{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4867e8f2",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of COVID-19 tweets\n",
    "\n",
    "Sanket Mayuresh Bhave, Dept. of Computer Science, Colorado State University\n",
    "\n",
    "sanket.bhave@colostate.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a5fad7",
   "metadata": {},
   "source": [
    "# Motivation and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503a3ff",
   "metadata": {},
   "source": [
    "Sentiment analysis is very important as it helps to understand people's sentiments and thoughts on a particular topic. This is visible through various examples like sentiment analysis of user reviews on an online retailing website, etc. Sentiment analysis can also be applied on various statements made on social media platforms like twitter. People react on social media and this can be helpful to understand people's sentiments about a topic, situation or incidence. This project aims to do sentiment analysis of such user tweets. These tweets are between the time frame February 2020 till March 2020 and related to COVID-19 pandemic. This has various applications like it helps understand public opinion and hence influences decision making. This can also be useful to decide how much moderation needs to be applied on social media to curtail misinformation. People's sentiments can be used to determine the next course of action and what the government needs to focus on for health and public management. \n",
    "\n",
    "This project can help build models that can do automatic sentiment analysis of user tweets and make the job easier for the policy-makers. Further, it also analyzes different models and their performance on categorizing the tweets based on their sentiments. \n",
    "\n",
    "\n",
    "Another motivation for this problem is to get the hands dirty on different important models not explored fully in the class. Yet another aim is to handle a dataset and build a model in a data scientist way by following all the steps- preprocessing, training, testing and reporting the results, on a huge dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a6dca1",
   "metadata": {},
   "source": [
    "For this task, a large-scale sentiment dataset, COVID-Senti has been used. Its use has been demostrated in [1] and this project aims to apply different categorization techniques on this dataset. The dataset contains 90K tweets with 6280 tweets labeled as positive, 16,335 as negative and 67,835 as neutral. Thus, the number of neutral tweets is larger than that of positive or negative. In the paper [1], the authors have used the TextBlob tool to label the tweets as positive, negative or neutral. \n",
    "\n",
    "We will start our analysis now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee8a458d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python-env/py39/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from math import ceil\n",
    "from random import Random\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.utils import simple_preprocess\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cefa004",
   "metadata": {},
   "source": [
    "The dataset is downloaded in CSV format in the file named COVIDSenti.csv. It can be found at [2]. We read the data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "009c5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_senti = pd.read_csv(\"COVIDSenti-main/COVIDSenti.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72744f34",
   "metadata": {},
   "source": [
    "# Preprocessing step\n",
    "\n",
    "As a part of preprocesing, several techniques are applied.\n",
    "\n",
    "1] First step is to remove the hyperlinks from the tweets. Many tweets contain hyperlinks that cite many websites. This information is not useful for us as it does not add any additional information for sentiment analysis. <br>\n",
    "2] The next step is to remove the @ mentions from the tweets. The reason is the same- it does not add any significant information on a sentence's sentiment. <br>\n",
    "3] and 4] The next steps are to remove the newlines and #tags from the tweets. Here, we won't remove the word associated with the # but only the special character #. It means, if a tweets contains #COVID, it will be converted to just COVID (no '#'). <br>\n",
    "5] The next step is to remove all the punctuations as they don't add any useful information for our models to work. <br>\n",
    "6] Further, all the stop words are removed from the data.<br>\n",
    "7] The final step is to tokenize the tweets using simple_preprocess() function from gensim. simple_preprocess() lowercases and tokenizes the documents into words. This step is very important as every tweet is a vector of words and hence needs to be tokenized for the same.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c839ddae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /s/chopin/a/grad/sanket96/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/tmp/ipykernel_1055771/1902245215.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  covid_senti['processed_tweet'] = covid_senti['tweet'].str.replace('http[^\\s]*',\"\")\n",
      "/tmp/ipykernel_1055771/1902245215.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  covid_senti['processed_tweet'] = covid_senti['tweet'].str.replace('@[^\\s]*',\"\")\n",
      "/tmp/ipykernel_1055771/1902245215.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  covid_senti['processed_tweet'] = covid_senti['processed_tweet'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "covid_senti['processed_tweet'] = covid_senti['tweet'].str.replace('http[^\\s]*',\"\")\n",
    "covid_senti['processed_tweet'] = covid_senti['tweet'].str.replace('@[^\\s]*',\"\")\n",
    "covid_senti = covid_senti.replace(r'\\n', '', regex=True)\n",
    "covid_senti = covid_senti.replace(r'#', '', regex=True)\n",
    "covid_senti['processed_tweet'] = covid_senti['processed_tweet'].str.replace('[^\\w\\s]','')\n",
    "covid_senti['processed_tweet'] = covid_senti['processed_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "covid_senti['tokenized_tweet'] = [simple_preprocess(line, deacc=True) for line in covid_senti['processed_tweet']]\n",
    "# covid_senti['tokenized_tweet'] = [[word.replace('\\n', '') for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "# covid_senti['tokenized_tweet'] = [[word.replace('#', '') for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "# covid_senti['tokenized_tweet'] = [[word.lower() for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "# covid_senti['tokenized_tweet'] = [[word.translate(table) for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti.to_csv('df.csv')\n",
    "# covid_senti['tokenized_tweet'] = [[covid_senti['tokenized_tweet'].apply(lambda x: word for word in line if word not in stop)] for line in  covid_senti['tokenized_tweet']] \n",
    "\n",
    "# print(covid_senti['tokenized_tweet'].sample(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d5f8a2",
   "metadata": {},
   "source": [
    "Below is the final step of preprocessing. Here word lemmatization is done. Word lemmatization is a process to associate different forms of the word to a single lemma or to its dictionary form. This is also a vital preprocessing step. The main aim behind this is to reduce the vocabulary size. For example, the words building and built will become a single word \"build\". Also, lemmatization is chosen over stemming as lemmatization is more accurate than the later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d527183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /s/chopin/a/grad/sanket96/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "covid_senti['lemmatized_tweet'] = [[wordnet_lemmatizer.lemmatize(word) for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "# print(covid_senti['lemmatized_tweet'].sample(n=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635dd8b",
   "metadata": {},
   "source": [
    "# Split of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1074910e",
   "metadata": {},
   "source": [
    "Here, the dataset is divided into train and test set. 80% of the data is used for training while 20% for testing. Below are the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942bea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(covid_senti)) < 0.8\n",
    "covid_senti_train = covid_senti[mask]\n",
    "covid_senti_test = covid_senti[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec32882",
   "metadata": {},
   "source": [
    "The number of samples in each class for the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d97bb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    53876\n",
       "neg    13097\n",
       "pos     4974\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_senti_train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "777e2313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    13509\n",
       "neg     3238\n",
       "pos     1306\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_senti_test[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0c408c",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db33ba20",
   "metadata": {},
   "source": [
    "Now, trying the Naive Bayes Classifier on the dataset. This is the same as the one used in PA1. Also, below is the table of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f0706d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.00      0.00      0.00      3238\n",
      "         pos       0.00      0.00      0.00      1306\n",
      "         neu       0.75      1.00      0.86     13509\n",
      "\n",
      "    accuracy                           0.75     18053\n",
      "   macro avg       0.25      0.33      0.29     18053\n",
      "weighted avg       0.56      0.75      0.64     18053\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayes():\n",
    "\n",
    "    def __init__(self):\n",
    "        # be sure to use the right class_dict for each data set\n",
    "        self.class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "        # self.class_dict = {'action': 0, 'comedy': 1}\n",
    "        self.feature_dict = {}\n",
    "        self.prior = np.zeros(len(self.class_dict))\n",
    "        self.likelihood = None\n",
    "    '''\n",
    "    Trains a multinomial Naive Bayes classifier on a training set.\n",
    "    Specifically, fills in self.prior and self.likelihood such that:\n",
    "    self.prior[class] = log(P(class))\n",
    "    self.likelihood[class][feature] = log(P(feature|class))\n",
    "    '''\n",
    "    def train(self, train_set):\n",
    "        self.feature_dict = self.select_features(train_set)\n",
    "        # iterate over training documents\n",
    "        self.likelihood = np.zeros((len(self.class_dict), len(self.feature_dict)))\n",
    "        doc_per_class = {}\n",
    "        word_count = {}\n",
    "        total_words_per_class = {}\n",
    "        vocabulary = set()\n",
    "        for index, row in train_set.iterrows():\n",
    "            class_name = row['label']\n",
    "            if (class_name in self.class_dict):\n",
    "                doc_per_class[class_name] = 1 + doc_per_class.get(class_name, 0)\n",
    "                    # collect class counts and feature counts\n",
    "                data = row['lemmatized_tweet']\n",
    "                for word in data:\n",
    "                    vocabulary.add(word)\n",
    "                    word_count[(word, class_name)] = 1 + word_count.get((word, class_name), 0)\n",
    "        # normalize counts to probabilities, and take logs\n",
    "        for class_name in self.class_dict:\n",
    "            counts = [v for k, v in word_count.items() if k[1] == class_name]\n",
    "            total_words_per_class[class_name] = sum(counts)\n",
    "        for word in self.feature_dict:\n",
    "            for class_name in self.class_dict:\n",
    "                self.likelihood[self.class_dict.get(class_name)][self.feature_dict.get(word)] = np.log(((word_count.get((word,\n",
    "                                                class_name), 0) + 1)/(total_words_per_class[class_name] + len(vocabulary))))\n",
    "        for class_name in self.class_dict:\n",
    "            self.prior[self.class_dict[class_name]] = np.log((doc_per_class[class_name] / sum(doc_per_class.values())))\n",
    "    '''\n",
    "    Tests the classifier on a development or test set.\n",
    "    Returns a dictionary of filenames mapped to their correct and predicted\n",
    "    classes such that:\n",
    "    results[filename]['correct'] = correct class\n",
    "    results[filename]['predicted'] = predicted class\n",
    "    '''\n",
    "    def test(self, dev_set):\n",
    "        pred_labels = []\n",
    "        true_labels = []\n",
    "        # iterate over testing documents\n",
    "        for index, row in dev_set.iterrows():\n",
    "            class_name = row['label']\n",
    "            # create feature vectors for each document\n",
    "            word_count = {}\n",
    "            true_labels.append(self.class_dict[class_name])\n",
    "            data = str(row['lemmatized_tweet'])\n",
    "            for word in data:\n",
    "                if word in self.feature_dict:\n",
    "                    word_count[word] = 1 + word_count.get(word, 0)\n",
    "            feature_vector = np.zeros((len(self.feature_dict), 1))\n",
    "            for i, word in enumerate(self.feature_dict):\n",
    "                feature_vector[i] = word_count.get(word, 0)\n",
    "            self.prior = np.reshape(self.prior, (self.prior.shape[0], 1))\n",
    "            probability = self.prior + np.matmul(self.likelihood, feature_vector)\n",
    "            pred_labels.append(np.argmax(probability))\n",
    "                # get most likely class\n",
    "        # print(dict(results))\n",
    "        return pred_labels, true_labels\n",
    "\n",
    "    '''\n",
    "    Given results, calculates the following:\n",
    "    Precision, Recall, F1 for each class\n",
    "    Accuracy overall\n",
    "    Also, prints evaluation metrics in readable format.\n",
    "    '''\n",
    "    def evaluate(self, results):\n",
    "        # you may find this helpful\n",
    "        target_names = ['neg', 'pos', 'neu']\n",
    "        print(classification_report(results[1], results[0], target_names=target_names))\n",
    "    '''\n",
    "    Performs feature selection.\n",
    "    Returns a dictionary of features.\n",
    "    '''\n",
    "    def select_features(self, train_set):\n",
    "        # almost any method of feature selection is fine here\n",
    "        doc_per_class = {}\n",
    "        word_count = {}\n",
    "        total_words_per_class = {}\n",
    "        vocabulary = set()\n",
    "        likelihood_ratio = {}\n",
    "        for index, row in train_set.iterrows():\n",
    "            class_name = row['label']\n",
    "            if (class_name in self.class_dict):\n",
    "                doc_per_class[class_name] = 1 + doc_per_class.get(class_name, 0)\n",
    "                    # collect class counts and feature counts\n",
    "                data = row['lemmatized_tweet']\n",
    "                for word in data:\n",
    "                    vocabulary.add(word)\n",
    "                    word_count[(word, class_name)] = 1 + word_count.get((word, class_name), 0)\n",
    "        # normalize counts to probabilities, and take logs\n",
    "        for class_name in self.class_dict:\n",
    "            counts = [v for k, v in word_count.items() if k[1] == class_name]\n",
    "            total_words_per_class[class_name] = sum(counts)\n",
    "        prob_class = np.zeros((3, 1))\n",
    "        for i, class_name in enumerate(self.class_dict):\n",
    "            prob_class[i] = (doc_per_class[class_name] / sum(doc_per_class.values()))\n",
    "        for word in vocabulary:\n",
    "            class_probs = [1] * len(self.class_dict)\n",
    "            for i, class_name in enumerate(self.class_dict):\n",
    "                class_probs[i] = (word_count.get((word,\n",
    "                                      class_name), 0) + 1) / (total_words_per_class[class_name] + len(vocabulary))\n",
    "                class_probs[i] = class_probs[i] / prob_class[i]\n",
    "            likelihood_ratio[word] = (1 / class_probs[0]) * (1 / class_probs[1]) * (1 / class_probs[2])\n",
    "        #likelihood_ratio_pos = dict(sorted(likelihood_ratio.items(), key=lambda item: item[1], reverse=True))\n",
    "        likelihood_ratio = dict(sorted(likelihood_ratio.items(), key=lambda item: item[1]))\n",
    "        words = []\n",
    "        words.extend(list(likelihood_ratio.keys())[:750])\n",
    "        #words.extend(list(likelihood_ratio_pos.keys())[:750])\n",
    "        # for class_name in self.class_dict:\n",
    "        #     self.prior[self.class_dict[class_name]] = np.log((doc_per_class[class_name] / sum(doc_per_class.values())))\n",
    "        features = {}\n",
    "        for i, word in enumerate(words):\n",
    "            features[word] = i\n",
    "        return features\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nb = NaiveBayes()\n",
    "    # make sure these point to the right directories\n",
    "    nb.train(covid_senti_train)\n",
    "    # nb.train('movie_reviews_small/train')\n",
    "    results = nb.test(covid_senti_test)\n",
    "    # results = nb.test('movie_reviews_small/test')\n",
    "    nb.evaluate(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77890f57",
   "metadata": {},
   "source": [
    "The overall accuracy achieved by Naive Bayes (75%) is satisfactory. But, the per-class precision is 0 which is not acceptable. It means Naive Bayes is biased entirely towards the \"neu\" class. This is expected as the number of samples labeled as \"neu\" is more than the other two classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c5b1d",
   "metadata": {},
   "source": [
    "# Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab1e2e8",
   "metadata": {},
   "source": [
    "Now, let's try Logistic Regression clasifier from our PA2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25052d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116452/623227358.py:111: RuntimeWarning: divide by zero encountered in log\n",
      "  loss += -((y @ np.log(y_hat)) + ((1 - y) @ np.log(1 - y_hat)))\n",
      "/tmp/ipykernel_116452/623227358.py:111: RuntimeWarning: invalid value encountered in matmul\n",
      "  loss += -((y @ np.log(y_hat)) + ((1 - y) @ np.log(1 - y_hat)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: nan\n",
      "Epoch 2 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 11 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 12 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 13 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 14 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 15 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 16 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 17 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 18 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 19 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 20 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 21 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 22 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 23 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 24 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 25 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 26 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 27 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 28 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 29 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 30 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 31 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 32 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 33 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 34 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 35 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 36 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 37 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 38 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 39 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 40 out of 40\n",
      "Average Train Loss: nan\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.00      0.00      0.00      3238\n",
      "         pos       0.07      1.00      0.13      1306\n",
      "         neu       0.00      0.00      0.00     13509\n",
      "\n",
      "    accuracy                           0.07     18053\n",
      "   macro avg       0.02      0.33      0.04     18053\n",
      "weighted avg       0.01      0.07      0.01     18053\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# CS542 Fall 2021 Programming Assignment 2\n",
    "# Logistic Regression Classifier\n",
    "\n",
    "'''\n",
    "Computes the logistic function.\n",
    "'''\n",
    "\n",
    "\n",
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, n_features=400):\n",
    "        # be sure to use the right class_dict for each data set\n",
    "        self.theta = None\n",
    "        self.n_features = n_features\n",
    "        self.feature_dict = None\n",
    "        self.class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "        # self.class_dict = {'action': 0, 'comedy': 1}\n",
    "        # use of self.feature_dict is optional for this assignment\n",
    "        self.feature_dict = self.select_features(covid_senti_train)\n",
    "\n",
    "    '''\n",
    "    Loads a dataset. Specifically, returns a list of filenames, and dictionaries\n",
    "    of classes and documents such that:\n",
    "    classes[filename] = class of the document\n",
    "    documents[filename] = feature vector for the document (use self.featurize)\n",
    "    '''\n",
    "\n",
    "    def select_features(self, data_set):\n",
    "        feature_count = {}\n",
    "        for index, row in data_set.iterrows():\n",
    "            data = str(row['lemmatized_tweet']).split()\n",
    "            for word in data:\n",
    "                feature_count[word] = 1 + feature_count.get(word, 0)\n",
    "\n",
    "        feature_count = list(dict(sorted(feature_count.items(), key=lambda v: v[1], reverse=True)).keys())[:500]\n",
    "        features = {}\n",
    "\n",
    "        for i, word in enumerate(feature_count):\n",
    "            features[word] = i\n",
    "        return features\n",
    "\n",
    "    def load_data(self, data_set):\n",
    "        filenames = []\n",
    "        classes = dict()\n",
    "        documents = dict()\n",
    "        # iterate over documents\n",
    "        for index, row in data_set.iterrows():\n",
    "            # your code here\n",
    "            # BEGIN STUDENT CODE\n",
    "            # if os.path.isfile(os.path.join(root, name)):\n",
    "            class_name = row['label']\n",
    "            classes[index] = self.class_dict[class_name]\n",
    "            documents[index] = self.featurize(row['lemmatized_tweet'])\n",
    "            # END STUDENT CODE\n",
    "        return classes, documents\n",
    "\n",
    "    '''\n",
    "    Given a document (as a list of words), returns a feature vector.\n",
    "    Note that the last element of the vector, corresponding to the bias, is a\n",
    "    \"dummy feature\" with value 1.\n",
    "    '''\n",
    "\n",
    "    def featurize(self, document):\n",
    "        vector = np.zeros(self.n_features + 1)\n",
    "        # BEGIN STUDENT CODE\n",
    "        for word in document:\n",
    "            if word in self.feature_dict:\n",
    "                if word not in w2v_model.wv.key_to_index:\n",
    "                    vector.extend([0] * 500)\n",
    "                else:\n",
    "                    vector.extend(w2v_model.wv[word])\n",
    "        # END STUDENT CODE\n",
    "        vector[-1] = 1\n",
    "        return vector\n",
    "\n",
    "    '''\n",
    "    Trains a logistic regression classifier on a training set.\n",
    "    '''\n",
    "\n",
    "    def train(self, train_set, batch_size=3, n_epochs=1, eta=0.1):\n",
    "        # if train_set == \"movie_reviews_small/train\":\n",
    "        #     self.feature_dict = {'fast': 0, 'couple': 1, 'shoot': 2, 'fly': 3}\n",
    "        # else:\n",
    "        #     self.feature_dict = self.select_features(train_set)\n",
    "        # self.n_features = len(self.feature_dict)\n",
    "        self.theta = np.zeros(self.n_features + 1)  # weights (and bias)\n",
    "        classes, documents = self.load_data(train_set)\n",
    "        n_minibatches = ceil(len(train_set) / batch_size)\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "            loss = 0\n",
    "            for i in range(n_minibatches):\n",
    "                # list of filenames in minibatch\n",
    "                minibatch = train_set[i * batch_size: (i + 1) * batch_size]\n",
    "                # BEGIN STUDENT CODE\n",
    "                # create and fill in matrix x and vector y\n",
    "                x = np.zeros((len(minibatch), self.n_features + 1))\n",
    "                y = np.zeros(len(minibatch))\n",
    "                k = 0\n",
    "                for j, row in minibatch.iterrows():\n",
    "                    x[k][:] = documents[j]\n",
    "                    y[k] = classes[j]\n",
    "                    k += 1\n",
    "                # compute y_hat\n",
    "                y_hat = sigma(np.dot(x, self.theta))\n",
    "                # update loss\n",
    "                loss += -((y @ np.log(y_hat)) + ((1 - y) @ np.log(1 - y_hat)))\n",
    "                # compute gradient\n",
    "                gradient = np.dot(x.T, np.subtract(y_hat, y)) / len(minibatch)\n",
    "                # update weights (and bias)\n",
    "                self.theta = self.theta - (eta * gradient)\n",
    "                # END STUDENT CODE\n",
    "            loss /= len(train_set)\n",
    "            print(\"Average Train Loss: {}\".format(loss))\n",
    "            # randomize order\n",
    "            #Random(epoch).shuffle(train_set)\n",
    "\n",
    "    '''\n",
    "    Tests the classifier on a development or test set.\n",
    "    Returns a dictionary of filenames mapped to their correct and predicted\n",
    "    classes such that:\n",
    "    results[filename]['correct'] = correct class\n",
    "    results[filename]['predicted'] = predicted class\n",
    "    '''\n",
    "\n",
    "    def test(self, dev_set):\n",
    "        pred_labels = []\n",
    "        true_labels = []\n",
    "        classes, documents = self.load_data(dev_set)\n",
    "        for index, row in dev_set.iterrows():\n",
    "            # BEGIN STUDENT CODE\n",
    "            # get most likely class (recall that P(y=1|x) = y_hat)\n",
    "            true_labels.append(classes[index])\n",
    "            prediction = sigma(np.dot(documents[index], self.theta))\n",
    "            pred_label = 1 if prediction > 0.5 else 0\n",
    "            pred_labels.append(pred_label)\n",
    "            # END STUDENT CODE\n",
    "        return pred_labels, true_labels\n",
    "\n",
    "    '''\n",
    "    Given results, calculates the following:\n",
    "    Precision, Recall, F1 for each class\n",
    "    Accuracy overall\n",
    "    Also, prints evaluation metrics in readable format.\n",
    "    '''\n",
    "\n",
    "    def evaluate(self, results):\n",
    "        # you can copy and paste your code from PA1 here\n",
    "        target_names = ['neg', 'pos', 'neu']\n",
    "        print(classification_report(results[1], results[0], target_names=target_names))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lr = LogisticRegression(n_features=750)\n",
    "    # make sure these point to the right directories\n",
    "    batch_size = [1, 2, 3, 8, 16, 32]\n",
    "    n_epochs = [1, 5, 10, 20, 30, 40]\n",
    "    eta = [0.025, 0.05, 0.1, 0.2, 0.4]\n",
    "\n",
    "    # code for grid search\n",
    "#     for b in batch_size:\n",
    "#         for n in n_epochs:\n",
    "#             for ler in eta:\n",
    "#                 lr.train(covid_senti_train, batch_size=b, n_epochs=n, eta=ler)\n",
    "#                 results = lr.test(covid_senti_test)\n",
    "#                 lr.evaluate(results)\n",
    "#                 print(\"Accuracy is for batch size: \", b, \", n_epochs: \", n, \"eta: \", ler)\n",
    "\n",
    "    # best features from grid search\n",
    "    lr.train(covid_senti_train, batch_size=3, n_epochs=40, eta=0.05)\n",
    "    results = lr.test(covid_senti_test)\n",
    "    # lr.train('movie_reviews_small/train', batch_size=3, n_epochs=1, eta=0.1)\n",
    "    # results = lr.test('movie_reviews_small/test')\n",
    "    lr.evaluate(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941e1c6c",
   "metadata": {},
   "source": [
    "From the results, it is visible that the accuracy is very poor with Logisitic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77038f69",
   "metadata": {},
   "source": [
    "# CNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e95fee",
   "metadata": {},
   "source": [
    "Now, we will try CNN as our next model. Here, the features extracted from the WordNet classifier will be used to train the CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecec1607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import gensim\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b387dd9",
   "metadata": {},
   "source": [
    "Below is the Word2Vec model from gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e73e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "tweets = list(covid_senti['lemmatized_tweet'].values)\n",
    "tweets.append(['pad'])\n",
    "w2v_model = Word2Vec(tweets, min_count = 1, vector_size = 500, workers = 3, window = 3, sg = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9157552",
   "metadata": {},
   "source": [
    "As our model understands only numbers, we will assign a number to each of our labels here. In the above cases, it was internally done in the code through the class_dict dictionary. Here, we will do it explicitly and add a new column to our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "734bfd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>lemmatized_tweet</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coronavirus | Human Coronavirus Types | CDC ht...</td>\n",
       "      <td>neu</td>\n",
       "      <td>Coronavirus Human Coronavirus Types CDC httpst...</td>\n",
       "      <td>[coronavirus, human, coronavirus, types, cdc]</td>\n",
       "      <td>[coronavirus, human, coronavirus, type, cdc]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@shehryar_taseer That‚Äôs üíØ true , Corona v...</td>\n",
       "      <td>neu</td>\n",
       "      <td>ThatÄôs üíØ true Corona virus swine flue Bird ...</td>\n",
       "      <td>[thataos, uiø, true, corona, virus, swine, flu...</td>\n",
       "      <td>[thataos, uiø, true, corona, virus, swine, flu...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TLDR: Not SARS, possibly new coronavirus. Diff...</td>\n",
       "      <td>neg</td>\n",
       "      <td>TLDR Not SARS possibly new coronavirus Difficu...</td>\n",
       "      <td>[tldr, not, sars, possibly, new, coronavirus, ...</td>\n",
       "      <td>[tldr, not, sars, possibly, new, coronavirus, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Disease outbreak news from the WHO: Middle Eas...</td>\n",
       "      <td>neu</td>\n",
       "      <td>Disease outbreak news WHO Middle East respirat...</td>\n",
       "      <td>[disease, outbreak, news, who, middle, east, r...</td>\n",
       "      <td>[disease, outbreak, news, who, middle, east, r...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>China - Media: WSJ says sources tell them myst...</td>\n",
       "      <td>neu</td>\n",
       "      <td>China Media WSJ says sources tell mystery pneu...</td>\n",
       "      <td>[china, media, wsj, says, sources, tell, myste...</td>\n",
       "      <td>[china, medium, wsj, say, source, tell, myster...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89995</th>\n",
       "      <td>@C_Racing48 The flu has a 2% death rate.. the ...</td>\n",
       "      <td>neu</td>\n",
       "      <td>The flu 2 death rate coronavirus 3 fine 3 risk...</td>\n",
       "      <td>[the, flu, death, rate, coronavirus, fine, ris...</td>\n",
       "      <td>[the, flu, death, rate, coronavirus, fine, ris...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89996</th>\n",
       "      <td>@realDonaldTrump We already know that but you‚...</td>\n",
       "      <td>neg</td>\n",
       "      <td>We already know youÄôre idiot bungled Coronavi...</td>\n",
       "      <td>[we, already, know, youaore, idiot, bungled, c...</td>\n",
       "      <td>[we, already, know, youaore, idiot, bungled, c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89997</th>\n",
       "      <td>First coronavirus case reported in St. Joseph ...</td>\n",
       "      <td>neu</td>\n",
       "      <td>First coronavirus case reported St Joseph Coun...</td>\n",
       "      <td>[first, coronavirus, case, reported, st, josep...</td>\n",
       "      <td>[first, coronavirus, case, reported, st, josep...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89998</th>\n",
       "      <td>If you ate ants when you were a child, you‚Äôr...</td>\n",
       "      <td>neu</td>\n",
       "      <td>If ate ants child youÄôre immune coronavirus</td>\n",
       "      <td>[if, ate, ants, child, youaore, immune, corona...</td>\n",
       "      <td>[if, ate, ant, child, youaore, immune, coronav...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89999</th>\n",
       "      <td>All this Coronavirus talk about to make me bea...</td>\n",
       "      <td>neu</td>\n",
       "      <td>All Coronavirus talk make beat dick hand sanit...</td>\n",
       "      <td>[all, coronavirus, talk, make, beat, dick, han...</td>\n",
       "      <td>[all, coronavirus, talk, make, beat, dick, han...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet label  \\\n",
       "0      Coronavirus | Human Coronavirus Types | CDC ht...   neu   \n",
       "1      @shehryar_taseer That‚Äôs üíØ true , Corona v...   neu   \n",
       "2      TLDR: Not SARS, possibly new coronavirus. Diff...   neg   \n",
       "3      Disease outbreak news from the WHO: Middle Eas...   neu   \n",
       "4      China - Media: WSJ says sources tell them myst...   neu   \n",
       "...                                                  ...   ...   \n",
       "89995  @C_Racing48 The flu has a 2% death rate.. the ...   neu   \n",
       "89996  @realDonaldTrump We already know that but you‚...   neg   \n",
       "89997  First coronavirus case reported in St. Joseph ...   neu   \n",
       "89998  If you ate ants when you were a child, you‚Äôr...   neu   \n",
       "89999  All this Coronavirus talk about to make me bea...   neu   \n",
       "\n",
       "                                         processed_tweet  \\\n",
       "0      Coronavirus Human Coronavirus Types CDC httpst...   \n",
       "1      ThatÄôs üíØ true Corona virus swine flue Bird ...   \n",
       "2      TLDR Not SARS possibly new coronavirus Difficu...   \n",
       "3      Disease outbreak news WHO Middle East respirat...   \n",
       "4      China Media WSJ says sources tell mystery pneu...   \n",
       "...                                                  ...   \n",
       "89995  The flu 2 death rate coronavirus 3 fine 3 risk...   \n",
       "89996  We already know youÄôre idiot bungled Coronavi...   \n",
       "89997  First coronavirus case reported St Joseph Coun...   \n",
       "89998       If ate ants child youÄôre immune coronavirus   \n",
       "89999  All Coronavirus talk make beat dick hand sanit...   \n",
       "\n",
       "                                         tokenized_tweet  \\\n",
       "0          [coronavirus, human, coronavirus, types, cdc]   \n",
       "1      [thataos, uiø, true, corona, virus, swine, flu...   \n",
       "2      [tldr, not, sars, possibly, new, coronavirus, ...   \n",
       "3      [disease, outbreak, news, who, middle, east, r...   \n",
       "4      [china, media, wsj, says, sources, tell, myste...   \n",
       "...                                                  ...   \n",
       "89995  [the, flu, death, rate, coronavirus, fine, ris...   \n",
       "89996  [we, already, know, youaore, idiot, bungled, c...   \n",
       "89997  [first, coronavirus, case, reported, st, josep...   \n",
       "89998  [if, ate, ants, child, youaore, immune, corona...   \n",
       "89999  [all, coronavirus, talk, make, beat, dick, han...   \n",
       "\n",
       "                                        lemmatized_tweet  label_num  \n",
       "0           [coronavirus, human, coronavirus, type, cdc]          2  \n",
       "1      [thataos, uiø, true, corona, virus, swine, flu...          2  \n",
       "2      [tldr, not, sars, possibly, new, coronavirus, ...          0  \n",
       "3      [disease, outbreak, news, who, middle, east, r...          2  \n",
       "4      [china, medium, wsj, say, source, tell, myster...          2  \n",
       "...                                                  ...        ...  \n",
       "89995  [the, flu, death, rate, coronavirus, fine, ris...          2  \n",
       "89996  [we, already, know, youaore, idiot, bungled, c...          0  \n",
       "89997  [first, coronavirus, case, reported, st, josep...          2  \n",
       "89998  [if, ate, ant, child, youaore, immune, coronav...          2  \n",
       "89999  [all, coronavirus, talk, make, beat, dick, han...          2  \n",
       "\n",
       "[90000 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "covid_senti['label_num'] = covid_senti.label.replace(class_dict)\n",
    "covid_senti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9553af",
   "metadata": {},
   "source": [
    "Again dividing our dataset into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "711fceaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    53870\n",
       "neg    13101\n",
       "pos     5061\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.random.rand(len(covid_senti)) < 0.8\n",
    "covid_senti_train = covid_senti[mask]\n",
    "covid_senti_test = covid_senti[~mask]\n",
    "covid_senti_train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ec09bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    13515\n",
       "neg     3234\n",
       "pos     1219\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_senti_test[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d6565",
   "metadata": {},
   "source": [
    "Below function return a vector representation of a sentence. That is, given a sentence, it looks for the index to every word and appends it to a list. The final list contains index for every word in the sentence. This list is then returned as a tensor. This index is a mapping to its actual embedding which will be looked in the first layer of the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "810eeb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = covid_senti['lemmatized_tweet'].map(len).max()\n",
    "def make_word_2_vec(sentence):\n",
    "    padding_idx = w2v_model.wv.key_to_index['pad']\n",
    "    padded_X = [padding_idx for i in range(max_len)]\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        if word not in w2v_model.wv.key_to_index:\n",
    "            padded_X[i] = 0\n",
    "            print(word)\n",
    "        else:\n",
    "            padded_X[i] = w2v_model.wv.key_to_index[word]\n",
    "        i += 1\n",
    "    return torch.tensor(padded_X, dtype=torch.long).view(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c7583",
   "metadata": {},
   "source": [
    "Below is the CNN model. This is taken from the lecture as it is. The aim is to evaluate the model on the tweet dataset. It has an embedding layer wherein it looks for word embeddings. Then, it has five convolution layers followed by a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb525eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 500\n",
    "NUM_FILTERS = 10\n",
    "\n",
    "class CnnTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, window_sizes=(1,2,3,5)):\n",
    "        super(CnnTextClassifier, self).__init__()\n",
    "        weights = w2v_model.wv\n",
    "        # With pretrained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors),\n",
    "                                                      padding_idx=w2v_model.wv.key_to_index['pad'])\n",
    "        # Without pretrained embeddings\n",
    "        # self.embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "                                   nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE],\n",
    "                                             padding=(window_size - 1, 0))\n",
    "                                   for window_size in window_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Apply a convolution + max_pool layer for each window size\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        xs = []\n",
    "        for conv in self.convs:\n",
    "            x2 = torch.tanh(conv(x))\n",
    "            x2 = torch.squeeze(x2, -1)\n",
    "            x2 = F.max_pool1d(x2, x2.size(2))\n",
    "            xs.append(x2)\n",
    "        x = torch.cat(xs, 2)\n",
    "\n",
    "        # FC\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a6d85",
   "metadata": {},
   "source": [
    "Now, we train our CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd0330e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch completed in: 169.0479 seconds\n",
      "1,0.8036848827393875\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Epoch completed in: 178.3237 seconds\n",
      "2,0.803582663023501\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Epoch completed in: 182.0451 seconds\n",
      "3,0.803582663023501\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 3\n",
    "VOCAB_SIZE = len(w2v_model.wv.key_to_index)\n",
    "\n",
    "cnn_model = CnnTextClassifier(vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\n",
    "# cnn_model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "\n",
    "# Open the file for writing loss\n",
    "class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "loss_file_name = 'cnn_class_big_loss_with_padding.csv'\n",
    "losses = []\n",
    "cnn_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch \" + str(epoch + 1))\n",
    "    train_loss = 0\n",
    "    for index, row in covid_senti_train.iterrows():\n",
    "        # Clearing the accumulated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make the bag of words vector for stemmed tokens \n",
    "        bow_vec = make_word_2_vec(row['lemmatized_tweet'])\n",
    "       \n",
    "        # Forward pass to get output\n",
    "        probs = cnn_model(bow_vec)\n",
    "\n",
    "        # Get the target label\n",
    "        target = torch.tensor([class_dict[row['label']]], dtype=torch.long)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = loss_function(probs, target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # if index == 0:\n",
    "    #     continue\n",
    "    print(\"Epoch completed in: %.4f seconds\" % (time.time()-start_time))\n",
    "    print(str((epoch+1)) + \",\" + str(train_loss / len(covid_senti_train)))\n",
    "    print('\\n')\n",
    "    train_loss = 0\n",
    "\n",
    "torch.save(cnn_model, 'cnn_big_model_500_with_padding.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c24e8",
   "metadata": {},
   "source": [
    "Let's evaluate the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ea9d459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.00      0.00      0.00         0\n",
      "         pos       0.00      0.00      0.00         0\n",
      "         neu       1.00      0.75      0.86     17968\n",
      "\n",
      "    accuracy                           0.75     17968\n",
      "   macro avg       0.33      0.25      0.29     17968\n",
      "weighted avg       1.00      0.75      0.86     17968\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions = []\n",
    "correct = []\n",
    "cnn_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = defaultdict(dict)\n",
    "    for index, row in covid_senti_test.iterrows():\n",
    "        bow_vec = make_word_2_vec(row['lemmatized_tweet'])\n",
    "        probs = cnn_model(bow_vec)\n",
    "        correct.append(row['label_num'])\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        predictions.append(predicted.numpy()[0])\n",
    "target_names = ['neg', 'pos', 'neu']\n",
    "print(classification_report(predictions, correct, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d06560",
   "metadata": {},
   "source": [
    "The results with CNN are same as that with Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20013f9",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c074e1c",
   "metadata": {},
   "source": [
    "Now, let's try on BERT. BERT is a very famous model released in 2018. It is based on Transformer having 12 Encoder layers and 12 Attention heads. It has around 110M parameters. Here, the BERT model is fine tuned for our dataset. We first split the dataset into train and test with 20% reserved for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b396c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(covid_senti.index.values, \n",
    "                                                    covid_senti.label_num.values, test_size=0.2,\n",
    "                                                   stratify=covid_senti.label_num.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6733b3",
   "metadata": {},
   "source": [
    "Here, we will mark each row as \"train\" or \"test\" based on which partition it is. This will help us in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6309ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_senti['data_type'] = ['not_set'] * covid_senti.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1a5544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_senti.loc[X_train, 'data_type'] = 'train'\n",
    "covid_senti.loc[X_test, 'data_type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21ac6a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faba1e5b",
   "metadata": {},
   "source": [
    "Below cells intialize and train a BERT uncased model. First, we will use th BERT Tokenizer to convert our dataset into the format BERT expects. This tokenizer will pad our inputs to length 512. The sequences larger than 512 are truncated. Also, a [SEP] token is placed between two sentences and a [CLS] token at the start of the sentences. The tokenizer also converts the words into the embeddings. Also, the tokenizer creates an attention mask that differentiates word token from padded tokens by marking them 1 and 0 repsectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57b70323",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be436e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/a/grad/sanket96/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2322: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='train'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')\n",
    "\n",
    "encoded_data_test = tokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='test'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7055d70",
   "metadata": {},
   "source": [
    "The below cells assigns labels, attention masks and input ids as per the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97677afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(covid_senti[covid_senti.data_type == 'train'].label_num.values)\n",
    "\n",
    "#validation set\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(covid_senti[covid_senti.data_type == 'test'].label_num.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5448cedc",
   "metadata": {},
   "source": [
    "This is the actual BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aa12fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(class_dict),\n",
    "                                                     output_attentions = False,\n",
    "                                                      output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad65572",
   "metadata": {},
   "source": [
    "Defining the Dataloader for our train and test set and starting the training proces.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18d7a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85639ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train,labels_train)\n",
    "\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b0e7d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56915f",
   "metadata": {},
   "source": [
    "BERT gets fine-tuned well if AdamW optimizer is used. In AdamW, weight decay and and learning rate are optimized separately. lr=1e-5 is the defualt learning rate for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d213a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d654d58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 3, Step: 0 / 2250 Loss: 1.1775\n",
      "Epoch: 1 / 3, Step: 1 / 2250 Loss: 1.1513\n",
      "Epoch: 1 / 3, Step: 2 / 2250 Loss: 1.1006\n",
      "Epoch: 1 / 3, Step: 3 / 2250 Loss: 0.9597\n",
      "Epoch: 1 / 3, Step: 4 / 2250 Loss: 1.0195\n",
      "Epoch: 1 / 3, Step: 5 / 2250 Loss: 0.9269\n",
      "Epoch: 1 / 3, Step: 6 / 2250 Loss: 0.8980\n",
      "Epoch: 1 / 3, Step: 7 / 2250 Loss: 0.8771\n",
      "Epoch: 1 / 3, Step: 8 / 2250 Loss: 0.8009\n",
      "Epoch: 1 / 3, Step: 9 / 2250 Loss: 0.8886\n",
      "Epoch: 1 / 3, Step: 10 / 2250 Loss: 0.7702\n",
      "Epoch: 1 / 3, Step: 11 / 2250 Loss: 0.6703\n",
      "Epoch: 1 / 3, Step: 12 / 2250 Loss: 0.7361\n",
      "Epoch: 1 / 3, Step: 13 / 2250 Loss: 0.7624\n",
      "Epoch: 1 / 3, Step: 14 / 2250 Loss: 1.0131\n",
      "Epoch: 1 / 3, Step: 15 / 2250 Loss: 0.5220\n",
      "Epoch: 1 / 3, Step: 16 / 2250 Loss: 0.8398\n",
      "Epoch: 1 / 3, Step: 17 / 2250 Loss: 0.6348\n",
      "Epoch: 1 / 3, Step: 18 / 2250 Loss: 0.7459\n",
      "Epoch: 1 / 3, Step: 19 / 2250 Loss: 0.7515\n",
      "Epoch: 1 / 3, Step: 20 / 2250 Loss: 0.6863\n",
      "Epoch: 1 / 3, Step: 21 / 2250 Loss: 0.8617\n",
      "Epoch: 1 / 3, Step: 22 / 2250 Loss: 0.5856\n",
      "Epoch: 1 / 3, Step: 23 / 2250 Loss: 0.7047\n",
      "Epoch: 1 / 3, Step: 24 / 2250 Loss: 0.8389\n",
      "Epoch: 1 / 3, Step: 25 / 2250 Loss: 0.5753\n",
      "Epoch: 1 / 3, Step: 26 / 2250 Loss: 0.8603\n",
      "Epoch: 1 / 3, Step: 27 / 2250 Loss: 0.9124\n",
      "Epoch: 1 / 3, Step: 28 / 2250 Loss: 0.5585\n",
      "Epoch: 1 / 3, Step: 29 / 2250 Loss: 0.9734\n",
      "Epoch: 1 / 3, Step: 30 / 2250 Loss: 0.6387\n",
      "Epoch: 1 / 3, Step: 31 / 2250 Loss: 0.7375\n",
      "Epoch: 1 / 3, Step: 32 / 2250 Loss: 0.5730\n",
      "Epoch: 1 / 3, Step: 33 / 2250 Loss: 0.7355\n",
      "Epoch: 1 / 3, Step: 34 / 2250 Loss: 0.5745\n",
      "Epoch: 1 / 3, Step: 35 / 2250 Loss: 0.7782\n",
      "Epoch: 1 / 3, Step: 36 / 2250 Loss: 0.7850\n",
      "Epoch: 1 / 3, Step: 37 / 2250 Loss: 0.7128\n",
      "Epoch: 1 / 3, Step: 38 / 2250 Loss: 0.7154\n",
      "Epoch: 1 / 3, Step: 39 / 2250 Loss: 0.7237\n",
      "Epoch: 1 / 3, Step: 40 / 2250 Loss: 0.5474\n",
      "Epoch: 1 / 3, Step: 41 / 2250 Loss: 0.8063\n",
      "Epoch: 1 / 3, Step: 42 / 2250 Loss: 0.4497\n",
      "Epoch: 1 / 3, Step: 43 / 2250 Loss: 0.5556\n",
      "Epoch: 1 / 3, Step: 44 / 2250 Loss: 0.8138\n",
      "Epoch: 1 / 3, Step: 45 / 2250 Loss: 0.6146\n",
      "Epoch: 1 / 3, Step: 46 / 2250 Loss: 0.6429\n",
      "Epoch: 1 / 3, Step: 47 / 2250 Loss: 0.6753\n",
      "Epoch: 1 / 3, Step: 48 / 2250 Loss: 0.8018\n",
      "Epoch: 1 / 3, Step: 49 / 2250 Loss: 0.7302\n",
      "Epoch: 1 / 3, Step: 50 / 2250 Loss: 0.9959\n",
      "Epoch: 1 / 3, Step: 51 / 2250 Loss: 0.9313\n",
      "Epoch: 1 / 3, Step: 52 / 2250 Loss: 0.9512\n",
      "Epoch: 1 / 3, Step: 53 / 2250 Loss: 0.7295\n",
      "Epoch: 1 / 3, Step: 54 / 2250 Loss: 0.8694\n",
      "Epoch: 1 / 3, Step: 55 / 2250 Loss: 0.5753\n",
      "Epoch: 1 / 3, Step: 56 / 2250 Loss: 0.6052\n",
      "Epoch: 1 / 3, Step: 57 / 2250 Loss: 0.6850\n",
      "Epoch: 1 / 3, Step: 58 / 2250 Loss: 0.7943\n",
      "Epoch: 1 / 3, Step: 59 / 2250 Loss: 0.8272\n",
      "Epoch: 1 / 3, Step: 60 / 2250 Loss: 0.6562\n",
      "Epoch: 1 / 3, Step: 61 / 2250 Loss: 0.6506\n",
      "Epoch: 1 / 3, Step: 62 / 2250 Loss: 0.8994\n",
      "Epoch: 1 / 3, Step: 63 / 2250 Loss: 0.8278\n",
      "Epoch: 1 / 3, Step: 64 / 2250 Loss: 0.5933\n",
      "Epoch: 1 / 3, Step: 65 / 2250 Loss: 0.8829\n",
      "Epoch: 1 / 3, Step: 66 / 2250 Loss: 0.6463\n",
      "Epoch: 1 / 3, Step: 67 / 2250 Loss: 0.4446\n",
      "Epoch: 1 / 3, Step: 68 / 2250 Loss: 0.7473\n",
      "Epoch: 1 / 3, Step: 69 / 2250 Loss: 1.0748\n",
      "Epoch: 1 / 3, Step: 70 / 2250 Loss: 0.8237\n",
      "Epoch: 1 / 3, Step: 71 / 2250 Loss: 0.4783\n",
      "Epoch: 1 / 3, Step: 72 / 2250 Loss: 0.6195\n",
      "Epoch: 1 / 3, Step: 73 / 2250 Loss: 0.7012\n",
      "Epoch: 1 / 3, Step: 74 / 2250 Loss: 0.5036\n",
      "Epoch: 1 / 3, Step: 75 / 2250 Loss: 0.6166\n",
      "Epoch: 1 / 3, Step: 76 / 2250 Loss: 0.8131\n",
      "Epoch: 1 / 3, Step: 77 / 2250 Loss: 0.8119\n",
      "Epoch: 1 / 3, Step: 78 / 2250 Loss: 0.7710\n",
      "Epoch: 1 / 3, Step: 79 / 2250 Loss: 0.6959\n",
      "Epoch: 1 / 3, Step: 80 / 2250 Loss: 0.5650\n",
      "Epoch: 1 / 3, Step: 81 / 2250 Loss: 0.4971\n",
      "Epoch: 1 / 3, Step: 82 / 2250 Loss: 0.7592\n",
      "Epoch: 1 / 3, Step: 83 / 2250 Loss: 0.6889\n",
      "Epoch: 1 / 3, Step: 84 / 2250 Loss: 0.4597\n",
      "Epoch: 1 / 3, Step: 85 / 2250 Loss: 0.7201\n",
      "Epoch: 1 / 3, Step: 86 / 2250 Loss: 0.8289\n",
      "Epoch: 1 / 3, Step: 87 / 2250 Loss: 0.5586\n",
      "Epoch: 1 / 3, Step: 88 / 2250 Loss: 0.5111\n",
      "Epoch: 1 / 3, Step: 89 / 2250 Loss: 0.7549\n",
      "Epoch: 1 / 3, Step: 90 / 2250 Loss: 0.5251\n",
      "Epoch: 1 / 3, Step: 91 / 2250 Loss: 0.7675\n",
      "Epoch: 1 / 3, Step: 92 / 2250 Loss: 0.7503\n",
      "Epoch: 1 / 3, Step: 93 / 2250 Loss: 0.7034\n",
      "Epoch: 1 / 3, Step: 94 / 2250 Loss: 0.9913\n",
      "Epoch: 1 / 3, Step: 95 / 2250 Loss: 0.6702\n",
      "Epoch: 1 / 3, Step: 96 / 2250 Loss: 0.9295\n",
      "Epoch: 1 / 3, Step: 97 / 2250 Loss: 0.6950\n",
      "Epoch: 1 / 3, Step: 98 / 2250 Loss: 0.6830\n",
      "Epoch: 1 / 3, Step: 99 / 2250 Loss: 0.8362\n",
      "Epoch: 1 / 3, Step: 100 / 2250 Loss: 0.6820\n",
      "Epoch: 1 / 3, Step: 101 / 2250 Loss: 0.8481\n",
      "Epoch: 1 / 3, Step: 102 / 2250 Loss: 0.7769\n",
      "Epoch: 1 / 3, Step: 103 / 2250 Loss: 0.7576\n",
      "Epoch: 1 / 3, Step: 104 / 2250 Loss: 0.7211\n",
      "Epoch: 1 / 3, Step: 105 / 2250 Loss: 0.7582\n",
      "Epoch: 1 / 3, Step: 106 / 2250 Loss: 0.5345\n",
      "Epoch: 1 / 3, Step: 107 / 2250 Loss: 0.6589\n",
      "Epoch: 1 / 3, Step: 108 / 2250 Loss: 0.4528\n",
      "Epoch: 1 / 3, Step: 109 / 2250 Loss: 0.7008\n",
      "Epoch: 1 / 3, Step: 110 / 2250 Loss: 0.6142\n",
      "Epoch: 1 / 3, Step: 111 / 2250 Loss: 0.5792\n",
      "Epoch: 1 / 3, Step: 112 / 2250 Loss: 0.9100\n",
      "Epoch: 1 / 3, Step: 113 / 2250 Loss: 0.5795\n",
      "Epoch: 1 / 3, Step: 114 / 2250 Loss: 1.1392\n",
      "Epoch: 1 / 3, Step: 115 / 2250 Loss: 0.7840\n",
      "Epoch: 1 / 3, Step: 116 / 2250 Loss: 0.8415\n",
      "Epoch: 1 / 3, Step: 117 / 2250 Loss: 0.7446\n",
      "Epoch: 1 / 3, Step: 118 / 2250 Loss: 0.8556\n",
      "Epoch: 1 / 3, Step: 119 / 2250 Loss: 0.8737\n",
      "Epoch: 1 / 3, Step: 120 / 2250 Loss: 0.5660\n",
      "Epoch: 1 / 3, Step: 121 / 2250 Loss: 0.6734\n",
      "Epoch: 1 / 3, Step: 122 / 2250 Loss: 0.7590\n",
      "Epoch: 1 / 3, Step: 123 / 2250 Loss: 0.7172\n",
      "Epoch: 1 / 3, Step: 124 / 2250 Loss: 1.1176\n",
      "Epoch: 1 / 3, Step: 125 / 2250 Loss: 0.7586\n",
      "Epoch: 1 / 3, Step: 126 / 2250 Loss: 0.8197\n",
      "Epoch: 1 / 3, Step: 127 / 2250 Loss: 0.6711\n",
      "Epoch: 1 / 3, Step: 128 / 2250 Loss: 0.6098\n",
      "Epoch: 1 / 3, Step: 129 / 2250 Loss: 0.7741\n",
      "Epoch: 1 / 3, Step: 130 / 2250 Loss: 0.6369\n",
      "Epoch: 1 / 3, Step: 131 / 2250 Loss: 0.5348\n",
      "Epoch: 1 / 3, Step: 132 / 2250 Loss: 0.7529\n",
      "Epoch: 1 / 3, Step: 133 / 2250 Loss: 0.6501\n",
      "Epoch: 1 / 3, Step: 134 / 2250 Loss: 0.8698\n",
      "Epoch: 1 / 3, Step: 135 / 2250 Loss: 0.6848\n",
      "Epoch: 1 / 3, Step: 136 / 2250 Loss: 0.6107\n",
      "Epoch: 1 / 3, Step: 137 / 2250 Loss: 0.8336\n",
      "Epoch: 1 / 3, Step: 138 / 2250 Loss: 0.5952\n",
      "Epoch: 1 / 3, Step: 139 / 2250 Loss: 0.6920\n",
      "Epoch: 1 / 3, Step: 140 / 2250 Loss: 0.4659\n",
      "Epoch: 1 / 3, Step: 141 / 2250 Loss: 0.5424\n",
      "Epoch: 1 / 3, Step: 142 / 2250 Loss: 0.4686\n",
      "Epoch: 1 / 3, Step: 143 / 2250 Loss: 0.7792\n",
      "Epoch: 1 / 3, Step: 144 / 2250 Loss: 0.4993\n",
      "Epoch: 1 / 3, Step: 145 / 2250 Loss: 0.9388\n",
      "Epoch: 1 / 3, Step: 146 / 2250 Loss: 0.4764\n",
      "Epoch: 1 / 3, Step: 147 / 2250 Loss: 0.5955\n",
      "Epoch: 1 / 3, Step: 148 / 2250 Loss: 0.7763\n",
      "Epoch: 1 / 3, Step: 149 / 2250 Loss: 0.4887\n",
      "Epoch: 1 / 3, Step: 150 / 2250 Loss: 0.6564\n",
      "Epoch: 1 / 3, Step: 151 / 2250 Loss: 0.8881\n",
      "Epoch: 1 / 3, Step: 152 / 2250 Loss: 0.6853\n",
      "Epoch: 1 / 3, Step: 153 / 2250 Loss: 0.8787\n",
      "Epoch: 1 / 3, Step: 154 / 2250 Loss: 0.7337\n",
      "Epoch: 1 / 3, Step: 155 / 2250 Loss: 0.8887\n",
      "Epoch: 1 / 3, Step: 156 / 2250 Loss: 1.0962\n",
      "Epoch: 1 / 3, Step: 157 / 2250 Loss: 0.7065\n",
      "Epoch: 1 / 3, Step: 158 / 2250 Loss: 0.6517\n",
      "Epoch: 1 / 3, Step: 159 / 2250 Loss: 0.8697\n",
      "Epoch: 1 / 3, Step: 160 / 2250 Loss: 0.7635\n",
      "Epoch: 1 / 3, Step: 161 / 2250 Loss: 0.7486\n",
      "Epoch: 1 / 3, Step: 162 / 2250 Loss: 0.7289\n",
      "Epoch: 1 / 3, Step: 163 / 2250 Loss: 0.7051\n",
      "Epoch: 1 / 3, Step: 164 / 2250 Loss: 0.5686\n",
      "Epoch: 1 / 3, Step: 165 / 2250 Loss: 0.7336\n",
      "Epoch: 1 / 3, Step: 166 / 2250 Loss: 0.7801\n",
      "Epoch: 1 / 3, Step: 167 / 2250 Loss: 0.8640\n",
      "Epoch: 1 / 3, Step: 168 / 2250 Loss: 0.8801\n",
      "Epoch: 1 / 3, Step: 169 / 2250 Loss: 0.7620\n",
      "Epoch: 1 / 3, Step: 170 / 2250 Loss: 0.6622\n",
      "Epoch: 1 / 3, Step: 171 / 2250 Loss: 0.6716\n",
      "Epoch: 1 / 3, Step: 172 / 2250 Loss: 0.7022\n",
      "Epoch: 1 / 3, Step: 173 / 2250 Loss: 0.5596\n",
      "Epoch: 1 / 3, Step: 174 / 2250 Loss: 0.7634\n",
      "Epoch: 1 / 3, Step: 175 / 2250 Loss: 0.6502\n",
      "Epoch: 1 / 3, Step: 176 / 2250 Loss: 0.7015\n",
      "Epoch: 1 / 3, Step: 177 / 2250 Loss: 0.9331\n",
      "Epoch: 1 / 3, Step: 178 / 2250 Loss: 0.7865\n",
      "Epoch: 1 / 3, Step: 179 / 2250 Loss: 0.5285\n",
      "Epoch: 1 / 3, Step: 180 / 2250 Loss: 0.7237\n",
      "Epoch: 1 / 3, Step: 181 / 2250 Loss: 0.8645\n",
      "Epoch: 1 / 3, Step: 182 / 2250 Loss: 0.9116\n",
      "Epoch: 1 / 3, Step: 183 / 2250 Loss: 0.6693\n",
      "Epoch: 1 / 3, Step: 184 / 2250 Loss: 0.8040\n",
      "Epoch: 1 / 3, Step: 185 / 2250 Loss: 0.9215\n",
      "Epoch: 1 / 3, Step: 186 / 2250 Loss: 0.6013\n",
      "Epoch: 1 / 3, Step: 187 / 2250 Loss: 0.7442\n",
      "Epoch: 1 / 3, Step: 188 / 2250 Loss: 0.7033\n",
      "Epoch: 1 / 3, Step: 189 / 2250 Loss: 0.7994\n",
      "Epoch: 1 / 3, Step: 190 / 2250 Loss: 0.6560\n",
      "Epoch: 1 / 3, Step: 191 / 2250 Loss: 0.6699\n",
      "Epoch: 1 / 3, Step: 192 / 2250 Loss: 0.5618\n",
      "Epoch: 1 / 3, Step: 193 / 2250 Loss: 0.6646\n",
      "Epoch: 1 / 3, Step: 194 / 2250 Loss: 0.7838\n",
      "Epoch: 1 / 3, Step: 195 / 2250 Loss: 0.6150\n",
      "Epoch: 1 / 3, Step: 196 / 2250 Loss: 0.5797\n",
      "Epoch: 1 / 3, Step: 197 / 2250 Loss: 0.5839\n",
      "Epoch: 1 / 3, Step: 198 / 2250 Loss: 0.5414\n",
      "Epoch: 1 / 3, Step: 199 / 2250 Loss: 0.7450\n",
      "Epoch: 1 / 3, Step: 200 / 2250 Loss: 0.7570\n",
      "Epoch: 1 / 3, Step: 201 / 2250 Loss: 0.8025\n",
      "Epoch: 1 / 3, Step: 202 / 2250 Loss: 0.6540\n",
      "Epoch: 1 / 3, Step: 203 / 2250 Loss: 0.6563\n",
      "Epoch: 1 / 3, Step: 204 / 2250 Loss: 0.6983\n",
      "Epoch: 1 / 3, Step: 205 / 2250 Loss: 0.8727\n",
      "Epoch: 1 / 3, Step: 206 / 2250 Loss: 0.8100\n",
      "Epoch: 1 / 3, Step: 207 / 2250 Loss: 0.6342\n",
      "Epoch: 1 / 3, Step: 208 / 2250 Loss: 0.6210\n",
      "Epoch: 1 / 3, Step: 209 / 2250 Loss: 0.5388\n",
      "Epoch: 1 / 3, Step: 210 / 2250 Loss: 0.6917\n",
      "Epoch: 1 / 3, Step: 211 / 2250 Loss: 0.4584\n",
      "Epoch: 1 / 3, Step: 212 / 2250 Loss: 0.7083\n",
      "Epoch: 1 / 3, Step: 213 / 2250 Loss: 0.7231\n",
      "Epoch: 1 / 3, Step: 214 / 2250 Loss: 0.7503\n",
      "Epoch: 1 / 3, Step: 215 / 2250 Loss: 0.5605\n",
      "Epoch: 1 / 3, Step: 216 / 2250 Loss: 0.5654\n",
      "Epoch: 1 / 3, Step: 217 / 2250 Loss: 0.5901\n",
      "Epoch: 1 / 3, Step: 218 / 2250 Loss: 0.7441\n",
      "Epoch: 1 / 3, Step: 219 / 2250 Loss: 0.7399\n",
      "Epoch: 1 / 3, Step: 220 / 2250 Loss: 0.5563\n",
      "Epoch: 1 / 3, Step: 221 / 2250 Loss: 0.4336\n",
      "Epoch: 1 / 3, Step: 222 / 2250 Loss: 0.5333\n",
      "Epoch: 1 / 3, Step: 223 / 2250 Loss: 1.1121\n",
      "Epoch: 1 / 3, Step: 224 / 2250 Loss: 0.4581\n",
      "Epoch: 1 / 3, Step: 225 / 2250 Loss: 0.7585\n",
      "Epoch: 1 / 3, Step: 226 / 2250 Loss: 0.5422\n",
      "Epoch: 1 / 3, Step: 227 / 2250 Loss: 0.6421\n",
      "Epoch: 1 / 3, Step: 228 / 2250 Loss: 0.6105\n",
      "Epoch: 1 / 3, Step: 229 / 2250 Loss: 0.5846\n",
      "Epoch: 1 / 3, Step: 230 / 2250 Loss: 0.5299\n",
      "Epoch: 1 / 3, Step: 231 / 2250 Loss: 0.5566\n",
      "Epoch: 1 / 3, Step: 232 / 2250 Loss: 0.3767\n",
      "Epoch: 1 / 3, Step: 233 / 2250 Loss: 0.7113\n",
      "Epoch: 1 / 3, Step: 234 / 2250 Loss: 0.6319\n",
      "Epoch: 1 / 3, Step: 235 / 2250 Loss: 0.7059\n",
      "Epoch: 1 / 3, Step: 236 / 2250 Loss: 0.2893\n",
      "Epoch: 1 / 3, Step: 237 / 2250 Loss: 0.5329\n",
      "Epoch: 1 / 3, Step: 238 / 2250 Loss: 0.5066\n",
      "Epoch: 1 / 3, Step: 239 / 2250 Loss: 0.9947\n",
      "Epoch: 1 / 3, Step: 240 / 2250 Loss: 0.7230\n",
      "Epoch: 1 / 3, Step: 241 / 2250 Loss: 0.5634\n",
      "Epoch: 1 / 3, Step: 242 / 2250 Loss: 0.7474\n",
      "Epoch: 1 / 3, Step: 243 / 2250 Loss: 0.5366\n",
      "Epoch: 1 / 3, Step: 244 / 2250 Loss: 0.6622\n",
      "Epoch: 1 / 3, Step: 245 / 2250 Loss: 0.5589\n",
      "Epoch: 1 / 3, Step: 246 / 2250 Loss: 0.7645\n",
      "Epoch: 1 / 3, Step: 247 / 2250 Loss: 0.6429\n",
      "Epoch: 1 / 3, Step: 248 / 2250 Loss: 0.4072\n",
      "Epoch: 1 / 3, Step: 249 / 2250 Loss: 0.7825\n",
      "Epoch: 1 / 3, Step: 250 / 2250 Loss: 0.4947\n",
      "Epoch: 1 / 3, Step: 251 / 2250 Loss: 0.7086\n",
      "Epoch: 1 / 3, Step: 252 / 2250 Loss: 0.3905\n",
      "Epoch: 1 / 3, Step: 253 / 2250 Loss: 0.7540\n",
      "Epoch: 1 / 3, Step: 254 / 2250 Loss: 0.2452\n",
      "Epoch: 1 / 3, Step: 255 / 2250 Loss: 0.6878\n",
      "Epoch: 1 / 3, Step: 256 / 2250 Loss: 0.7828\n",
      "Epoch: 1 / 3, Step: 257 / 2250 Loss: 0.5858\n",
      "Epoch: 1 / 3, Step: 258 / 2250 Loss: 0.8341\n",
      "Epoch: 1 / 3, Step: 259 / 2250 Loss: 0.3582\n",
      "Epoch: 1 / 3, Step: 260 / 2250 Loss: 0.7964\n",
      "Epoch: 1 / 3, Step: 261 / 2250 Loss: 0.6958\n",
      "Epoch: 1 / 3, Step: 262 / 2250 Loss: 0.6196\n",
      "Epoch: 1 / 3, Step: 263 / 2250 Loss: 0.6828\n",
      "Epoch: 1 / 3, Step: 264 / 2250 Loss: 0.8673\n",
      "Epoch: 1 / 3, Step: 265 / 2250 Loss: 0.6874\n",
      "Epoch: 1 / 3, Step: 266 / 2250 Loss: 0.4621\n",
      "Epoch: 1 / 3, Step: 267 / 2250 Loss: 0.2697\n",
      "Epoch: 1 / 3, Step: 268 / 2250 Loss: 0.5609\n",
      "Epoch: 1 / 3, Step: 269 / 2250 Loss: 0.4401\n",
      "Epoch: 1 / 3, Step: 270 / 2250 Loss: 0.6774\n",
      "Epoch: 1 / 3, Step: 271 / 2250 Loss: 0.7204\n",
      "Epoch: 1 / 3, Step: 272 / 2250 Loss: 0.4820\n",
      "Epoch: 1 / 3, Step: 273 / 2250 Loss: 0.5748\n",
      "Epoch: 1 / 3, Step: 274 / 2250 Loss: 0.5878\n",
      "Epoch: 1 / 3, Step: 275 / 2250 Loss: 0.6528\n",
      "Epoch: 1 / 3, Step: 276 / 2250 Loss: 0.8356\n",
      "Epoch: 1 / 3, Step: 277 / 2250 Loss: 0.4375\n",
      "Epoch: 1 / 3, Step: 278 / 2250 Loss: 0.4624\n",
      "Epoch: 1 / 3, Step: 279 / 2250 Loss: 0.4841\n",
      "Epoch: 1 / 3, Step: 280 / 2250 Loss: 0.6576\n",
      "Epoch: 1 / 3, Step: 281 / 2250 Loss: 0.3575\n",
      "Epoch: 1 / 3, Step: 282 / 2250 Loss: 0.4562\n",
      "Epoch: 1 / 3, Step: 283 / 2250 Loss: 0.5951\n",
      "Epoch: 1 / 3, Step: 284 / 2250 Loss: 0.3048\n",
      "Epoch: 1 / 3, Step: 285 / 2250 Loss: 0.6506\n",
      "Epoch: 1 / 3, Step: 286 / 2250 Loss: 0.5384\n",
      "Epoch: 1 / 3, Step: 287 / 2250 Loss: 0.5358\n",
      "Epoch: 1 / 3, Step: 288 / 2250 Loss: 0.4270\n",
      "Epoch: 1 / 3, Step: 289 / 2250 Loss: 0.7890\n",
      "Epoch: 1 / 3, Step: 290 / 2250 Loss: 0.4527\n",
      "Epoch: 1 / 3, Step: 291 / 2250 Loss: 0.6149\n",
      "Epoch: 1 / 3, Step: 292 / 2250 Loss: 0.4644\n",
      "Epoch: 1 / 3, Step: 293 / 2250 Loss: 0.5066\n",
      "Epoch: 1 / 3, Step: 294 / 2250 Loss: 0.4551\n",
      "Epoch: 1 / 3, Step: 295 / 2250 Loss: 0.5116\n",
      "Epoch: 1 / 3, Step: 296 / 2250 Loss: 0.7041\n",
      "Epoch: 1 / 3, Step: 297 / 2250 Loss: 0.3031\n",
      "Epoch: 1 / 3, Step: 298 / 2250 Loss: 0.4025\n",
      "Epoch: 1 / 3, Step: 299 / 2250 Loss: 0.5265\n",
      "Epoch: 1 / 3, Step: 300 / 2250 Loss: 0.5893\n",
      "Epoch: 1 / 3, Step: 301 / 2250 Loss: 0.7611\n",
      "Epoch: 1 / 3, Step: 302 / 2250 Loss: 0.4391\n",
      "Epoch: 1 / 3, Step: 303 / 2250 Loss: 0.3379\n",
      "Epoch: 1 / 3, Step: 304 / 2250 Loss: 0.5465\n",
      "Epoch: 1 / 3, Step: 305 / 2250 Loss: 0.6670\n",
      "Epoch: 1 / 3, Step: 306 / 2250 Loss: 0.4312\n",
      "Epoch: 1 / 3, Step: 307 / 2250 Loss: 0.5855\n",
      "Epoch: 1 / 3, Step: 308 / 2250 Loss: 0.6791\n",
      "Epoch: 1 / 3, Step: 309 / 2250 Loss: 0.5764\n",
      "Epoch: 1 / 3, Step: 310 / 2250 Loss: 0.5164\n",
      "Epoch: 1 / 3, Step: 311 / 2250 Loss: 0.5879\n",
      "Epoch: 1 / 3, Step: 312 / 2250 Loss: 0.4243\n",
      "Epoch: 1 / 3, Step: 313 / 2250 Loss: 0.8034\n",
      "Epoch: 1 / 3, Step: 314 / 2250 Loss: 0.4951\n",
      "Epoch: 1 / 3, Step: 315 / 2250 Loss: 0.2811\n",
      "Epoch: 1 / 3, Step: 316 / 2250 Loss: 0.9123\n",
      "Epoch: 1 / 3, Step: 317 / 2250 Loss: 0.5158\n",
      "Epoch: 1 / 3, Step: 318 / 2250 Loss: 0.6576\n",
      "Epoch: 1 / 3, Step: 319 / 2250 Loss: 0.4313\n",
      "Epoch: 1 / 3, Step: 320 / 2250 Loss: 0.4755\n",
      "Epoch: 1 / 3, Step: 321 / 2250 Loss: 0.3747\n",
      "Epoch: 1 / 3, Step: 322 / 2250 Loss: 0.4227\n",
      "Epoch: 1 / 3, Step: 323 / 2250 Loss: 0.7015\n",
      "Epoch: 1 / 3, Step: 324 / 2250 Loss: 0.6157\n",
      "Epoch: 1 / 3, Step: 325 / 2250 Loss: 0.5123\n",
      "Epoch: 1 / 3, Step: 326 / 2250 Loss: 0.5578\n",
      "Epoch: 1 / 3, Step: 327 / 2250 Loss: 0.6780\n",
      "Epoch: 1 / 3, Step: 328 / 2250 Loss: 0.6479\n",
      "Epoch: 1 / 3, Step: 329 / 2250 Loss: 0.5536\n",
      "Epoch: 1 / 3, Step: 330 / 2250 Loss: 0.8783\n",
      "Epoch: 1 / 3, Step: 331 / 2250 Loss: 0.3663\n",
      "Epoch: 1 / 3, Step: 332 / 2250 Loss: 0.4404\n",
      "Epoch: 1 / 3, Step: 333 / 2250 Loss: 0.6973\n",
      "Epoch: 1 / 3, Step: 334 / 2250 Loss: 0.6434\n",
      "Epoch: 1 / 3, Step: 335 / 2250 Loss: 0.7571\n",
      "Epoch: 1 / 3, Step: 336 / 2250 Loss: 0.6340\n",
      "Epoch: 1 / 3, Step: 337 / 2250 Loss: 0.4564\n",
      "Epoch: 1 / 3, Step: 338 / 2250 Loss: 0.4570\n",
      "Epoch: 1 / 3, Step: 339 / 2250 Loss: 0.4252\n",
      "Epoch: 1 / 3, Step: 340 / 2250 Loss: 0.6809\n",
      "Epoch: 1 / 3, Step: 341 / 2250 Loss: 0.4902\n",
      "Epoch: 1 / 3, Step: 342 / 2250 Loss: 0.6878\n",
      "Epoch: 1 / 3, Step: 343 / 2250 Loss: 0.6229\n",
      "Epoch: 1 / 3, Step: 344 / 2250 Loss: 0.3818\n",
      "Epoch: 1 / 3, Step: 345 / 2250 Loss: 0.6226\n",
      "Epoch: 1 / 3, Step: 346 / 2250 Loss: 0.3338\n",
      "Epoch: 1 / 3, Step: 347 / 2250 Loss: 0.6009\n",
      "Epoch: 1 / 3, Step: 348 / 2250 Loss: 0.4624\n",
      "Epoch: 1 / 3, Step: 349 / 2250 Loss: 0.5671\n",
      "Epoch: 1 / 3, Step: 350 / 2250 Loss: 0.6687\n",
      "Epoch: 1 / 3, Step: 351 / 2250 Loss: 0.5610\n",
      "Epoch: 1 / 3, Step: 352 / 2250 Loss: 0.6456\n",
      "Epoch: 1 / 3, Step: 353 / 2250 Loss: 0.5125\n",
      "Epoch: 1 / 3, Step: 354 / 2250 Loss: 0.8281\n",
      "Epoch: 1 / 3, Step: 355 / 2250 Loss: 0.6221\n",
      "Epoch: 1 / 3, Step: 356 / 2250 Loss: 0.6144\n",
      "Epoch: 1 / 3, Step: 357 / 2250 Loss: 0.7450\n",
      "Epoch: 1 / 3, Step: 358 / 2250 Loss: 0.3252\n",
      "Epoch: 1 / 3, Step: 359 / 2250 Loss: 0.7137\n",
      "Epoch: 1 / 3, Step: 360 / 2250 Loss: 0.6290\n",
      "Epoch: 1 / 3, Step: 361 / 2250 Loss: 0.8303\n",
      "Epoch: 1 / 3, Step: 362 / 2250 Loss: 0.6649\n",
      "Epoch: 1 / 3, Step: 363 / 2250 Loss: 0.4928\n",
      "Epoch: 1 / 3, Step: 364 / 2250 Loss: 0.4123\n",
      "Epoch: 1 / 3, Step: 365 / 2250 Loss: 0.4319\n",
      "Epoch: 1 / 3, Step: 366 / 2250 Loss: 0.3589\n",
      "Epoch: 1 / 3, Step: 367 / 2250 Loss: 0.7171\n",
      "Epoch: 1 / 3, Step: 368 / 2250 Loss: 0.6787\n",
      "Epoch: 1 / 3, Step: 369 / 2250 Loss: 0.3787\n",
      "Epoch: 1 / 3, Step: 370 / 2250 Loss: 0.5448\n",
      "Epoch: 1 / 3, Step: 371 / 2250 Loss: 0.5373\n",
      "Epoch: 1 / 3, Step: 372 / 2250 Loss: 0.4033\n",
      "Epoch: 1 / 3, Step: 373 / 2250 Loss: 0.5725\n",
      "Epoch: 1 / 3, Step: 374 / 2250 Loss: 0.5405\n",
      "Epoch: 1 / 3, Step: 375 / 2250 Loss: 0.3525\n",
      "Epoch: 1 / 3, Step: 376 / 2250 Loss: 0.5492\n",
      "Epoch: 1 / 3, Step: 377 / 2250 Loss: 0.8984\n",
      "Epoch: 1 / 3, Step: 378 / 2250 Loss: 0.5952\n",
      "Epoch: 1 / 3, Step: 379 / 2250 Loss: 0.4450\n",
      "Epoch: 1 / 3, Step: 380 / 2250 Loss: 0.6092\n",
      "Epoch: 1 / 3, Step: 381 / 2250 Loss: 0.3774\n",
      "Epoch: 1 / 3, Step: 382 / 2250 Loss: 0.5563\n",
      "Epoch: 1 / 3, Step: 383 / 2250 Loss: 0.4747\n",
      "Epoch: 1 / 3, Step: 384 / 2250 Loss: 0.4037\n",
      "Epoch: 1 / 3, Step: 385 / 2250 Loss: 0.3087\n",
      "Epoch: 1 / 3, Step: 386 / 2250 Loss: 0.3001\n",
      "Epoch: 1 / 3, Step: 387 / 2250 Loss: 0.4144\n",
      "Epoch: 1 / 3, Step: 388 / 2250 Loss: 0.3641\n",
      "Epoch: 1 / 3, Step: 389 / 2250 Loss: 0.2918\n",
      "Epoch: 1 / 3, Step: 390 / 2250 Loss: 0.8291\n",
      "Epoch: 1 / 3, Step: 391 / 2250 Loss: 0.4789\n",
      "Epoch: 1 / 3, Step: 392 / 2250 Loss: 0.4959\n",
      "Epoch: 1 / 3, Step: 393 / 2250 Loss: 0.6516\n",
      "Epoch: 1 / 3, Step: 394 / 2250 Loss: 0.5413\n",
      "Epoch: 1 / 3, Step: 395 / 2250 Loss: 0.4898\n",
      "Epoch: 1 / 3, Step: 396 / 2250 Loss: 0.4694\n",
      "Epoch: 1 / 3, Step: 397 / 2250 Loss: 0.2178\n",
      "Epoch: 1 / 3, Step: 398 / 2250 Loss: 0.3886\n",
      "Epoch: 1 / 3, Step: 399 / 2250 Loss: 0.7389\n",
      "Epoch: 1 / 3, Step: 400 / 2250 Loss: 0.3053\n",
      "Epoch: 1 / 3, Step: 401 / 2250 Loss: 0.4256\n",
      "Epoch: 1 / 3, Step: 402 / 2250 Loss: 0.4413\n",
      "Epoch: 1 / 3, Step: 403 / 2250 Loss: 0.4426\n",
      "Epoch: 1 / 3, Step: 404 / 2250 Loss: 0.3413\n",
      "Epoch: 1 / 3, Step: 405 / 2250 Loss: 0.5986\n",
      "Epoch: 1 / 3, Step: 406 / 2250 Loss: 0.4095\n",
      "Epoch: 1 / 3, Step: 407 / 2250 Loss: 0.4507\n",
      "Epoch: 1 / 3, Step: 408 / 2250 Loss: 0.5963\n",
      "Epoch: 1 / 3, Step: 409 / 2250 Loss: 0.3418\n",
      "Epoch: 1 / 3, Step: 410 / 2250 Loss: 0.4144\n",
      "Epoch: 1 / 3, Step: 411 / 2250 Loss: 0.5487\n",
      "Epoch: 1 / 3, Step: 412 / 2250 Loss: 0.4017\n",
      "Epoch: 1 / 3, Step: 413 / 2250 Loss: 0.4895\n",
      "Epoch: 1 / 3, Step: 414 / 2250 Loss: 0.4617\n",
      "Epoch: 1 / 3, Step: 415 / 2250 Loss: 0.5119\n",
      "Epoch: 1 / 3, Step: 416 / 2250 Loss: 0.3598\n",
      "Epoch: 1 / 3, Step: 417 / 2250 Loss: 0.3227\n",
      "Epoch: 1 / 3, Step: 418 / 2250 Loss: 0.3888\n",
      "Epoch: 1 / 3, Step: 419 / 2250 Loss: 0.4842\n",
      "Epoch: 1 / 3, Step: 420 / 2250 Loss: 0.5037\n",
      "Epoch: 1 / 3, Step: 421 / 2250 Loss: 0.6463\n",
      "Epoch: 1 / 3, Step: 422 / 2250 Loss: 0.3752\n",
      "Epoch: 1 / 3, Step: 423 / 2250 Loss: 0.3710\n",
      "Epoch: 1 / 3, Step: 424 / 2250 Loss: 0.4931\n",
      "Epoch: 1 / 3, Step: 425 / 2250 Loss: 0.6335\n",
      "Epoch: 1 / 3, Step: 426 / 2250 Loss: 0.6159\n",
      "Epoch: 1 / 3, Step: 427 / 2250 Loss: 0.4021\n",
      "Epoch: 1 / 3, Step: 428 / 2250 Loss: 0.5729\n",
      "Epoch: 1 / 3, Step: 429 / 2250 Loss: 0.4524\n",
      "Epoch: 1 / 3, Step: 430 / 2250 Loss: 0.3571\n",
      "Epoch: 1 / 3, Step: 431 / 2250 Loss: 0.5063\n",
      "Epoch: 1 / 3, Step: 432 / 2250 Loss: 0.5542\n",
      "Epoch: 1 / 3, Step: 433 / 2250 Loss: 0.6682\n",
      "Epoch: 1 / 3, Step: 434 / 2250 Loss: 0.4943\n",
      "Epoch: 1 / 3, Step: 435 / 2250 Loss: 0.4664\n",
      "Epoch: 1 / 3, Step: 436 / 2250 Loss: 0.4438\n",
      "Epoch: 1 / 3, Step: 437 / 2250 Loss: 0.5167\n",
      "Epoch: 1 / 3, Step: 438 / 2250 Loss: 0.4639\n",
      "Epoch: 1 / 3, Step: 439 / 2250 Loss: 0.4407\n",
      "Epoch: 1 / 3, Step: 440 / 2250 Loss: 0.3644\n",
      "Epoch: 1 / 3, Step: 441 / 2250 Loss: 0.4712\n",
      "Epoch: 1 / 3, Step: 442 / 2250 Loss: 0.3951\n",
      "Epoch: 1 / 3, Step: 443 / 2250 Loss: 0.4697\n",
      "Epoch: 1 / 3, Step: 444 / 2250 Loss: 0.4718\n",
      "Epoch: 1 / 3, Step: 445 / 2250 Loss: 0.2893\n",
      "Epoch: 1 / 3, Step: 446 / 2250 Loss: 0.5011\n",
      "Epoch: 1 / 3, Step: 447 / 2250 Loss: 0.4729\n",
      "Epoch: 1 / 3, Step: 448 / 2250 Loss: 0.4259\n",
      "Epoch: 1 / 3, Step: 449 / 2250 Loss: 0.5063\n",
      "Epoch: 1 / 3, Step: 450 / 2250 Loss: 0.5284\n",
      "Epoch: 1 / 3, Step: 451 / 2250 Loss: 0.4820\n",
      "Epoch: 1 / 3, Step: 452 / 2250 Loss: 0.2568\n",
      "Epoch: 1 / 3, Step: 453 / 2250 Loss: 0.4490\n",
      "Epoch: 1 / 3, Step: 454 / 2250 Loss: 0.4631\n",
      "Epoch: 1 / 3, Step: 455 / 2250 Loss: 0.3290\n",
      "Epoch: 1 / 3, Step: 456 / 2250 Loss: 0.7353\n",
      "Epoch: 1 / 3, Step: 457 / 2250 Loss: 0.6631\n",
      "Epoch: 1 / 3, Step: 458 / 2250 Loss: 0.2854\n",
      "Epoch: 1 / 3, Step: 459 / 2250 Loss: 0.6188\n",
      "Epoch: 1 / 3, Step: 460 / 2250 Loss: 0.6196\n",
      "Epoch: 1 / 3, Step: 461 / 2250 Loss: 0.5522\n",
      "Epoch: 1 / 3, Step: 462 / 2250 Loss: 0.7128\n",
      "Epoch: 1 / 3, Step: 463 / 2250 Loss: 0.5065\n",
      "Epoch: 1 / 3, Step: 464 / 2250 Loss: 0.5521\n",
      "Epoch: 1 / 3, Step: 465 / 2250 Loss: 0.5591\n",
      "Epoch: 1 / 3, Step: 466 / 2250 Loss: 0.4872\n",
      "Epoch: 1 / 3, Step: 467 / 2250 Loss: 0.5383\n",
      "Epoch: 1 / 3, Step: 468 / 2250 Loss: 0.3293\n",
      "Epoch: 1 / 3, Step: 469 / 2250 Loss: 0.4624\n",
      "Epoch: 1 / 3, Step: 470 / 2250 Loss: 0.3322\n",
      "Epoch: 1 / 3, Step: 471 / 2250 Loss: 0.4398\n",
      "Epoch: 1 / 3, Step: 472 / 2250 Loss: 0.5983\n",
      "Epoch: 1 / 3, Step: 473 / 2250 Loss: 0.3543\n",
      "Epoch: 1 / 3, Step: 474 / 2250 Loss: 0.4711\n",
      "Epoch: 1 / 3, Step: 475 / 2250 Loss: 0.4547\n",
      "Epoch: 1 / 3, Step: 476 / 2250 Loss: 0.5765\n",
      "Epoch: 1 / 3, Step: 477 / 2250 Loss: 0.3148\n",
      "Epoch: 1 / 3, Step: 478 / 2250 Loss: 0.3409\n",
      "Epoch: 1 / 3, Step: 479 / 2250 Loss: 0.4788\n",
      "Epoch: 1 / 3, Step: 480 / 2250 Loss: 0.2674\n",
      "Epoch: 1 / 3, Step: 481 / 2250 Loss: 0.3741\n",
      "Epoch: 1 / 3, Step: 482 / 2250 Loss: 0.3467\n",
      "Epoch: 1 / 3, Step: 483 / 2250 Loss: 0.4563\n",
      "Epoch: 1 / 3, Step: 484 / 2250 Loss: 0.5499\n",
      "Epoch: 1 / 3, Step: 485 / 2250 Loss: 0.1978\n",
      "Epoch: 1 / 3, Step: 486 / 2250 Loss: 0.3926\n",
      "Epoch: 1 / 3, Step: 487 / 2250 Loss: 0.6687\n",
      "Epoch: 1 / 3, Step: 488 / 2250 Loss: 0.3561\n",
      "Epoch: 1 / 3, Step: 489 / 2250 Loss: 0.4443\n",
      "Epoch: 1 / 3, Step: 490 / 2250 Loss: 0.3947\n",
      "Epoch: 1 / 3, Step: 491 / 2250 Loss: 0.3854\n",
      "Epoch: 1 / 3, Step: 492 / 2250 Loss: 0.8078\n",
      "Epoch: 1 / 3, Step: 493 / 2250 Loss: 0.6305\n",
      "Epoch: 1 / 3, Step: 494 / 2250 Loss: 0.5860\n",
      "Epoch: 1 / 3, Step: 495 / 2250 Loss: 0.4953\n",
      "Epoch: 1 / 3, Step: 496 / 2250 Loss: 0.4994\n",
      "Epoch: 1 / 3, Step: 497 / 2250 Loss: 0.3317\n",
      "Epoch: 1 / 3, Step: 498 / 2250 Loss: 0.5209\n",
      "Epoch: 1 / 3, Step: 499 / 2250 Loss: 0.4031\n",
      "Epoch: 1 / 3, Step: 500 / 2250 Loss: 0.4122\n",
      "Epoch: 1 / 3, Step: 501 / 2250 Loss: 0.2893\n",
      "Epoch: 1 / 3, Step: 502 / 2250 Loss: 0.2879\n",
      "Epoch: 1 / 3, Step: 503 / 2250 Loss: 0.6573\n",
      "Epoch: 1 / 3, Step: 504 / 2250 Loss: 0.4084\n",
      "Epoch: 1 / 3, Step: 505 / 2250 Loss: 0.5692\n",
      "Epoch: 1 / 3, Step: 506 / 2250 Loss: 0.4428\n",
      "Epoch: 1 / 3, Step: 507 / 2250 Loss: 0.4647\n",
      "Epoch: 1 / 3, Step: 508 / 2250 Loss: 0.4426\n",
      "Epoch: 1 / 3, Step: 509 / 2250 Loss: 0.4738\n",
      "Epoch: 1 / 3, Step: 510 / 2250 Loss: 0.5340\n",
      "Epoch: 1 / 3, Step: 511 / 2250 Loss: 0.4683\n",
      "Epoch: 1 / 3, Step: 512 / 2250 Loss: 0.5590\n",
      "Epoch: 1 / 3, Step: 513 / 2250 Loss: 0.4847\n",
      "Epoch: 1 / 3, Step: 514 / 2250 Loss: 0.3966\n",
      "Epoch: 1 / 3, Step: 515 / 2250 Loss: 0.3330\n",
      "Epoch: 1 / 3, Step: 516 / 2250 Loss: 0.4549\n",
      "Epoch: 1 / 3, Step: 517 / 2250 Loss: 0.5101\n",
      "Epoch: 1 / 3, Step: 518 / 2250 Loss: 0.6410\n",
      "Epoch: 1 / 3, Step: 519 / 2250 Loss: 0.4485\n",
      "Epoch: 1 / 3, Step: 520 / 2250 Loss: 0.5156\n",
      "Epoch: 1 / 3, Step: 521 / 2250 Loss: 0.4520\n",
      "Epoch: 1 / 3, Step: 522 / 2250 Loss: 0.4104\n",
      "Epoch: 1 / 3, Step: 523 / 2250 Loss: 0.3652\n",
      "Epoch: 1 / 3, Step: 524 / 2250 Loss: 0.6055\n",
      "Epoch: 1 / 3, Step: 525 / 2250 Loss: 0.4192\n",
      "Epoch: 1 / 3, Step: 526 / 2250 Loss: 0.4805\n",
      "Epoch: 1 / 3, Step: 527 / 2250 Loss: 0.5294\n",
      "Epoch: 1 / 3, Step: 528 / 2250 Loss: 0.3423\n",
      "Epoch: 1 / 3, Step: 529 / 2250 Loss: 0.3358\n",
      "Epoch: 1 / 3, Step: 530 / 2250 Loss: 0.5375\n",
      "Epoch: 1 / 3, Step: 531 / 2250 Loss: 0.3541\n",
      "Epoch: 1 / 3, Step: 532 / 2250 Loss: 0.4673\n",
      "Epoch: 1 / 3, Step: 533 / 2250 Loss: 0.4590\n",
      "Epoch: 1 / 3, Step: 534 / 2250 Loss: 0.6182\n",
      "Epoch: 1 / 3, Step: 535 / 2250 Loss: 0.4573\n",
      "Epoch: 1 / 3, Step: 536 / 2250 Loss: 0.5544\n",
      "Epoch: 1 / 3, Step: 537 / 2250 Loss: 0.3583\n",
      "Epoch: 1 / 3, Step: 538 / 2250 Loss: 0.3407\n",
      "Epoch: 1 / 3, Step: 539 / 2250 Loss: 0.5064\n",
      "Epoch: 1 / 3, Step: 540 / 2250 Loss: 0.5142\n",
      "Epoch: 1 / 3, Step: 541 / 2250 Loss: 0.3540\n",
      "Epoch: 1 / 3, Step: 542 / 2250 Loss: 0.3784\n",
      "Epoch: 1 / 3, Step: 543 / 2250 Loss: 0.3917\n",
      "Epoch: 1 / 3, Step: 544 / 2250 Loss: 0.3366\n",
      "Epoch: 1 / 3, Step: 545 / 2250 Loss: 0.4515\n",
      "Epoch: 1 / 3, Step: 546 / 2250 Loss: 0.3186\n",
      "Epoch: 1 / 3, Step: 547 / 2250 Loss: 0.3625\n",
      "Epoch: 1 / 3, Step: 548 / 2250 Loss: 0.2647\n",
      "Epoch: 1 / 3, Step: 549 / 2250 Loss: 0.3772\n",
      "Epoch: 1 / 3, Step: 550 / 2250 Loss: 0.3308\n",
      "Epoch: 1 / 3, Step: 551 / 2250 Loss: 0.4213\n",
      "Epoch: 1 / 3, Step: 552 / 2250 Loss: 0.4143\n",
      "Epoch: 1 / 3, Step: 553 / 2250 Loss: 0.3856\n",
      "Epoch: 1 / 3, Step: 554 / 2250 Loss: 0.3504\n",
      "Epoch: 1 / 3, Step: 555 / 2250 Loss: 0.4001\n",
      "Epoch: 1 / 3, Step: 556 / 2250 Loss: 0.4544\n",
      "Epoch: 1 / 3, Step: 557 / 2250 Loss: 0.6021\n",
      "Epoch: 1 / 3, Step: 558 / 2250 Loss: 0.4229\n",
      "Epoch: 1 / 3, Step: 559 / 2250 Loss: 0.4327\n",
      "Epoch: 1 / 3, Step: 560 / 2250 Loss: 0.3360\n",
      "Epoch: 1 / 3, Step: 561 / 2250 Loss: 0.4720\n",
      "Epoch: 1 / 3, Step: 562 / 2250 Loss: 0.5596\n",
      "Epoch: 1 / 3, Step: 563 / 2250 Loss: 0.5283\n",
      "Epoch: 1 / 3, Step: 564 / 2250 Loss: 0.2846\n",
      "Epoch: 1 / 3, Step: 565 / 2250 Loss: 0.3884\n",
      "Epoch: 1 / 3, Step: 566 / 2250 Loss: 0.2156\n",
      "Epoch: 1 / 3, Step: 567 / 2250 Loss: 0.3505\n",
      "Epoch: 1 / 3, Step: 568 / 2250 Loss: 0.2538\n",
      "Epoch: 1 / 3, Step: 569 / 2250 Loss: 0.6372\n",
      "Epoch: 1 / 3, Step: 570 / 2250 Loss: 0.2546\n",
      "Epoch: 1 / 3, Step: 571 / 2250 Loss: 0.4613\n",
      "Epoch: 1 / 3, Step: 572 / 2250 Loss: 0.5161\n",
      "Epoch: 1 / 3, Step: 573 / 2250 Loss: 0.4048\n",
      "Epoch: 1 / 3, Step: 574 / 2250 Loss: 0.4712\n",
      "Epoch: 1 / 3, Step: 575 / 2250 Loss: 0.5144\n",
      "Epoch: 1 / 3, Step: 576 / 2250 Loss: 0.2425\n",
      "Epoch: 1 / 3, Step: 577 / 2250 Loss: 0.7164\n",
      "Epoch: 1 / 3, Step: 578 / 2250 Loss: 0.3953\n",
      "Epoch: 1 / 3, Step: 579 / 2250 Loss: 0.2574\n",
      "Epoch: 1 / 3, Step: 580 / 2250 Loss: 0.2116\n",
      "Epoch: 1 / 3, Step: 581 / 2250 Loss: 0.5125\n",
      "Epoch: 1 / 3, Step: 582 / 2250 Loss: 0.5760\n",
      "Epoch: 1 / 3, Step: 583 / 2250 Loss: 0.3280\n",
      "Epoch: 1 / 3, Step: 584 / 2250 Loss: 0.3578\n",
      "Epoch: 1 / 3, Step: 585 / 2250 Loss: 0.4916\n",
      "Epoch: 1 / 3, Step: 586 / 2250 Loss: 0.5962\n",
      "Epoch: 1 / 3, Step: 587 / 2250 Loss: 0.2952\n",
      "Epoch: 1 / 3, Step: 588 / 2250 Loss: 0.3764\n",
      "Epoch: 1 / 3, Step: 589 / 2250 Loss: 0.4174\n",
      "Epoch: 1 / 3, Step: 590 / 2250 Loss: 0.4652\n",
      "Epoch: 1 / 3, Step: 591 / 2250 Loss: 0.1447\n",
      "Epoch: 1 / 3, Step: 592 / 2250 Loss: 0.2802\n",
      "Epoch: 1 / 3, Step: 593 / 2250 Loss: 0.2613\n",
      "Epoch: 1 / 3, Step: 594 / 2250 Loss: 0.1698\n",
      "Epoch: 1 / 3, Step: 595 / 2250 Loss: 0.5001\n",
      "Epoch: 1 / 3, Step: 596 / 2250 Loss: 0.2230\n",
      "Epoch: 1 / 3, Step: 597 / 2250 Loss: 0.3503\n",
      "Epoch: 1 / 3, Step: 598 / 2250 Loss: 0.2975\n",
      "Epoch: 1 / 3, Step: 599 / 2250 Loss: 0.3172\n",
      "Epoch: 1 / 3, Step: 600 / 2250 Loss: 0.8129\n",
      "Epoch: 1 / 3, Step: 601 / 2250 Loss: 0.3189\n",
      "Epoch: 1 / 3, Step: 602 / 2250 Loss: 0.3565\n",
      "Epoch: 1 / 3, Step: 603 / 2250 Loss: 0.3767\n",
      "Epoch: 1 / 3, Step: 604 / 2250 Loss: 0.5098\n",
      "Epoch: 1 / 3, Step: 605 / 2250 Loss: 0.3004\n",
      "Epoch: 1 / 3, Step: 606 / 2250 Loss: 0.3914\n",
      "Epoch: 1 / 3, Step: 607 / 2250 Loss: 0.4651\n",
      "Epoch: 1 / 3, Step: 608 / 2250 Loss: 0.1901\n",
      "Epoch: 1 / 3, Step: 609 / 2250 Loss: 0.1968\n",
      "Epoch: 1 / 3, Step: 610 / 2250 Loss: 0.3201\n",
      "Epoch: 1 / 3, Step: 611 / 2250 Loss: 0.3265\n",
      "Epoch: 1 / 3, Step: 612 / 2250 Loss: 0.3463\n",
      "Epoch: 1 / 3, Step: 613 / 2250 Loss: 0.3781\n",
      "Epoch: 1 / 3, Step: 614 / 2250 Loss: 0.3194\n",
      "Epoch: 1 / 3, Step: 615 / 2250 Loss: 0.5179\n",
      "Epoch: 1 / 3, Step: 616 / 2250 Loss: 0.2835\n",
      "Epoch: 1 / 3, Step: 617 / 2250 Loss: 0.2714\n",
      "Epoch: 1 / 3, Step: 618 / 2250 Loss: 0.2727\n",
      "Epoch: 1 / 3, Step: 619 / 2250 Loss: 0.3613\n",
      "Epoch: 1 / 3, Step: 620 / 2250 Loss: 0.2643\n",
      "Epoch: 1 / 3, Step: 621 / 2250 Loss: 0.3499\n",
      "Epoch: 1 / 3, Step: 622 / 2250 Loss: 0.3765\n",
      "Epoch: 1 / 3, Step: 623 / 2250 Loss: 0.2713\n",
      "Epoch: 1 / 3, Step: 624 / 2250 Loss: 0.2930\n",
      "Epoch: 1 / 3, Step: 625 / 2250 Loss: 0.2295\n",
      "Epoch: 1 / 3, Step: 626 / 2250 Loss: 0.6598\n",
      "Epoch: 1 / 3, Step: 627 / 2250 Loss: 0.3849\n",
      "Epoch: 1 / 3, Step: 628 / 2250 Loss: 0.4226\n",
      "Epoch: 1 / 3, Step: 629 / 2250 Loss: 0.4964\n",
      "Epoch: 1 / 3, Step: 630 / 2250 Loss: 0.5453\n",
      "Epoch: 1 / 3, Step: 631 / 2250 Loss: 0.1497\n",
      "Epoch: 1 / 3, Step: 632 / 2250 Loss: 0.4317\n",
      "Epoch: 1 / 3, Step: 633 / 2250 Loss: 0.1008\n",
      "Epoch: 1 / 3, Step: 634 / 2250 Loss: 0.5014\n",
      "Epoch: 1 / 3, Step: 635 / 2250 Loss: 0.5314\n",
      "Epoch: 1 / 3, Step: 636 / 2250 Loss: 0.5920\n",
      "Epoch: 1 / 3, Step: 637 / 2250 Loss: 0.4399\n",
      "Epoch: 1 / 3, Step: 638 / 2250 Loss: 0.4083\n",
      "Epoch: 1 / 3, Step: 639 / 2250 Loss: 0.2735\n",
      "Epoch: 1 / 3, Step: 640 / 2250 Loss: 0.2300\n",
      "Epoch: 1 / 3, Step: 641 / 2250 Loss: 0.2558\n",
      "Epoch: 1 / 3, Step: 642 / 2250 Loss: 0.3756\n",
      "Epoch: 1 / 3, Step: 643 / 2250 Loss: 0.3578\n",
      "Epoch: 1 / 3, Step: 644 / 2250 Loss: 0.3861\n",
      "Epoch: 1 / 3, Step: 645 / 2250 Loss: 0.4316\n",
      "Epoch: 1 / 3, Step: 646 / 2250 Loss: 0.3939\n",
      "Epoch: 1 / 3, Step: 647 / 2250 Loss: 0.4526\n",
      "Epoch: 1 / 3, Step: 648 / 2250 Loss: 0.5689\n",
      "Epoch: 1 / 3, Step: 649 / 2250 Loss: 0.1930\n",
      "Epoch: 1 / 3, Step: 650 / 2250 Loss: 0.3606\n",
      "Epoch: 1 / 3, Step: 651 / 2250 Loss: 0.3903\n",
      "Epoch: 1 / 3, Step: 652 / 2250 Loss: 0.2646\n",
      "Epoch: 1 / 3, Step: 653 / 2250 Loss: 0.2931\n",
      "Epoch: 1 / 3, Step: 654 / 2250 Loss: 0.4709\n",
      "Epoch: 1 / 3, Step: 655 / 2250 Loss: 0.3879\n",
      "Epoch: 1 / 3, Step: 656 / 2250 Loss: 0.3757\n",
      "Epoch: 1 / 3, Step: 657 / 2250 Loss: 0.4987\n",
      "Epoch: 1 / 3, Step: 658 / 2250 Loss: 0.5770\n",
      "Epoch: 1 / 3, Step: 659 / 2250 Loss: 0.2538\n",
      "Epoch: 1 / 3, Step: 660 / 2250 Loss: 0.2952\n",
      "Epoch: 1 / 3, Step: 661 / 2250 Loss: 0.3662\n",
      "Epoch: 1 / 3, Step: 662 / 2250 Loss: 0.3782\n",
      "Epoch: 1 / 3, Step: 663 / 2250 Loss: 0.3327\n",
      "Epoch: 1 / 3, Step: 664 / 2250 Loss: 0.5026\n",
      "Epoch: 1 / 3, Step: 665 / 2250 Loss: 0.3107\n",
      "Epoch: 1 / 3, Step: 666 / 2250 Loss: 0.3598\n",
      "Epoch: 1 / 3, Step: 667 / 2250 Loss: 0.2711\n",
      "Epoch: 1 / 3, Step: 668 / 2250 Loss: 0.1554\n",
      "Epoch: 1 / 3, Step: 669 / 2250 Loss: 0.2589\n",
      "Epoch: 1 / 3, Step: 670 / 2250 Loss: 0.2334\n",
      "Epoch: 1 / 3, Step: 671 / 2250 Loss: 0.6187\n",
      "Epoch: 1 / 3, Step: 672 / 2250 Loss: 0.5286\n",
      "Epoch: 1 / 3, Step: 673 / 2250 Loss: 0.3760\n",
      "Epoch: 1 / 3, Step: 674 / 2250 Loss: 0.4935\n",
      "Epoch: 1 / 3, Step: 675 / 2250 Loss: 0.5061\n",
      "Epoch: 1 / 3, Step: 676 / 2250 Loss: 0.2444\n",
      "Epoch: 1 / 3, Step: 677 / 2250 Loss: 0.3577\n",
      "Epoch: 1 / 3, Step: 678 / 2250 Loss: 0.4716\n",
      "Epoch: 1 / 3, Step: 679 / 2250 Loss: 0.2858\n",
      "Epoch: 1 / 3, Step: 680 / 2250 Loss: 0.2513\n",
      "Epoch: 1 / 3, Step: 681 / 2250 Loss: 0.5000\n",
      "Epoch: 1 / 3, Step: 682 / 2250 Loss: 0.2264\n",
      "Epoch: 1 / 3, Step: 683 / 2250 Loss: 0.2983\n",
      "Epoch: 1 / 3, Step: 684 / 2250 Loss: 0.2804\n",
      "Epoch: 1 / 3, Step: 685 / 2250 Loss: 0.2021\n",
      "Epoch: 1 / 3, Step: 686 / 2250 Loss: 0.4306\n",
      "Epoch: 1 / 3, Step: 687 / 2250 Loss: 0.1913\n",
      "Epoch: 1 / 3, Step: 688 / 2250 Loss: 0.1339\n",
      "Epoch: 1 / 3, Step: 689 / 2250 Loss: 0.1850\n",
      "Epoch: 1 / 3, Step: 690 / 2250 Loss: 0.1987\n",
      "Epoch: 1 / 3, Step: 691 / 2250 Loss: 0.2907\n",
      "Epoch: 1 / 3, Step: 692 / 2250 Loss: 0.4700\n",
      "Epoch: 1 / 3, Step: 693 / 2250 Loss: 0.2471\n",
      "Epoch: 1 / 3, Step: 694 / 2250 Loss: 0.3590\n",
      "Epoch: 1 / 3, Step: 695 / 2250 Loss: 0.2148\n",
      "Epoch: 1 / 3, Step: 696 / 2250 Loss: 0.2544\n",
      "Epoch: 1 / 3, Step: 697 / 2250 Loss: 0.3672\n",
      "Epoch: 1 / 3, Step: 698 / 2250 Loss: 0.4668\n",
      "Epoch: 1 / 3, Step: 699 / 2250 Loss: 0.3024\n",
      "Epoch: 1 / 3, Step: 700 / 2250 Loss: 0.3067\n",
      "Epoch: 1 / 3, Step: 701 / 2250 Loss: 0.6689\n",
      "Epoch: 1 / 3, Step: 702 / 2250 Loss: 0.5399\n",
      "Epoch: 1 / 3, Step: 703 / 2250 Loss: 0.3113\n",
      "Epoch: 1 / 3, Step: 704 / 2250 Loss: 0.4649\n",
      "Epoch: 1 / 3, Step: 705 / 2250 Loss: 0.1688\n",
      "Epoch: 1 / 3, Step: 706 / 2250 Loss: 0.5397\n",
      "Epoch: 1 / 3, Step: 707 / 2250 Loss: 0.6282\n",
      "Epoch: 1 / 3, Step: 708 / 2250 Loss: 0.2142\n",
      "Epoch: 1 / 3, Step: 709 / 2250 Loss: 0.1901\n",
      "Epoch: 1 / 3, Step: 710 / 2250 Loss: 0.1944\n",
      "Epoch: 1 / 3, Step: 711 / 2250 Loss: 0.2243\n",
      "Epoch: 1 / 3, Step: 712 / 2250 Loss: 0.4094\n",
      "Epoch: 1 / 3, Step: 713 / 2250 Loss: 0.4701\n",
      "Epoch: 1 / 3, Step: 714 / 2250 Loss: 0.2026\n",
      "Epoch: 1 / 3, Step: 715 / 2250 Loss: 0.2174\n",
      "Epoch: 1 / 3, Step: 716 / 2250 Loss: 0.5364\n",
      "Epoch: 1 / 3, Step: 717 / 2250 Loss: 0.5842\n",
      "Epoch: 1 / 3, Step: 718 / 2250 Loss: 0.1432\n",
      "Epoch: 1 / 3, Step: 719 / 2250 Loss: 0.2557\n",
      "Epoch: 1 / 3, Step: 720 / 2250 Loss: 0.2423\n",
      "Epoch: 1 / 3, Step: 721 / 2250 Loss: 0.4402\n",
      "Epoch: 1 / 3, Step: 722 / 2250 Loss: 0.1155\n",
      "Epoch: 1 / 3, Step: 723 / 2250 Loss: 0.1499\n",
      "Epoch: 1 / 3, Step: 724 / 2250 Loss: 0.3640\n",
      "Epoch: 1 / 3, Step: 725 / 2250 Loss: 0.2026\n",
      "Epoch: 1 / 3, Step: 726 / 2250 Loss: 0.1975\n",
      "Epoch: 1 / 3, Step: 727 / 2250 Loss: 0.5060\n",
      "Epoch: 1 / 3, Step: 728 / 2250 Loss: 0.7251\n",
      "Epoch: 1 / 3, Step: 729 / 2250 Loss: 0.3085\n",
      "Epoch: 1 / 3, Step: 730 / 2250 Loss: 0.5126\n",
      "Epoch: 1 / 3, Step: 731 / 2250 Loss: 0.4807\n",
      "Epoch: 1 / 3, Step: 732 / 2250 Loss: 0.4273\n",
      "Epoch: 1 / 3, Step: 733 / 2250 Loss: 0.1268\n",
      "Epoch: 1 / 3, Step: 734 / 2250 Loss: 0.5810\n",
      "Epoch: 1 / 3, Step: 735 / 2250 Loss: 0.2662\n",
      "Epoch: 1 / 3, Step: 736 / 2250 Loss: 0.5439\n",
      "Epoch: 1 / 3, Step: 737 / 2250 Loss: 0.2681\n",
      "Epoch: 1 / 3, Step: 738 / 2250 Loss: 0.3612\n",
      "Epoch: 1 / 3, Step: 739 / 2250 Loss: 0.2489\n",
      "Epoch: 1 / 3, Step: 740 / 2250 Loss: 0.2011\n",
      "Epoch: 1 / 3, Step: 741 / 2250 Loss: 0.3382\n",
      "Epoch: 1 / 3, Step: 742 / 2250 Loss: 0.5773\n",
      "Epoch: 1 / 3, Step: 743 / 2250 Loss: 0.4087\n",
      "Epoch: 1 / 3, Step: 744 / 2250 Loss: 0.3924\n",
      "Epoch: 1 / 3, Step: 745 / 2250 Loss: 0.2365\n",
      "Epoch: 1 / 3, Step: 746 / 2250 Loss: 0.3640\n",
      "Epoch: 1 / 3, Step: 747 / 2250 Loss: 0.2310\n",
      "Epoch: 1 / 3, Step: 748 / 2250 Loss: 0.3169\n",
      "Epoch: 1 / 3, Step: 749 / 2250 Loss: 0.4845\n",
      "Epoch: 1 / 3, Step: 750 / 2250 Loss: 0.4102\n",
      "Epoch: 1 / 3, Step: 751 / 2250 Loss: 0.1872\n",
      "Epoch: 1 / 3, Step: 752 / 2250 Loss: 0.5746\n",
      "Epoch: 1 / 3, Step: 753 / 2250 Loss: 0.5932\n",
      "Epoch: 1 / 3, Step: 754 / 2250 Loss: 0.1424\n",
      "Epoch: 1 / 3, Step: 755 / 2250 Loss: 0.2124\n",
      "Epoch: 1 / 3, Step: 756 / 2250 Loss: 0.4528\n",
      "Epoch: 1 / 3, Step: 757 / 2250 Loss: 0.2038\n",
      "Epoch: 1 / 3, Step: 758 / 2250 Loss: 0.2354\n",
      "Epoch: 1 / 3, Step: 759 / 2250 Loss: 0.1703\n",
      "Epoch: 1 / 3, Step: 760 / 2250 Loss: 0.3983\n",
      "Epoch: 1 / 3, Step: 761 / 2250 Loss: 0.2117\n",
      "Epoch: 1 / 3, Step: 762 / 2250 Loss: 0.1784\n",
      "Epoch: 1 / 3, Step: 763 / 2250 Loss: 0.3206\n",
      "Epoch: 1 / 3, Step: 764 / 2250 Loss: 0.1928\n",
      "Epoch: 1 / 3, Step: 765 / 2250 Loss: 0.2566\n",
      "Epoch: 1 / 3, Step: 766 / 2250 Loss: 0.3725\n",
      "Epoch: 1 / 3, Step: 767 / 2250 Loss: 0.3867\n",
      "Epoch: 1 / 3, Step: 768 / 2250 Loss: 0.2309\n",
      "Epoch: 1 / 3, Step: 769 / 2250 Loss: 0.4605\n",
      "Epoch: 1 / 3, Step: 770 / 2250 Loss: 0.2077\n",
      "Epoch: 1 / 3, Step: 771 / 2250 Loss: 0.6363\n",
      "Epoch: 1 / 3, Step: 772 / 2250 Loss: 0.4809\n",
      "Epoch: 1 / 3, Step: 773 / 2250 Loss: 0.2907\n",
      "Epoch: 1 / 3, Step: 774 / 2250 Loss: 0.6333\n",
      "Epoch: 1 / 3, Step: 775 / 2250 Loss: 0.2451\n",
      "Epoch: 1 / 3, Step: 776 / 2250 Loss: 0.3900\n",
      "Epoch: 1 / 3, Step: 777 / 2250 Loss: 0.2201\n",
      "Epoch: 1 / 3, Step: 778 / 2250 Loss: 0.5631\n",
      "Epoch: 1 / 3, Step: 779 / 2250 Loss: 0.5914\n",
      "Epoch: 1 / 3, Step: 780 / 2250 Loss: 0.1907\n",
      "Epoch: 1 / 3, Step: 781 / 2250 Loss: 0.3149\n",
      "Epoch: 1 / 3, Step: 782 / 2250 Loss: 0.2866\n",
      "Epoch: 1 / 3, Step: 783 / 2250 Loss: 0.5146\n",
      "Epoch: 1 / 3, Step: 784 / 2250 Loss: 0.2534\n",
      "Epoch: 1 / 3, Step: 785 / 2250 Loss: 0.3984\n",
      "Epoch: 1 / 3, Step: 786 / 2250 Loss: 0.7035\n",
      "Epoch: 1 / 3, Step: 787 / 2250 Loss: 0.3308\n",
      "Epoch: 1 / 3, Step: 788 / 2250 Loss: 0.2025\n",
      "Epoch: 1 / 3, Step: 789 / 2250 Loss: 0.1793\n",
      "Epoch: 1 / 3, Step: 790 / 2250 Loss: 0.2829\n",
      "Epoch: 1 / 3, Step: 791 / 2250 Loss: 0.2504\n",
      "Epoch: 1 / 3, Step: 792 / 2250 Loss: 0.5479\n",
      "Epoch: 1 / 3, Step: 793 / 2250 Loss: 0.4631\n",
      "Epoch: 1 / 3, Step: 794 / 2250 Loss: 0.2347\n",
      "Epoch: 1 / 3, Step: 795 / 2250 Loss: 0.3081\n",
      "Epoch: 1 / 3, Step: 796 / 2250 Loss: 0.2265\n",
      "Epoch: 1 / 3, Step: 797 / 2250 Loss: 0.2025\n",
      "Epoch: 1 / 3, Step: 798 / 2250 Loss: 0.2428\n",
      "Epoch: 1 / 3, Step: 799 / 2250 Loss: 0.4243\n",
      "Epoch: 1 / 3, Step: 800 / 2250 Loss: 0.3993\n",
      "Epoch: 1 / 3, Step: 801 / 2250 Loss: 0.5726\n",
      "Epoch: 1 / 3, Step: 802 / 2250 Loss: 0.4345\n",
      "Epoch: 1 / 3, Step: 803 / 2250 Loss: 0.4090\n",
      "Epoch: 1 / 3, Step: 804 / 2250 Loss: 0.2202\n",
      "Epoch: 1 / 3, Step: 805 / 2250 Loss: 0.2390\n",
      "Epoch: 1 / 3, Step: 806 / 2250 Loss: 0.3547\n",
      "Epoch: 1 / 3, Step: 807 / 2250 Loss: 0.2427\n",
      "Epoch: 1 / 3, Step: 808 / 2250 Loss: 0.2014\n",
      "Epoch: 1 / 3, Step: 809 / 2250 Loss: 0.3695\n",
      "Epoch: 1 / 3, Step: 810 / 2250 Loss: 0.4748\n",
      "Epoch: 1 / 3, Step: 811 / 2250 Loss: 0.3718\n",
      "Epoch: 1 / 3, Step: 812 / 2250 Loss: 0.1469\n",
      "Epoch: 1 / 3, Step: 813 / 2250 Loss: 0.4348\n",
      "Epoch: 1 / 3, Step: 814 / 2250 Loss: 0.4672\n",
      "Epoch: 1 / 3, Step: 815 / 2250 Loss: 0.3300\n",
      "Epoch: 1 / 3, Step: 816 / 2250 Loss: 0.4270\n",
      "Epoch: 1 / 3, Step: 817 / 2250 Loss: 0.3376\n",
      "Epoch: 1 / 3, Step: 818 / 2250 Loss: 0.3876\n",
      "Epoch: 1 / 3, Step: 819 / 2250 Loss: 0.1604\n",
      "Epoch: 1 / 3, Step: 820 / 2250 Loss: 0.3242\n",
      "Epoch: 1 / 3, Step: 821 / 2250 Loss: 0.4734\n",
      "Epoch: 1 / 3, Step: 822 / 2250 Loss: 0.2288\n",
      "Epoch: 1 / 3, Step: 823 / 2250 Loss: 0.6671\n",
      "Epoch: 1 / 3, Step: 824 / 2250 Loss: 0.2539\n",
      "Epoch: 1 / 3, Step: 825 / 2250 Loss: 0.1764\n",
      "Epoch: 1 / 3, Step: 826 / 2250 Loss: 0.3086\n",
      "Epoch: 1 / 3, Step: 827 / 2250 Loss: 0.1598\n",
      "Epoch: 1 / 3, Step: 828 / 2250 Loss: 0.5129\n",
      "Epoch: 1 / 3, Step: 829 / 2250 Loss: 0.4710\n",
      "Epoch: 1 / 3, Step: 830 / 2250 Loss: 0.4442\n",
      "Epoch: 1 / 3, Step: 831 / 2250 Loss: 0.3264\n",
      "Epoch: 1 / 3, Step: 832 / 2250 Loss: 0.3678\n",
      "Epoch: 1 / 3, Step: 833 / 2250 Loss: 0.3228\n",
      "Epoch: 1 / 3, Step: 834 / 2250 Loss: 0.3398\n",
      "Epoch: 1 / 3, Step: 835 / 2250 Loss: 0.2375\n",
      "Epoch: 1 / 3, Step: 836 / 2250 Loss: 0.1728\n",
      "Epoch: 1 / 3, Step: 837 / 2250 Loss: 0.2692\n",
      "Epoch: 1 / 3, Step: 838 / 2250 Loss: 0.3524\n",
      "Epoch: 1 / 3, Step: 839 / 2250 Loss: 0.1354\n",
      "Epoch: 1 / 3, Step: 840 / 2250 Loss: 0.3581\n",
      "Epoch: 1 / 3, Step: 841 / 2250 Loss: 0.3232\n",
      "Epoch: 1 / 3, Step: 842 / 2250 Loss: 0.2291\n",
      "Epoch: 1 / 3, Step: 843 / 2250 Loss: 0.4344\n",
      "Epoch: 1 / 3, Step: 844 / 2250 Loss: 0.3333\n",
      "Epoch: 1 / 3, Step: 845 / 2250 Loss: 0.1660\n",
      "Epoch: 1 / 3, Step: 846 / 2250 Loss: 0.4028\n",
      "Epoch: 1 / 3, Step: 847 / 2250 Loss: 0.3052\n",
      "Epoch: 1 / 3, Step: 848 / 2250 Loss: 0.3215\n",
      "Epoch: 1 / 3, Step: 849 / 2250 Loss: 0.4768\n",
      "Epoch: 1 / 3, Step: 850 / 2250 Loss: 0.4833\n",
      "Epoch: 1 / 3, Step: 851 / 2250 Loss: 0.1185\n",
      "Epoch: 1 / 3, Step: 852 / 2250 Loss: 0.1736\n",
      "Epoch: 1 / 3, Step: 853 / 2250 Loss: 0.1626\n",
      "Epoch: 1 / 3, Step: 854 / 2250 Loss: 0.2136\n",
      "Epoch: 1 / 3, Step: 855 / 2250 Loss: 0.2515\n",
      "Epoch: 1 / 3, Step: 856 / 2250 Loss: 0.3157\n",
      "Epoch: 1 / 3, Step: 857 / 2250 Loss: 0.6217\n",
      "Epoch: 1 / 3, Step: 858 / 2250 Loss: 0.0933\n",
      "Epoch: 1 / 3, Step: 859 / 2250 Loss: 0.6879\n",
      "Epoch: 1 / 3, Step: 860 / 2250 Loss: 0.1937\n",
      "Epoch: 1 / 3, Step: 861 / 2250 Loss: 0.2582\n",
      "Epoch: 1 / 3, Step: 862 / 2250 Loss: 0.2471\n",
      "Epoch: 1 / 3, Step: 863 / 2250 Loss: 0.3154\n",
      "Epoch: 1 / 3, Step: 864 / 2250 Loss: 0.3227\n",
      "Epoch: 1 / 3, Step: 865 / 2250 Loss: 0.2453\n",
      "Epoch: 1 / 3, Step: 866 / 2250 Loss: 0.3950\n",
      "Epoch: 1 / 3, Step: 867 / 2250 Loss: 0.2564\n",
      "Epoch: 1 / 3, Step: 868 / 2250 Loss: 0.4094\n",
      "Epoch: 1 / 3, Step: 869 / 2250 Loss: 0.5378\n",
      "Epoch: 1 / 3, Step: 870 / 2250 Loss: 0.5163\n",
      "Epoch: 1 / 3, Step: 871 / 2250 Loss: 0.3649\n",
      "Epoch: 1 / 3, Step: 872 / 2250 Loss: 0.3283\n",
      "Epoch: 1 / 3, Step: 873 / 2250 Loss: 0.2472\n",
      "Epoch: 1 / 3, Step: 874 / 2250 Loss: 0.2422\n",
      "Epoch: 1 / 3, Step: 875 / 2250 Loss: 0.4856\n",
      "Epoch: 1 / 3, Step: 876 / 2250 Loss: 0.3710\n",
      "Epoch: 1 / 3, Step: 877 / 2250 Loss: 0.4841\n",
      "Epoch: 1 / 3, Step: 878 / 2250 Loss: 0.2896\n",
      "Epoch: 1 / 3, Step: 879 / 2250 Loss: 0.3025\n",
      "Epoch: 1 / 3, Step: 880 / 2250 Loss: 0.1488\n",
      "Epoch: 1 / 3, Step: 881 / 2250 Loss: 0.5414\n",
      "Epoch: 1 / 3, Step: 882 / 2250 Loss: 0.3320\n",
      "Epoch: 1 / 3, Step: 883 / 2250 Loss: 0.5134\n",
      "Epoch: 1 / 3, Step: 884 / 2250 Loss: 0.2640\n",
      "Epoch: 1 / 3, Step: 885 / 2250 Loss: 0.5066\n",
      "Epoch: 1 / 3, Step: 886 / 2250 Loss: 0.1695\n",
      "Epoch: 1 / 3, Step: 887 / 2250 Loss: 0.4663\n",
      "Epoch: 1 / 3, Step: 888 / 2250 Loss: 0.3520\n",
      "Epoch: 1 / 3, Step: 889 / 2250 Loss: 0.4196\n",
      "Epoch: 1 / 3, Step: 890 / 2250 Loss: 0.4562\n",
      "Epoch: 1 / 3, Step: 891 / 2250 Loss: 0.5124\n",
      "Epoch: 1 / 3, Step: 892 / 2250 Loss: 0.3610\n",
      "Epoch: 1 / 3, Step: 893 / 2250 Loss: 0.3341\n",
      "Epoch: 1 / 3, Step: 894 / 2250 Loss: 0.1628\n",
      "Epoch: 1 / 3, Step: 895 / 2250 Loss: 0.5739\n",
      "Epoch: 1 / 3, Step: 896 / 2250 Loss: 0.6538\n",
      "Epoch: 1 / 3, Step: 897 / 2250 Loss: 0.4323\n",
      "Epoch: 1 / 3, Step: 898 / 2250 Loss: 0.3625\n",
      "Epoch: 1 / 3, Step: 899 / 2250 Loss: 0.2538\n",
      "Epoch: 1 / 3, Step: 900 / 2250 Loss: 0.4916\n",
      "Epoch: 1 / 3, Step: 901 / 2250 Loss: 0.2647\n",
      "Epoch: 1 / 3, Step: 902 / 2250 Loss: 0.3054\n",
      "Epoch: 1 / 3, Step: 903 / 2250 Loss: 0.2417\n",
      "Epoch: 1 / 3, Step: 904 / 2250 Loss: 0.3574\n",
      "Epoch: 1 / 3, Step: 905 / 2250 Loss: 0.2043\n",
      "Epoch: 1 / 3, Step: 906 / 2250 Loss: 0.3811\n",
      "Epoch: 1 / 3, Step: 907 / 2250 Loss: 0.5427\n",
      "Epoch: 1 / 3, Step: 908 / 2250 Loss: 0.2635\n",
      "Epoch: 1 / 3, Step: 909 / 2250 Loss: 0.2950\n",
      "Epoch: 1 / 3, Step: 910 / 2250 Loss: 0.3047\n",
      "Epoch: 1 / 3, Step: 911 / 2250 Loss: 0.2237\n",
      "Epoch: 1 / 3, Step: 912 / 2250 Loss: 0.4772\n",
      "Epoch: 1 / 3, Step: 913 / 2250 Loss: 0.3034\n",
      "Epoch: 1 / 3, Step: 914 / 2250 Loss: 0.1056\n",
      "Epoch: 1 / 3, Step: 915 / 2250 Loss: 0.4993\n",
      "Epoch: 1 / 3, Step: 916 / 2250 Loss: 0.2192\n",
      "Epoch: 1 / 3, Step: 917 / 2250 Loss: 0.3534\n",
      "Epoch: 1 / 3, Step: 918 / 2250 Loss: 0.1535\n",
      "Epoch: 1 / 3, Step: 919 / 2250 Loss: 0.3823\n",
      "Epoch: 1 / 3, Step: 920 / 2250 Loss: 0.1347\n",
      "Epoch: 1 / 3, Step: 921 / 2250 Loss: 0.4939\n",
      "Epoch: 1 / 3, Step: 922 / 2250 Loss: 0.3950\n",
      "Epoch: 1 / 3, Step: 923 / 2250 Loss: 0.1165\n",
      "Epoch: 1 / 3, Step: 924 / 2250 Loss: 0.2291\n",
      "Epoch: 1 / 3, Step: 925 / 2250 Loss: 0.1255\n",
      "Epoch: 1 / 3, Step: 926 / 2250 Loss: 0.2012\n",
      "Epoch: 1 / 3, Step: 927 / 2250 Loss: 0.0916\n",
      "Epoch: 1 / 3, Step: 928 / 2250 Loss: 0.2814\n",
      "Epoch: 1 / 3, Step: 929 / 2250 Loss: 0.2846\n",
      "Epoch: 1 / 3, Step: 930 / 2250 Loss: 0.4727\n",
      "Epoch: 1 / 3, Step: 931 / 2250 Loss: 0.1575\n",
      "Epoch: 1 / 3, Step: 932 / 2250 Loss: 0.2320\n",
      "Epoch: 1 / 3, Step: 933 / 2250 Loss: 0.3481\n",
      "Epoch: 1 / 3, Step: 934 / 2250 Loss: 0.4869\n",
      "Epoch: 1 / 3, Step: 935 / 2250 Loss: 0.3095\n",
      "Epoch: 1 / 3, Step: 936 / 2250 Loss: 0.2540\n",
      "Epoch: 1 / 3, Step: 937 / 2250 Loss: 0.2731\n",
      "Epoch: 1 / 3, Step: 938 / 2250 Loss: 0.1885\n",
      "Epoch: 1 / 3, Step: 939 / 2250 Loss: 0.1688\n",
      "Epoch: 1 / 3, Step: 940 / 2250 Loss: 0.3594\n",
      "Epoch: 1 / 3, Step: 941 / 2250 Loss: 0.5967\n",
      "Epoch: 1 / 3, Step: 942 / 2250 Loss: 0.2442\n",
      "Epoch: 1 / 3, Step: 943 / 2250 Loss: 0.3867\n",
      "Epoch: 1 / 3, Step: 944 / 2250 Loss: 0.3403\n",
      "Epoch: 1 / 3, Step: 945 / 2250 Loss: 0.4753\n",
      "Epoch: 1 / 3, Step: 946 / 2250 Loss: 0.2336\n",
      "Epoch: 1 / 3, Step: 947 / 2250 Loss: 0.0782\n",
      "Epoch: 1 / 3, Step: 948 / 2250 Loss: 0.5939\n",
      "Epoch: 1 / 3, Step: 949 / 2250 Loss: 0.2762\n",
      "Epoch: 1 / 3, Step: 950 / 2250 Loss: 0.2799\n",
      "Epoch: 1 / 3, Step: 951 / 2250 Loss: 0.4371\n",
      "Epoch: 1 / 3, Step: 952 / 2250 Loss: 0.3412\n",
      "Epoch: 1 / 3, Step: 953 / 2250 Loss: 0.2245\n",
      "Epoch: 1 / 3, Step: 954 / 2250 Loss: 0.1682\n",
      "Epoch: 1 / 3, Step: 955 / 2250 Loss: 0.3490\n",
      "Epoch: 1 / 3, Step: 956 / 2250 Loss: 0.2741\n",
      "Epoch: 1 / 3, Step: 957 / 2250 Loss: 0.2293\n",
      "Epoch: 1 / 3, Step: 958 / 2250 Loss: 0.3201\n",
      "Epoch: 1 / 3, Step: 959 / 2250 Loss: 0.3453\n",
      "Epoch: 1 / 3, Step: 960 / 2250 Loss: 0.2920\n",
      "Epoch: 1 / 3, Step: 961 / 2250 Loss: 0.3118\n",
      "Epoch: 1 / 3, Step: 962 / 2250 Loss: 0.1218\n",
      "Epoch: 1 / 3, Step: 963 / 2250 Loss: 0.2022\n",
      "Epoch: 1 / 3, Step: 964 / 2250 Loss: 0.3060\n",
      "Epoch: 1 / 3, Step: 965 / 2250 Loss: 0.2866\n",
      "Epoch: 1 / 3, Step: 966 / 2250 Loss: 0.5150\n",
      "Epoch: 1 / 3, Step: 967 / 2250 Loss: 0.4853\n",
      "Epoch: 1 / 3, Step: 968 / 2250 Loss: 0.3544\n",
      "Epoch: 1 / 3, Step: 969 / 2250 Loss: 0.2120\n",
      "Epoch: 1 / 3, Step: 970 / 2250 Loss: 0.3184\n",
      "Epoch: 1 / 3, Step: 971 / 2250 Loss: 0.2819\n",
      "Epoch: 1 / 3, Step: 972 / 2250 Loss: 0.3658\n",
      "Epoch: 1 / 3, Step: 973 / 2250 Loss: 0.4471\n",
      "Epoch: 1 / 3, Step: 974 / 2250 Loss: 0.2639\n",
      "Epoch: 1 / 3, Step: 975 / 2250 Loss: 0.1590\n",
      "Epoch: 1 / 3, Step: 976 / 2250 Loss: 0.4354\n",
      "Epoch: 1 / 3, Step: 977 / 2250 Loss: 0.2182\n",
      "Epoch: 1 / 3, Step: 978 / 2250 Loss: 0.0976\n",
      "Epoch: 1 / 3, Step: 979 / 2250 Loss: 0.3065\n",
      "Epoch: 1 / 3, Step: 980 / 2250 Loss: 0.2250\n",
      "Epoch: 1 / 3, Step: 981 / 2250 Loss: 0.2749\n",
      "Epoch: 1 / 3, Step: 982 / 2250 Loss: 0.2383\n",
      "Epoch: 1 / 3, Step: 983 / 2250 Loss: 0.3129\n",
      "Epoch: 1 / 3, Step: 984 / 2250 Loss: 0.5475\n",
      "Epoch: 1 / 3, Step: 985 / 2250 Loss: 0.2176\n",
      "Epoch: 1 / 3, Step: 986 / 2250 Loss: 0.6033\n",
      "Epoch: 1 / 3, Step: 987 / 2250 Loss: 0.3778\n",
      "Epoch: 1 / 3, Step: 988 / 2250 Loss: 0.4016\n",
      "Epoch: 1 / 3, Step: 989 / 2250 Loss: 0.1912\n",
      "Epoch: 1 / 3, Step: 990 / 2250 Loss: 0.1971\n",
      "Epoch: 1 / 3, Step: 991 / 2250 Loss: 0.2018\n",
      "Epoch: 1 / 3, Step: 992 / 2250 Loss: 0.2349\n",
      "Epoch: 1 / 3, Step: 993 / 2250 Loss: 0.2968\n",
      "Epoch: 1 / 3, Step: 994 / 2250 Loss: 0.3646\n",
      "Epoch: 1 / 3, Step: 995 / 2250 Loss: 0.1818\n",
      "Epoch: 1 / 3, Step: 996 / 2250 Loss: 0.4358\n",
      "Epoch: 1 / 3, Step: 997 / 2250 Loss: 0.4067\n",
      "Epoch: 1 / 3, Step: 998 / 2250 Loss: 0.1761\n",
      "Epoch: 1 / 3, Step: 999 / 2250 Loss: 0.1797\n",
      "Epoch: 1 / 3, Step: 1000 / 2250 Loss: 0.1480\n",
      "Epoch: 1 / 3, Step: 1001 / 2250 Loss: 0.6992\n",
      "Epoch: 1 / 3, Step: 1002 / 2250 Loss: 0.2848\n",
      "Epoch: 1 / 3, Step: 1003 / 2250 Loss: 0.3709\n",
      "Epoch: 1 / 3, Step: 1004 / 2250 Loss: 0.1778\n",
      "Epoch: 1 / 3, Step: 1005 / 2250 Loss: 0.4649\n",
      "Epoch: 1 / 3, Step: 1006 / 2250 Loss: 0.3718\n",
      "Epoch: 1 / 3, Step: 1007 / 2250 Loss: 0.2320\n",
      "Epoch: 1 / 3, Step: 1008 / 2250 Loss: 0.2594\n",
      "Epoch: 1 / 3, Step: 1009 / 2250 Loss: 0.2737\n",
      "Epoch: 1 / 3, Step: 1010 / 2250 Loss: 0.3086\n",
      "Epoch: 1 / 3, Step: 1011 / 2250 Loss: 0.2936\n",
      "Epoch: 1 / 3, Step: 1012 / 2250 Loss: 0.4559\n",
      "Epoch: 1 / 3, Step: 1013 / 2250 Loss: 0.3527\n",
      "Epoch: 1 / 3, Step: 1014 / 2250 Loss: 0.3403\n",
      "Epoch: 1 / 3, Step: 1015 / 2250 Loss: 0.3455\n",
      "Epoch: 1 / 3, Step: 1016 / 2250 Loss: 0.2399\n",
      "Epoch: 1 / 3, Step: 1017 / 2250 Loss: 0.0762\n",
      "Epoch: 1 / 3, Step: 1018 / 2250 Loss: 0.3638\n",
      "Epoch: 1 / 3, Step: 1019 / 2250 Loss: 0.1709\n",
      "Epoch: 1 / 3, Step: 1020 / 2250 Loss: 0.2245\n",
      "Epoch: 1 / 3, Step: 1021 / 2250 Loss: 0.3124\n",
      "Epoch: 1 / 3, Step: 1022 / 2250 Loss: 0.2160\n",
      "Epoch: 1 / 3, Step: 1023 / 2250 Loss: 0.1668\n",
      "Epoch: 1 / 3, Step: 1024 / 2250 Loss: 0.4151\n",
      "Epoch: 1 / 3, Step: 1025 / 2250 Loss: 0.3425\n",
      "Epoch: 1 / 3, Step: 1026 / 2250 Loss: 0.1059\n",
      "Epoch: 1 / 3, Step: 1027 / 2250 Loss: 0.0869\n",
      "Epoch: 1 / 3, Step: 1028 / 2250 Loss: 0.3817\n",
      "Epoch: 1 / 3, Step: 1029 / 2250 Loss: 0.4408\n",
      "Epoch: 1 / 3, Step: 1030 / 2250 Loss: 0.3764\n",
      "Epoch: 1 / 3, Step: 1031 / 2250 Loss: 0.3865\n",
      "Epoch: 1 / 3, Step: 1032 / 2250 Loss: 0.1428\n",
      "Epoch: 1 / 3, Step: 1033 / 2250 Loss: 0.1308\n",
      "Epoch: 1 / 3, Step: 1034 / 2250 Loss: 0.3663\n",
      "Epoch: 1 / 3, Step: 1035 / 2250 Loss: 0.1911\n",
      "Epoch: 1 / 3, Step: 1036 / 2250 Loss: 0.2532\n",
      "Epoch: 1 / 3, Step: 1037 / 2250 Loss: 0.1459\n",
      "Epoch: 1 / 3, Step: 1038 / 2250 Loss: 0.2271\n",
      "Epoch: 1 / 3, Step: 1039 / 2250 Loss: 0.1375\n",
      "Epoch: 1 / 3, Step: 1040 / 2250 Loss: 0.2521\n",
      "Epoch: 1 / 3, Step: 1041 / 2250 Loss: 0.1322\n",
      "Epoch: 1 / 3, Step: 1042 / 2250 Loss: 0.1613\n",
      "Epoch: 1 / 3, Step: 1043 / 2250 Loss: 0.1581\n",
      "Epoch: 1 / 3, Step: 1044 / 2250 Loss: 0.1566\n",
      "Epoch: 1 / 3, Step: 1045 / 2250 Loss: 0.2820\n",
      "Epoch: 1 / 3, Step: 1046 / 2250 Loss: 0.3429\n",
      "Epoch: 1 / 3, Step: 1047 / 2250 Loss: 0.2010\n",
      "Epoch: 1 / 3, Step: 1048 / 2250 Loss: 0.2230\n",
      "Epoch: 1 / 3, Step: 1049 / 2250 Loss: 0.2002\n",
      "Epoch: 1 / 3, Step: 1050 / 2250 Loss: 0.2026\n",
      "Epoch: 1 / 3, Step: 1051 / 2250 Loss: 0.2373\n",
      "Epoch: 1 / 3, Step: 1052 / 2250 Loss: 0.1944\n",
      "Epoch: 1 / 3, Step: 1053 / 2250 Loss: 0.0667\n",
      "Epoch: 1 / 3, Step: 1054 / 2250 Loss: 0.1799\n",
      "Epoch: 1 / 3, Step: 1055 / 2250 Loss: 0.1803\n",
      "Epoch: 1 / 3, Step: 1056 / 2250 Loss: 0.2059\n",
      "Epoch: 1 / 3, Step: 1057 / 2250 Loss: 0.3195\n",
      "Epoch: 1 / 3, Step: 1058 / 2250 Loss: 0.3303\n",
      "Epoch: 1 / 3, Step: 1059 / 2250 Loss: 0.3541\n",
      "Epoch: 1 / 3, Step: 1060 / 2250 Loss: 0.7328\n",
      "Epoch: 1 / 3, Step: 1061 / 2250 Loss: 0.2809\n",
      "Epoch: 1 / 3, Step: 1062 / 2250 Loss: 0.1319\n",
      "Epoch: 1 / 3, Step: 1063 / 2250 Loss: 0.3518\n",
      "Epoch: 1 / 3, Step: 1064 / 2250 Loss: 0.1680\n",
      "Epoch: 1 / 3, Step: 1065 / 2250 Loss: 0.1935\n",
      "Epoch: 1 / 3, Step: 1066 / 2250 Loss: 0.4070\n",
      "Epoch: 1 / 3, Step: 1067 / 2250 Loss: 0.3014\n",
      "Epoch: 1 / 3, Step: 1068 / 2250 Loss: 0.1921\n",
      "Epoch: 1 / 3, Step: 1069 / 2250 Loss: 0.3680\n",
      "Epoch: 1 / 3, Step: 1070 / 2250 Loss: 0.2842\n",
      "Epoch: 1 / 3, Step: 1071 / 2250 Loss: 0.3383\n",
      "Epoch: 1 / 3, Step: 1072 / 2250 Loss: 0.3476\n",
      "Epoch: 1 / 3, Step: 1073 / 2250 Loss: 0.4171\n",
      "Epoch: 1 / 3, Step: 1074 / 2250 Loss: 0.2516\n",
      "Epoch: 1 / 3, Step: 1075 / 2250 Loss: 0.2459\n",
      "Epoch: 1 / 3, Step: 1076 / 2250 Loss: 0.4313\n",
      "Epoch: 1 / 3, Step: 1077 / 2250 Loss: 0.7043\n",
      "Epoch: 1 / 3, Step: 1078 / 2250 Loss: 0.1494\n",
      "Epoch: 1 / 3, Step: 1079 / 2250 Loss: 0.3765\n",
      "Epoch: 1 / 3, Step: 1080 / 2250 Loss: 0.1961\n",
      "Epoch: 1 / 3, Step: 1081 / 2250 Loss: 0.3392\n",
      "Epoch: 1 / 3, Step: 1082 / 2250 Loss: 0.4033\n",
      "Epoch: 1 / 3, Step: 1083 / 2250 Loss: 0.4184\n",
      "Epoch: 1 / 3, Step: 1084 / 2250 Loss: 0.4271\n",
      "Epoch: 1 / 3, Step: 1085 / 2250 Loss: 0.1414\n",
      "Epoch: 1 / 3, Step: 1086 / 2250 Loss: 0.1610\n",
      "Epoch: 1 / 3, Step: 1087 / 2250 Loss: 0.2586\n",
      "Epoch: 1 / 3, Step: 1088 / 2250 Loss: 0.1039\n",
      "Epoch: 1 / 3, Step: 1089 / 2250 Loss: 0.3341\n",
      "Epoch: 1 / 3, Step: 1090 / 2250 Loss: 0.4203\n",
      "Epoch: 1 / 3, Step: 1091 / 2250 Loss: 0.4778\n",
      "Epoch: 1 / 3, Step: 1092 / 2250 Loss: 0.0753\n",
      "Epoch: 1 / 3, Step: 1093 / 2250 Loss: 0.2540\n",
      "Epoch: 1 / 3, Step: 1094 / 2250 Loss: 0.3703\n",
      "Epoch: 1 / 3, Step: 1095 / 2250 Loss: 0.2738\n",
      "Epoch: 1 / 3, Step: 1096 / 2250 Loss: 0.3924\n",
      "Epoch: 1 / 3, Step: 1097 / 2250 Loss: 0.2595\n",
      "Epoch: 1 / 3, Step: 1098 / 2250 Loss: 0.4022\n",
      "Epoch: 1 / 3, Step: 1099 / 2250 Loss: 0.2496\n",
      "Epoch: 1 / 3, Step: 1100 / 2250 Loss: 0.1860\n",
      "Epoch: 1 / 3, Step: 1101 / 2250 Loss: 0.0908\n",
      "Epoch: 1 / 3, Step: 1102 / 2250 Loss: 0.4746\n",
      "Epoch: 1 / 3, Step: 1103 / 2250 Loss: 0.1772\n",
      "Epoch: 1 / 3, Step: 1104 / 2250 Loss: 0.2266\n",
      "Epoch: 1 / 3, Step: 1105 / 2250 Loss: 0.1666\n",
      "Epoch: 1 / 3, Step: 1106 / 2250 Loss: 0.1641\n",
      "Epoch: 1 / 3, Step: 1107 / 2250 Loss: 0.1901\n",
      "Epoch: 1 / 3, Step: 1108 / 2250 Loss: 0.4755\n",
      "Epoch: 1 / 3, Step: 1109 / 2250 Loss: 0.1864\n",
      "Epoch: 1 / 3, Step: 1110 / 2250 Loss: 0.3005\n",
      "Epoch: 1 / 3, Step: 1111 / 2250 Loss: 0.3375\n",
      "Epoch: 1 / 3, Step: 1112 / 2250 Loss: 0.5049\n",
      "Epoch: 1 / 3, Step: 1113 / 2250 Loss: 0.4026\n",
      "Epoch: 1 / 3, Step: 1114 / 2250 Loss: 0.3135\n",
      "Epoch: 1 / 3, Step: 1115 / 2250 Loss: 0.2285\n",
      "Epoch: 1 / 3, Step: 1116 / 2250 Loss: 0.1799\n",
      "Epoch: 1 / 3, Step: 1117 / 2250 Loss: 0.2559\n",
      "Epoch: 1 / 3, Step: 1118 / 2250 Loss: 0.1319\n",
      "Epoch: 1 / 3, Step: 1119 / 2250 Loss: 0.2261\n",
      "Epoch: 1 / 3, Step: 1120 / 2250 Loss: 0.3461\n",
      "Epoch: 1 / 3, Step: 1121 / 2250 Loss: 0.1300\n",
      "Epoch: 1 / 3, Step: 1122 / 2250 Loss: 0.2904\n",
      "Epoch: 1 / 3, Step: 1123 / 2250 Loss: 0.5094\n",
      "Epoch: 1 / 3, Step: 1124 / 2250 Loss: 0.3109\n",
      "Epoch: 1 / 3, Step: 1125 / 2250 Loss: 0.1151\n",
      "Epoch: 1 / 3, Step: 1126 / 2250 Loss: 0.5653\n",
      "Epoch: 1 / 3, Step: 1127 / 2250 Loss: 0.1883\n",
      "Epoch: 1 / 3, Step: 1128 / 2250 Loss: 0.1261\n",
      "Epoch: 1 / 3, Step: 1129 / 2250 Loss: 0.3072\n",
      "Epoch: 1 / 3, Step: 1130 / 2250 Loss: 0.2006\n",
      "Epoch: 1 / 3, Step: 1131 / 2250 Loss: 0.2335\n",
      "Epoch: 1 / 3, Step: 1132 / 2250 Loss: 0.3391\n",
      "Epoch: 1 / 3, Step: 1133 / 2250 Loss: 0.0940\n",
      "Epoch: 1 / 3, Step: 1134 / 2250 Loss: 0.3944\n",
      "Epoch: 1 / 3, Step: 1135 / 2250 Loss: 0.3997\n",
      "Epoch: 1 / 3, Step: 1136 / 2250 Loss: 0.2058\n",
      "Epoch: 1 / 3, Step: 1137 / 2250 Loss: 0.0845\n",
      "Epoch: 1 / 3, Step: 1138 / 2250 Loss: 0.3304\n",
      "Epoch: 1 / 3, Step: 1139 / 2250 Loss: 0.2022\n",
      "Epoch: 1 / 3, Step: 1140 / 2250 Loss: 0.2363\n",
      "Epoch: 1 / 3, Step: 1141 / 2250 Loss: 0.1794\n",
      "Epoch: 1 / 3, Step: 1142 / 2250 Loss: 0.3522\n",
      "Epoch: 1 / 3, Step: 1143 / 2250 Loss: 0.1739\n",
      "Epoch: 1 / 3, Step: 1144 / 2250 Loss: 0.2088\n",
      "Epoch: 1 / 3, Step: 1145 / 2250 Loss: 0.1912\n",
      "Epoch: 1 / 3, Step: 1146 / 2250 Loss: 0.4042\n",
      "Epoch: 1 / 3, Step: 1147 / 2250 Loss: 0.2691\n",
      "Epoch: 1 / 3, Step: 1148 / 2250 Loss: 0.3411\n",
      "Epoch: 1 / 3, Step: 1149 / 2250 Loss: 0.1542\n",
      "Epoch: 1 / 3, Step: 1150 / 2250 Loss: 0.3088\n",
      "Epoch: 1 / 3, Step: 1151 / 2250 Loss: 0.1722\n",
      "Epoch: 1 / 3, Step: 1152 / 2250 Loss: 0.2587\n",
      "Epoch: 1 / 3, Step: 1153 / 2250 Loss: 0.2057\n",
      "Epoch: 1 / 3, Step: 1154 / 2250 Loss: 0.1963\n",
      "Epoch: 1 / 3, Step: 1155 / 2250 Loss: 0.0844\n",
      "Epoch: 1 / 3, Step: 1156 / 2250 Loss: 0.1917\n",
      "Epoch: 1 / 3, Step: 1157 / 2250 Loss: 0.4147\n",
      "Epoch: 1 / 3, Step: 1158 / 2250 Loss: 0.3635\n",
      "Epoch: 1 / 3, Step: 1159 / 2250 Loss: 0.4186\n",
      "Epoch: 1 / 3, Step: 1160 / 2250 Loss: 0.2470\n",
      "Epoch: 1 / 3, Step: 1161 / 2250 Loss: 0.4024\n",
      "Epoch: 1 / 3, Step: 1162 / 2250 Loss: 0.4667\n",
      "Epoch: 1 / 3, Step: 1163 / 2250 Loss: 0.3203\n",
      "Epoch: 1 / 3, Step: 1164 / 2250 Loss: 0.5750\n",
      "Epoch: 1 / 3, Step: 1165 / 2250 Loss: 0.1544\n",
      "Epoch: 1 / 3, Step: 1166 / 2250 Loss: 0.1028\n",
      "Epoch: 1 / 3, Step: 1167 / 2250 Loss: 0.3583\n",
      "Epoch: 1 / 3, Step: 1168 / 2250 Loss: 0.1602\n",
      "Epoch: 1 / 3, Step: 1169 / 2250 Loss: 0.2695\n",
      "Epoch: 1 / 3, Step: 1170 / 2250 Loss: 0.2485\n",
      "Epoch: 1 / 3, Step: 1171 / 2250 Loss: 0.4535\n",
      "Epoch: 1 / 3, Step: 1172 / 2250 Loss: 0.2493\n",
      "Epoch: 1 / 3, Step: 1173 / 2250 Loss: 0.1973\n",
      "Epoch: 1 / 3, Step: 1174 / 2250 Loss: 0.1984\n",
      "Epoch: 1 / 3, Step: 1175 / 2250 Loss: 0.4162\n",
      "Epoch: 1 / 3, Step: 1176 / 2250 Loss: 0.2080\n",
      "Epoch: 1 / 3, Step: 1177 / 2250 Loss: 0.5812\n",
      "Epoch: 1 / 3, Step: 1178 / 2250 Loss: 0.1404\n",
      "Epoch: 1 / 3, Step: 1179 / 2250 Loss: 0.4150\n",
      "Epoch: 1 / 3, Step: 1180 / 2250 Loss: 0.1778\n",
      "Epoch: 1 / 3, Step: 1181 / 2250 Loss: 0.1767\n",
      "Epoch: 1 / 3, Step: 1182 / 2250 Loss: 0.4179\n",
      "Epoch: 1 / 3, Step: 1183 / 2250 Loss: 0.3677\n",
      "Epoch: 1 / 3, Step: 1184 / 2250 Loss: 0.3380\n",
      "Epoch: 1 / 3, Step: 1185 / 2250 Loss: 0.6152\n",
      "Epoch: 1 / 3, Step: 1186 / 2250 Loss: 0.3885\n",
      "Epoch: 1 / 3, Step: 1187 / 2250 Loss: 0.2137\n",
      "Epoch: 1 / 3, Step: 1188 / 2250 Loss: 0.4121\n",
      "Epoch: 1 / 3, Step: 1189 / 2250 Loss: 0.1874\n",
      "Epoch: 1 / 3, Step: 1190 / 2250 Loss: 0.3933\n",
      "Epoch: 1 / 3, Step: 1191 / 2250 Loss: 0.3797\n",
      "Epoch: 1 / 3, Step: 1192 / 2250 Loss: 0.1471\n",
      "Epoch: 1 / 3, Step: 1193 / 2250 Loss: 0.2662\n",
      "Epoch: 1 / 3, Step: 1194 / 2250 Loss: 0.1412\n",
      "Epoch: 1 / 3, Step: 1195 / 2250 Loss: 0.1962\n",
      "Epoch: 1 / 3, Step: 1196 / 2250 Loss: 0.3822\n",
      "Epoch: 1 / 3, Step: 1197 / 2250 Loss: 0.1659\n",
      "Epoch: 1 / 3, Step: 1198 / 2250 Loss: 0.3194\n",
      "Epoch: 1 / 3, Step: 1199 / 2250 Loss: 0.3460\n",
      "Epoch: 1 / 3, Step: 1200 / 2250 Loss: 0.1003\n",
      "Epoch: 1 / 3, Step: 1201 / 2250 Loss: 0.3572\n",
      "Epoch: 1 / 3, Step: 1202 / 2250 Loss: 0.3643\n",
      "Epoch: 1 / 3, Step: 1203 / 2250 Loss: 0.4453\n",
      "Epoch: 1 / 3, Step: 1204 / 2250 Loss: 0.0752\n",
      "Epoch: 1 / 3, Step: 1205 / 2250 Loss: 0.1661\n",
      "Epoch: 1 / 3, Step: 1206 / 2250 Loss: 0.3917\n",
      "Epoch: 1 / 3, Step: 1207 / 2250 Loss: 0.2673\n",
      "Epoch: 1 / 3, Step: 1208 / 2250 Loss: 0.3824\n",
      "Epoch: 1 / 3, Step: 1209 / 2250 Loss: 0.1471\n",
      "Epoch: 1 / 3, Step: 1210 / 2250 Loss: 0.2686\n",
      "Epoch: 1 / 3, Step: 1211 / 2250 Loss: 0.4858\n",
      "Epoch: 1 / 3, Step: 1212 / 2250 Loss: 0.2503\n",
      "Epoch: 1 / 3, Step: 1213 / 2250 Loss: 0.3065\n",
      "Epoch: 1 / 3, Step: 1214 / 2250 Loss: 0.0747\n",
      "Epoch: 1 / 3, Step: 1215 / 2250 Loss: 0.5531\n",
      "Epoch: 1 / 3, Step: 1216 / 2250 Loss: 0.3491\n",
      "Epoch: 1 / 3, Step: 1217 / 2250 Loss: 0.1205\n",
      "Epoch: 1 / 3, Step: 1218 / 2250 Loss: 0.4251\n",
      "Epoch: 1 / 3, Step: 1219 / 2250 Loss: 0.3173\n",
      "Epoch: 1 / 3, Step: 1220 / 2250 Loss: 0.1660\n",
      "Epoch: 1 / 3, Step: 1221 / 2250 Loss: 0.3078\n",
      "Epoch: 1 / 3, Step: 1222 / 2250 Loss: 0.2303\n",
      "Epoch: 1 / 3, Step: 1223 / 2250 Loss: 0.1908\n",
      "Epoch: 1 / 3, Step: 1224 / 2250 Loss: 0.2767\n",
      "Epoch: 1 / 3, Step: 1225 / 2250 Loss: 0.2822\n",
      "Epoch: 1 / 3, Step: 1226 / 2250 Loss: 0.3199\n",
      "Epoch: 1 / 3, Step: 1227 / 2250 Loss: 0.1852\n",
      "Epoch: 1 / 3, Step: 1228 / 2250 Loss: 0.2207\n",
      "Epoch: 1 / 3, Step: 1229 / 2250 Loss: 0.5580\n",
      "Epoch: 1 / 3, Step: 1230 / 2250 Loss: 0.1734\n",
      "Epoch: 1 / 3, Step: 1231 / 2250 Loss: 0.2983\n",
      "Epoch: 1 / 3, Step: 1232 / 2250 Loss: 0.2080\n",
      "Epoch: 1 / 3, Step: 1233 / 2250 Loss: 0.3683\n",
      "Epoch: 1 / 3, Step: 1234 / 2250 Loss: 0.2087\n",
      "Epoch: 1 / 3, Step: 1235 / 2250 Loss: 0.1275\n",
      "Epoch: 1 / 3, Step: 1236 / 2250 Loss: 0.3488\n",
      "Epoch: 1 / 3, Step: 1237 / 2250 Loss: 0.3079\n",
      "Epoch: 1 / 3, Step: 1238 / 2250 Loss: 0.1230\n",
      "Epoch: 1 / 3, Step: 1239 / 2250 Loss: 0.4074\n",
      "Epoch: 1 / 3, Step: 1240 / 2250 Loss: 0.2459\n",
      "Epoch: 1 / 3, Step: 1241 / 2250 Loss: 0.3114\n",
      "Epoch: 1 / 3, Step: 1242 / 2250 Loss: 0.1743\n",
      "Epoch: 1 / 3, Step: 1243 / 2250 Loss: 0.2867\n",
      "Epoch: 1 / 3, Step: 1244 / 2250 Loss: 0.3124\n",
      "Epoch: 1 / 3, Step: 1245 / 2250 Loss: 0.3348\n",
      "Epoch: 1 / 3, Step: 1246 / 2250 Loss: 0.2016\n",
      "Epoch: 1 / 3, Step: 1247 / 2250 Loss: 0.1332\n",
      "Epoch: 1 / 3, Step: 1248 / 2250 Loss: 0.1119\n",
      "Epoch: 1 / 3, Step: 1249 / 2250 Loss: 0.1386\n",
      "Epoch: 1 / 3, Step: 1250 / 2250 Loss: 0.1307\n",
      "Epoch: 1 / 3, Step: 1251 / 2250 Loss: 0.1942\n",
      "Epoch: 1 / 3, Step: 1252 / 2250 Loss: 0.1816\n",
      "Epoch: 1 / 3, Step: 1253 / 2250 Loss: 0.1143\n",
      "Epoch: 1 / 3, Step: 1254 / 2250 Loss: 0.0998\n",
      "Epoch: 1 / 3, Step: 1255 / 2250 Loss: 0.0912\n",
      "Epoch: 1 / 3, Step: 1256 / 2250 Loss: 0.3469\n",
      "Epoch: 1 / 3, Step: 1257 / 2250 Loss: 0.4201\n",
      "Epoch: 1 / 3, Step: 1258 / 2250 Loss: 0.2311\n",
      "Epoch: 1 / 3, Step: 1259 / 2250 Loss: 0.3152\n",
      "Epoch: 1 / 3, Step: 1260 / 2250 Loss: 0.2088\n",
      "Epoch: 1 / 3, Step: 1261 / 2250 Loss: 0.0772\n",
      "Epoch: 1 / 3, Step: 1262 / 2250 Loss: 0.2695\n",
      "Epoch: 1 / 3, Step: 1263 / 2250 Loss: 0.1003\n",
      "Epoch: 1 / 3, Step: 1264 / 2250 Loss: 0.1665\n",
      "Epoch: 1 / 3, Step: 1265 / 2250 Loss: 0.2049\n",
      "Epoch: 1 / 3, Step: 1266 / 2250 Loss: 0.2866\n",
      "Epoch: 1 / 3, Step: 1267 / 2250 Loss: 0.2400\n",
      "Epoch: 1 / 3, Step: 1268 / 2250 Loss: 0.5290\n",
      "Epoch: 1 / 3, Step: 1269 / 2250 Loss: 0.4232\n",
      "Epoch: 1 / 3, Step: 1270 / 2250 Loss: 0.4092\n",
      "Epoch: 1 / 3, Step: 1271 / 2250 Loss: 0.3007\n",
      "Epoch: 1 / 3, Step: 1272 / 2250 Loss: 0.2634\n",
      "Epoch: 1 / 3, Step: 1273 / 2250 Loss: 0.5306\n",
      "Epoch: 1 / 3, Step: 1274 / 2250 Loss: 0.1617\n",
      "Epoch: 1 / 3, Step: 1275 / 2250 Loss: 0.2366\n",
      "Epoch: 1 / 3, Step: 1276 / 2250 Loss: 0.3285\n",
      "Epoch: 1 / 3, Step: 1277 / 2250 Loss: 0.1451\n",
      "Epoch: 1 / 3, Step: 1278 / 2250 Loss: 0.3624\n",
      "Epoch: 1 / 3, Step: 1279 / 2250 Loss: 0.0799\n",
      "Epoch: 1 / 3, Step: 1280 / 2250 Loss: 0.1811\n",
      "Epoch: 1 / 3, Step: 1281 / 2250 Loss: 0.2331\n",
      "Epoch: 1 / 3, Step: 1282 / 2250 Loss: 0.2584\n",
      "Epoch: 1 / 3, Step: 1283 / 2250 Loss: 0.3393\n",
      "Epoch: 1 / 3, Step: 1284 / 2250 Loss: 0.3712\n",
      "Epoch: 1 / 3, Step: 1285 / 2250 Loss: 0.6343\n",
      "Epoch: 1 / 3, Step: 1286 / 2250 Loss: 0.0838\n",
      "Epoch: 1 / 3, Step: 1287 / 2250 Loss: 0.2216\n",
      "Epoch: 1 / 3, Step: 1288 / 2250 Loss: 0.3779\n",
      "Epoch: 1 / 3, Step: 1289 / 2250 Loss: 0.3252\n",
      "Epoch: 1 / 3, Step: 1290 / 2250 Loss: 0.3618\n",
      "Epoch: 1 / 3, Step: 1291 / 2250 Loss: 0.3772\n",
      "Epoch: 1 / 3, Step: 1292 / 2250 Loss: 0.2414\n",
      "Epoch: 1 / 3, Step: 1293 / 2250 Loss: 0.2975\n",
      "Epoch: 1 / 3, Step: 1294 / 2250 Loss: 0.3520\n",
      "Epoch: 1 / 3, Step: 1295 / 2250 Loss: 0.4290\n",
      "Epoch: 1 / 3, Step: 1296 / 2250 Loss: 0.2539\n",
      "Epoch: 1 / 3, Step: 1297 / 2250 Loss: 0.1962\n",
      "Epoch: 1 / 3, Step: 1298 / 2250 Loss: 0.1972\n",
      "Epoch: 1 / 3, Step: 1299 / 2250 Loss: 0.1947\n",
      "Epoch: 1 / 3, Step: 1300 / 2250 Loss: 0.1768\n",
      "Epoch: 1 / 3, Step: 1301 / 2250 Loss: 0.1909\n",
      "Epoch: 1 / 3, Step: 1302 / 2250 Loss: 0.2961\n",
      "Epoch: 1 / 3, Step: 1303 / 2250 Loss: 0.3697\n",
      "Epoch: 1 / 3, Step: 1304 / 2250 Loss: 0.2192\n",
      "Epoch: 1 / 3, Step: 1305 / 2250 Loss: 0.3030\n",
      "Epoch: 1 / 3, Step: 1306 / 2250 Loss: 0.4204\n",
      "Epoch: 1 / 3, Step: 1307 / 2250 Loss: 0.1613\n",
      "Epoch: 1 / 3, Step: 1308 / 2250 Loss: 0.1676\n",
      "Epoch: 1 / 3, Step: 1309 / 2250 Loss: 0.4272\n",
      "Epoch: 1 / 3, Step: 1310 / 2250 Loss: 0.0873\n",
      "Epoch: 1 / 3, Step: 1311 / 2250 Loss: 0.1651\n",
      "Epoch: 1 / 3, Step: 1312 / 2250 Loss: 0.3326\n",
      "Epoch: 1 / 3, Step: 1313 / 2250 Loss: 0.2272\n",
      "Epoch: 1 / 3, Step: 1314 / 2250 Loss: 0.1366\n",
      "Epoch: 1 / 3, Step: 1315 / 2250 Loss: 0.2893\n",
      "Epoch: 1 / 3, Step: 1316 / 2250 Loss: 0.1032\n",
      "Epoch: 1 / 3, Step: 1317 / 2250 Loss: 0.2666\n",
      "Epoch: 1 / 3, Step: 1318 / 2250 Loss: 0.5014\n",
      "Epoch: 1 / 3, Step: 1319 / 2250 Loss: 0.1880\n",
      "Epoch: 1 / 3, Step: 1320 / 2250 Loss: 0.1138\n",
      "Epoch: 1 / 3, Step: 1321 / 2250 Loss: 0.3955\n",
      "Epoch: 1 / 3, Step: 1322 / 2250 Loss: 0.1489\n",
      "Epoch: 1 / 3, Step: 1323 / 2250 Loss: 0.4042\n",
      "Epoch: 1 / 3, Step: 1324 / 2250 Loss: 0.1967\n",
      "Epoch: 1 / 3, Step: 1325 / 2250 Loss: 0.3180\n",
      "Epoch: 1 / 3, Step: 1326 / 2250 Loss: 0.1248\n",
      "Epoch: 1 / 3, Step: 1327 / 2250 Loss: 0.1767\n",
      "Epoch: 1 / 3, Step: 1328 / 2250 Loss: 0.1478\n",
      "Epoch: 1 / 3, Step: 1329 / 2250 Loss: 0.2374\n",
      "Epoch: 1 / 3, Step: 1330 / 2250 Loss: 0.2913\n",
      "Epoch: 1 / 3, Step: 1331 / 2250 Loss: 0.3086\n",
      "Epoch: 1 / 3, Step: 1332 / 2250 Loss: 0.1511\n",
      "Epoch: 1 / 3, Step: 1333 / 2250 Loss: 0.3633\n",
      "Epoch: 1 / 3, Step: 1334 / 2250 Loss: 0.2634\n",
      "Epoch: 1 / 3, Step: 1335 / 2250 Loss: 0.3209\n",
      "Epoch: 1 / 3, Step: 1336 / 2250 Loss: 0.2968\n",
      "Epoch: 1 / 3, Step: 1337 / 2250 Loss: 0.5741\n",
      "Epoch: 1 / 3, Step: 1338 / 2250 Loss: 0.1791\n",
      "Epoch: 1 / 3, Step: 1339 / 2250 Loss: 0.1981\n",
      "Epoch: 1 / 3, Step: 1340 / 2250 Loss: 0.2427\n",
      "Epoch: 1 / 3, Step: 1341 / 2250 Loss: 0.1071\n",
      "Epoch: 1 / 3, Step: 1342 / 2250 Loss: 0.3000\n",
      "Epoch: 1 / 3, Step: 1343 / 2250 Loss: 0.4028\n",
      "Epoch: 1 / 3, Step: 1344 / 2250 Loss: 0.2752\n",
      "Epoch: 1 / 3, Step: 1345 / 2250 Loss: 0.2781\n",
      "Epoch: 1 / 3, Step: 1346 / 2250 Loss: 0.3018\n",
      "Epoch: 1 / 3, Step: 1347 / 2250 Loss: 0.1586\n",
      "Epoch: 1 / 3, Step: 1348 / 2250 Loss: 0.1413\n",
      "Epoch: 1 / 3, Step: 1349 / 2250 Loss: 0.1951\n",
      "Epoch: 1 / 3, Step: 1350 / 2250 Loss: 0.2072\n",
      "Epoch: 1 / 3, Step: 1351 / 2250 Loss: 0.2324\n",
      "Epoch: 1 / 3, Step: 1352 / 2250 Loss: 0.3813\n",
      "Epoch: 1 / 3, Step: 1353 / 2250 Loss: 0.3902\n",
      "Epoch: 1 / 3, Step: 1354 / 2250 Loss: 0.2148\n",
      "Epoch: 1 / 3, Step: 1355 / 2250 Loss: 0.4197\n",
      "Epoch: 1 / 3, Step: 1356 / 2250 Loss: 0.3889\n",
      "Epoch: 1 / 3, Step: 1357 / 2250 Loss: 0.3168\n",
      "Epoch: 1 / 3, Step: 1358 / 2250 Loss: 0.1542\n",
      "Epoch: 1 / 3, Step: 1359 / 2250 Loss: 0.2441\n",
      "Epoch: 1 / 3, Step: 1360 / 2250 Loss: 0.2674\n",
      "Epoch: 1 / 3, Step: 1361 / 2250 Loss: 0.3017\n",
      "Epoch: 1 / 3, Step: 1362 / 2250 Loss: 0.3563\n",
      "Epoch: 1 / 3, Step: 1363 / 2250 Loss: 0.1629\n",
      "Epoch: 1 / 3, Step: 1364 / 2250 Loss: 0.3286\n",
      "Epoch: 1 / 3, Step: 1365 / 2250 Loss: 0.1696\n",
      "Epoch: 1 / 3, Step: 1366 / 2250 Loss: 0.2575\n",
      "Epoch: 1 / 3, Step: 1367 / 2250 Loss: 0.2722\n",
      "Epoch: 1 / 3, Step: 1368 / 2250 Loss: 0.1345\n",
      "Epoch: 1 / 3, Step: 1369 / 2250 Loss: 0.3814\n",
      "Epoch: 1 / 3, Step: 1370 / 2250 Loss: 0.2692\n",
      "Epoch: 1 / 3, Step: 1371 / 2250 Loss: 0.2337\n",
      "Epoch: 1 / 3, Step: 1372 / 2250 Loss: 0.3928\n",
      "Epoch: 1 / 3, Step: 1373 / 2250 Loss: 0.3474\n",
      "Epoch: 1 / 3, Step: 1374 / 2250 Loss: 0.2315\n",
      "Epoch: 1 / 3, Step: 1375 / 2250 Loss: 0.2126\n",
      "Epoch: 1 / 3, Step: 1376 / 2250 Loss: 0.2141\n",
      "Epoch: 1 / 3, Step: 1377 / 2250 Loss: 0.1994\n",
      "Epoch: 1 / 3, Step: 1378 / 2250 Loss: 0.2598\n",
      "Epoch: 1 / 3, Step: 1379 / 2250 Loss: 0.3798\n",
      "Epoch: 1 / 3, Step: 1380 / 2250 Loss: 0.3959\n",
      "Epoch: 1 / 3, Step: 1381 / 2250 Loss: 0.3643\n",
      "Epoch: 1 / 3, Step: 1382 / 2250 Loss: 0.3175\n",
      "Epoch: 1 / 3, Step: 1383 / 2250 Loss: 0.2477\n",
      "Epoch: 1 / 3, Step: 1384 / 2250 Loss: 0.2716\n",
      "Epoch: 1 / 3, Step: 1385 / 2250 Loss: 0.2404\n",
      "Epoch: 1 / 3, Step: 1386 / 2250 Loss: 0.1085\n",
      "Epoch: 1 / 3, Step: 1387 / 2250 Loss: 0.1660\n",
      "Epoch: 1 / 3, Step: 1388 / 2250 Loss: 0.1869\n",
      "Epoch: 1 / 3, Step: 1389 / 2250 Loss: 0.2781\n",
      "Epoch: 1 / 3, Step: 1390 / 2250 Loss: 0.3230\n",
      "Epoch: 1 / 3, Step: 1391 / 2250 Loss: 0.2436\n",
      "Epoch: 1 / 3, Step: 1392 / 2250 Loss: 0.1482\n",
      "Epoch: 1 / 3, Step: 1393 / 2250 Loss: 0.2167\n",
      "Epoch: 1 / 3, Step: 1394 / 2250 Loss: 0.3404\n",
      "Epoch: 1 / 3, Step: 1395 / 2250 Loss: 0.1356\n",
      "Epoch: 1 / 3, Step: 1396 / 2250 Loss: 0.2526\n",
      "Epoch: 1 / 3, Step: 1397 / 2250 Loss: 0.1427\n",
      "Epoch: 1 / 3, Step: 1398 / 2250 Loss: 0.1314\n",
      "Epoch: 1 / 3, Step: 1399 / 2250 Loss: 0.1710\n",
      "Epoch: 1 / 3, Step: 1400 / 2250 Loss: 0.1681\n",
      "Epoch: 1 / 3, Step: 1401 / 2250 Loss: 0.2770\n",
      "Epoch: 1 / 3, Step: 1402 / 2250 Loss: 0.4840\n",
      "Epoch: 1 / 3, Step: 1403 / 2250 Loss: 0.0834\n",
      "Epoch: 1 / 3, Step: 1404 / 2250 Loss: 0.4528\n",
      "Epoch: 1 / 3, Step: 1405 / 2250 Loss: 0.2271\n",
      "Epoch: 1 / 3, Step: 1406 / 2250 Loss: 0.1779\n",
      "Epoch: 1 / 3, Step: 1407 / 2250 Loss: 0.1747\n",
      "Epoch: 1 / 3, Step: 1408 / 2250 Loss: 0.1237\n",
      "Epoch: 1 / 3, Step: 1409 / 2250 Loss: 0.2702\n",
      "Epoch: 1 / 3, Step: 1410 / 2250 Loss: 0.3515\n",
      "Epoch: 1 / 3, Step: 1411 / 2250 Loss: 0.0977\n",
      "Epoch: 1 / 3, Step: 1412 / 2250 Loss: 0.3900\n",
      "Epoch: 1 / 3, Step: 1413 / 2250 Loss: 0.1093\n",
      "Epoch: 1 / 3, Step: 1414 / 2250 Loss: 0.2413\n",
      "Epoch: 1 / 3, Step: 1415 / 2250 Loss: 0.2117\n",
      "Epoch: 1 / 3, Step: 1416 / 2250 Loss: 0.2006\n",
      "Epoch: 1 / 3, Step: 1417 / 2250 Loss: 0.2738\n",
      "Epoch: 1 / 3, Step: 1418 / 2250 Loss: 0.3671\n",
      "Epoch: 1 / 3, Step: 1419 / 2250 Loss: 0.1791\n",
      "Epoch: 1 / 3, Step: 1420 / 2250 Loss: 0.1937\n",
      "Epoch: 1 / 3, Step: 1421 / 2250 Loss: 0.1932\n",
      "Epoch: 1 / 3, Step: 1422 / 2250 Loss: 0.2124\n",
      "Epoch: 1 / 3, Step: 1423 / 2250 Loss: 0.3509\n",
      "Epoch: 1 / 3, Step: 1424 / 2250 Loss: 0.0703\n",
      "Epoch: 1 / 3, Step: 1425 / 2250 Loss: 0.3852\n",
      "Epoch: 1 / 3, Step: 1426 / 2250 Loss: 0.1201\n",
      "Epoch: 1 / 3, Step: 1427 / 2250 Loss: 0.2910\n",
      "Epoch: 1 / 3, Step: 1428 / 2250 Loss: 0.2283\n",
      "Epoch: 1 / 3, Step: 1429 / 2250 Loss: 0.1139\n",
      "Epoch: 1 / 3, Step: 1430 / 2250 Loss: 0.2589\n",
      "Epoch: 1 / 3, Step: 1431 / 2250 Loss: 0.1944\n",
      "Epoch: 1 / 3, Step: 1432 / 2250 Loss: 0.2425\n",
      "Epoch: 1 / 3, Step: 1433 / 2250 Loss: 0.3561\n",
      "Epoch: 1 / 3, Step: 1434 / 2250 Loss: 0.2507\n",
      "Epoch: 1 / 3, Step: 1435 / 2250 Loss: 0.3380\n",
      "Epoch: 1 / 3, Step: 1436 / 2250 Loss: 0.2666\n",
      "Epoch: 1 / 3, Step: 1437 / 2250 Loss: 0.3856\n",
      "Epoch: 1 / 3, Step: 1438 / 2250 Loss: 0.1175\n",
      "Epoch: 1 / 3, Step: 1439 / 2250 Loss: 0.2679\n",
      "Epoch: 1 / 3, Step: 1440 / 2250 Loss: 0.1646\n",
      "Epoch: 1 / 3, Step: 1441 / 2250 Loss: 0.1131\n",
      "Epoch: 1 / 3, Step: 1442 / 2250 Loss: 0.3232\n",
      "Epoch: 1 / 3, Step: 1443 / 2250 Loss: 0.1403\n",
      "Epoch: 1 / 3, Step: 1444 / 2250 Loss: 0.1331\n",
      "Epoch: 1 / 3, Step: 1445 / 2250 Loss: 0.0820\n",
      "Epoch: 1 / 3, Step: 1446 / 2250 Loss: 0.3330\n",
      "Epoch: 1 / 3, Step: 1447 / 2250 Loss: 0.1765\n",
      "Epoch: 1 / 3, Step: 1448 / 2250 Loss: 0.1249\n",
      "Epoch: 1 / 3, Step: 1449 / 2250 Loss: 0.1540\n",
      "Epoch: 1 / 3, Step: 1450 / 2250 Loss: 0.1703\n",
      "Epoch: 1 / 3, Step: 1451 / 2250 Loss: 0.3344\n",
      "Epoch: 1 / 3, Step: 1452 / 2250 Loss: 0.0892\n",
      "Epoch: 1 / 3, Step: 1453 / 2250 Loss: 0.1884\n",
      "Epoch: 1 / 3, Step: 1454 / 2250 Loss: 0.2536\n",
      "Epoch: 1 / 3, Step: 1455 / 2250 Loss: 0.2687\n",
      "Epoch: 1 / 3, Step: 1456 / 2250 Loss: 0.0775\n",
      "Epoch: 1 / 3, Step: 1457 / 2250 Loss: 0.1306\n",
      "Epoch: 1 / 3, Step: 1458 / 2250 Loss: 0.1968\n",
      "Epoch: 1 / 3, Step: 1459 / 2250 Loss: 0.0397\n",
      "Epoch: 1 / 3, Step: 1460 / 2250 Loss: 0.0637\n",
      "Epoch: 1 / 3, Step: 1461 / 2250 Loss: 0.4012\n",
      "Epoch: 1 / 3, Step: 1462 / 2250 Loss: 0.1896\n",
      "Epoch: 1 / 3, Step: 1463 / 2250 Loss: 0.1944\n",
      "Epoch: 1 / 3, Step: 1464 / 2250 Loss: 0.2620\n",
      "Epoch: 1 / 3, Step: 1465 / 2250 Loss: 0.6240\n",
      "Epoch: 1 / 3, Step: 1466 / 2250 Loss: 0.1652\n",
      "Epoch: 1 / 3, Step: 1467 / 2250 Loss: 0.1293\n",
      "Epoch: 1 / 3, Step: 1468 / 2250 Loss: 0.2443\n",
      "Epoch: 1 / 3, Step: 1469 / 2250 Loss: 0.3734\n",
      "Epoch: 1 / 3, Step: 1470 / 2250 Loss: 0.1544\n",
      "Epoch: 1 / 3, Step: 1471 / 2250 Loss: 0.2058\n",
      "Epoch: 1 / 3, Step: 1472 / 2250 Loss: 0.2314\n",
      "Epoch: 1 / 3, Step: 1473 / 2250 Loss: 0.4550\n",
      "Epoch: 1 / 3, Step: 1474 / 2250 Loss: 0.0982\n",
      "Epoch: 1 / 3, Step: 1475 / 2250 Loss: 0.1810\n",
      "Epoch: 1 / 3, Step: 1476 / 2250 Loss: 0.2405\n",
      "Epoch: 1 / 3, Step: 1477 / 2250 Loss: 0.2146\n",
      "Epoch: 1 / 3, Step: 1478 / 2250 Loss: 0.3627\n",
      "Epoch: 1 / 3, Step: 1479 / 2250 Loss: 0.0789\n",
      "Epoch: 1 / 3, Step: 1480 / 2250 Loss: 0.2508\n",
      "Epoch: 1 / 3, Step: 1481 / 2250 Loss: 0.3973\n",
      "Epoch: 1 / 3, Step: 1482 / 2250 Loss: 0.3471\n",
      "Epoch: 1 / 3, Step: 1483 / 2250 Loss: 0.4667\n",
      "Epoch: 1 / 3, Step: 1484 / 2250 Loss: 0.1673\n",
      "Epoch: 1 / 3, Step: 1485 / 2250 Loss: 0.2295\n",
      "Epoch: 1 / 3, Step: 1486 / 2250 Loss: 0.1252\n",
      "Epoch: 1 / 3, Step: 1487 / 2250 Loss: 0.1410\n",
      "Epoch: 1 / 3, Step: 1488 / 2250 Loss: 0.1561\n",
      "Epoch: 1 / 3, Step: 1489 / 2250 Loss: 0.2622\n",
      "Epoch: 1 / 3, Step: 1490 / 2250 Loss: 0.3202\n",
      "Epoch: 1 / 3, Step: 1491 / 2250 Loss: 0.3244\n",
      "Epoch: 1 / 3, Step: 1492 / 2250 Loss: 0.1455\n",
      "Epoch: 1 / 3, Step: 1493 / 2250 Loss: 0.1278\n",
      "Epoch: 1 / 3, Step: 1494 / 2250 Loss: 0.3308\n",
      "Epoch: 1 / 3, Step: 1495 / 2250 Loss: 0.1143\n",
      "Epoch: 1 / 3, Step: 1496 / 2250 Loss: 0.2904\n",
      "Epoch: 1 / 3, Step: 1497 / 2250 Loss: 0.3264\n",
      "Epoch: 1 / 3, Step: 1498 / 2250 Loss: 0.2439\n",
      "Epoch: 1 / 3, Step: 1499 / 2250 Loss: 0.2114\n",
      "Epoch: 1 / 3, Step: 1500 / 2250 Loss: 0.0495\n",
      "Epoch: 1 / 3, Step: 1501 / 2250 Loss: 0.2991\n",
      "Epoch: 1 / 3, Step: 1502 / 2250 Loss: 0.2468\n",
      "Epoch: 1 / 3, Step: 1503 / 2250 Loss: 0.2779\n",
      "Epoch: 1 / 3, Step: 1504 / 2250 Loss: 0.4001\n",
      "Epoch: 1 / 3, Step: 1505 / 2250 Loss: 0.2194\n",
      "Epoch: 1 / 3, Step: 1506 / 2250 Loss: 0.5060\n",
      "Epoch: 1 / 3, Step: 1507 / 2250 Loss: 0.1652\n",
      "Epoch: 1 / 3, Step: 1508 / 2250 Loss: 0.1993\n",
      "Epoch: 1 / 3, Step: 1509 / 2250 Loss: 0.2681\n",
      "Epoch: 1 / 3, Step: 1510 / 2250 Loss: 0.1157\n",
      "Epoch: 1 / 3, Step: 1511 / 2250 Loss: 0.0701\n",
      "Epoch: 1 / 3, Step: 1512 / 2250 Loss: 0.1188\n",
      "Epoch: 1 / 3, Step: 1513 / 2250 Loss: 0.2735\n",
      "Epoch: 1 / 3, Step: 1514 / 2250 Loss: 0.2014\n",
      "Epoch: 1 / 3, Step: 1515 / 2250 Loss: 0.2780\n",
      "Epoch: 1 / 3, Step: 1516 / 2250 Loss: 0.1070\n",
      "Epoch: 1 / 3, Step: 1517 / 2250 Loss: 0.1427\n",
      "Epoch: 1 / 3, Step: 1518 / 2250 Loss: 0.1621\n",
      "Epoch: 1 / 3, Step: 1519 / 2250 Loss: 0.2885\n",
      "Epoch: 1 / 3, Step: 1520 / 2250 Loss: 0.3529\n",
      "Epoch: 1 / 3, Step: 1521 / 2250 Loss: 0.2417\n",
      "Epoch: 1 / 3, Step: 1522 / 2250 Loss: 0.1698\n",
      "Epoch: 1 / 3, Step: 1523 / 2250 Loss: 0.1865\n",
      "Epoch: 1 / 3, Step: 1524 / 2250 Loss: 0.1884\n",
      "Epoch: 1 / 3, Step: 1525 / 2250 Loss: 0.4640\n",
      "Epoch: 1 / 3, Step: 1526 / 2250 Loss: 0.2300\n",
      "Epoch: 1 / 3, Step: 1527 / 2250 Loss: 0.1592\n",
      "Epoch: 1 / 3, Step: 1528 / 2250 Loss: 0.2903\n",
      "Epoch: 1 / 3, Step: 1529 / 2250 Loss: 0.1751\n",
      "Epoch: 1 / 3, Step: 1530 / 2250 Loss: 0.3525\n",
      "Epoch: 1 / 3, Step: 1531 / 2250 Loss: 0.3115\n",
      "Epoch: 1 / 3, Step: 1532 / 2250 Loss: 0.3860\n",
      "Epoch: 1 / 3, Step: 1533 / 2250 Loss: 0.1118\n",
      "Epoch: 1 / 3, Step: 1534 / 2250 Loss: 0.2100\n",
      "Epoch: 1 / 3, Step: 1535 / 2250 Loss: 0.3291\n",
      "Epoch: 1 / 3, Step: 1536 / 2250 Loss: 0.2718\n",
      "Epoch: 1 / 3, Step: 1537 / 2250 Loss: 0.3469\n",
      "Epoch: 1 / 3, Step: 1538 / 2250 Loss: 0.2237\n",
      "Epoch: 1 / 3, Step: 1539 / 2250 Loss: 0.2933\n",
      "Epoch: 1 / 3, Step: 1540 / 2250 Loss: 0.2606\n",
      "Epoch: 1 / 3, Step: 1541 / 2250 Loss: 0.3080\n",
      "Epoch: 1 / 3, Step: 1542 / 2250 Loss: 0.3469\n",
      "Epoch: 1 / 3, Step: 1543 / 2250 Loss: 0.4165\n",
      "Epoch: 1 / 3, Step: 1544 / 2250 Loss: 0.3884\n",
      "Epoch: 1 / 3, Step: 1545 / 2250 Loss: 0.1787\n",
      "Epoch: 1 / 3, Step: 1546 / 2250 Loss: 0.3824\n",
      "Epoch: 1 / 3, Step: 1547 / 2250 Loss: 0.1590\n",
      "Epoch: 1 / 3, Step: 1548 / 2250 Loss: 0.2892\n",
      "Epoch: 1 / 3, Step: 1549 / 2250 Loss: 0.2465\n",
      "Epoch: 1 / 3, Step: 1550 / 2250 Loss: 0.3567\n",
      "Epoch: 1 / 3, Step: 1551 / 2250 Loss: 0.2321\n",
      "Epoch: 1 / 3, Step: 1552 / 2250 Loss: 0.2151\n",
      "Epoch: 1 / 3, Step: 1553 / 2250 Loss: 0.4147\n",
      "Epoch: 1 / 3, Step: 1554 / 2250 Loss: 0.2759\n",
      "Epoch: 1 / 3, Step: 1555 / 2250 Loss: 0.1035\n",
      "Epoch: 1 / 3, Step: 1556 / 2250 Loss: 0.1960\n",
      "Epoch: 1 / 3, Step: 1557 / 2250 Loss: 0.1100\n",
      "Epoch: 1 / 3, Step: 1558 / 2250 Loss: 0.1786\n",
      "Epoch: 1 / 3, Step: 1559 / 2250 Loss: 0.2291\n",
      "Epoch: 1 / 3, Step: 1560 / 2250 Loss: 0.1980\n",
      "Epoch: 1 / 3, Step: 1561 / 2250 Loss: 0.0860\n",
      "Epoch: 1 / 3, Step: 1562 / 2250 Loss: 0.1548\n",
      "Epoch: 1 / 3, Step: 1563 / 2250 Loss: 0.1158\n",
      "Epoch: 1 / 3, Step: 1564 / 2250 Loss: 0.1751\n",
      "Epoch: 1 / 3, Step: 1565 / 2250 Loss: 0.5684\n",
      "Epoch: 1 / 3, Step: 1566 / 2250 Loss: 0.2348\n",
      "Epoch: 1 / 3, Step: 1567 / 2250 Loss: 0.3065\n",
      "Epoch: 1 / 3, Step: 1568 / 2250 Loss: 0.2598\n",
      "Epoch: 1 / 3, Step: 1569 / 2250 Loss: 0.0687\n",
      "Epoch: 1 / 3, Step: 1570 / 2250 Loss: 0.1888\n",
      "Epoch: 1 / 3, Step: 1571 / 2250 Loss: 0.3729\n",
      "Epoch: 1 / 3, Step: 1572 / 2250 Loss: 0.1977\n",
      "Epoch: 1 / 3, Step: 1573 / 2250 Loss: 0.1098\n",
      "Epoch: 1 / 3, Step: 1574 / 2250 Loss: 0.2567\n",
      "Epoch: 1 / 3, Step: 1575 / 2250 Loss: 0.1438\n",
      "Epoch: 1 / 3, Step: 1576 / 2250 Loss: 0.1879\n",
      "Epoch: 1 / 3, Step: 1577 / 2250 Loss: 0.0898\n",
      "Epoch: 1 / 3, Step: 1578 / 2250 Loss: 0.1350\n",
      "Epoch: 1 / 3, Step: 1579 / 2250 Loss: 0.3601\n",
      "Epoch: 1 / 3, Step: 1580 / 2250 Loss: 0.2983\n",
      "Epoch: 1 / 3, Step: 1581 / 2250 Loss: 0.3047\n",
      "Epoch: 1 / 3, Step: 1582 / 2250 Loss: 0.1383\n",
      "Epoch: 1 / 3, Step: 1583 / 2250 Loss: 0.0554\n",
      "Epoch: 1 / 3, Step: 1584 / 2250 Loss: 0.0575\n",
      "Epoch: 1 / 3, Step: 1585 / 2250 Loss: 0.1149\n",
      "Epoch: 1 / 3, Step: 1586 / 2250 Loss: 0.3342\n",
      "Epoch: 1 / 3, Step: 1587 / 2250 Loss: 0.3272\n",
      "Epoch: 1 / 3, Step: 1588 / 2250 Loss: 0.1572\n",
      "Epoch: 1 / 3, Step: 1589 / 2250 Loss: 0.1486\n",
      "Epoch: 1 / 3, Step: 1590 / 2250 Loss: 0.0948\n",
      "Epoch: 1 / 3, Step: 1591 / 2250 Loss: 0.1477\n",
      "Epoch: 1 / 3, Step: 1592 / 2250 Loss: 0.1050\n",
      "Epoch: 1 / 3, Step: 1593 / 2250 Loss: 0.2015\n",
      "Epoch: 1 / 3, Step: 1594 / 2250 Loss: 0.1008\n",
      "Epoch: 1 / 3, Step: 1595 / 2250 Loss: 0.3104\n",
      "Epoch: 1 / 3, Step: 1596 / 2250 Loss: 0.3188\n",
      "Epoch: 1 / 3, Step: 1597 / 2250 Loss: 0.0830\n",
      "Epoch: 1 / 3, Step: 1598 / 2250 Loss: 0.2109\n",
      "Epoch: 1 / 3, Step: 1599 / 2250 Loss: 0.0388\n",
      "Epoch: 1 / 3, Step: 1600 / 2250 Loss: 0.1391\n",
      "Epoch: 1 / 3, Step: 1601 / 2250 Loss: 0.1128\n",
      "Epoch: 1 / 3, Step: 1602 / 2250 Loss: 0.3414\n",
      "Epoch: 1 / 3, Step: 1603 / 2250 Loss: 0.2597\n",
      "Epoch: 1 / 3, Step: 1604 / 2250 Loss: 0.3343\n",
      "Epoch: 1 / 3, Step: 1605 / 2250 Loss: 0.1391\n",
      "Epoch: 1 / 3, Step: 1606 / 2250 Loss: 0.2183\n",
      "Epoch: 1 / 3, Step: 1607 / 2250 Loss: 0.3163\n",
      "Epoch: 1 / 3, Step: 1608 / 2250 Loss: 0.1650\n",
      "Epoch: 1 / 3, Step: 1609 / 2250 Loss: 0.1631\n",
      "Epoch: 1 / 3, Step: 1610 / 2250 Loss: 0.2551\n",
      "Epoch: 1 / 3, Step: 1611 / 2250 Loss: 0.1925\n",
      "Epoch: 1 / 3, Step: 1612 / 2250 Loss: 0.1658\n",
      "Epoch: 1 / 3, Step: 1613 / 2250 Loss: 0.1396\n",
      "Epoch: 1 / 3, Step: 1614 / 2250 Loss: 0.4414\n",
      "Epoch: 1 / 3, Step: 1615 / 2250 Loss: 0.2323\n",
      "Epoch: 1 / 3, Step: 1616 / 2250 Loss: 0.3013\n",
      "Epoch: 1 / 3, Step: 1617 / 2250 Loss: 0.2102\n",
      "Epoch: 1 / 3, Step: 1618 / 2250 Loss: 0.3242\n",
      "Epoch: 1 / 3, Step: 1619 / 2250 Loss: 0.3712\n",
      "Epoch: 1 / 3, Step: 1620 / 2250 Loss: 0.1838\n",
      "Epoch: 1 / 3, Step: 1621 / 2250 Loss: 0.0901\n",
      "Epoch: 1 / 3, Step: 1622 / 2250 Loss: 0.3438\n",
      "Epoch: 1 / 3, Step: 1623 / 2250 Loss: 0.0620\n",
      "Epoch: 1 / 3, Step: 1624 / 2250 Loss: 0.9139\n",
      "Epoch: 1 / 3, Step: 1625 / 2250 Loss: 0.3715\n",
      "Epoch: 1 / 3, Step: 1626 / 2250 Loss: 0.0920\n",
      "Epoch: 1 / 3, Step: 1627 / 2250 Loss: 0.3553\n",
      "Epoch: 1 / 3, Step: 1628 / 2250 Loss: 0.2279\n",
      "Epoch: 1 / 3, Step: 1629 / 2250 Loss: 0.1017\n",
      "Epoch: 1 / 3, Step: 1630 / 2250 Loss: 0.2656\n",
      "Epoch: 1 / 3, Step: 1631 / 2250 Loss: 0.0644\n",
      "Epoch: 1 / 3, Step: 1632 / 2250 Loss: 0.0694\n",
      "Epoch: 1 / 3, Step: 1633 / 2250 Loss: 0.1417\n",
      "Epoch: 1 / 3, Step: 1634 / 2250 Loss: 0.2112\n",
      "Epoch: 1 / 3, Step: 1635 / 2250 Loss: 0.1934\n",
      "Epoch: 1 / 3, Step: 1636 / 2250 Loss: 0.3145\n",
      "Epoch: 1 / 3, Step: 1637 / 2250 Loss: 0.1274\n",
      "Epoch: 1 / 3, Step: 1638 / 2250 Loss: 0.0590\n",
      "Epoch: 1 / 3, Step: 1639 / 2250 Loss: 0.4274\n",
      "Epoch: 1 / 3, Step: 1640 / 2250 Loss: 0.2313\n",
      "Epoch: 1 / 3, Step: 1641 / 2250 Loss: 0.2280\n",
      "Epoch: 1 / 3, Step: 1642 / 2250 Loss: 0.1512\n",
      "Epoch: 1 / 3, Step: 1643 / 2250 Loss: 0.0865\n",
      "Epoch: 1 / 3, Step: 1644 / 2250 Loss: 0.1613\n",
      "Epoch: 1 / 3, Step: 1645 / 2250 Loss: 0.2447\n",
      "Epoch: 1 / 3, Step: 1646 / 2250 Loss: 0.4704\n",
      "Epoch: 1 / 3, Step: 1647 / 2250 Loss: 0.1867\n",
      "Epoch: 1 / 3, Step: 1648 / 2250 Loss: 0.2287\n",
      "Epoch: 1 / 3, Step: 1649 / 2250 Loss: 0.1842\n",
      "Epoch: 1 / 3, Step: 1650 / 2250 Loss: 0.1326\n",
      "Epoch: 1 / 3, Step: 1651 / 2250 Loss: 0.7509\n",
      "Epoch: 1 / 3, Step: 1652 / 2250 Loss: 0.2176\n",
      "Epoch: 1 / 3, Step: 1653 / 2250 Loss: 0.2108\n",
      "Epoch: 1 / 3, Step: 1654 / 2250 Loss: 0.0921\n",
      "Epoch: 1 / 3, Step: 1655 / 2250 Loss: 0.3555\n",
      "Epoch: 1 / 3, Step: 1656 / 2250 Loss: 0.2592\n",
      "Epoch: 1 / 3, Step: 1657 / 2250 Loss: 0.2704\n",
      "Epoch: 1 / 3, Step: 1658 / 2250 Loss: 0.3950\n",
      "Epoch: 1 / 3, Step: 1659 / 2250 Loss: 0.0628\n",
      "Epoch: 1 / 3, Step: 1660 / 2250 Loss: 0.4388\n",
      "Epoch: 1 / 3, Step: 1661 / 2250 Loss: 0.2288\n",
      "Epoch: 1 / 3, Step: 1662 / 2250 Loss: 0.3528\n",
      "Epoch: 1 / 3, Step: 1663 / 2250 Loss: 0.1390\n",
      "Epoch: 1 / 3, Step: 1664 / 2250 Loss: 0.3644\n",
      "Epoch: 1 / 3, Step: 1665 / 2250 Loss: 0.1864\n",
      "Epoch: 1 / 3, Step: 1666 / 2250 Loss: 0.2907\n",
      "Epoch: 1 / 3, Step: 1667 / 2250 Loss: 0.4482\n",
      "Epoch: 1 / 3, Step: 1668 / 2250 Loss: 0.1696\n",
      "Epoch: 1 / 3, Step: 1669 / 2250 Loss: 0.3653\n",
      "Epoch: 1 / 3, Step: 1670 / 2250 Loss: 0.0938\n",
      "Epoch: 1 / 3, Step: 1671 / 2250 Loss: 0.3605\n",
      "Epoch: 1 / 3, Step: 1672 / 2250 Loss: 0.1292\n",
      "Epoch: 1 / 3, Step: 1673 / 2250 Loss: 0.1533\n",
      "Epoch: 1 / 3, Step: 1674 / 2250 Loss: 0.2683\n",
      "Epoch: 1 / 3, Step: 1675 / 2250 Loss: 0.1881\n",
      "Epoch: 1 / 3, Step: 1676 / 2250 Loss: 0.2921\n",
      "Epoch: 1 / 3, Step: 1677 / 2250 Loss: 0.0936\n",
      "Epoch: 1 / 3, Step: 1678 / 2250 Loss: 0.2969\n",
      "Epoch: 1 / 3, Step: 1679 / 2250 Loss: 0.0579\n",
      "Epoch: 1 / 3, Step: 1680 / 2250 Loss: 0.2492\n",
      "Epoch: 1 / 3, Step: 1681 / 2250 Loss: 0.5024\n",
      "Epoch: 1 / 3, Step: 1682 / 2250 Loss: 0.1447\n",
      "Epoch: 1 / 3, Step: 1683 / 2250 Loss: 0.0592\n",
      "Epoch: 1 / 3, Step: 1684 / 2250 Loss: 0.2675\n",
      "Epoch: 1 / 3, Step: 1685 / 2250 Loss: 0.3140\n",
      "Epoch: 1 / 3, Step: 1686 / 2250 Loss: 0.3472\n",
      "Epoch: 1 / 3, Step: 1687 / 2250 Loss: 0.0993\n",
      "Epoch: 1 / 3, Step: 1688 / 2250 Loss: 0.2737\n",
      "Epoch: 1 / 3, Step: 1689 / 2250 Loss: 0.1906\n",
      "Epoch: 1 / 3, Step: 1690 / 2250 Loss: 0.1374\n",
      "Epoch: 1 / 3, Step: 1691 / 2250 Loss: 0.2622\n",
      "Epoch: 1 / 3, Step: 1692 / 2250 Loss: 0.1241\n",
      "Epoch: 1 / 3, Step: 1693 / 2250 Loss: 0.1350\n",
      "Epoch: 1 / 3, Step: 1694 / 2250 Loss: 0.1381\n",
      "Epoch: 1 / 3, Step: 1695 / 2250 Loss: 0.4258\n",
      "Epoch: 1 / 3, Step: 1696 / 2250 Loss: 0.2575\n",
      "Epoch: 1 / 3, Step: 1697 / 2250 Loss: 0.4819\n",
      "Epoch: 1 / 3, Step: 1698 / 2250 Loss: 0.0852\n",
      "Epoch: 1 / 3, Step: 1699 / 2250 Loss: 0.1108\n",
      "Epoch: 1 / 3, Step: 1700 / 2250 Loss: 0.1644\n",
      "Epoch: 1 / 3, Step: 1701 / 2250 Loss: 0.0860\n",
      "Epoch: 1 / 3, Step: 1702 / 2250 Loss: 0.2185\n",
      "Epoch: 1 / 3, Step: 1703 / 2250 Loss: 0.1690\n",
      "Epoch: 1 / 3, Step: 1704 / 2250 Loss: 0.1694\n",
      "Epoch: 1 / 3, Step: 1705 / 2250 Loss: 0.1726\n",
      "Epoch: 1 / 3, Step: 1706 / 2250 Loss: 0.2706\n",
      "Epoch: 1 / 3, Step: 1707 / 2250 Loss: 0.4286\n",
      "Epoch: 1 / 3, Step: 1708 / 2250 Loss: 0.1583\n",
      "Epoch: 1 / 3, Step: 1709 / 2250 Loss: 0.1809\n",
      "Epoch: 1 / 3, Step: 1710 / 2250 Loss: 0.2498\n",
      "Epoch: 1 / 3, Step: 1711 / 2250 Loss: 0.4762\n",
      "Epoch: 1 / 3, Step: 1712 / 2250 Loss: 0.1187\n",
      "Epoch: 1 / 3, Step: 1713 / 2250 Loss: 0.1573\n",
      "Epoch: 1 / 3, Step: 1714 / 2250 Loss: 0.2439\n",
      "Epoch: 1 / 3, Step: 1715 / 2250 Loss: 0.3322\n",
      "Epoch: 1 / 3, Step: 1716 / 2250 Loss: 0.1502\n",
      "Epoch: 1 / 3, Step: 1717 / 2250 Loss: 0.1063\n",
      "Epoch: 1 / 3, Step: 1718 / 2250 Loss: 0.2206\n",
      "Epoch: 1 / 3, Step: 1719 / 2250 Loss: 0.4487\n",
      "Epoch: 1 / 3, Step: 1720 / 2250 Loss: 0.0944\n",
      "Epoch: 1 / 3, Step: 1721 / 2250 Loss: 0.1068\n",
      "Epoch: 1 / 3, Step: 1722 / 2250 Loss: 0.1785\n",
      "Epoch: 1 / 3, Step: 1723 / 2250 Loss: 0.1081\n",
      "Epoch: 1 / 3, Step: 1724 / 2250 Loss: 0.3690\n",
      "Epoch: 1 / 3, Step: 1725 / 2250 Loss: 0.2556\n",
      "Epoch: 1 / 3, Step: 1726 / 2250 Loss: 0.4537\n",
      "Epoch: 1 / 3, Step: 1727 / 2250 Loss: 0.1884\n",
      "Epoch: 1 / 3, Step: 1728 / 2250 Loss: 0.2364\n",
      "Epoch: 1 / 3, Step: 1729 / 2250 Loss: 0.2520\n",
      "Epoch: 1 / 3, Step: 1730 / 2250 Loss: 0.2909\n",
      "Epoch: 1 / 3, Step: 1731 / 2250 Loss: 0.1835\n",
      "Epoch: 1 / 3, Step: 1732 / 2250 Loss: 0.1184\n",
      "Epoch: 1 / 3, Step: 1733 / 2250 Loss: 0.0838\n",
      "Epoch: 1 / 3, Step: 1734 / 2250 Loss: 0.2381\n",
      "Epoch: 1 / 3, Step: 1735 / 2250 Loss: 0.1030\n",
      "Epoch: 1 / 3, Step: 1736 / 2250 Loss: 0.2040\n",
      "Epoch: 1 / 3, Step: 1737 / 2250 Loss: 0.2138\n",
      "Epoch: 1 / 3, Step: 1738 / 2250 Loss: 0.2257\n",
      "Epoch: 1 / 3, Step: 1739 / 2250 Loss: 0.1558\n",
      "Epoch: 1 / 3, Step: 1740 / 2250 Loss: 0.0904\n",
      "Epoch: 1 / 3, Step: 1741 / 2250 Loss: 0.0687\n",
      "Epoch: 1 / 3, Step: 1742 / 2250 Loss: 0.2806\n",
      "Epoch: 1 / 3, Step: 1743 / 2250 Loss: 0.0575\n",
      "Epoch: 1 / 3, Step: 1744 / 2250 Loss: 0.0943\n",
      "Epoch: 1 / 3, Step: 1745 / 2250 Loss: 0.1561\n",
      "Epoch: 1 / 3, Step: 1746 / 2250 Loss: 0.1877\n",
      "Epoch: 1 / 3, Step: 1747 / 2250 Loss: 0.0993\n",
      "Epoch: 1 / 3, Step: 1748 / 2250 Loss: 0.1344\n",
      "Epoch: 1 / 3, Step: 1749 / 2250 Loss: 0.0996\n",
      "Epoch: 1 / 3, Step: 1750 / 2250 Loss: 0.4199\n",
      "Epoch: 1 / 3, Step: 1751 / 2250 Loss: 0.1692\n",
      "Epoch: 1 / 3, Step: 1752 / 2250 Loss: 0.1214\n",
      "Epoch: 1 / 3, Step: 1753 / 2250 Loss: 0.1929\n",
      "Epoch: 1 / 3, Step: 1754 / 2250 Loss: 0.2505\n",
      "Epoch: 1 / 3, Step: 1755 / 2250 Loss: 0.4252\n",
      "Epoch: 1 / 3, Step: 1756 / 2250 Loss: 0.0985\n",
      "Epoch: 1 / 3, Step: 1757 / 2250 Loss: 0.2421\n",
      "Epoch: 1 / 3, Step: 1758 / 2250 Loss: 0.1485\n",
      "Epoch: 1 / 3, Step: 1759 / 2250 Loss: 0.1279\n",
      "Epoch: 1 / 3, Step: 1760 / 2250 Loss: 0.2180\n",
      "Epoch: 1 / 3, Step: 1761 / 2250 Loss: 0.4638\n",
      "Epoch: 1 / 3, Step: 1762 / 2250 Loss: 0.5477\n",
      "Epoch: 1 / 3, Step: 1763 / 2250 Loss: 0.1586\n",
      "Epoch: 1 / 3, Step: 1764 / 2250 Loss: 0.0796\n",
      "Epoch: 1 / 3, Step: 1765 / 2250 Loss: 0.2956\n",
      "Epoch: 1 / 3, Step: 1766 / 2250 Loss: 0.0661\n",
      "Epoch: 1 / 3, Step: 1767 / 2250 Loss: 0.2837\n",
      "Epoch: 1 / 3, Step: 1768 / 2250 Loss: 0.2443\n",
      "Epoch: 1 / 3, Step: 1769 / 2250 Loss: 0.2526\n",
      "Epoch: 1 / 3, Step: 1770 / 2250 Loss: 0.1448\n",
      "Epoch: 1 / 3, Step: 1771 / 2250 Loss: 0.0750\n",
      "Epoch: 1 / 3, Step: 1772 / 2250 Loss: 0.0613\n",
      "Epoch: 1 / 3, Step: 1773 / 2250 Loss: 0.1014\n",
      "Epoch: 1 / 3, Step: 1774 / 2250 Loss: 0.2583\n",
      "Epoch: 1 / 3, Step: 1775 / 2250 Loss: 0.4037\n",
      "Epoch: 1 / 3, Step: 1776 / 2250 Loss: 0.2311\n",
      "Epoch: 1 / 3, Step: 1777 / 2250 Loss: 0.3728\n",
      "Epoch: 1 / 3, Step: 1778 / 2250 Loss: 0.0487\n",
      "Epoch: 1 / 3, Step: 1779 / 2250 Loss: 0.1640\n",
      "Epoch: 1 / 3, Step: 1780 / 2250 Loss: 0.3641\n",
      "Epoch: 1 / 3, Step: 1781 / 2250 Loss: 0.2608\n",
      "Epoch: 1 / 3, Step: 1782 / 2250 Loss: 0.2737\n",
      "Epoch: 1 / 3, Step: 1783 / 2250 Loss: 0.2715\n",
      "Epoch: 1 / 3, Step: 1784 / 2250 Loss: 0.1954\n",
      "Epoch: 1 / 3, Step: 1785 / 2250 Loss: 0.1200\n",
      "Epoch: 1 / 3, Step: 1786 / 2250 Loss: 0.0687\n",
      "Epoch: 1 / 3, Step: 1787 / 2250 Loss: 0.1405\n",
      "Epoch: 1 / 3, Step: 1788 / 2250 Loss: 0.1364\n",
      "Epoch: 1 / 3, Step: 1789 / 2250 Loss: 0.3687\n",
      "Epoch: 1 / 3, Step: 1790 / 2250 Loss: 0.3672\n",
      "Epoch: 1 / 3, Step: 1791 / 2250 Loss: 0.2964\n",
      "Epoch: 1 / 3, Step: 1792 / 2250 Loss: 0.2619\n",
      "Epoch: 1 / 3, Step: 1793 / 2250 Loss: 0.1138\n",
      "Epoch: 1 / 3, Step: 1794 / 2250 Loss: 0.1143\n",
      "Epoch: 1 / 3, Step: 1795 / 2250 Loss: 0.2358\n",
      "Epoch: 1 / 3, Step: 1796 / 2250 Loss: 0.2355\n",
      "Epoch: 1 / 3, Step: 1797 / 2250 Loss: 0.1078\n",
      "Epoch: 1 / 3, Step: 1798 / 2250 Loss: 0.3087\n",
      "Epoch: 1 / 3, Step: 1799 / 2250 Loss: 0.1632\n",
      "Epoch: 1 / 3, Step: 1800 / 2250 Loss: 0.1935\n",
      "Epoch: 1 / 3, Step: 1801 / 2250 Loss: 0.2657\n",
      "Epoch: 1 / 3, Step: 1802 / 2250 Loss: 0.1487\n",
      "Epoch: 1 / 3, Step: 1803 / 2250 Loss: 0.1022\n",
      "Epoch: 1 / 3, Step: 1804 / 2250 Loss: 0.0642\n",
      "Epoch: 1 / 3, Step: 1805 / 2250 Loss: 0.0954\n",
      "Epoch: 1 / 3, Step: 1806 / 2250 Loss: 0.3668\n",
      "Epoch: 1 / 3, Step: 1807 / 2250 Loss: 0.0750\n",
      "Epoch: 1 / 3, Step: 1808 / 2250 Loss: 0.0296\n",
      "Epoch: 1 / 3, Step: 1809 / 2250 Loss: 0.2844\n",
      "Epoch: 1 / 3, Step: 1810 / 2250 Loss: 0.3005\n",
      "Epoch: 1 / 3, Step: 1811 / 2250 Loss: 0.1154\n",
      "Epoch: 1 / 3, Step: 1812 / 2250 Loss: 0.0548\n",
      "Epoch: 1 / 3, Step: 1813 / 2250 Loss: 0.1561\n",
      "Epoch: 1 / 3, Step: 1814 / 2250 Loss: 0.3086\n",
      "Epoch: 1 / 3, Step: 1815 / 2250 Loss: 0.3944\n",
      "Epoch: 1 / 3, Step: 1816 / 2250 Loss: 0.3570\n",
      "Epoch: 1 / 3, Step: 1817 / 2250 Loss: 0.0440\n",
      "Epoch: 1 / 3, Step: 1818 / 2250 Loss: 0.7211\n",
      "Epoch: 1 / 3, Step: 1819 / 2250 Loss: 0.3420\n",
      "Epoch: 1 / 3, Step: 1820 / 2250 Loss: 0.2229\n",
      "Epoch: 1 / 3, Step: 1821 / 2250 Loss: 0.1012\n",
      "Epoch: 1 / 3, Step: 1822 / 2250 Loss: 0.2360\n",
      "Epoch: 1 / 3, Step: 1823 / 2250 Loss: 0.3698\n",
      "Epoch: 1 / 3, Step: 1824 / 2250 Loss: 0.0801\n",
      "Epoch: 1 / 3, Step: 1825 / 2250 Loss: 0.1483\n",
      "Epoch: 1 / 3, Step: 1826 / 2250 Loss: 0.1526\n",
      "Epoch: 1 / 3, Step: 1827 / 2250 Loss: 0.3795\n",
      "Epoch: 1 / 3, Step: 1828 / 2250 Loss: 0.1110\n",
      "Epoch: 1 / 3, Step: 1829 / 2250 Loss: 0.0494\n",
      "Epoch: 1 / 3, Step: 1830 / 2250 Loss: 0.1880\n",
      "Epoch: 1 / 3, Step: 1831 / 2250 Loss: 0.2955\n",
      "Epoch: 1 / 3, Step: 1832 / 2250 Loss: 0.1353\n",
      "Epoch: 1 / 3, Step: 1833 / 2250 Loss: 0.1644\n",
      "Epoch: 1 / 3, Step: 1834 / 2250 Loss: 0.2217\n",
      "Epoch: 1 / 3, Step: 1835 / 2250 Loss: 0.2194\n",
      "Epoch: 1 / 3, Step: 1836 / 2250 Loss: 0.1254\n",
      "Epoch: 1 / 3, Step: 1837 / 2250 Loss: 0.1414\n",
      "Epoch: 1 / 3, Step: 1838 / 2250 Loss: 0.2398\n",
      "Epoch: 1 / 3, Step: 1839 / 2250 Loss: 0.2568\n",
      "Epoch: 1 / 3, Step: 1840 / 2250 Loss: 0.4318\n",
      "Epoch: 1 / 3, Step: 1841 / 2250 Loss: 0.0921\n",
      "Epoch: 1 / 3, Step: 1842 / 2250 Loss: 0.1644\n",
      "Epoch: 1 / 3, Step: 1843 / 2250 Loss: 0.2690\n",
      "Epoch: 1 / 3, Step: 1844 / 2250 Loss: 0.0498\n",
      "Epoch: 1 / 3, Step: 1845 / 2250 Loss: 0.1415\n",
      "Epoch: 1 / 3, Step: 1846 / 2250 Loss: 0.2655\n",
      "Epoch: 1 / 3, Step: 1847 / 2250 Loss: 0.0840\n",
      "Epoch: 1 / 3, Step: 1848 / 2250 Loss: 0.2579\n",
      "Epoch: 1 / 3, Step: 1849 / 2250 Loss: 0.3087\n",
      "Epoch: 1 / 3, Step: 1850 / 2250 Loss: 0.2219\n",
      "Epoch: 1 / 3, Step: 1851 / 2250 Loss: 0.1282\n",
      "Epoch: 1 / 3, Step: 1852 / 2250 Loss: 0.1826\n",
      "Epoch: 1 / 3, Step: 1853 / 2250 Loss: 0.0798\n",
      "Epoch: 1 / 3, Step: 1854 / 2250 Loss: 0.2643\n",
      "Epoch: 1 / 3, Step: 1855 / 2250 Loss: 0.1431\n",
      "Epoch: 1 / 3, Step: 1856 / 2250 Loss: 0.1863\n",
      "Epoch: 1 / 3, Step: 1857 / 2250 Loss: 0.2897\n",
      "Epoch: 1 / 3, Step: 1858 / 2250 Loss: 0.1759\n",
      "Epoch: 1 / 3, Step: 1859 / 2250 Loss: 0.2695\n",
      "Epoch: 1 / 3, Step: 1860 / 2250 Loss: 0.4182\n",
      "Epoch: 1 / 3, Step: 1861 / 2250 Loss: 0.1093\n",
      "Epoch: 1 / 3, Step: 1862 / 2250 Loss: 0.3946\n",
      "Epoch: 1 / 3, Step: 1863 / 2250 Loss: 0.1910\n",
      "Epoch: 1 / 3, Step: 1864 / 2250 Loss: 0.0891\n",
      "Epoch: 1 / 3, Step: 1865 / 2250 Loss: 0.2018\n",
      "Epoch: 1 / 3, Step: 1866 / 2250 Loss: 0.0747\n",
      "Epoch: 1 / 3, Step: 1867 / 2250 Loss: 0.2777\n",
      "Epoch: 1 / 3, Step: 1868 / 2250 Loss: 0.1368\n",
      "Epoch: 1 / 3, Step: 1869 / 2250 Loss: 0.1813\n",
      "Epoch: 1 / 3, Step: 1870 / 2250 Loss: 0.2073\n",
      "Epoch: 1 / 3, Step: 1871 / 2250 Loss: 0.2040\n",
      "Epoch: 1 / 3, Step: 1872 / 2250 Loss: 0.3518\n",
      "Epoch: 1 / 3, Step: 1873 / 2250 Loss: 0.1099\n",
      "Epoch: 1 / 3, Step: 1874 / 2250 Loss: 0.1408\n",
      "Epoch: 1 / 3, Step: 1875 / 2250 Loss: 0.1878\n",
      "Epoch: 1 / 3, Step: 1876 / 2250 Loss: 0.0962\n",
      "Epoch: 1 / 3, Step: 1877 / 2250 Loss: 0.2979\n",
      "Epoch: 1 / 3, Step: 1878 / 2250 Loss: 0.1843\n",
      "Epoch: 1 / 3, Step: 1879 / 2250 Loss: 0.2837\n",
      "Epoch: 1 / 3, Step: 1880 / 2250 Loss: 0.2261\n",
      "Epoch: 1 / 3, Step: 1881 / 2250 Loss: 0.1055\n",
      "Epoch: 1 / 3, Step: 1882 / 2250 Loss: 0.2162\n",
      "Epoch: 1 / 3, Step: 1883 / 2250 Loss: 0.1383\n",
      "Epoch: 1 / 3, Step: 1884 / 2250 Loss: 0.3503\n",
      "Epoch: 1 / 3, Step: 1885 / 2250 Loss: 0.1605\n",
      "Epoch: 1 / 3, Step: 1886 / 2250 Loss: 0.0569\n",
      "Epoch: 1 / 3, Step: 1887 / 2250 Loss: 0.2201\n",
      "Epoch: 1 / 3, Step: 1888 / 2250 Loss: 0.3330\n",
      "Epoch: 1 / 3, Step: 1889 / 2250 Loss: 0.1953\n",
      "Epoch: 1 / 3, Step: 1890 / 2250 Loss: 0.0584\n",
      "Epoch: 1 / 3, Step: 1891 / 2250 Loss: 0.1623\n",
      "Epoch: 1 / 3, Step: 1892 / 2250 Loss: 0.0422\n",
      "Epoch: 1 / 3, Step: 1893 / 2250 Loss: 0.0507\n",
      "Epoch: 1 / 3, Step: 1894 / 2250 Loss: 0.1181\n",
      "Epoch: 1 / 3, Step: 1895 / 2250 Loss: 0.2692\n",
      "Epoch: 1 / 3, Step: 1896 / 2250 Loss: 0.1575\n",
      "Epoch: 1 / 3, Step: 1897 / 2250 Loss: 0.4301\n",
      "Epoch: 1 / 3, Step: 1898 / 2250 Loss: 0.1007\n",
      "Epoch: 1 / 3, Step: 1899 / 2250 Loss: 0.2640\n",
      "Epoch: 1 / 3, Step: 1900 / 2250 Loss: 0.0892\n",
      "Epoch: 1 / 3, Step: 1901 / 2250 Loss: 0.3408\n",
      "Epoch: 1 / 3, Step: 1902 / 2250 Loss: 0.1855\n",
      "Epoch: 1 / 3, Step: 1903 / 2250 Loss: 0.0565\n",
      "Epoch: 1 / 3, Step: 1904 / 2250 Loss: 0.3375\n",
      "Epoch: 1 / 3, Step: 1905 / 2250 Loss: 0.2234\n",
      "Epoch: 1 / 3, Step: 1906 / 2250 Loss: 0.1872\n",
      "Epoch: 1 / 3, Step: 1907 / 2250 Loss: 0.2311\n",
      "Epoch: 1 / 3, Step: 1908 / 2250 Loss: 0.1893\n",
      "Epoch: 1 / 3, Step: 1909 / 2250 Loss: 0.2920\n",
      "Epoch: 1 / 3, Step: 1910 / 2250 Loss: 0.1635\n",
      "Epoch: 1 / 3, Step: 1911 / 2250 Loss: 0.4275\n",
      "Epoch: 1 / 3, Step: 1912 / 2250 Loss: 0.3361\n",
      "Epoch: 1 / 3, Step: 1913 / 2250 Loss: 0.0864\n",
      "Epoch: 1 / 3, Step: 1914 / 2250 Loss: 0.0697\n",
      "Epoch: 1 / 3, Step: 1915 / 2250 Loss: 0.0835\n",
      "Epoch: 1 / 3, Step: 1916 / 2250 Loss: 0.3381\n",
      "Epoch: 1 / 3, Step: 1917 / 2250 Loss: 0.1533\n",
      "Epoch: 1 / 3, Step: 1918 / 2250 Loss: 0.0377\n",
      "Epoch: 1 / 3, Step: 1919 / 2250 Loss: 0.2096\n",
      "Epoch: 1 / 3, Step: 1920 / 2250 Loss: 0.1601\n",
      "Epoch: 1 / 3, Step: 1921 / 2250 Loss: 0.5127\n",
      "Epoch: 1 / 3, Step: 1922 / 2250 Loss: 0.0961\n",
      "Epoch: 1 / 3, Step: 1923 / 2250 Loss: 0.0994\n",
      "Epoch: 1 / 3, Step: 1924 / 2250 Loss: 0.1002\n",
      "Epoch: 1 / 3, Step: 1925 / 2250 Loss: 0.1785\n",
      "Epoch: 1 / 3, Step: 1926 / 2250 Loss: 0.0448\n",
      "Epoch: 1 / 3, Step: 1927 / 2250 Loss: 0.0495\n",
      "Epoch: 1 / 3, Step: 1928 / 2250 Loss: 0.1884\n",
      "Epoch: 1 / 3, Step: 1929 / 2250 Loss: 0.1011\n",
      "Epoch: 1 / 3, Step: 1930 / 2250 Loss: 0.1423\n",
      "Epoch: 1 / 3, Step: 1931 / 2250 Loss: 0.0895\n",
      "Epoch: 1 / 3, Step: 1932 / 2250 Loss: 0.1892\n",
      "Epoch: 1 / 3, Step: 1933 / 2250 Loss: 0.2993\n",
      "Epoch: 1 / 3, Step: 1934 / 2250 Loss: 0.3024\n",
      "Epoch: 1 / 3, Step: 1935 / 2250 Loss: 0.0969\n",
      "Epoch: 1 / 3, Step: 1936 / 2250 Loss: 0.2991\n",
      "Epoch: 1 / 3, Step: 1937 / 2250 Loss: 0.2608\n",
      "Epoch: 1 / 3, Step: 1938 / 2250 Loss: 0.3100\n",
      "Epoch: 1 / 3, Step: 1939 / 2250 Loss: 0.0471\n",
      "Epoch: 1 / 3, Step: 1940 / 2250 Loss: 0.1489\n",
      "Epoch: 1 / 3, Step: 1941 / 2250 Loss: 0.3039\n",
      "Epoch: 1 / 3, Step: 1942 / 2250 Loss: 0.2983\n",
      "Epoch: 1 / 3, Step: 1943 / 2250 Loss: 0.1557\n",
      "Epoch: 1 / 3, Step: 1944 / 2250 Loss: 0.2827\n",
      "Epoch: 1 / 3, Step: 1945 / 2250 Loss: 0.1138\n",
      "Epoch: 1 / 3, Step: 1946 / 2250 Loss: 0.2102\n",
      "Epoch: 1 / 3, Step: 1947 / 2250 Loss: 0.2839\n",
      "Epoch: 1 / 3, Step: 1948 / 2250 Loss: 0.3458\n",
      "Epoch: 1 / 3, Step: 1949 / 2250 Loss: 0.3168\n",
      "Epoch: 1 / 3, Step: 1950 / 2250 Loss: 0.0936\n",
      "Epoch: 1 / 3, Step: 1951 / 2250 Loss: 0.1475\n",
      "Epoch: 1 / 3, Step: 1952 / 2250 Loss: 0.2593\n",
      "Epoch: 1 / 3, Step: 1953 / 2250 Loss: 0.4071\n",
      "Epoch: 1 / 3, Step: 1954 / 2250 Loss: 0.5296\n",
      "Epoch: 1 / 3, Step: 1955 / 2250 Loss: 0.2850\n",
      "Epoch: 1 / 3, Step: 1956 / 2250 Loss: 0.3597\n",
      "Epoch: 1 / 3, Step: 1957 / 2250 Loss: 0.1010\n",
      "Epoch: 1 / 3, Step: 1958 / 2250 Loss: 0.0473\n",
      "Epoch: 1 / 3, Step: 1959 / 2250 Loss: 0.3905\n",
      "Epoch: 1 / 3, Step: 1960 / 2250 Loss: 0.1182\n",
      "Epoch: 1 / 3, Step: 1961 / 2250 Loss: 0.2683\n",
      "Epoch: 1 / 3, Step: 1962 / 2250 Loss: 0.1475\n",
      "Epoch: 1 / 3, Step: 1963 / 2250 Loss: 0.2096\n",
      "Epoch: 1 / 3, Step: 1964 / 2250 Loss: 0.2392\n",
      "Epoch: 1 / 3, Step: 1965 / 2250 Loss: 0.1038\n",
      "Epoch: 1 / 3, Step: 1966 / 2250 Loss: 0.2735\n",
      "Epoch: 1 / 3, Step: 1967 / 2250 Loss: 0.0999\n",
      "Epoch: 1 / 3, Step: 1968 / 2250 Loss: 0.3145\n",
      "Epoch: 1 / 3, Step: 1969 / 2250 Loss: 0.2800\n",
      "Epoch: 1 / 3, Step: 1970 / 2250 Loss: 0.1056\n",
      "Epoch: 1 / 3, Step: 1971 / 2250 Loss: 0.1586\n",
      "Epoch: 1 / 3, Step: 1972 / 2250 Loss: 0.1586\n",
      "Epoch: 1 / 3, Step: 1973 / 2250 Loss: 0.0612\n",
      "Epoch: 1 / 3, Step: 1974 / 2250 Loss: 0.1314\n",
      "Epoch: 1 / 3, Step: 1975 / 2250 Loss: 0.2653\n",
      "Epoch: 1 / 3, Step: 1976 / 2250 Loss: 0.4207\n",
      "Epoch: 1 / 3, Step: 1977 / 2250 Loss: 0.3134\n",
      "Epoch: 1 / 3, Step: 1978 / 2250 Loss: 0.0651\n",
      "Epoch: 1 / 3, Step: 1979 / 2250 Loss: 0.2867\n",
      "Epoch: 1 / 3, Step: 1980 / 2250 Loss: 0.4286\n",
      "Epoch: 1 / 3, Step: 1981 / 2250 Loss: 0.0704\n",
      "Epoch: 1 / 3, Step: 1982 / 2250 Loss: 0.1787\n",
      "Epoch: 1 / 3, Step: 1983 / 2250 Loss: 0.2421\n",
      "Epoch: 1 / 3, Step: 1984 / 2250 Loss: 0.2780\n",
      "Epoch: 1 / 3, Step: 1985 / 2250 Loss: 0.1690\n",
      "Epoch: 1 / 3, Step: 1986 / 2250 Loss: 0.2631\n",
      "Epoch: 1 / 3, Step: 1987 / 2250 Loss: 0.2129\n",
      "Epoch: 1 / 3, Step: 1988 / 2250 Loss: 0.1270\n",
      "Epoch: 1 / 3, Step: 1989 / 2250 Loss: 0.3090\n",
      "Epoch: 1 / 3, Step: 1990 / 2250 Loss: 0.3455\n",
      "Epoch: 1 / 3, Step: 1991 / 2250 Loss: 0.1028\n",
      "Epoch: 1 / 3, Step: 1992 / 2250 Loss: 0.0970\n",
      "Epoch: 1 / 3, Step: 1993 / 2250 Loss: 0.2050\n",
      "Epoch: 1 / 3, Step: 1994 / 2250 Loss: 0.1113\n",
      "Epoch: 1 / 3, Step: 1995 / 2250 Loss: 0.1191\n",
      "Epoch: 1 / 3, Step: 1996 / 2250 Loss: 0.1521\n",
      "Epoch: 1 / 3, Step: 1997 / 2250 Loss: 0.3062\n",
      "Epoch: 1 / 3, Step: 1998 / 2250 Loss: 0.1407\n",
      "Epoch: 1 / 3, Step: 1999 / 2250 Loss: 0.2263\n",
      "Epoch: 1 / 3, Step: 2000 / 2250 Loss: 0.0735\n",
      "Epoch: 1 / 3, Step: 2001 / 2250 Loss: 0.1385\n",
      "Epoch: 1 / 3, Step: 2002 / 2250 Loss: 0.2814\n",
      "Epoch: 1 / 3, Step: 2003 / 2250 Loss: 0.0802\n",
      "Epoch: 1 / 3, Step: 2004 / 2250 Loss: 0.2419\n",
      "Epoch: 1 / 3, Step: 2005 / 2250 Loss: 0.3833\n",
      "Epoch: 1 / 3, Step: 2006 / 2250 Loss: 0.2277\n",
      "Epoch: 1 / 3, Step: 2007 / 2250 Loss: 0.3294\n",
      "Epoch: 1 / 3, Step: 2008 / 2250 Loss: 0.3349\n",
      "Epoch: 1 / 3, Step: 2009 / 2250 Loss: 0.0568\n",
      "Epoch: 1 / 3, Step: 2010 / 2250 Loss: 0.0787\n",
      "Epoch: 1 / 3, Step: 2011 / 2250 Loss: 0.1466\n",
      "Epoch: 1 / 3, Step: 2012 / 2250 Loss: 0.2767\n",
      "Epoch: 1 / 3, Step: 2013 / 2250 Loss: 0.1456\n",
      "Epoch: 1 / 3, Step: 2014 / 2250 Loss: 0.2152\n",
      "Epoch: 1 / 3, Step: 2015 / 2250 Loss: 0.2283\n",
      "Epoch: 1 / 3, Step: 2016 / 2250 Loss: 0.2243\n",
      "Epoch: 1 / 3, Step: 2017 / 2250 Loss: 0.3664\n",
      "Epoch: 1 / 3, Step: 2018 / 2250 Loss: 0.3133\n",
      "Epoch: 1 / 3, Step: 2019 / 2250 Loss: 0.6066\n",
      "Epoch: 1 / 3, Step: 2020 / 2250 Loss: 0.1714\n",
      "Epoch: 1 / 3, Step: 2021 / 2250 Loss: 0.4045\n",
      "Epoch: 1 / 3, Step: 2022 / 2250 Loss: 0.1653\n",
      "Epoch: 1 / 3, Step: 2023 / 2250 Loss: 0.1922\n",
      "Epoch: 1 / 3, Step: 2024 / 2250 Loss: 0.0888\n",
      "Epoch: 1 / 3, Step: 2025 / 2250 Loss: 0.3020\n",
      "Epoch: 1 / 3, Step: 2026 / 2250 Loss: 0.0813\n",
      "Epoch: 1 / 3, Step: 2027 / 2250 Loss: 0.0492\n",
      "Epoch: 1 / 3, Step: 2028 / 2250 Loss: 0.0620\n",
      "Epoch: 1 / 3, Step: 2029 / 2250 Loss: 0.0930\n",
      "Epoch: 1 / 3, Step: 2030 / 2250 Loss: 0.2023\n",
      "Epoch: 1 / 3, Step: 2031 / 2250 Loss: 0.3347\n",
      "Epoch: 1 / 3, Step: 2032 / 2250 Loss: 0.1160\n",
      "Epoch: 1 / 3, Step: 2033 / 2250 Loss: 0.4266\n",
      "Epoch: 1 / 3, Step: 2034 / 2250 Loss: 0.1318\n",
      "Epoch: 1 / 3, Step: 2035 / 2250 Loss: 0.1486\n",
      "Epoch: 1 / 3, Step: 2036 / 2250 Loss: 0.3018\n",
      "Epoch: 1 / 3, Step: 2037 / 2250 Loss: 0.2048\n",
      "Epoch: 1 / 3, Step: 2038 / 2250 Loss: 0.0888\n",
      "Epoch: 1 / 3, Step: 2039 / 2250 Loss: 0.1804\n",
      "Epoch: 1 / 3, Step: 2040 / 2250 Loss: 0.2284\n",
      "Epoch: 1 / 3, Step: 2041 / 2250 Loss: 0.1107\n",
      "Epoch: 1 / 3, Step: 2042 / 2250 Loss: 0.2644\n",
      "Epoch: 1 / 3, Step: 2043 / 2250 Loss: 0.1837\n",
      "Epoch: 1 / 3, Step: 2044 / 2250 Loss: 0.1997\n",
      "Epoch: 1 / 3, Step: 2045 / 2250 Loss: 0.1201\n",
      "Epoch: 1 / 3, Step: 2046 / 2250 Loss: 0.3129\n",
      "Epoch: 1 / 3, Step: 2047 / 2250 Loss: 0.2104\n",
      "Epoch: 1 / 3, Step: 2048 / 2250 Loss: 0.2883\n",
      "Epoch: 1 / 3, Step: 2049 / 2250 Loss: 0.1424\n",
      "Epoch: 1 / 3, Step: 2050 / 2250 Loss: 0.0968\n",
      "Epoch: 1 / 3, Step: 2051 / 2250 Loss: 0.1643\n",
      "Epoch: 1 / 3, Step: 2052 / 2250 Loss: 0.2507\n",
      "Epoch: 1 / 3, Step: 2053 / 2250 Loss: 0.2085\n",
      "Epoch: 1 / 3, Step: 2054 / 2250 Loss: 0.1712\n",
      "Epoch: 1 / 3, Step: 2055 / 2250 Loss: 0.1386\n",
      "Epoch: 1 / 3, Step: 2056 / 2250 Loss: 0.0296\n",
      "Epoch: 1 / 3, Step: 2057 / 2250 Loss: 0.1020\n",
      "Epoch: 1 / 3, Step: 2058 / 2250 Loss: 0.1420\n",
      "Epoch: 1 / 3, Step: 2059 / 2250 Loss: 0.3659\n",
      "Epoch: 1 / 3, Step: 2060 / 2250 Loss: 0.4025\n",
      "Epoch: 1 / 3, Step: 2061 / 2250 Loss: 0.1993\n",
      "Epoch: 1 / 3, Step: 2062 / 2250 Loss: 0.3836\n",
      "Epoch: 1 / 3, Step: 2063 / 2250 Loss: 0.2159\n",
      "Epoch: 1 / 3, Step: 2064 / 2250 Loss: 0.2496\n",
      "Epoch: 1 / 3, Step: 2065 / 2250 Loss: 0.1215\n",
      "Epoch: 1 / 3, Step: 2066 / 2250 Loss: 0.2088\n",
      "Epoch: 1 / 3, Step: 2067 / 2250 Loss: 0.1651\n",
      "Epoch: 1 / 3, Step: 2068 / 2250 Loss: 0.1670\n",
      "Epoch: 1 / 3, Step: 2069 / 2250 Loss: 0.0706\n",
      "Epoch: 1 / 3, Step: 2070 / 2250 Loss: 0.0767\n",
      "Epoch: 1 / 3, Step: 2071 / 2250 Loss: 0.4231\n",
      "Epoch: 1 / 3, Step: 2072 / 2250 Loss: 0.0757\n",
      "Epoch: 1 / 3, Step: 2073 / 2250 Loss: 0.1885\n",
      "Epoch: 1 / 3, Step: 2074 / 2250 Loss: 0.3840\n",
      "Epoch: 1 / 3, Step: 2075 / 2250 Loss: 0.3849\n",
      "Epoch: 1 / 3, Step: 2076 / 2250 Loss: 0.2337\n",
      "Epoch: 1 / 3, Step: 2077 / 2250 Loss: 0.0341\n",
      "Epoch: 1 / 3, Step: 2078 / 2250 Loss: 0.0882\n",
      "Epoch: 1 / 3, Step: 2079 / 2250 Loss: 0.1455\n",
      "Epoch: 1 / 3, Step: 2080 / 2250 Loss: 0.1846\n",
      "Epoch: 1 / 3, Step: 2081 / 2250 Loss: 0.1504\n",
      "Epoch: 1 / 3, Step: 2082 / 2250 Loss: 0.0822\n",
      "Epoch: 1 / 3, Step: 2083 / 2250 Loss: 0.0516\n",
      "Epoch: 1 / 3, Step: 2084 / 2250 Loss: 0.1184\n",
      "Epoch: 1 / 3, Step: 2085 / 2250 Loss: 0.2761\n",
      "Epoch: 1 / 3, Step: 2086 / 2250 Loss: 0.1849\n",
      "Epoch: 1 / 3, Step: 2087 / 2250 Loss: 0.1277\n",
      "Epoch: 1 / 3, Step: 2088 / 2250 Loss: 0.0927\n",
      "Epoch: 1 / 3, Step: 2089 / 2250 Loss: 0.2820\n",
      "Epoch: 1 / 3, Step: 2090 / 2250 Loss: 0.1416\n",
      "Epoch: 1 / 3, Step: 2091 / 2250 Loss: 0.0292\n",
      "Epoch: 1 / 3, Step: 2092 / 2250 Loss: 0.1672\n",
      "Epoch: 1 / 3, Step: 2093 / 2250 Loss: 0.1169\n",
      "Epoch: 1 / 3, Step: 2094 / 2250 Loss: 0.1270\n",
      "Epoch: 1 / 3, Step: 2095 / 2250 Loss: 0.5335\n",
      "Epoch: 1 / 3, Step: 2096 / 2250 Loss: 0.2459\n",
      "Epoch: 1 / 3, Step: 2097 / 2250 Loss: 0.1109\n",
      "Epoch: 1 / 3, Step: 2098 / 2250 Loss: 0.1967\n",
      "Epoch: 1 / 3, Step: 2099 / 2250 Loss: 0.3747\n",
      "Epoch: 1 / 3, Step: 2100 / 2250 Loss: 0.3493\n",
      "Epoch: 1 / 3, Step: 2101 / 2250 Loss: 0.1282\n",
      "Epoch: 1 / 3, Step: 2102 / 2250 Loss: 0.0740\n",
      "Epoch: 1 / 3, Step: 2103 / 2250 Loss: 0.1753\n",
      "Epoch: 1 / 3, Step: 2104 / 2250 Loss: 0.2550\n",
      "Epoch: 1 / 3, Step: 2105 / 2250 Loss: 0.2716\n",
      "Epoch: 1 / 3, Step: 2106 / 2250 Loss: 0.4091\n",
      "Epoch: 1 / 3, Step: 2107 / 2250 Loss: 0.0756\n",
      "Epoch: 1 / 3, Step: 2108 / 2250 Loss: 0.2972\n",
      "Epoch: 1 / 3, Step: 2109 / 2250 Loss: 0.3883\n",
      "Epoch: 1 / 3, Step: 2110 / 2250 Loss: 0.1797\n",
      "Epoch: 1 / 3, Step: 2111 / 2250 Loss: 0.4439\n",
      "Epoch: 1 / 3, Step: 2112 / 2250 Loss: 0.2878\n",
      "Epoch: 1 / 3, Step: 2113 / 2250 Loss: 0.1281\n",
      "Epoch: 1 / 3, Step: 2114 / 2250 Loss: 0.2339\n",
      "Epoch: 1 / 3, Step: 2115 / 2250 Loss: 0.3745\n",
      "Epoch: 1 / 3, Step: 2116 / 2250 Loss: 0.1841\n",
      "Epoch: 1 / 3, Step: 2117 / 2250 Loss: 0.0855\n",
      "Epoch: 1 / 3, Step: 2118 / 2250 Loss: 0.0906\n",
      "Epoch: 1 / 3, Step: 2119 / 2250 Loss: 0.2408\n",
      "Epoch: 1 / 3, Step: 2120 / 2250 Loss: 0.2698\n",
      "Epoch: 1 / 3, Step: 2121 / 2250 Loss: 0.4418\n",
      "Epoch: 1 / 3, Step: 2122 / 2250 Loss: 0.2045\n",
      "Epoch: 1 / 3, Step: 2123 / 2250 Loss: 0.0741\n",
      "Epoch: 1 / 3, Step: 2124 / 2250 Loss: 0.3426\n",
      "Epoch: 1 / 3, Step: 2125 / 2250 Loss: 0.0920\n",
      "Epoch: 1 / 3, Step: 2126 / 2250 Loss: 0.1415\n",
      "Epoch: 1 / 3, Step: 2127 / 2250 Loss: 0.3346\n",
      "Epoch: 1 / 3, Step: 2128 / 2250 Loss: 0.1252\n",
      "Epoch: 1 / 3, Step: 2129 / 2250 Loss: 0.1421\n",
      "Epoch: 1 / 3, Step: 2130 / 2250 Loss: 0.5164\n",
      "Epoch: 1 / 3, Step: 2131 / 2250 Loss: 0.1459\n",
      "Epoch: 1 / 3, Step: 2132 / 2250 Loss: 0.5630\n",
      "Epoch: 1 / 3, Step: 2133 / 2250 Loss: 0.1970\n",
      "Epoch: 1 / 3, Step: 2134 / 2250 Loss: 0.0486\n",
      "Epoch: 1 / 3, Step: 2135 / 2250 Loss: 0.0984\n",
      "Epoch: 1 / 3, Step: 2136 / 2250 Loss: 0.1656\n",
      "Epoch: 1 / 3, Step: 2137 / 2250 Loss: 0.3002\n",
      "Epoch: 1 / 3, Step: 2138 / 2250 Loss: 0.5193\n",
      "Epoch: 1 / 3, Step: 2139 / 2250 Loss: 0.2225\n",
      "Epoch: 1 / 3, Step: 2140 / 2250 Loss: 0.2499\n",
      "Epoch: 1 / 3, Step: 2141 / 2250 Loss: 0.1247\n",
      "Epoch: 1 / 3, Step: 2142 / 2250 Loss: 0.2311\n",
      "Epoch: 1 / 3, Step: 2143 / 2250 Loss: 0.2258\n",
      "Epoch: 1 / 3, Step: 2144 / 2250 Loss: 0.2925\n",
      "Epoch: 1 / 3, Step: 2145 / 2250 Loss: 0.1311\n",
      "Epoch: 1 / 3, Step: 2146 / 2250 Loss: 0.0426\n",
      "Epoch: 1 / 3, Step: 2147 / 2250 Loss: 0.2008\n",
      "Epoch: 1 / 3, Step: 2148 / 2250 Loss: 0.0636\n",
      "Epoch: 1 / 3, Step: 2149 / 2250 Loss: 0.1307\n",
      "Epoch: 1 / 3, Step: 2150 / 2250 Loss: 0.2003\n",
      "Epoch: 1 / 3, Step: 2151 / 2250 Loss: 0.1660\n",
      "Epoch: 1 / 3, Step: 2152 / 2250 Loss: 0.1775\n",
      "Epoch: 1 / 3, Step: 2153 / 2250 Loss: 0.0460\n",
      "Epoch: 1 / 3, Step: 2154 / 2250 Loss: 0.1939\n",
      "Epoch: 1 / 3, Step: 2155 / 2250 Loss: 0.1780\n",
      "Epoch: 1 / 3, Step: 2156 / 2250 Loss: 0.1247\n",
      "Epoch: 1 / 3, Step: 2157 / 2250 Loss: 0.2011\n",
      "Epoch: 1 / 3, Step: 2158 / 2250 Loss: 0.0608\n",
      "Epoch: 1 / 3, Step: 2159 / 2250 Loss: 0.2153\n",
      "Epoch: 1 / 3, Step: 2160 / 2250 Loss: 0.1773\n",
      "Epoch: 1 / 3, Step: 2161 / 2250 Loss: 0.0371\n",
      "Epoch: 1 / 3, Step: 2162 / 2250 Loss: 0.1626\n",
      "Epoch: 1 / 3, Step: 2163 / 2250 Loss: 0.1034\n",
      "Epoch: 1 / 3, Step: 2164 / 2250 Loss: 0.1586\n",
      "Epoch: 1 / 3, Step: 2165 / 2250 Loss: 0.1321\n",
      "Epoch: 1 / 3, Step: 2166 / 2250 Loss: 0.2971\n",
      "Epoch: 1 / 3, Step: 2167 / 2250 Loss: 0.2629\n",
      "Epoch: 1 / 3, Step: 2168 / 2250 Loss: 0.0852\n",
      "Epoch: 1 / 3, Step: 2169 / 2250 Loss: 0.2606\n",
      "Epoch: 1 / 3, Step: 2170 / 2250 Loss: 0.1804\n",
      "Epoch: 1 / 3, Step: 2171 / 2250 Loss: 0.3264\n",
      "Epoch: 1 / 3, Step: 2172 / 2250 Loss: 0.1085\n",
      "Epoch: 1 / 3, Step: 2173 / 2250 Loss: 0.4328\n",
      "Epoch: 1 / 3, Step: 2174 / 2250 Loss: 0.7039\n",
      "Epoch: 1 / 3, Step: 2175 / 2250 Loss: 0.2160\n",
      "Epoch: 1 / 3, Step: 2176 / 2250 Loss: 0.2111\n",
      "Epoch: 1 / 3, Step: 2177 / 2250 Loss: 0.0763\n",
      "Epoch: 1 / 3, Step: 2178 / 2250 Loss: 0.3663\n",
      "Epoch: 1 / 3, Step: 2179 / 2250 Loss: 0.1018\n",
      "Epoch: 1 / 3, Step: 2180 / 2250 Loss: 0.2864\n",
      "Epoch: 1 / 3, Step: 2181 / 2250 Loss: 0.2844\n",
      "Epoch: 1 / 3, Step: 2182 / 2250 Loss: 0.1408\n",
      "Epoch: 1 / 3, Step: 2183 / 2250 Loss: 0.2418\n",
      "Epoch: 1 / 3, Step: 2184 / 2250 Loss: 0.1475\n",
      "Epoch: 1 / 3, Step: 2185 / 2250 Loss: 0.1230\n",
      "Epoch: 1 / 3, Step: 2186 / 2250 Loss: 0.1870\n",
      "Epoch: 1 / 3, Step: 2187 / 2250 Loss: 0.1044\n",
      "Epoch: 1 / 3, Step: 2188 / 2250 Loss: 0.2690\n",
      "Epoch: 1 / 3, Step: 2189 / 2250 Loss: 0.2963\n",
      "Epoch: 1 / 3, Step: 2190 / 2250 Loss: 0.2266\n",
      "Epoch: 1 / 3, Step: 2191 / 2250 Loss: 0.2769\n",
      "Epoch: 1 / 3, Step: 2192 / 2250 Loss: 0.1519\n",
      "Epoch: 1 / 3, Step: 2193 / 2250 Loss: 0.1089\n",
      "Epoch: 1 / 3, Step: 2194 / 2250 Loss: 0.1481\n",
      "Epoch: 1 / 3, Step: 2195 / 2250 Loss: 0.1044\n",
      "Epoch: 1 / 3, Step: 2196 / 2250 Loss: 0.3681\n",
      "Epoch: 1 / 3, Step: 2197 / 2250 Loss: 0.2552\n",
      "Epoch: 1 / 3, Step: 2198 / 2250 Loss: 0.0822\n",
      "Epoch: 1 / 3, Step: 2199 / 2250 Loss: 0.0329\n",
      "Epoch: 1 / 3, Step: 2200 / 2250 Loss: 0.2465\n",
      "Epoch: 1 / 3, Step: 2201 / 2250 Loss: 0.2204\n",
      "Epoch: 1 / 3, Step: 2202 / 2250 Loss: 0.4408\n",
      "Epoch: 1 / 3, Step: 2203 / 2250 Loss: 0.1282\n",
      "Epoch: 1 / 3, Step: 2204 / 2250 Loss: 0.3007\n",
      "Epoch: 1 / 3, Step: 2205 / 2250 Loss: 0.1753\n",
      "Epoch: 1 / 3, Step: 2206 / 2250 Loss: 0.0627\n",
      "Epoch: 1 / 3, Step: 2207 / 2250 Loss: 0.2043\n",
      "Epoch: 1 / 3, Step: 2208 / 2250 Loss: 0.1660\n",
      "Epoch: 1 / 3, Step: 2209 / 2250 Loss: 0.1426\n",
      "Epoch: 1 / 3, Step: 2210 / 2250 Loss: 0.1742\n",
      "Epoch: 1 / 3, Step: 2211 / 2250 Loss: 0.0888\n",
      "Epoch: 1 / 3, Step: 2212 / 2250 Loss: 0.3591\n",
      "Epoch: 1 / 3, Step: 2213 / 2250 Loss: 0.2615\n",
      "Epoch: 1 / 3, Step: 2214 / 2250 Loss: 0.1157\n",
      "Epoch: 1 / 3, Step: 2215 / 2250 Loss: 0.3054\n",
      "Epoch: 1 / 3, Step: 2216 / 2250 Loss: 0.2757\n",
      "Epoch: 1 / 3, Step: 2217 / 2250 Loss: 0.3718\n",
      "Epoch: 1 / 3, Step: 2218 / 2250 Loss: 0.1721\n",
      "Epoch: 1 / 3, Step: 2219 / 2250 Loss: 0.0436\n",
      "Epoch: 1 / 3, Step: 2220 / 2250 Loss: 0.0694\n",
      "Epoch: 1 / 3, Step: 2221 / 2250 Loss: 0.2165\n",
      "Epoch: 1 / 3, Step: 2222 / 2250 Loss: 0.2717\n",
      "Epoch: 1 / 3, Step: 2223 / 2250 Loss: 0.2002\n",
      "Epoch: 1 / 3, Step: 2224 / 2250 Loss: 0.0581\n",
      "Epoch: 1 / 3, Step: 2225 / 2250 Loss: 0.2473\n",
      "Epoch: 1 / 3, Step: 2226 / 2250 Loss: 0.2144\n",
      "Epoch: 1 / 3, Step: 2227 / 2250 Loss: 0.2082\n",
      "Epoch: 1 / 3, Step: 2228 / 2250 Loss: 0.1086\n",
      "Epoch: 1 / 3, Step: 2229 / 2250 Loss: 0.2446\n",
      "Epoch: 1 / 3, Step: 2230 / 2250 Loss: 0.1202\n",
      "Epoch: 1 / 3, Step: 2231 / 2250 Loss: 0.2118\n",
      "Epoch: 1 / 3, Step: 2232 / 2250 Loss: 0.0912\n",
      "Epoch: 1 / 3, Step: 2233 / 2250 Loss: 0.2520\n",
      "Epoch: 1 / 3, Step: 2234 / 2250 Loss: 0.2090\n",
      "Epoch: 1 / 3, Step: 2235 / 2250 Loss: 0.4000\n",
      "Epoch: 1 / 3, Step: 2236 / 2250 Loss: 0.0556\n",
      "Epoch: 1 / 3, Step: 2237 / 2250 Loss: 0.0729\n",
      "Epoch: 1 / 3, Step: 2238 / 2250 Loss: 0.1900\n",
      "Epoch: 1 / 3, Step: 2239 / 2250 Loss: 0.1758\n",
      "Epoch: 1 / 3, Step: 2240 / 2250 Loss: 0.0968\n",
      "Epoch: 1 / 3, Step: 2241 / 2250 Loss: 0.7727\n",
      "Epoch: 1 / 3, Step: 2242 / 2250 Loss: 0.1228\n",
      "Epoch: 1 / 3, Step: 2243 / 2250 Loss: 0.0759\n",
      "Epoch: 1 / 3, Step: 2244 / 2250 Loss: 0.2568\n",
      "Epoch: 1 / 3, Step: 2245 / 2250 Loss: 0.2373\n",
      "Epoch: 1 / 3, Step: 2246 / 2250 Loss: 0.1004\n",
      "Epoch: 1 / 3, Step: 2247 / 2250 Loss: 0.1220\n",
      "Epoch: 1 / 3, Step: 2248 / 2250 Loss: 0.1475\n",
      "Epoch: 1 / 3, Step: 2249 / 2250 Loss: 0.0842\n",
      "Epoch: 2 / 3, Step: 0 / 2250 Loss: 0.1823\n",
      "Epoch: 2 / 3, Step: 1 / 2250 Loss: 0.1191\n",
      "Epoch: 2 / 3, Step: 2 / 2250 Loss: 0.2767\n",
      "Epoch: 2 / 3, Step: 3 / 2250 Loss: 0.0549\n",
      "Epoch: 2 / 3, Step: 4 / 2250 Loss: 0.1704\n",
      "Epoch: 2 / 3, Step: 5 / 2250 Loss: 0.1888\n",
      "Epoch: 2 / 3, Step: 6 / 2250 Loss: 0.2647\n",
      "Epoch: 2 / 3, Step: 7 / 2250 Loss: 0.1556\n",
      "Epoch: 2 / 3, Step: 8 / 2250 Loss: 0.2045\n",
      "Epoch: 2 / 3, Step: 9 / 2250 Loss: 0.0620\n",
      "Epoch: 2 / 3, Step: 10 / 2250 Loss: 0.1302\n",
      "Epoch: 2 / 3, Step: 11 / 2250 Loss: 0.0926\n",
      "Epoch: 2 / 3, Step: 12 / 2250 Loss: 0.0878\n",
      "Epoch: 2 / 3, Step: 13 / 2250 Loss: 0.0347\n",
      "Epoch: 2 / 3, Step: 14 / 2250 Loss: 0.1410\n",
      "Epoch: 2 / 3, Step: 15 / 2250 Loss: 0.0979\n",
      "Epoch: 2 / 3, Step: 16 / 2250 Loss: 0.2152\n",
      "Epoch: 2 / 3, Step: 17 / 2250 Loss: 0.1659\n",
      "Epoch: 2 / 3, Step: 18 / 2250 Loss: 0.1239\n",
      "Epoch: 2 / 3, Step: 19 / 2250 Loss: 0.0829\n",
      "Epoch: 2 / 3, Step: 20 / 2250 Loss: 0.1774\n",
      "Epoch: 2 / 3, Step: 21 / 2250 Loss: 0.1253\n",
      "Epoch: 2 / 3, Step: 22 / 2250 Loss: 0.2716\n",
      "Epoch: 2 / 3, Step: 23 / 2250 Loss: 0.2498\n",
      "Epoch: 2 / 3, Step: 24 / 2250 Loss: 0.1796\n",
      "Epoch: 2 / 3, Step: 25 / 2250 Loss: 0.3536\n",
      "Epoch: 2 / 3, Step: 26 / 2250 Loss: 0.3079\n",
      "Epoch: 2 / 3, Step: 27 / 2250 Loss: 0.0761\n",
      "Epoch: 2 / 3, Step: 28 / 2250 Loss: 0.4082\n",
      "Epoch: 2 / 3, Step: 29 / 2250 Loss: 0.4585\n",
      "Epoch: 2 / 3, Step: 30 / 2250 Loss: 0.0917\n",
      "Epoch: 2 / 3, Step: 31 / 2250 Loss: 0.2775\n",
      "Epoch: 2 / 3, Step: 32 / 2250 Loss: 0.0577\n",
      "Epoch: 2 / 3, Step: 33 / 2250 Loss: 0.1756\n",
      "Epoch: 2 / 3, Step: 34 / 2250 Loss: 0.1508\n",
      "Epoch: 2 / 3, Step: 35 / 2250 Loss: 0.1189\n",
      "Epoch: 2 / 3, Step: 36 / 2250 Loss: 0.2424\n",
      "Epoch: 2 / 3, Step: 37 / 2250 Loss: 0.2127\n",
      "Epoch: 2 / 3, Step: 38 / 2250 Loss: 0.1667\n",
      "Epoch: 2 / 3, Step: 39 / 2250 Loss: 0.1109\n",
      "Epoch: 2 / 3, Step: 40 / 2250 Loss: 0.1366\n",
      "Epoch: 2 / 3, Step: 41 / 2250 Loss: 0.0251\n",
      "Epoch: 2 / 3, Step: 42 / 2250 Loss: 0.1320\n",
      "Epoch: 2 / 3, Step: 43 / 2250 Loss: 0.2623\n",
      "Epoch: 2 / 3, Step: 44 / 2250 Loss: 0.5296\n",
      "Epoch: 2 / 3, Step: 45 / 2250 Loss: 0.0471\n",
      "Epoch: 2 / 3, Step: 46 / 2250 Loss: 0.0849\n",
      "Epoch: 2 / 3, Step: 47 / 2250 Loss: 0.2111\n",
      "Epoch: 2 / 3, Step: 48 / 2250 Loss: 0.1949\n",
      "Epoch: 2 / 3, Step: 49 / 2250 Loss: 0.0783\n",
      "Epoch: 2 / 3, Step: 50 / 2250 Loss: 0.1336\n",
      "Epoch: 2 / 3, Step: 51 / 2250 Loss: 0.0762\n",
      "Epoch: 2 / 3, Step: 52 / 2250 Loss: 0.0837\n",
      "Epoch: 2 / 3, Step: 53 / 2250 Loss: 0.1054\n",
      "Epoch: 2 / 3, Step: 54 / 2250 Loss: 0.0845\n",
      "Epoch: 2 / 3, Step: 55 / 2250 Loss: 0.0860\n",
      "Epoch: 2 / 3, Step: 56 / 2250 Loss: 0.0848\n",
      "Epoch: 2 / 3, Step: 57 / 2250 Loss: 0.2949\n",
      "Epoch: 2 / 3, Step: 58 / 2250 Loss: 0.0433\n",
      "Epoch: 2 / 3, Step: 59 / 2250 Loss: 0.2136\n",
      "Epoch: 2 / 3, Step: 60 / 2250 Loss: 0.0665\n",
      "Epoch: 2 / 3, Step: 61 / 2250 Loss: 0.2158\n",
      "Epoch: 2 / 3, Step: 62 / 2250 Loss: 0.2685\n",
      "Epoch: 2 / 3, Step: 63 / 2250 Loss: 0.1936\n",
      "Epoch: 2 / 3, Step: 64 / 2250 Loss: 0.0981\n",
      "Epoch: 2 / 3, Step: 65 / 2250 Loss: 0.1194\n",
      "Epoch: 2 / 3, Step: 66 / 2250 Loss: 0.2706\n",
      "Epoch: 2 / 3, Step: 67 / 2250 Loss: 0.1791\n",
      "Epoch: 2 / 3, Step: 68 / 2250 Loss: 0.3850\n",
      "Epoch: 2 / 3, Step: 69 / 2250 Loss: 0.4324\n",
      "Epoch: 2 / 3, Step: 70 / 2250 Loss: 0.0338\n",
      "Epoch: 2 / 3, Step: 71 / 2250 Loss: 0.2354\n",
      "Epoch: 2 / 3, Step: 72 / 2250 Loss: 0.1363\n",
      "Epoch: 2 / 3, Step: 73 / 2250 Loss: 0.0452\n",
      "Epoch: 2 / 3, Step: 74 / 2250 Loss: 0.1007\n",
      "Epoch: 2 / 3, Step: 75 / 2250 Loss: 0.1628\n",
      "Epoch: 2 / 3, Step: 76 / 2250 Loss: 0.0924\n",
      "Epoch: 2 / 3, Step: 77 / 2250 Loss: 0.1025\n",
      "Epoch: 2 / 3, Step: 78 / 2250 Loss: 0.1190\n",
      "Epoch: 2 / 3, Step: 79 / 2250 Loss: 0.0914\n",
      "Epoch: 2 / 3, Step: 80 / 2250 Loss: 0.1391\n",
      "Epoch: 2 / 3, Step: 81 / 2250 Loss: 0.1021\n",
      "Epoch: 2 / 3, Step: 82 / 2250 Loss: 0.0516\n",
      "Epoch: 2 / 3, Step: 83 / 2250 Loss: 0.1128\n",
      "Epoch: 2 / 3, Step: 84 / 2250 Loss: 0.1089\n",
      "Epoch: 2 / 3, Step: 85 / 2250 Loss: 0.4273\n",
      "Epoch: 2 / 3, Step: 86 / 2250 Loss: 0.0943\n",
      "Epoch: 2 / 3, Step: 87 / 2250 Loss: 0.1079\n",
      "Epoch: 2 / 3, Step: 88 / 2250 Loss: 0.0513\n",
      "Epoch: 2 / 3, Step: 89 / 2250 Loss: 0.2185\n",
      "Epoch: 2 / 3, Step: 90 / 2250 Loss: 0.3172\n",
      "Epoch: 2 / 3, Step: 91 / 2250 Loss: 0.0707\n",
      "Epoch: 2 / 3, Step: 92 / 2250 Loss: 0.2068\n",
      "Epoch: 2 / 3, Step: 93 / 2250 Loss: 0.2505\n",
      "Epoch: 2 / 3, Step: 94 / 2250 Loss: 0.0782\n",
      "Epoch: 2 / 3, Step: 95 / 2250 Loss: 0.2017\n",
      "Epoch: 2 / 3, Step: 96 / 2250 Loss: 0.4157\n",
      "Epoch: 2 / 3, Step: 97 / 2250 Loss: 0.0352\n",
      "Epoch: 2 / 3, Step: 98 / 2250 Loss: 0.2906\n",
      "Epoch: 2 / 3, Step: 99 / 2250 Loss: 0.0496\n",
      "Epoch: 2 / 3, Step: 100 / 2250 Loss: 0.0532\n",
      "Epoch: 2 / 3, Step: 101 / 2250 Loss: 0.0746\n",
      "Epoch: 2 / 3, Step: 102 / 2250 Loss: 0.1876\n",
      "Epoch: 2 / 3, Step: 103 / 2250 Loss: 0.1411\n",
      "Epoch: 2 / 3, Step: 104 / 2250 Loss: 0.2352\n",
      "Epoch: 2 / 3, Step: 105 / 2250 Loss: 0.0346\n",
      "Epoch: 2 / 3, Step: 106 / 2250 Loss: 0.1340\n",
      "Epoch: 2 / 3, Step: 107 / 2250 Loss: 0.4687\n",
      "Epoch: 2 / 3, Step: 108 / 2250 Loss: 0.1269\n",
      "Epoch: 2 / 3, Step: 109 / 2250 Loss: 0.1958\n",
      "Epoch: 2 / 3, Step: 110 / 2250 Loss: 0.1166\n",
      "Epoch: 2 / 3, Step: 111 / 2250 Loss: 0.0946\n",
      "Epoch: 2 / 3, Step: 112 / 2250 Loss: 0.1578\n",
      "Epoch: 2 / 3, Step: 113 / 2250 Loss: 0.1089\n",
      "Epoch: 2 / 3, Step: 114 / 2250 Loss: 0.2038\n",
      "Epoch: 2 / 3, Step: 115 / 2250 Loss: 0.2216\n",
      "Epoch: 2 / 3, Step: 116 / 2250 Loss: 0.1642\n",
      "Epoch: 2 / 3, Step: 117 / 2250 Loss: 0.1150\n",
      "Epoch: 2 / 3, Step: 118 / 2250 Loss: 0.0668\n",
      "Epoch: 2 / 3, Step: 119 / 2250 Loss: 0.2072\n",
      "Epoch: 2 / 3, Step: 120 / 2250 Loss: 0.1506\n",
      "Epoch: 2 / 3, Step: 121 / 2250 Loss: 0.1071\n",
      "Epoch: 2 / 3, Step: 122 / 2250 Loss: 0.1395\n",
      "Epoch: 2 / 3, Step: 123 / 2250 Loss: 0.0743\n",
      "Epoch: 2 / 3, Step: 124 / 2250 Loss: 0.0559\n",
      "Epoch: 2 / 3, Step: 125 / 2250 Loss: 0.0823\n",
      "Epoch: 2 / 3, Step: 126 / 2250 Loss: 0.0747\n",
      "Epoch: 2 / 3, Step: 127 / 2250 Loss: 0.1634\n",
      "Epoch: 2 / 3, Step: 128 / 2250 Loss: 0.0680\n",
      "Epoch: 2 / 3, Step: 129 / 2250 Loss: 0.0933\n",
      "Epoch: 2 / 3, Step: 130 / 2250 Loss: 0.1287\n",
      "Epoch: 2 / 3, Step: 131 / 2250 Loss: 0.0816\n",
      "Epoch: 2 / 3, Step: 132 / 2250 Loss: 0.5242\n",
      "Epoch: 2 / 3, Step: 133 / 2250 Loss: 0.1496\n",
      "Epoch: 2 / 3, Step: 134 / 2250 Loss: 0.1581\n",
      "Epoch: 2 / 3, Step: 135 / 2250 Loss: 0.2940\n",
      "Epoch: 2 / 3, Step: 136 / 2250 Loss: 0.1833\n",
      "Epoch: 2 / 3, Step: 137 / 2250 Loss: 0.1391\n",
      "Epoch: 2 / 3, Step: 138 / 2250 Loss: 0.1539\n",
      "Epoch: 2 / 3, Step: 139 / 2250 Loss: 0.0242\n",
      "Epoch: 2 / 3, Step: 140 / 2250 Loss: 0.3604\n",
      "Epoch: 2 / 3, Step: 141 / 2250 Loss: 0.2319\n",
      "Epoch: 2 / 3, Step: 142 / 2250 Loss: 0.0509\n",
      "Epoch: 2 / 3, Step: 143 / 2250 Loss: 0.3031\n",
      "Epoch: 2 / 3, Step: 144 / 2250 Loss: 0.1257\n",
      "Epoch: 2 / 3, Step: 145 / 2250 Loss: 0.2089\n",
      "Epoch: 2 / 3, Step: 146 / 2250 Loss: 0.0415\n",
      "Epoch: 2 / 3, Step: 147 / 2250 Loss: 0.3298\n",
      "Epoch: 2 / 3, Step: 148 / 2250 Loss: 0.2504\n",
      "Epoch: 2 / 3, Step: 149 / 2250 Loss: 0.2031\n",
      "Epoch: 2 / 3, Step: 150 / 2250 Loss: 0.1347\n",
      "Epoch: 2 / 3, Step: 151 / 2250 Loss: 0.2175\n",
      "Epoch: 2 / 3, Step: 152 / 2250 Loss: 0.1430\n",
      "Epoch: 2 / 3, Step: 153 / 2250 Loss: 0.2028\n",
      "Epoch: 2 / 3, Step: 154 / 2250 Loss: 0.2880\n",
      "Epoch: 2 / 3, Step: 155 / 2250 Loss: 0.0664\n",
      "Epoch: 2 / 3, Step: 156 / 2250 Loss: 0.0975\n",
      "Epoch: 2 / 3, Step: 157 / 2250 Loss: 0.0484\n",
      "Epoch: 2 / 3, Step: 158 / 2250 Loss: 0.0984\n",
      "Epoch: 2 / 3, Step: 159 / 2250 Loss: 0.0484\n",
      "Epoch: 2 / 3, Step: 160 / 2250 Loss: 0.1035\n",
      "Epoch: 2 / 3, Step: 161 / 2250 Loss: 0.1922\n",
      "Epoch: 2 / 3, Step: 162 / 2250 Loss: 0.0348\n",
      "Epoch: 2 / 3, Step: 163 / 2250 Loss: 0.0656\n",
      "Epoch: 2 / 3, Step: 164 / 2250 Loss: 0.1968\n",
      "Epoch: 2 / 3, Step: 165 / 2250 Loss: 0.1607\n",
      "Epoch: 2 / 3, Step: 166 / 2250 Loss: 0.2888\n",
      "Epoch: 2 / 3, Step: 167 / 2250 Loss: 0.1837\n",
      "Epoch: 2 / 3, Step: 168 / 2250 Loss: 0.0906\n",
      "Epoch: 2 / 3, Step: 169 / 2250 Loss: 0.2281\n",
      "Epoch: 2 / 3, Step: 170 / 2250 Loss: 0.1905\n",
      "Epoch: 2 / 3, Step: 171 / 2250 Loss: 0.3182\n",
      "Epoch: 2 / 3, Step: 172 / 2250 Loss: 0.2928\n",
      "Epoch: 2 / 3, Step: 173 / 2250 Loss: 0.1158\n",
      "Epoch: 2 / 3, Step: 174 / 2250 Loss: 0.0867\n",
      "Epoch: 2 / 3, Step: 175 / 2250 Loss: 0.0432\n",
      "Epoch: 2 / 3, Step: 176 / 2250 Loss: 0.2276\n",
      "Epoch: 2 / 3, Step: 177 / 2250 Loss: 0.2478\n",
      "Epoch: 2 / 3, Step: 178 / 2250 Loss: 0.2163\n",
      "Epoch: 2 / 3, Step: 179 / 2250 Loss: 0.2744\n",
      "Epoch: 2 / 3, Step: 180 / 2250 Loss: 0.1389\n",
      "Epoch: 2 / 3, Step: 181 / 2250 Loss: 0.2931\n",
      "Epoch: 2 / 3, Step: 182 / 2250 Loss: 0.2504\n",
      "Epoch: 2 / 3, Step: 183 / 2250 Loss: 0.0321\n",
      "Epoch: 2 / 3, Step: 184 / 2250 Loss: 0.1964\n",
      "Epoch: 2 / 3, Step: 185 / 2250 Loss: 0.1489\n",
      "Epoch: 2 / 3, Step: 186 / 2250 Loss: 0.0500\n",
      "Epoch: 2 / 3, Step: 187 / 2250 Loss: 0.1290\n",
      "Epoch: 2 / 3, Step: 188 / 2250 Loss: 0.1017\n",
      "Epoch: 2 / 3, Step: 189 / 2250 Loss: 0.1429\n",
      "Epoch: 2 / 3, Step: 190 / 2250 Loss: 0.1455\n",
      "Epoch: 2 / 3, Step: 191 / 2250 Loss: 0.1089\n",
      "Epoch: 2 / 3, Step: 192 / 2250 Loss: 0.0499\n",
      "Epoch: 2 / 3, Step: 193 / 2250 Loss: 0.0897\n",
      "Epoch: 2 / 3, Step: 194 / 2250 Loss: 0.0311\n",
      "Epoch: 2 / 3, Step: 195 / 2250 Loss: 0.1583\n",
      "Epoch: 2 / 3, Step: 196 / 2250 Loss: 0.2253\n",
      "Epoch: 2 / 3, Step: 197 / 2250 Loss: 0.2643\n",
      "Epoch: 2 / 3, Step: 198 / 2250 Loss: 0.2402\n",
      "Epoch: 2 / 3, Step: 199 / 2250 Loss: 0.3737\n",
      "Epoch: 2 / 3, Step: 200 / 2250 Loss: 0.4756\n",
      "Epoch: 2 / 3, Step: 201 / 2250 Loss: 0.1173\n",
      "Epoch: 2 / 3, Step: 202 / 2250 Loss: 0.2006\n",
      "Epoch: 2 / 3, Step: 203 / 2250 Loss: 0.0696\n",
      "Epoch: 2 / 3, Step: 204 / 2250 Loss: 0.1527\n",
      "Epoch: 2 / 3, Step: 205 / 2250 Loss: 0.0734\n",
      "Epoch: 2 / 3, Step: 206 / 2250 Loss: 0.1600\n",
      "Epoch: 2 / 3, Step: 207 / 2250 Loss: 0.0406\n",
      "Epoch: 2 / 3, Step: 208 / 2250 Loss: 0.0753\n",
      "Epoch: 2 / 3, Step: 209 / 2250 Loss: 0.1753\n",
      "Epoch: 2 / 3, Step: 210 / 2250 Loss: 0.1374\n",
      "Epoch: 2 / 3, Step: 211 / 2250 Loss: 0.0458\n",
      "Epoch: 2 / 3, Step: 212 / 2250 Loss: 0.2294\n",
      "Epoch: 2 / 3, Step: 213 / 2250 Loss: 0.2078\n",
      "Epoch: 2 / 3, Step: 214 / 2250 Loss: 0.2138\n",
      "Epoch: 2 / 3, Step: 215 / 2250 Loss: 0.0901\n",
      "Epoch: 2 / 3, Step: 216 / 2250 Loss: 0.1316\n",
      "Epoch: 2 / 3, Step: 217 / 2250 Loss: 0.4736\n",
      "Epoch: 2 / 3, Step: 218 / 2250 Loss: 0.1729\n",
      "Epoch: 2 / 3, Step: 219 / 2250 Loss: 0.2475\n",
      "Epoch: 2 / 3, Step: 220 / 2250 Loss: 0.0716\n",
      "Epoch: 2 / 3, Step: 221 / 2250 Loss: 0.0636\n",
      "Epoch: 2 / 3, Step: 222 / 2250 Loss: 0.1403\n",
      "Epoch: 2 / 3, Step: 223 / 2250 Loss: 0.1041\n",
      "Epoch: 2 / 3, Step: 224 / 2250 Loss: 0.0908\n",
      "Epoch: 2 / 3, Step: 225 / 2250 Loss: 0.0925\n",
      "Epoch: 2 / 3, Step: 226 / 2250 Loss: 0.1172\n",
      "Epoch: 2 / 3, Step: 227 / 2250 Loss: 0.1027\n",
      "Epoch: 2 / 3, Step: 228 / 2250 Loss: 0.2614\n",
      "Epoch: 2 / 3, Step: 229 / 2250 Loss: 0.1066\n",
      "Epoch: 2 / 3, Step: 230 / 2250 Loss: 0.1185\n",
      "Epoch: 2 / 3, Step: 231 / 2250 Loss: 0.1839\n",
      "Epoch: 2 / 3, Step: 232 / 2250 Loss: 0.1596\n",
      "Epoch: 2 / 3, Step: 233 / 2250 Loss: 0.0563\n",
      "Epoch: 2 / 3, Step: 234 / 2250 Loss: 0.3339\n",
      "Epoch: 2 / 3, Step: 235 / 2250 Loss: 0.3115\n",
      "Epoch: 2 / 3, Step: 236 / 2250 Loss: 0.1272\n",
      "Epoch: 2 / 3, Step: 237 / 2250 Loss: 0.3579\n",
      "Epoch: 2 / 3, Step: 238 / 2250 Loss: 0.1022\n",
      "Epoch: 2 / 3, Step: 239 / 2250 Loss: 0.1155\n",
      "Epoch: 2 / 3, Step: 240 / 2250 Loss: 0.3297\n",
      "Epoch: 2 / 3, Step: 241 / 2250 Loss: 0.2116\n",
      "Epoch: 2 / 3, Step: 242 / 2250 Loss: 0.1640\n",
      "Epoch: 2 / 3, Step: 243 / 2250 Loss: 0.2280\n",
      "Epoch: 2 / 3, Step: 244 / 2250 Loss: 0.1228\n",
      "Epoch: 2 / 3, Step: 245 / 2250 Loss: 0.1050\n",
      "Epoch: 2 / 3, Step: 246 / 2250 Loss: 0.0338\n",
      "Epoch: 2 / 3, Step: 247 / 2250 Loss: 0.4199\n",
      "Epoch: 2 / 3, Step: 248 / 2250 Loss: 0.0507\n",
      "Epoch: 2 / 3, Step: 249 / 2250 Loss: 0.2022\n",
      "Epoch: 2 / 3, Step: 250 / 2250 Loss: 0.1679\n",
      "Epoch: 2 / 3, Step: 251 / 2250 Loss: 0.2952\n",
      "Epoch: 2 / 3, Step: 252 / 2250 Loss: 0.3184\n",
      "Epoch: 2 / 3, Step: 253 / 2250 Loss: 0.2412\n",
      "Epoch: 2 / 3, Step: 254 / 2250 Loss: 0.0607\n",
      "Epoch: 2 / 3, Step: 255 / 2250 Loss: 0.0565\n",
      "Epoch: 2 / 3, Step: 256 / 2250 Loss: 0.1632\n",
      "Epoch: 2 / 3, Step: 257 / 2250 Loss: 0.0843\n",
      "Epoch: 2 / 3, Step: 258 / 2250 Loss: 0.1700\n",
      "Epoch: 2 / 3, Step: 259 / 2250 Loss: 0.1747\n",
      "Epoch: 2 / 3, Step: 260 / 2250 Loss: 0.1441\n",
      "Epoch: 2 / 3, Step: 261 / 2250 Loss: 0.4096\n",
      "Epoch: 2 / 3, Step: 262 / 2250 Loss: 0.1920\n",
      "Epoch: 2 / 3, Step: 263 / 2250 Loss: 0.1447\n",
      "Epoch: 2 / 3, Step: 264 / 2250 Loss: 0.1100\n",
      "Epoch: 2 / 3, Step: 265 / 2250 Loss: 0.3190\n",
      "Epoch: 2 / 3, Step: 266 / 2250 Loss: 0.0754\n",
      "Epoch: 2 / 3, Step: 267 / 2250 Loss: 0.0900\n",
      "Epoch: 2 / 3, Step: 268 / 2250 Loss: 0.3119\n",
      "Epoch: 2 / 3, Step: 269 / 2250 Loss: 0.2710\n",
      "Epoch: 2 / 3, Step: 270 / 2250 Loss: 0.2649\n",
      "Epoch: 2 / 3, Step: 271 / 2250 Loss: 0.0404\n",
      "Epoch: 2 / 3, Step: 272 / 2250 Loss: 0.1669\n",
      "Epoch: 2 / 3, Step: 273 / 2250 Loss: 0.2031\n",
      "Epoch: 2 / 3, Step: 274 / 2250 Loss: 0.1424\n",
      "Epoch: 2 / 3, Step: 275 / 2250 Loss: 0.3284\n",
      "Epoch: 2 / 3, Step: 276 / 2250 Loss: 0.0812\n",
      "Epoch: 2 / 3, Step: 277 / 2250 Loss: 0.1928\n",
      "Epoch: 2 / 3, Step: 278 / 2250 Loss: 0.1106\n",
      "Epoch: 2 / 3, Step: 279 / 2250 Loss: 0.1570\n",
      "Epoch: 2 / 3, Step: 280 / 2250 Loss: 0.0575\n",
      "Epoch: 2 / 3, Step: 281 / 2250 Loss: 0.2228\n",
      "Epoch: 2 / 3, Step: 282 / 2250 Loss: 0.1963\n",
      "Epoch: 2 / 3, Step: 283 / 2250 Loss: 0.2329\n",
      "Epoch: 2 / 3, Step: 284 / 2250 Loss: 0.0671\n",
      "Epoch: 2 / 3, Step: 285 / 2250 Loss: 0.0570\n",
      "Epoch: 2 / 3, Step: 286 / 2250 Loss: 0.0655\n",
      "Epoch: 2 / 3, Step: 287 / 2250 Loss: 0.2752\n",
      "Epoch: 2 / 3, Step: 288 / 2250 Loss: 0.0612\n",
      "Epoch: 2 / 3, Step: 289 / 2250 Loss: 0.1442\n",
      "Epoch: 2 / 3, Step: 290 / 2250 Loss: 0.1081\n",
      "Epoch: 2 / 3, Step: 291 / 2250 Loss: 0.1496\n",
      "Epoch: 2 / 3, Step: 292 / 2250 Loss: 0.2980\n",
      "Epoch: 2 / 3, Step: 293 / 2250 Loss: 0.0364\n",
      "Epoch: 2 / 3, Step: 294 / 2250 Loss: 0.1405\n",
      "Epoch: 2 / 3, Step: 295 / 2250 Loss: 0.0493\n",
      "Epoch: 2 / 3, Step: 296 / 2250 Loss: 0.0583\n",
      "Epoch: 2 / 3, Step: 297 / 2250 Loss: 0.1078\n",
      "Epoch: 2 / 3, Step: 298 / 2250 Loss: 0.0656\n",
      "Epoch: 2 / 3, Step: 299 / 2250 Loss: 0.1091\n",
      "Epoch: 2 / 3, Step: 300 / 2250 Loss: 0.0520\n",
      "Epoch: 2 / 3, Step: 301 / 2250 Loss: 0.4078\n",
      "Epoch: 2 / 3, Step: 302 / 2250 Loss: 0.0814\n",
      "Epoch: 2 / 3, Step: 303 / 2250 Loss: 0.2862\n",
      "Epoch: 2 / 3, Step: 304 / 2250 Loss: 0.0738\n",
      "Epoch: 2 / 3, Step: 305 / 2250 Loss: 0.2260\n",
      "Epoch: 2 / 3, Step: 306 / 2250 Loss: 0.2632\n",
      "Epoch: 2 / 3, Step: 307 / 2250 Loss: 0.1703\n",
      "Epoch: 2 / 3, Step: 308 / 2250 Loss: 0.0242\n",
      "Epoch: 2 / 3, Step: 309 / 2250 Loss: 0.0458\n",
      "Epoch: 2 / 3, Step: 310 / 2250 Loss: 0.1733\n",
      "Epoch: 2 / 3, Step: 311 / 2250 Loss: 0.0324\n",
      "Epoch: 2 / 3, Step: 312 / 2250 Loss: 0.2588\n",
      "Epoch: 2 / 3, Step: 313 / 2250 Loss: 0.0309\n",
      "Epoch: 2 / 3, Step: 314 / 2250 Loss: 0.1854\n",
      "Epoch: 2 / 3, Step: 315 / 2250 Loss: 0.5254\n",
      "Epoch: 2 / 3, Step: 316 / 2250 Loss: 0.1984\n",
      "Epoch: 2 / 3, Step: 317 / 2250 Loss: 0.2098\n",
      "Epoch: 2 / 3, Step: 318 / 2250 Loss: 0.1677\n",
      "Epoch: 2 / 3, Step: 319 / 2250 Loss: 0.2103\n",
      "Epoch: 2 / 3, Step: 320 / 2250 Loss: 0.2812\n",
      "Epoch: 2 / 3, Step: 321 / 2250 Loss: 0.0504\n",
      "Epoch: 2 / 3, Step: 322 / 2250 Loss: 0.0336\n",
      "Epoch: 2 / 3, Step: 323 / 2250 Loss: 0.0520\n",
      "Epoch: 2 / 3, Step: 324 / 2250 Loss: 0.0417\n",
      "Epoch: 2 / 3, Step: 325 / 2250 Loss: 0.2148\n",
      "Epoch: 2 / 3, Step: 326 / 2250 Loss: 0.3511\n",
      "Epoch: 2 / 3, Step: 327 / 2250 Loss: 0.0856\n",
      "Epoch: 2 / 3, Step: 328 / 2250 Loss: 0.0773\n",
      "Epoch: 2 / 3, Step: 329 / 2250 Loss: 0.1265\n",
      "Epoch: 2 / 3, Step: 330 / 2250 Loss: 0.4187\n",
      "Epoch: 2 / 3, Step: 331 / 2250 Loss: 0.1613\n",
      "Epoch: 2 / 3, Step: 332 / 2250 Loss: 0.0857\n",
      "Epoch: 2 / 3, Step: 333 / 2250 Loss: 0.1101\n",
      "Epoch: 2 / 3, Step: 334 / 2250 Loss: 0.2640\n",
      "Epoch: 2 / 3, Step: 335 / 2250 Loss: 0.0238\n",
      "Epoch: 2 / 3, Step: 336 / 2250 Loss: 0.1658\n",
      "Epoch: 2 / 3, Step: 337 / 2250 Loss: 0.3169\n",
      "Epoch: 2 / 3, Step: 338 / 2250 Loss: 0.1692\n",
      "Epoch: 2 / 3, Step: 339 / 2250 Loss: 0.1361\n",
      "Epoch: 2 / 3, Step: 340 / 2250 Loss: 0.3396\n",
      "Epoch: 2 / 3, Step: 341 / 2250 Loss: 0.1988\n",
      "Epoch: 2 / 3, Step: 342 / 2250 Loss: 0.2096\n",
      "Epoch: 2 / 3, Step: 343 / 2250 Loss: 0.2160\n",
      "Epoch: 2 / 3, Step: 344 / 2250 Loss: 0.2301\n",
      "Epoch: 2 / 3, Step: 345 / 2250 Loss: 0.2380\n",
      "Epoch: 2 / 3, Step: 346 / 2250 Loss: 0.1526\n",
      "Epoch: 2 / 3, Step: 347 / 2250 Loss: 0.1493\n",
      "Epoch: 2 / 3, Step: 348 / 2250 Loss: 0.1131\n",
      "Epoch: 2 / 3, Step: 349 / 2250 Loss: 0.0897\n",
      "Epoch: 2 / 3, Step: 350 / 2250 Loss: 0.4254\n",
      "Epoch: 2 / 3, Step: 351 / 2250 Loss: 0.2432\n",
      "Epoch: 2 / 3, Step: 352 / 2250 Loss: 0.3479\n",
      "Epoch: 2 / 3, Step: 353 / 2250 Loss: 0.0675\n",
      "Epoch: 2 / 3, Step: 354 / 2250 Loss: 0.0712\n",
      "Epoch: 2 / 3, Step: 355 / 2250 Loss: 0.3470\n",
      "Epoch: 2 / 3, Step: 356 / 2250 Loss: 0.1025\n",
      "Epoch: 2 / 3, Step: 357 / 2250 Loss: 0.0659\n",
      "Epoch: 2 / 3, Step: 358 / 2250 Loss: 0.1097\n",
      "Epoch: 2 / 3, Step: 359 / 2250 Loss: 0.0827\n",
      "Epoch: 2 / 3, Step: 360 / 2250 Loss: 0.2413\n",
      "Epoch: 2 / 3, Step: 361 / 2250 Loss: 0.3357\n",
      "Epoch: 2 / 3, Step: 362 / 2250 Loss: 0.1336\n",
      "Epoch: 2 / 3, Step: 363 / 2250 Loss: 0.2624\n",
      "Epoch: 2 / 3, Step: 364 / 2250 Loss: 0.2922\n",
      "Epoch: 2 / 3, Step: 365 / 2250 Loss: 0.0491\n",
      "Epoch: 2 / 3, Step: 366 / 2250 Loss: 0.1720\n",
      "Epoch: 2 / 3, Step: 367 / 2250 Loss: 0.1628\n",
      "Epoch: 2 / 3, Step: 368 / 2250 Loss: 0.4101\n",
      "Epoch: 2 / 3, Step: 369 / 2250 Loss: 0.0483\n",
      "Epoch: 2 / 3, Step: 370 / 2250 Loss: 0.0936\n",
      "Epoch: 2 / 3, Step: 371 / 2250 Loss: 0.2865\n",
      "Epoch: 2 / 3, Step: 372 / 2250 Loss: 0.0506\n",
      "Epoch: 2 / 3, Step: 373 / 2250 Loss: 0.1280\n",
      "Epoch: 2 / 3, Step: 374 / 2250 Loss: 0.3355\n",
      "Epoch: 2 / 3, Step: 375 / 2250 Loss: 0.3236\n",
      "Epoch: 2 / 3, Step: 376 / 2250 Loss: 0.3108\n",
      "Epoch: 2 / 3, Step: 377 / 2250 Loss: 0.0795\n",
      "Epoch: 2 / 3, Step: 378 / 2250 Loss: 0.0673\n",
      "Epoch: 2 / 3, Step: 379 / 2250 Loss: 0.2506\n",
      "Epoch: 2 / 3, Step: 380 / 2250 Loss: 0.1864\n",
      "Epoch: 2 / 3, Step: 381 / 2250 Loss: 0.2187\n",
      "Epoch: 2 / 3, Step: 382 / 2250 Loss: 0.2528\n",
      "Epoch: 2 / 3, Step: 383 / 2250 Loss: 0.0277\n",
      "Epoch: 2 / 3, Step: 384 / 2250 Loss: 0.1585\n",
      "Epoch: 2 / 3, Step: 385 / 2250 Loss: 0.1998\n",
      "Epoch: 2 / 3, Step: 386 / 2250 Loss: 0.1260\n",
      "Epoch: 2 / 3, Step: 387 / 2250 Loss: 0.1096\n",
      "Epoch: 2 / 3, Step: 388 / 2250 Loss: 0.1720\n",
      "Epoch: 2 / 3, Step: 389 / 2250 Loss: 0.0265\n",
      "Epoch: 2 / 3, Step: 390 / 2250 Loss: 0.0361\n",
      "Epoch: 2 / 3, Step: 391 / 2250 Loss: 0.2033\n",
      "Epoch: 2 / 3, Step: 392 / 2250 Loss: 0.0888\n",
      "Epoch: 2 / 3, Step: 393 / 2250 Loss: 0.1325\n",
      "Epoch: 2 / 3, Step: 394 / 2250 Loss: 0.0719\n",
      "Epoch: 2 / 3, Step: 395 / 2250 Loss: 0.0733\n",
      "Epoch: 2 / 3, Step: 396 / 2250 Loss: 0.1002\n",
      "Epoch: 2 / 3, Step: 397 / 2250 Loss: 0.1858\n",
      "Epoch: 2 / 3, Step: 398 / 2250 Loss: 0.2193\n",
      "Epoch: 2 / 3, Step: 399 / 2250 Loss: 0.0479\n",
      "Epoch: 2 / 3, Step: 400 / 2250 Loss: 0.1016\n",
      "Epoch: 2 / 3, Step: 401 / 2250 Loss: 0.1497\n",
      "Epoch: 2 / 3, Step: 402 / 2250 Loss: 0.2005\n",
      "Epoch: 2 / 3, Step: 403 / 2250 Loss: 0.2023\n",
      "Epoch: 2 / 3, Step: 404 / 2250 Loss: 0.3138\n",
      "Epoch: 2 / 3, Step: 405 / 2250 Loss: 0.1975\n",
      "Epoch: 2 / 3, Step: 406 / 2250 Loss: 0.1706\n",
      "Epoch: 2 / 3, Step: 407 / 2250 Loss: 0.1573\n",
      "Epoch: 2 / 3, Step: 408 / 2250 Loss: 0.2544\n",
      "Epoch: 2 / 3, Step: 409 / 2250 Loss: 0.0963\n",
      "Epoch: 2 / 3, Step: 410 / 2250 Loss: 0.1966\n",
      "Epoch: 2 / 3, Step: 411 / 2250 Loss: 0.0632\n",
      "Epoch: 2 / 3, Step: 412 / 2250 Loss: 0.1919\n",
      "Epoch: 2 / 3, Step: 413 / 2250 Loss: 0.1120\n",
      "Epoch: 2 / 3, Step: 414 / 2250 Loss: 0.0306\n",
      "Epoch: 2 / 3, Step: 415 / 2250 Loss: 0.2921\n",
      "Epoch: 2 / 3, Step: 416 / 2250 Loss: 0.1202\n",
      "Epoch: 2 / 3, Step: 417 / 2250 Loss: 0.2555\n",
      "Epoch: 2 / 3, Step: 418 / 2250 Loss: 0.1097\n",
      "Epoch: 2 / 3, Step: 419 / 2250 Loss: 0.0443\n",
      "Epoch: 2 / 3, Step: 420 / 2250 Loss: 0.1563\n",
      "Epoch: 2 / 3, Step: 421 / 2250 Loss: 0.3458\n",
      "Epoch: 2 / 3, Step: 422 / 2250 Loss: 0.0912\n",
      "Epoch: 2 / 3, Step: 423 / 2250 Loss: 0.2279\n",
      "Epoch: 2 / 3, Step: 424 / 2250 Loss: 0.0986\n",
      "Epoch: 2 / 3, Step: 425 / 2250 Loss: 0.1152\n",
      "Epoch: 2 / 3, Step: 426 / 2250 Loss: 0.1254\n",
      "Epoch: 2 / 3, Step: 427 / 2250 Loss: 0.1963\n",
      "Epoch: 2 / 3, Step: 428 / 2250 Loss: 0.2098\n",
      "Epoch: 2 / 3, Step: 429 / 2250 Loss: 0.0451\n",
      "Epoch: 2 / 3, Step: 430 / 2250 Loss: 0.0711\n",
      "Epoch: 2 / 3, Step: 431 / 2250 Loss: 0.0576\n",
      "Epoch: 2 / 3, Step: 432 / 2250 Loss: 0.5914\n",
      "Epoch: 2 / 3, Step: 433 / 2250 Loss: 0.0786\n",
      "Epoch: 2 / 3, Step: 434 / 2250 Loss: 0.1075\n",
      "Epoch: 2 / 3, Step: 435 / 2250 Loss: 0.0953\n",
      "Epoch: 2 / 3, Step: 436 / 2250 Loss: 0.3092\n",
      "Epoch: 2 / 3, Step: 437 / 2250 Loss: 0.1863\n",
      "Epoch: 2 / 3, Step: 438 / 2250 Loss: 0.1151\n",
      "Epoch: 2 / 3, Step: 439 / 2250 Loss: 0.2991\n",
      "Epoch: 2 / 3, Step: 440 / 2250 Loss: 0.1237\n",
      "Epoch: 2 / 3, Step: 441 / 2250 Loss: 0.1675\n",
      "Epoch: 2 / 3, Step: 442 / 2250 Loss: 0.2010\n",
      "Epoch: 2 / 3, Step: 443 / 2250 Loss: 0.2573\n",
      "Epoch: 2 / 3, Step: 444 / 2250 Loss: 0.1469\n",
      "Epoch: 2 / 3, Step: 445 / 2250 Loss: 0.0483\n",
      "Epoch: 2 / 3, Step: 446 / 2250 Loss: 0.2349\n",
      "Epoch: 2 / 3, Step: 447 / 2250 Loss: 0.2304\n",
      "Epoch: 2 / 3, Step: 448 / 2250 Loss: 0.2289\n",
      "Epoch: 2 / 3, Step: 449 / 2250 Loss: 0.1121\n",
      "Epoch: 2 / 3, Step: 450 / 2250 Loss: 0.0529\n",
      "Epoch: 2 / 3, Step: 451 / 2250 Loss: 0.2370\n",
      "Epoch: 2 / 3, Step: 452 / 2250 Loss: 0.2236\n",
      "Epoch: 2 / 3, Step: 453 / 2250 Loss: 0.0790\n",
      "Epoch: 2 / 3, Step: 454 / 2250 Loss: 0.1706\n",
      "Epoch: 2 / 3, Step: 455 / 2250 Loss: 0.0833\n",
      "Epoch: 2 / 3, Step: 456 / 2250 Loss: 0.1558\n",
      "Epoch: 2 / 3, Step: 457 / 2250 Loss: 0.3010\n",
      "Epoch: 2 / 3, Step: 458 / 2250 Loss: 0.0438\n",
      "Epoch: 2 / 3, Step: 459 / 2250 Loss: 0.2339\n",
      "Epoch: 2 / 3, Step: 460 / 2250 Loss: 0.0620\n",
      "Epoch: 2 / 3, Step: 461 / 2250 Loss: 0.0843\n",
      "Epoch: 2 / 3, Step: 462 / 2250 Loss: 0.2622\n",
      "Epoch: 2 / 3, Step: 463 / 2250 Loss: 0.3676\n",
      "Epoch: 2 / 3, Step: 464 / 2250 Loss: 0.3553\n",
      "Epoch: 2 / 3, Step: 465 / 2250 Loss: 0.2210\n",
      "Epoch: 2 / 3, Step: 466 / 2250 Loss: 0.0671\n",
      "Epoch: 2 / 3, Step: 467 / 2250 Loss: 0.1761\n",
      "Epoch: 2 / 3, Step: 468 / 2250 Loss: 0.2213\n",
      "Epoch: 2 / 3, Step: 469 / 2250 Loss: 0.0528\n",
      "Epoch: 2 / 3, Step: 470 / 2250 Loss: 0.2365\n",
      "Epoch: 2 / 3, Step: 471 / 2250 Loss: 0.0428\n",
      "Epoch: 2 / 3, Step: 472 / 2250 Loss: 0.0317\n",
      "Epoch: 2 / 3, Step: 473 / 2250 Loss: 0.1151\n",
      "Epoch: 2 / 3, Step: 474 / 2250 Loss: 0.2061\n",
      "Epoch: 2 / 3, Step: 475 / 2250 Loss: 0.0443\n",
      "Epoch: 2 / 3, Step: 476 / 2250 Loss: 0.1924\n",
      "Epoch: 2 / 3, Step: 477 / 2250 Loss: 0.1277\n",
      "Epoch: 2 / 3, Step: 478 / 2250 Loss: 0.1378\n",
      "Epoch: 2 / 3, Step: 479 / 2250 Loss: 0.0616\n",
      "Epoch: 2 / 3, Step: 480 / 2250 Loss: 0.1353\n",
      "Epoch: 2 / 3, Step: 481 / 2250 Loss: 0.1511\n",
      "Epoch: 2 / 3, Step: 482 / 2250 Loss: 0.1517\n",
      "Epoch: 2 / 3, Step: 483 / 2250 Loss: 0.0410\n",
      "Epoch: 2 / 3, Step: 484 / 2250 Loss: 0.1593\n",
      "Epoch: 2 / 3, Step: 485 / 2250 Loss: 0.1822\n",
      "Epoch: 2 / 3, Step: 486 / 2250 Loss: 0.1101\n",
      "Epoch: 2 / 3, Step: 487 / 2250 Loss: 0.3027\n",
      "Epoch: 2 / 3, Step: 488 / 2250 Loss: 0.0350\n",
      "Epoch: 2 / 3, Step: 489 / 2250 Loss: 0.1860\n",
      "Epoch: 2 / 3, Step: 490 / 2250 Loss: 0.0584\n",
      "Epoch: 2 / 3, Step: 491 / 2250 Loss: 0.2192\n",
      "Epoch: 2 / 3, Step: 492 / 2250 Loss: 0.5191\n",
      "Epoch: 2 / 3, Step: 493 / 2250 Loss: 0.0557\n",
      "Epoch: 2 / 3, Step: 494 / 2250 Loss: 0.1190\n",
      "Epoch: 2 / 3, Step: 495 / 2250 Loss: 0.0971\n",
      "Epoch: 2 / 3, Step: 496 / 2250 Loss: 0.3899\n",
      "Epoch: 2 / 3, Step: 497 / 2250 Loss: 0.0596\n",
      "Epoch: 2 / 3, Step: 498 / 2250 Loss: 0.0761\n",
      "Epoch: 2 / 3, Step: 499 / 2250 Loss: 0.4740\n",
      "Epoch: 2 / 3, Step: 500 / 2250 Loss: 0.0651\n",
      "Epoch: 2 / 3, Step: 501 / 2250 Loss: 0.1349\n",
      "Epoch: 2 / 3, Step: 502 / 2250 Loss: 0.3130\n",
      "Epoch: 2 / 3, Step: 503 / 2250 Loss: 0.1631\n",
      "Epoch: 2 / 3, Step: 504 / 2250 Loss: 0.0508\n",
      "Epoch: 2 / 3, Step: 505 / 2250 Loss: 0.0932\n",
      "Epoch: 2 / 3, Step: 506 / 2250 Loss: 0.1913\n",
      "Epoch: 2 / 3, Step: 507 / 2250 Loss: 0.0406\n",
      "Epoch: 2 / 3, Step: 508 / 2250 Loss: 0.1904\n",
      "Epoch: 2 / 3, Step: 509 / 2250 Loss: 0.3125\n",
      "Epoch: 2 / 3, Step: 510 / 2250 Loss: 0.4240\n",
      "Epoch: 2 / 3, Step: 511 / 2250 Loss: 0.0663\n",
      "Epoch: 2 / 3, Step: 512 / 2250 Loss: 0.0741\n",
      "Epoch: 2 / 3, Step: 513 / 2250 Loss: 0.0816\n",
      "Epoch: 2 / 3, Step: 514 / 2250 Loss: 0.1176\n",
      "Epoch: 2 / 3, Step: 515 / 2250 Loss: 0.0665\n",
      "Epoch: 2 / 3, Step: 516 / 2250 Loss: 0.0705\n",
      "Epoch: 2 / 3, Step: 517 / 2250 Loss: 0.3429\n",
      "Epoch: 2 / 3, Step: 518 / 2250 Loss: 0.2057\n",
      "Epoch: 2 / 3, Step: 519 / 2250 Loss: 0.3789\n",
      "Epoch: 2 / 3, Step: 520 / 2250 Loss: 0.2095\n",
      "Epoch: 2 / 3, Step: 521 / 2250 Loss: 0.1100\n",
      "Epoch: 2 / 3, Step: 522 / 2250 Loss: 0.0415\n",
      "Epoch: 2 / 3, Step: 523 / 2250 Loss: 0.2345\n",
      "Epoch: 2 / 3, Step: 524 / 2250 Loss: 0.2514\n",
      "Epoch: 2 / 3, Step: 525 / 2250 Loss: 0.1227\n",
      "Epoch: 2 / 3, Step: 526 / 2250 Loss: 0.2984\n",
      "Epoch: 2 / 3, Step: 527 / 2250 Loss: 0.1355\n",
      "Epoch: 2 / 3, Step: 528 / 2250 Loss: 0.2873\n",
      "Epoch: 2 / 3, Step: 529 / 2250 Loss: 0.1161\n",
      "Epoch: 2 / 3, Step: 530 / 2250 Loss: 0.0342\n",
      "Epoch: 2 / 3, Step: 531 / 2250 Loss: 0.2027\n",
      "Epoch: 2 / 3, Step: 532 / 2250 Loss: 0.0758\n",
      "Epoch: 2 / 3, Step: 533 / 2250 Loss: 0.0664\n",
      "Epoch: 2 / 3, Step: 534 / 2250 Loss: 0.1942\n",
      "Epoch: 2 / 3, Step: 535 / 2250 Loss: 0.0504\n",
      "Epoch: 2 / 3, Step: 536 / 2250 Loss: 0.1775\n",
      "Epoch: 2 / 3, Step: 537 / 2250 Loss: 0.2900\n",
      "Epoch: 2 / 3, Step: 538 / 2250 Loss: 0.1112\n",
      "Epoch: 2 / 3, Step: 539 / 2250 Loss: 0.2628\n",
      "Epoch: 2 / 3, Step: 540 / 2250 Loss: 0.1102\n",
      "Epoch: 2 / 3, Step: 541 / 2250 Loss: 0.0448\n",
      "Epoch: 2 / 3, Step: 542 / 2250 Loss: 0.1741\n",
      "Epoch: 2 / 3, Step: 543 / 2250 Loss: 0.0380\n",
      "Epoch: 2 / 3, Step: 544 / 2250 Loss: 0.1929\n",
      "Epoch: 2 / 3, Step: 545 / 2250 Loss: 0.4088\n",
      "Epoch: 2 / 3, Step: 546 / 2250 Loss: 0.2070\n",
      "Epoch: 2 / 3, Step: 547 / 2250 Loss: 0.1224\n",
      "Epoch: 2 / 3, Step: 548 / 2250 Loss: 0.1339\n",
      "Epoch: 2 / 3, Step: 549 / 2250 Loss: 0.0917\n",
      "Epoch: 2 / 3, Step: 550 / 2250 Loss: 0.2423\n",
      "Epoch: 2 / 3, Step: 551 / 2250 Loss: 0.0477\n",
      "Epoch: 2 / 3, Step: 552 / 2250 Loss: 0.1783\n",
      "Epoch: 2 / 3, Step: 553 / 2250 Loss: 0.0646\n",
      "Epoch: 2 / 3, Step: 554 / 2250 Loss: 0.1457\n",
      "Epoch: 2 / 3, Step: 555 / 2250 Loss: 0.1442\n",
      "Epoch: 2 / 3, Step: 556 / 2250 Loss: 0.0739\n",
      "Epoch: 2 / 3, Step: 557 / 2250 Loss: 0.2006\n",
      "Epoch: 2 / 3, Step: 558 / 2250 Loss: 0.1549\n",
      "Epoch: 2 / 3, Step: 559 / 2250 Loss: 0.1242\n",
      "Epoch: 2 / 3, Step: 560 / 2250 Loss: 0.2423\n",
      "Epoch: 2 / 3, Step: 561 / 2250 Loss: 0.1432\n",
      "Epoch: 2 / 3, Step: 562 / 2250 Loss: 0.0762\n",
      "Epoch: 2 / 3, Step: 563 / 2250 Loss: 0.0484\n",
      "Epoch: 2 / 3, Step: 564 / 2250 Loss: 0.1885\n",
      "Epoch: 2 / 3, Step: 565 / 2250 Loss: 0.2685\n",
      "Epoch: 2 / 3, Step: 566 / 2250 Loss: 0.0268\n",
      "Epoch: 2 / 3, Step: 567 / 2250 Loss: 0.2633\n",
      "Epoch: 2 / 3, Step: 568 / 2250 Loss: 0.0382\n",
      "Epoch: 2 / 3, Step: 569 / 2250 Loss: 0.0653\n",
      "Epoch: 2 / 3, Step: 570 / 2250 Loss: 0.1926\n",
      "Epoch: 2 / 3, Step: 571 / 2250 Loss: 0.2521\n",
      "Epoch: 2 / 3, Step: 572 / 2250 Loss: 0.0759\n",
      "Epoch: 2 / 3, Step: 573 / 2250 Loss: 0.0609\n",
      "Epoch: 2 / 3, Step: 574 / 2250 Loss: 0.2050\n",
      "Epoch: 2 / 3, Step: 575 / 2250 Loss: 0.1222\n",
      "Epoch: 2 / 3, Step: 576 / 2250 Loss: 0.0535\n",
      "Epoch: 2 / 3, Step: 577 / 2250 Loss: 0.0758\n",
      "Epoch: 2 / 3, Step: 578 / 2250 Loss: 0.0514\n",
      "Epoch: 2 / 3, Step: 579 / 2250 Loss: 0.4172\n",
      "Epoch: 2 / 3, Step: 580 / 2250 Loss: 0.1725\n",
      "Epoch: 2 / 3, Step: 581 / 2250 Loss: 0.0333\n",
      "Epoch: 2 / 3, Step: 582 / 2250 Loss: 0.1834\n",
      "Epoch: 2 / 3, Step: 583 / 2250 Loss: 0.0203\n",
      "Epoch: 2 / 3, Step: 584 / 2250 Loss: 0.0235\n",
      "Epoch: 2 / 3, Step: 585 / 2250 Loss: 0.0626\n",
      "Epoch: 2 / 3, Step: 586 / 2250 Loss: 0.1438\n",
      "Epoch: 2 / 3, Step: 587 / 2250 Loss: 0.4146\n",
      "Epoch: 2 / 3, Step: 588 / 2250 Loss: 0.0624\n",
      "Epoch: 2 / 3, Step: 589 / 2250 Loss: 0.0738\n",
      "Epoch: 2 / 3, Step: 590 / 2250 Loss: 0.0924\n",
      "Epoch: 2 / 3, Step: 591 / 2250 Loss: 0.0377\n",
      "Epoch: 2 / 3, Step: 592 / 2250 Loss: 0.2658\n",
      "Epoch: 2 / 3, Step: 593 / 2250 Loss: 0.0367\n",
      "Epoch: 2 / 3, Step: 594 / 2250 Loss: 0.0920\n",
      "Epoch: 2 / 3, Step: 595 / 2250 Loss: 0.0366\n",
      "Epoch: 2 / 3, Step: 596 / 2250 Loss: 0.0218\n",
      "Epoch: 2 / 3, Step: 597 / 2250 Loss: 0.2164\n",
      "Epoch: 2 / 3, Step: 598 / 2250 Loss: 0.0587\n",
      "Epoch: 2 / 3, Step: 599 / 2250 Loss: 0.2000\n",
      "Epoch: 2 / 3, Step: 600 / 2250 Loss: 0.0227\n",
      "Epoch: 2 / 3, Step: 601 / 2250 Loss: 0.0810\n",
      "Epoch: 2 / 3, Step: 602 / 2250 Loss: 0.0482\n",
      "Epoch: 2 / 3, Step: 603 / 2250 Loss: 0.4439\n",
      "Epoch: 2 / 3, Step: 604 / 2250 Loss: 0.1153\n",
      "Epoch: 2 / 3, Step: 605 / 2250 Loss: 0.2818\n",
      "Epoch: 2 / 3, Step: 606 / 2250 Loss: 0.2887\n",
      "Epoch: 2 / 3, Step: 607 / 2250 Loss: 0.2884\n",
      "Epoch: 2 / 3, Step: 608 / 2250 Loss: 0.2538\n",
      "Epoch: 2 / 3, Step: 609 / 2250 Loss: 0.1157\n",
      "Epoch: 2 / 3, Step: 610 / 2250 Loss: 0.0236\n",
      "Epoch: 2 / 3, Step: 611 / 2250 Loss: 0.4875\n",
      "Epoch: 2 / 3, Step: 612 / 2250 Loss: 0.1400\n",
      "Epoch: 2 / 3, Step: 613 / 2250 Loss: 0.2008\n",
      "Epoch: 2 / 3, Step: 614 / 2250 Loss: 0.1198\n",
      "Epoch: 2 / 3, Step: 615 / 2250 Loss: 0.0516\n",
      "Epoch: 2 / 3, Step: 616 / 2250 Loss: 0.2341\n",
      "Epoch: 2 / 3, Step: 617 / 2250 Loss: 0.1023\n",
      "Epoch: 2 / 3, Step: 618 / 2250 Loss: 0.2149\n",
      "Epoch: 2 / 3, Step: 619 / 2250 Loss: 0.0686\n",
      "Epoch: 2 / 3, Step: 620 / 2250 Loss: 0.0473\n",
      "Epoch: 2 / 3, Step: 621 / 2250 Loss: 0.2835\n",
      "Epoch: 2 / 3, Step: 622 / 2250 Loss: 0.2019\n",
      "Epoch: 2 / 3, Step: 623 / 2250 Loss: 0.0352\n",
      "Epoch: 2 / 3, Step: 624 / 2250 Loss: 0.0610\n",
      "Epoch: 2 / 3, Step: 625 / 2250 Loss: 0.0907\n",
      "Epoch: 2 / 3, Step: 626 / 2250 Loss: 0.0342\n",
      "Epoch: 2 / 3, Step: 627 / 2250 Loss: 0.1570\n",
      "Epoch: 2 / 3, Step: 628 / 2250 Loss: 0.4846\n",
      "Epoch: 2 / 3, Step: 629 / 2250 Loss: 0.0412\n",
      "Epoch: 2 / 3, Step: 630 / 2250 Loss: 0.2537\n",
      "Epoch: 2 / 3, Step: 631 / 2250 Loss: 0.0724\n",
      "Epoch: 2 / 3, Step: 632 / 2250 Loss: 0.0817\n",
      "Epoch: 2 / 3, Step: 633 / 2250 Loss: 0.1989\n",
      "Epoch: 2 / 3, Step: 634 / 2250 Loss: 0.0831\n",
      "Epoch: 2 / 3, Step: 635 / 2250 Loss: 0.2027\n",
      "Epoch: 2 / 3, Step: 636 / 2250 Loss: 0.0710\n",
      "Epoch: 2 / 3, Step: 637 / 2250 Loss: 0.3350\n",
      "Epoch: 2 / 3, Step: 638 / 2250 Loss: 0.1132\n",
      "Epoch: 2 / 3, Step: 639 / 2250 Loss: 0.1446\n",
      "Epoch: 2 / 3, Step: 640 / 2250 Loss: 0.0581\n",
      "Epoch: 2 / 3, Step: 641 / 2250 Loss: 0.0821\n",
      "Epoch: 2 / 3, Step: 642 / 2250 Loss: 0.1437\n",
      "Epoch: 2 / 3, Step: 643 / 2250 Loss: 0.0442\n",
      "Epoch: 2 / 3, Step: 644 / 2250 Loss: 0.0287\n",
      "Epoch: 2 / 3, Step: 645 / 2250 Loss: 0.2340\n",
      "Epoch: 2 / 3, Step: 646 / 2250 Loss: 0.0605\n",
      "Epoch: 2 / 3, Step: 647 / 2250 Loss: 0.0767\n",
      "Epoch: 2 / 3, Step: 648 / 2250 Loss: 0.2422\n",
      "Epoch: 2 / 3, Step: 649 / 2250 Loss: 0.1844\n",
      "Epoch: 2 / 3, Step: 650 / 2250 Loss: 0.1078\n",
      "Epoch: 2 / 3, Step: 651 / 2250 Loss: 0.1196\n",
      "Epoch: 2 / 3, Step: 652 / 2250 Loss: 0.3284\n",
      "Epoch: 2 / 3, Step: 653 / 2250 Loss: 0.0473\n",
      "Epoch: 2 / 3, Step: 654 / 2250 Loss: 0.2868\n",
      "Epoch: 2 / 3, Step: 655 / 2250 Loss: 0.0515\n",
      "Epoch: 2 / 3, Step: 656 / 2250 Loss: 0.0296\n",
      "Epoch: 2 / 3, Step: 657 / 2250 Loss: 0.0214\n",
      "Epoch: 2 / 3, Step: 658 / 2250 Loss: 0.0496\n",
      "Epoch: 2 / 3, Step: 659 / 2250 Loss: 0.0651\n",
      "Epoch: 2 / 3, Step: 660 / 2250 Loss: 0.0194\n",
      "Epoch: 2 / 3, Step: 661 / 2250 Loss: 0.0584\n",
      "Epoch: 2 / 3, Step: 662 / 2250 Loss: 0.2161\n",
      "Epoch: 2 / 3, Step: 663 / 2250 Loss: 0.2789\n",
      "Epoch: 2 / 3, Step: 664 / 2250 Loss: 0.2208\n",
      "Epoch: 2 / 3, Step: 665 / 2250 Loss: 0.1616\n",
      "Epoch: 2 / 3, Step: 666 / 2250 Loss: 0.2124\n",
      "Epoch: 2 / 3, Step: 667 / 2250 Loss: 0.0513\n",
      "Epoch: 2 / 3, Step: 668 / 2250 Loss: 0.0687\n",
      "Epoch: 2 / 3, Step: 669 / 2250 Loss: 0.1773\n",
      "Epoch: 2 / 3, Step: 670 / 2250 Loss: 0.1225\n",
      "Epoch: 2 / 3, Step: 671 / 2250 Loss: 0.1632\n",
      "Epoch: 2 / 3, Step: 672 / 2250 Loss: 0.2744\n",
      "Epoch: 2 / 3, Step: 673 / 2250 Loss: 0.0908\n",
      "Epoch: 2 / 3, Step: 674 / 2250 Loss: 0.1044\n",
      "Epoch: 2 / 3, Step: 675 / 2250 Loss: 0.3686\n",
      "Epoch: 2 / 3, Step: 676 / 2250 Loss: 0.1683\n",
      "Epoch: 2 / 3, Step: 677 / 2250 Loss: 0.0741\n",
      "Epoch: 2 / 3, Step: 678 / 2250 Loss: 0.0485\n",
      "Epoch: 2 / 3, Step: 679 / 2250 Loss: 0.0537\n",
      "Epoch: 2 / 3, Step: 680 / 2250 Loss: 0.1170\n",
      "Epoch: 2 / 3, Step: 681 / 2250 Loss: 0.0398\n",
      "Epoch: 2 / 3, Step: 682 / 2250 Loss: 0.1157\n",
      "Epoch: 2 / 3, Step: 683 / 2250 Loss: 0.5700\n",
      "Epoch: 2 / 3, Step: 684 / 2250 Loss: 0.2744\n",
      "Epoch: 2 / 3, Step: 685 / 2250 Loss: 0.0460\n",
      "Epoch: 2 / 3, Step: 686 / 2250 Loss: 0.1445\n",
      "Epoch: 2 / 3, Step: 687 / 2250 Loss: 0.2227\n",
      "Epoch: 2 / 3, Step: 688 / 2250 Loss: 0.2096\n",
      "Epoch: 2 / 3, Step: 689 / 2250 Loss: 0.1388\n",
      "Epoch: 2 / 3, Step: 690 / 2250 Loss: 0.0728\n",
      "Epoch: 2 / 3, Step: 691 / 2250 Loss: 0.1288\n",
      "Epoch: 2 / 3, Step: 692 / 2250 Loss: 0.1183\n",
      "Epoch: 2 / 3, Step: 693 / 2250 Loss: 0.0834\n",
      "Epoch: 2 / 3, Step: 694 / 2250 Loss: 0.0514\n",
      "Epoch: 2 / 3, Step: 695 / 2250 Loss: 0.0881\n",
      "Epoch: 2 / 3, Step: 696 / 2250 Loss: 0.0453\n",
      "Epoch: 2 / 3, Step: 697 / 2250 Loss: 0.0893\n",
      "Epoch: 2 / 3, Step: 698 / 2250 Loss: 0.0729\n",
      "Epoch: 2 / 3, Step: 699 / 2250 Loss: 0.0463\n",
      "Epoch: 2 / 3, Step: 700 / 2250 Loss: 0.0373\n",
      "Epoch: 2 / 3, Step: 701 / 2250 Loss: 0.2297\n",
      "Epoch: 2 / 3, Step: 702 / 2250 Loss: 0.0909\n",
      "Epoch: 2 / 3, Step: 703 / 2250 Loss: 0.0506\n",
      "Epoch: 2 / 3, Step: 704 / 2250 Loss: 0.0807\n",
      "Epoch: 2 / 3, Step: 705 / 2250 Loss: 0.0355\n",
      "Epoch: 2 / 3, Step: 706 / 2250 Loss: 0.0355\n",
      "Epoch: 2 / 3, Step: 707 / 2250 Loss: 0.0730\n",
      "Epoch: 2 / 3, Step: 708 / 2250 Loss: 0.1715\n",
      "Epoch: 2 / 3, Step: 709 / 2250 Loss: 0.1523\n",
      "Epoch: 2 / 3, Step: 710 / 2250 Loss: 0.1013\n",
      "Epoch: 2 / 3, Step: 711 / 2250 Loss: 0.0660\n",
      "Epoch: 2 / 3, Step: 712 / 2250 Loss: 0.0203\n",
      "Epoch: 2 / 3, Step: 713 / 2250 Loss: 0.1406\n",
      "Epoch: 2 / 3, Step: 714 / 2250 Loss: 0.4050\n",
      "Epoch: 2 / 3, Step: 715 / 2250 Loss: 0.0569\n",
      "Epoch: 2 / 3, Step: 716 / 2250 Loss: 0.3344\n",
      "Epoch: 2 / 3, Step: 717 / 2250 Loss: 0.1096\n",
      "Epoch: 2 / 3, Step: 718 / 2250 Loss: 0.1463\n",
      "Epoch: 2 / 3, Step: 719 / 2250 Loss: 0.0335\n",
      "Epoch: 2 / 3, Step: 720 / 2250 Loss: 0.0200\n",
      "Epoch: 2 / 3, Step: 721 / 2250 Loss: 0.0195\n",
      "Epoch: 2 / 3, Step: 722 / 2250 Loss: 0.0759\n",
      "Epoch: 2 / 3, Step: 723 / 2250 Loss: 0.0481\n",
      "Epoch: 2 / 3, Step: 724 / 2250 Loss: 0.2641\n",
      "Epoch: 2 / 3, Step: 725 / 2250 Loss: 0.0591\n",
      "Epoch: 2 / 3, Step: 726 / 2250 Loss: 0.0341\n",
      "Epoch: 2 / 3, Step: 727 / 2250 Loss: 0.1167\n",
      "Epoch: 2 / 3, Step: 728 / 2250 Loss: 0.1524\n",
      "Epoch: 2 / 3, Step: 729 / 2250 Loss: 0.1551\n",
      "Epoch: 2 / 3, Step: 730 / 2250 Loss: 0.4176\n",
      "Epoch: 2 / 3, Step: 731 / 2250 Loss: 0.1564\n",
      "Epoch: 2 / 3, Step: 732 / 2250 Loss: 0.2855\n",
      "Epoch: 2 / 3, Step: 733 / 2250 Loss: 0.0523\n",
      "Epoch: 2 / 3, Step: 734 / 2250 Loss: 0.0405\n",
      "Epoch: 2 / 3, Step: 735 / 2250 Loss: 0.5572\n",
      "Epoch: 2 / 3, Step: 736 / 2250 Loss: 0.2242\n",
      "Epoch: 2 / 3, Step: 737 / 2250 Loss: 0.0282\n",
      "Epoch: 2 / 3, Step: 738 / 2250 Loss: 0.0397\n",
      "Epoch: 2 / 3, Step: 739 / 2250 Loss: 0.0353\n",
      "Epoch: 2 / 3, Step: 740 / 2250 Loss: 0.1046\n",
      "Epoch: 2 / 3, Step: 741 / 2250 Loss: 0.1382\n",
      "Epoch: 2 / 3, Step: 742 / 2250 Loss: 0.2050\n",
      "Epoch: 2 / 3, Step: 743 / 2250 Loss: 0.0604\n",
      "Epoch: 2 / 3, Step: 744 / 2250 Loss: 0.1031\n",
      "Epoch: 2 / 3, Step: 745 / 2250 Loss: 0.3396\n",
      "Epoch: 2 / 3, Step: 746 / 2250 Loss: 0.2300\n",
      "Epoch: 2 / 3, Step: 747 / 2250 Loss: 0.0200\n",
      "Epoch: 2 / 3, Step: 748 / 2250 Loss: 0.2284\n",
      "Epoch: 2 / 3, Step: 749 / 2250 Loss: 0.0249\n",
      "Epoch: 2 / 3, Step: 750 / 2250 Loss: 0.1296\n",
      "Epoch: 2 / 3, Step: 751 / 2250 Loss: 0.0602\n",
      "Epoch: 2 / 3, Step: 752 / 2250 Loss: 0.1514\n",
      "Epoch: 2 / 3, Step: 753 / 2250 Loss: 0.0429\n",
      "Epoch: 2 / 3, Step: 754 / 2250 Loss: 0.2140\n",
      "Epoch: 2 / 3, Step: 755 / 2250 Loss: 0.0455\n",
      "Epoch: 2 / 3, Step: 756 / 2250 Loss: 0.0462\n",
      "Epoch: 2 / 3, Step: 757 / 2250 Loss: 0.2037\n",
      "Epoch: 2 / 3, Step: 758 / 2250 Loss: 0.1891\n",
      "Epoch: 2 / 3, Step: 759 / 2250 Loss: 0.6312\n",
      "Epoch: 2 / 3, Step: 760 / 2250 Loss: 0.1399\n",
      "Epoch: 2 / 3, Step: 761 / 2250 Loss: 0.0797\n",
      "Epoch: 2 / 3, Step: 762 / 2250 Loss: 0.0551\n",
      "Epoch: 2 / 3, Step: 763 / 2250 Loss: 0.4278\n",
      "Epoch: 2 / 3, Step: 764 / 2250 Loss: 0.0982\n",
      "Epoch: 2 / 3, Step: 765 / 2250 Loss: 0.1284\n",
      "Epoch: 2 / 3, Step: 766 / 2250 Loss: 0.5330\n",
      "Epoch: 2 / 3, Step: 767 / 2250 Loss: 0.1709\n",
      "Epoch: 2 / 3, Step: 768 / 2250 Loss: 0.3472\n",
      "Epoch: 2 / 3, Step: 769 / 2250 Loss: 0.2151\n",
      "Epoch: 2 / 3, Step: 770 / 2250 Loss: 0.0570\n",
      "Epoch: 2 / 3, Step: 771 / 2250 Loss: 0.0538\n",
      "Epoch: 2 / 3, Step: 772 / 2250 Loss: 0.0672\n",
      "Epoch: 2 / 3, Step: 773 / 2250 Loss: 0.1026\n",
      "Epoch: 2 / 3, Step: 774 / 2250 Loss: 0.1253\n",
      "Epoch: 2 / 3, Step: 775 / 2250 Loss: 0.2797\n",
      "Epoch: 2 / 3, Step: 776 / 2250 Loss: 0.0797\n",
      "Epoch: 2 / 3, Step: 777 / 2250 Loss: 0.3007\n",
      "Epoch: 2 / 3, Step: 778 / 2250 Loss: 0.2040\n",
      "Epoch: 2 / 3, Step: 779 / 2250 Loss: 0.0668\n",
      "Epoch: 2 / 3, Step: 780 / 2250 Loss: 0.2663\n",
      "Epoch: 2 / 3, Step: 781 / 2250 Loss: 0.1032\n",
      "Epoch: 2 / 3, Step: 782 / 2250 Loss: 0.1170\n",
      "Epoch: 2 / 3, Step: 783 / 2250 Loss: 0.0786\n",
      "Epoch: 2 / 3, Step: 784 / 2250 Loss: 0.2345\n",
      "Epoch: 2 / 3, Step: 785 / 2250 Loss: 0.2920\n",
      "Epoch: 2 / 3, Step: 786 / 2250 Loss: 0.0811\n",
      "Epoch: 2 / 3, Step: 787 / 2250 Loss: 0.1077\n",
      "Epoch: 2 / 3, Step: 788 / 2250 Loss: 0.1266\n",
      "Epoch: 2 / 3, Step: 789 / 2250 Loss: 0.1319\n",
      "Epoch: 2 / 3, Step: 790 / 2250 Loss: 0.0729\n",
      "Epoch: 2 / 3, Step: 791 / 2250 Loss: 0.0935\n",
      "Epoch: 2 / 3, Step: 792 / 2250 Loss: 0.0539\n",
      "Epoch: 2 / 3, Step: 793 / 2250 Loss: 0.0855\n",
      "Epoch: 2 / 3, Step: 794 / 2250 Loss: 0.1128\n",
      "Epoch: 2 / 3, Step: 795 / 2250 Loss: 0.0944\n",
      "Epoch: 2 / 3, Step: 796 / 2250 Loss: 0.0506\n",
      "Epoch: 2 / 3, Step: 797 / 2250 Loss: 0.1818\n",
      "Epoch: 2 / 3, Step: 798 / 2250 Loss: 0.2830\n",
      "Epoch: 2 / 3, Step: 799 / 2250 Loss: 0.2673\n",
      "Epoch: 2 / 3, Step: 800 / 2250 Loss: 0.1935\n",
      "Epoch: 2 / 3, Step: 801 / 2250 Loss: 0.0831\n",
      "Epoch: 2 / 3, Step: 802 / 2250 Loss: 0.1799\n",
      "Epoch: 2 / 3, Step: 803 / 2250 Loss: 0.1941\n",
      "Epoch: 2 / 3, Step: 804 / 2250 Loss: 0.0646\n",
      "Epoch: 2 / 3, Step: 805 / 2250 Loss: 0.1976\n",
      "Epoch: 2 / 3, Step: 806 / 2250 Loss: 0.0482\n",
      "Epoch: 2 / 3, Step: 807 / 2250 Loss: 0.1342\n",
      "Epoch: 2 / 3, Step: 808 / 2250 Loss: 0.1496\n",
      "Epoch: 2 / 3, Step: 809 / 2250 Loss: 0.0495\n",
      "Epoch: 2 / 3, Step: 810 / 2250 Loss: 0.2192\n",
      "Epoch: 2 / 3, Step: 811 / 2250 Loss: 0.0704\n",
      "Epoch: 2 / 3, Step: 812 / 2250 Loss: 0.1540\n",
      "Epoch: 2 / 3, Step: 813 / 2250 Loss: 0.0653\n",
      "Epoch: 2 / 3, Step: 814 / 2250 Loss: 0.1832\n",
      "Epoch: 2 / 3, Step: 815 / 2250 Loss: 0.1813\n",
      "Epoch: 2 / 3, Step: 816 / 2250 Loss: 0.0788\n",
      "Epoch: 2 / 3, Step: 817 / 2250 Loss: 0.2497\n",
      "Epoch: 2 / 3, Step: 818 / 2250 Loss: 0.0448\n",
      "Epoch: 2 / 3, Step: 819 / 2250 Loss: 0.0586\n",
      "Epoch: 2 / 3, Step: 820 / 2250 Loss: 0.1218\n",
      "Epoch: 2 / 3, Step: 821 / 2250 Loss: 0.0967\n",
      "Epoch: 2 / 3, Step: 822 / 2250 Loss: 0.1813\n",
      "Epoch: 2 / 3, Step: 823 / 2250 Loss: 0.0538\n",
      "Epoch: 2 / 3, Step: 824 / 2250 Loss: 0.1880\n",
      "Epoch: 2 / 3, Step: 825 / 2250 Loss: 0.1881\n",
      "Epoch: 2 / 3, Step: 826 / 2250 Loss: 0.1737\n",
      "Epoch: 2 / 3, Step: 827 / 2250 Loss: 0.2327\n",
      "Epoch: 2 / 3, Step: 828 / 2250 Loss: 0.1356\n",
      "Epoch: 2 / 3, Step: 829 / 2250 Loss: 0.1106\n",
      "Epoch: 2 / 3, Step: 830 / 2250 Loss: 0.0606\n",
      "Epoch: 2 / 3, Step: 831 / 2250 Loss: 0.1736\n",
      "Epoch: 2 / 3, Step: 832 / 2250 Loss: 0.0924\n",
      "Epoch: 2 / 3, Step: 833 / 2250 Loss: 0.0892\n",
      "Epoch: 2 / 3, Step: 834 / 2250 Loss: 0.1300\n",
      "Epoch: 2 / 3, Step: 835 / 2250 Loss: 0.2006\n",
      "Epoch: 2 / 3, Step: 836 / 2250 Loss: 0.0519\n",
      "Epoch: 2 / 3, Step: 837 / 2250 Loss: 0.3110\n",
      "Epoch: 2 / 3, Step: 838 / 2250 Loss: 0.0460\n",
      "Epoch: 2 / 3, Step: 839 / 2250 Loss: 0.0946\n",
      "Epoch: 2 / 3, Step: 840 / 2250 Loss: 0.2310\n",
      "Epoch: 2 / 3, Step: 841 / 2250 Loss: 0.0415\n",
      "Epoch: 2 / 3, Step: 842 / 2250 Loss: 0.1346\n",
      "Epoch: 2 / 3, Step: 843 / 2250 Loss: 0.2762\n",
      "Epoch: 2 / 3, Step: 844 / 2250 Loss: 0.4343\n",
      "Epoch: 2 / 3, Step: 845 / 2250 Loss: 0.0479\n",
      "Epoch: 2 / 3, Step: 846 / 2250 Loss: 0.0829\n",
      "Epoch: 2 / 3, Step: 847 / 2250 Loss: 0.0728\n",
      "Epoch: 2 / 3, Step: 848 / 2250 Loss: 0.0589\n",
      "Epoch: 2 / 3, Step: 849 / 2250 Loss: 0.3044\n",
      "Epoch: 2 / 3, Step: 850 / 2250 Loss: 0.0882\n",
      "Epoch: 2 / 3, Step: 851 / 2250 Loss: 0.2389\n",
      "Epoch: 2 / 3, Step: 852 / 2250 Loss: 0.4425\n",
      "Epoch: 2 / 3, Step: 853 / 2250 Loss: 0.0552\n",
      "Epoch: 2 / 3, Step: 854 / 2250 Loss: 0.2055\n",
      "Epoch: 2 / 3, Step: 855 / 2250 Loss: 0.0667\n",
      "Epoch: 2 / 3, Step: 856 / 2250 Loss: 0.0803\n",
      "Epoch: 2 / 3, Step: 857 / 2250 Loss: 0.2037\n",
      "Epoch: 2 / 3, Step: 858 / 2250 Loss: 0.0756\n",
      "Epoch: 2 / 3, Step: 859 / 2250 Loss: 0.0700\n",
      "Epoch: 2 / 3, Step: 860 / 2250 Loss: 0.1235\n",
      "Epoch: 2 / 3, Step: 861 / 2250 Loss: 0.1221\n",
      "Epoch: 2 / 3, Step: 862 / 2250 Loss: 0.0347\n",
      "Epoch: 2 / 3, Step: 863 / 2250 Loss: 0.1734\n",
      "Epoch: 2 / 3, Step: 864 / 2250 Loss: 0.1963\n",
      "Epoch: 2 / 3, Step: 865 / 2250 Loss: 0.2695\n",
      "Epoch: 2 / 3, Step: 866 / 2250 Loss: 0.4889\n",
      "Epoch: 2 / 3, Step: 867 / 2250 Loss: 0.0287\n",
      "Epoch: 2 / 3, Step: 868 / 2250 Loss: 0.0315\n",
      "Epoch: 2 / 3, Step: 869 / 2250 Loss: 0.0702\n",
      "Epoch: 2 / 3, Step: 870 / 2250 Loss: 0.1314\n",
      "Epoch: 2 / 3, Step: 871 / 2250 Loss: 0.1755\n",
      "Epoch: 2 / 3, Step: 872 / 2250 Loss: 0.1825\n",
      "Epoch: 2 / 3, Step: 873 / 2250 Loss: 0.2481\n",
      "Epoch: 2 / 3, Step: 874 / 2250 Loss: 0.1105\n",
      "Epoch: 2 / 3, Step: 875 / 2250 Loss: 0.0315\n",
      "Epoch: 2 / 3, Step: 876 / 2250 Loss: 0.1203\n",
      "Epoch: 2 / 3, Step: 877 / 2250 Loss: 0.0449\n",
      "Epoch: 2 / 3, Step: 878 / 2250 Loss: 0.3431\n",
      "Epoch: 2 / 3, Step: 879 / 2250 Loss: 0.0890\n",
      "Epoch: 2 / 3, Step: 880 / 2250 Loss: 0.2657\n",
      "Epoch: 2 / 3, Step: 881 / 2250 Loss: 0.1851\n",
      "Epoch: 2 / 3, Step: 882 / 2250 Loss: 0.1210\n",
      "Epoch: 2 / 3, Step: 883 / 2250 Loss: 0.2395\n",
      "Epoch: 2 / 3, Step: 884 / 2250 Loss: 0.0634\n",
      "Epoch: 2 / 3, Step: 885 / 2250 Loss: 0.0370\n",
      "Epoch: 2 / 3, Step: 886 / 2250 Loss: 0.1568\n",
      "Epoch: 2 / 3, Step: 887 / 2250 Loss: 0.0367\n",
      "Epoch: 2 / 3, Step: 888 / 2250 Loss: 0.1268\n",
      "Epoch: 2 / 3, Step: 889 / 2250 Loss: 0.2363\n",
      "Epoch: 2 / 3, Step: 890 / 2250 Loss: 0.3621\n",
      "Epoch: 2 / 3, Step: 891 / 2250 Loss: 0.1777\n",
      "Epoch: 2 / 3, Step: 892 / 2250 Loss: 0.1294\n",
      "Epoch: 2 / 3, Step: 893 / 2250 Loss: 0.0956\n",
      "Epoch: 2 / 3, Step: 894 / 2250 Loss: 0.2243\n",
      "Epoch: 2 / 3, Step: 895 / 2250 Loss: 0.1650\n",
      "Epoch: 2 / 3, Step: 896 / 2250 Loss: 0.0914\n",
      "Epoch: 2 / 3, Step: 897 / 2250 Loss: 0.0175\n",
      "Epoch: 2 / 3, Step: 898 / 2250 Loss: 0.2168\n",
      "Epoch: 2 / 3, Step: 899 / 2250 Loss: 0.1936\n",
      "Epoch: 2 / 3, Step: 900 / 2250 Loss: 0.0449\n",
      "Epoch: 2 / 3, Step: 901 / 2250 Loss: 0.1828\n",
      "Epoch: 2 / 3, Step: 902 / 2250 Loss: 0.1120\n",
      "Epoch: 2 / 3, Step: 903 / 2250 Loss: 0.0855\n",
      "Epoch: 2 / 3, Step: 904 / 2250 Loss: 0.0640\n",
      "Epoch: 2 / 3, Step: 905 / 2250 Loss: 0.3238\n",
      "Epoch: 2 / 3, Step: 906 / 2250 Loss: 0.3392\n",
      "Epoch: 2 / 3, Step: 907 / 2250 Loss: 0.1764\n",
      "Epoch: 2 / 3, Step: 908 / 2250 Loss: 0.5774\n",
      "Epoch: 2 / 3, Step: 909 / 2250 Loss: 0.0875\n",
      "Epoch: 2 / 3, Step: 910 / 2250 Loss: 0.1575\n",
      "Epoch: 2 / 3, Step: 911 / 2250 Loss: 0.0841\n",
      "Epoch: 2 / 3, Step: 912 / 2250 Loss: 0.0420\n",
      "Epoch: 2 / 3, Step: 913 / 2250 Loss: 0.0740\n",
      "Epoch: 2 / 3, Step: 914 / 2250 Loss: 0.4553\n",
      "Epoch: 2 / 3, Step: 915 / 2250 Loss: 0.0296\n",
      "Epoch: 2 / 3, Step: 916 / 2250 Loss: 0.1221\n",
      "Epoch: 2 / 3, Step: 917 / 2250 Loss: 0.0509\n",
      "Epoch: 2 / 3, Step: 918 / 2250 Loss: 0.2741\n",
      "Epoch: 2 / 3, Step: 919 / 2250 Loss: 0.2811\n",
      "Epoch: 2 / 3, Step: 920 / 2250 Loss: 0.1706\n",
      "Epoch: 2 / 3, Step: 921 / 2250 Loss: 0.1022\n",
      "Epoch: 2 / 3, Step: 922 / 2250 Loss: 0.0609\n",
      "Epoch: 2 / 3, Step: 923 / 2250 Loss: 0.2303\n",
      "Epoch: 2 / 3, Step: 924 / 2250 Loss: 0.0377\n",
      "Epoch: 2 / 3, Step: 925 / 2250 Loss: 0.0621\n",
      "Epoch: 2 / 3, Step: 926 / 2250 Loss: 0.1084\n",
      "Epoch: 2 / 3, Step: 927 / 2250 Loss: 0.1162\n",
      "Epoch: 2 / 3, Step: 928 / 2250 Loss: 0.0338\n",
      "Epoch: 2 / 3, Step: 929 / 2250 Loss: 0.1196\n",
      "Epoch: 2 / 3, Step: 930 / 2250 Loss: 0.0525\n",
      "Epoch: 2 / 3, Step: 931 / 2250 Loss: 0.1086\n",
      "Epoch: 2 / 3, Step: 932 / 2250 Loss: 0.0679\n",
      "Epoch: 2 / 3, Step: 933 / 2250 Loss: 0.0914\n",
      "Epoch: 2 / 3, Step: 934 / 2250 Loss: 0.1877\n",
      "Epoch: 2 / 3, Step: 935 / 2250 Loss: 0.1562\n",
      "Epoch: 2 / 3, Step: 936 / 2250 Loss: 0.0547\n",
      "Epoch: 2 / 3, Step: 937 / 2250 Loss: 0.2889\n",
      "Epoch: 2 / 3, Step: 938 / 2250 Loss: 0.1800\n",
      "Epoch: 2 / 3, Step: 939 / 2250 Loss: 0.1970\n",
      "Epoch: 2 / 3, Step: 940 / 2250 Loss: 0.0849\n",
      "Epoch: 2 / 3, Step: 941 / 2250 Loss: 0.2035\n",
      "Epoch: 2 / 3, Step: 942 / 2250 Loss: 0.1572\n",
      "Epoch: 2 / 3, Step: 943 / 2250 Loss: 0.1150\n",
      "Epoch: 2 / 3, Step: 944 / 2250 Loss: 0.1538\n",
      "Epoch: 2 / 3, Step: 945 / 2250 Loss: 0.0561\n",
      "Epoch: 2 / 3, Step: 946 / 2250 Loss: 0.3005\n",
      "Epoch: 2 / 3, Step: 947 / 2250 Loss: 0.1798\n",
      "Epoch: 2 / 3, Step: 948 / 2250 Loss: 0.1428\n",
      "Epoch: 2 / 3, Step: 949 / 2250 Loss: 0.1737\n",
      "Epoch: 2 / 3, Step: 950 / 2250 Loss: 0.2485\n",
      "Epoch: 2 / 3, Step: 951 / 2250 Loss: 0.0252\n",
      "Epoch: 2 / 3, Step: 952 / 2250 Loss: 0.1493\n",
      "Epoch: 2 / 3, Step: 953 / 2250 Loss: 0.1291\n",
      "Epoch: 2 / 3, Step: 954 / 2250 Loss: 0.0780\n",
      "Epoch: 2 / 3, Step: 955 / 2250 Loss: 0.2795\n",
      "Epoch: 2 / 3, Step: 956 / 2250 Loss: 0.0713\n",
      "Epoch: 2 / 3, Step: 957 / 2250 Loss: 0.1313\n",
      "Epoch: 2 / 3, Step: 958 / 2250 Loss: 0.1971\n",
      "Epoch: 2 / 3, Step: 959 / 2250 Loss: 0.1198\n",
      "Epoch: 2 / 3, Step: 960 / 2250 Loss: 0.0212\n",
      "Epoch: 2 / 3, Step: 961 / 2250 Loss: 0.0813\n",
      "Epoch: 2 / 3, Step: 962 / 2250 Loss: 0.0194\n",
      "Epoch: 2 / 3, Step: 963 / 2250 Loss: 0.1503\n",
      "Epoch: 2 / 3, Step: 964 / 2250 Loss: 0.1786\n",
      "Epoch: 2 / 3, Step: 965 / 2250 Loss: 0.0230\n",
      "Epoch: 2 / 3, Step: 966 / 2250 Loss: 0.0615\n",
      "Epoch: 2 / 3, Step: 967 / 2250 Loss: 0.2250\n",
      "Epoch: 2 / 3, Step: 968 / 2250 Loss: 0.1394\n",
      "Epoch: 2 / 3, Step: 969 / 2250 Loss: 0.1379\n",
      "Epoch: 2 / 3, Step: 970 / 2250 Loss: 0.7283\n",
      "Epoch: 2 / 3, Step: 971 / 2250 Loss: 0.1557\n",
      "Epoch: 2 / 3, Step: 972 / 2250 Loss: 0.1031\n",
      "Epoch: 2 / 3, Step: 973 / 2250 Loss: 0.1615\n",
      "Epoch: 2 / 3, Step: 974 / 2250 Loss: 0.1319\n",
      "Epoch: 2 / 3, Step: 975 / 2250 Loss: 0.2685\n",
      "Epoch: 2 / 3, Step: 976 / 2250 Loss: 0.0168\n",
      "Epoch: 2 / 3, Step: 977 / 2250 Loss: 0.1896\n",
      "Epoch: 2 / 3, Step: 978 / 2250 Loss: 0.1142\n",
      "Epoch: 2 / 3, Step: 979 / 2250 Loss: 0.3045\n",
      "Epoch: 2 / 3, Step: 980 / 2250 Loss: 0.1793\n",
      "Epoch: 2 / 3, Step: 981 / 2250 Loss: 0.1675\n",
      "Epoch: 2 / 3, Step: 982 / 2250 Loss: 0.3644\n",
      "Epoch: 2 / 3, Step: 983 / 2250 Loss: 0.1720\n",
      "Epoch: 2 / 3, Step: 984 / 2250 Loss: 0.0222\n",
      "Epoch: 2 / 3, Step: 985 / 2250 Loss: 0.0768\n",
      "Epoch: 2 / 3, Step: 986 / 2250 Loss: 0.1797\n",
      "Epoch: 2 / 3, Step: 987 / 2250 Loss: 0.0358\n",
      "Epoch: 2 / 3, Step: 988 / 2250 Loss: 0.1656\n",
      "Epoch: 2 / 3, Step: 989 / 2250 Loss: 0.0979\n",
      "Epoch: 2 / 3, Step: 990 / 2250 Loss: 0.0600\n",
      "Epoch: 2 / 3, Step: 991 / 2250 Loss: 0.3663\n",
      "Epoch: 2 / 3, Step: 992 / 2250 Loss: 0.3409\n",
      "Epoch: 2 / 3, Step: 993 / 2250 Loss: 0.2100\n",
      "Epoch: 2 / 3, Step: 994 / 2250 Loss: 0.0480\n",
      "Epoch: 2 / 3, Step: 995 / 2250 Loss: 0.0703\n",
      "Epoch: 2 / 3, Step: 996 / 2250 Loss: 0.0666\n",
      "Epoch: 2 / 3, Step: 997 / 2250 Loss: 0.2037\n",
      "Epoch: 2 / 3, Step: 998 / 2250 Loss: 0.1938\n",
      "Epoch: 2 / 3, Step: 999 / 2250 Loss: 0.0709\n",
      "Epoch: 2 / 3, Step: 1000 / 2250 Loss: 0.0697\n",
      "Epoch: 2 / 3, Step: 1001 / 2250 Loss: 0.2613\n",
      "Epoch: 2 / 3, Step: 1002 / 2250 Loss: 0.0985\n",
      "Epoch: 2 / 3, Step: 1003 / 2250 Loss: 0.1737\n",
      "Epoch: 2 / 3, Step: 1004 / 2250 Loss: 0.1696\n",
      "Epoch: 2 / 3, Step: 1005 / 2250 Loss: 0.1507\n",
      "Epoch: 2 / 3, Step: 1006 / 2250 Loss: 0.0430\n",
      "Epoch: 2 / 3, Step: 1007 / 2250 Loss: 0.0377\n",
      "Epoch: 2 / 3, Step: 1008 / 2250 Loss: 0.0434\n",
      "Epoch: 2 / 3, Step: 1009 / 2250 Loss: 0.1860\n",
      "Epoch: 2 / 3, Step: 1010 / 2250 Loss: 0.1495\n",
      "Epoch: 2 / 3, Step: 1011 / 2250 Loss: 0.1566\n",
      "Epoch: 2 / 3, Step: 1012 / 2250 Loss: 0.1177\n",
      "Epoch: 2 / 3, Step: 1013 / 2250 Loss: 0.1129\n",
      "Epoch: 2 / 3, Step: 1014 / 2250 Loss: 0.0613\n",
      "Epoch: 2 / 3, Step: 1015 / 2250 Loss: 0.0921\n",
      "Epoch: 2 / 3, Step: 1016 / 2250 Loss: 0.0475\n",
      "Epoch: 2 / 3, Step: 1017 / 2250 Loss: 0.1733\n",
      "Epoch: 2 / 3, Step: 1018 / 2250 Loss: 0.2225\n",
      "Epoch: 2 / 3, Step: 1019 / 2250 Loss: 0.1208\n",
      "Epoch: 2 / 3, Step: 1020 / 2250 Loss: 0.4267\n",
      "Epoch: 2 / 3, Step: 1021 / 2250 Loss: 0.1590\n",
      "Epoch: 2 / 3, Step: 1022 / 2250 Loss: 0.3781\n",
      "Epoch: 2 / 3, Step: 1023 / 2250 Loss: 0.0293\n",
      "Epoch: 2 / 3, Step: 1024 / 2250 Loss: 0.1291\n",
      "Epoch: 2 / 3, Step: 1025 / 2250 Loss: 0.0321\n",
      "Epoch: 2 / 3, Step: 1026 / 2250 Loss: 0.0672\n",
      "Epoch: 2 / 3, Step: 1027 / 2250 Loss: 0.0420\n",
      "Epoch: 2 / 3, Step: 1028 / 2250 Loss: 0.0895\n",
      "Epoch: 2 / 3, Step: 1029 / 2250 Loss: 0.2082\n",
      "Epoch: 2 / 3, Step: 1030 / 2250 Loss: 0.1381\n",
      "Epoch: 2 / 3, Step: 1031 / 2250 Loss: 0.2729\n",
      "Epoch: 2 / 3, Step: 1032 / 2250 Loss: 0.2116\n",
      "Epoch: 2 / 3, Step: 1033 / 2250 Loss: 0.0760\n",
      "Epoch: 2 / 3, Step: 1034 / 2250 Loss: 0.2966\n",
      "Epoch: 2 / 3, Step: 1035 / 2250 Loss: 0.0320\n",
      "Epoch: 2 / 3, Step: 1036 / 2250 Loss: 0.0765\n",
      "Epoch: 2 / 3, Step: 1037 / 2250 Loss: 0.0739\n",
      "Epoch: 2 / 3, Step: 1038 / 2250 Loss: 0.1258\n",
      "Epoch: 2 / 3, Step: 1039 / 2250 Loss: 0.0992\n",
      "Epoch: 2 / 3, Step: 1040 / 2250 Loss: 0.0941\n",
      "Epoch: 2 / 3, Step: 1041 / 2250 Loss: 0.1131\n",
      "Epoch: 2 / 3, Step: 1042 / 2250 Loss: 0.0766\n",
      "Epoch: 2 / 3, Step: 1043 / 2250 Loss: 0.1038\n",
      "Epoch: 2 / 3, Step: 1044 / 2250 Loss: 0.0748\n",
      "Epoch: 2 / 3, Step: 1045 / 2250 Loss: 0.0810\n",
      "Epoch: 2 / 3, Step: 1046 / 2250 Loss: 0.2585\n",
      "Epoch: 2 / 3, Step: 1047 / 2250 Loss: 0.0948\n",
      "Epoch: 2 / 3, Step: 1048 / 2250 Loss: 0.1140\n",
      "Epoch: 2 / 3, Step: 1049 / 2250 Loss: 0.1029\n",
      "Epoch: 2 / 3, Step: 1050 / 2250 Loss: 0.0294\n",
      "Epoch: 2 / 3, Step: 1051 / 2250 Loss: 0.0773\n",
      "Epoch: 2 / 3, Step: 1052 / 2250 Loss: 0.0852\n",
      "Epoch: 2 / 3, Step: 1053 / 2250 Loss: 0.2139\n",
      "Epoch: 2 / 3, Step: 1054 / 2250 Loss: 0.0586\n",
      "Epoch: 2 / 3, Step: 1055 / 2250 Loss: 0.0737\n",
      "Epoch: 2 / 3, Step: 1056 / 2250 Loss: 0.1058\n",
      "Epoch: 2 / 3, Step: 1057 / 2250 Loss: 0.0600\n",
      "Epoch: 2 / 3, Step: 1058 / 2250 Loss: 0.1650\n",
      "Epoch: 2 / 3, Step: 1059 / 2250 Loss: 0.0161\n",
      "Epoch: 2 / 3, Step: 1060 / 2250 Loss: 0.2023\n",
      "Epoch: 2 / 3, Step: 1061 / 2250 Loss: 0.2040\n",
      "Epoch: 2 / 3, Step: 1062 / 2250 Loss: 0.2650\n",
      "Epoch: 2 / 3, Step: 1063 / 2250 Loss: 0.3711\n",
      "Epoch: 2 / 3, Step: 1064 / 2250 Loss: 0.1703\n",
      "Epoch: 2 / 3, Step: 1065 / 2250 Loss: 0.0603\n",
      "Epoch: 2 / 3, Step: 1066 / 2250 Loss: 0.0249\n",
      "Epoch: 2 / 3, Step: 1067 / 2250 Loss: 0.2150\n",
      "Epoch: 2 / 3, Step: 1068 / 2250 Loss: 0.0397\n",
      "Epoch: 2 / 3, Step: 1069 / 2250 Loss: 0.2089\n",
      "Epoch: 2 / 3, Step: 1070 / 2250 Loss: 0.0595\n",
      "Epoch: 2 / 3, Step: 1071 / 2250 Loss: 0.0219\n",
      "Epoch: 2 / 3, Step: 1072 / 2250 Loss: 0.0556\n",
      "Epoch: 2 / 3, Step: 1073 / 2250 Loss: 0.3334\n",
      "Epoch: 2 / 3, Step: 1074 / 2250 Loss: 0.2334\n",
      "Epoch: 2 / 3, Step: 1075 / 2250 Loss: 0.3995\n",
      "Epoch: 2 / 3, Step: 1076 / 2250 Loss: 0.1562\n",
      "Epoch: 2 / 3, Step: 1077 / 2250 Loss: 0.0635\n",
      "Epoch: 2 / 3, Step: 1078 / 2250 Loss: 0.0389\n",
      "Epoch: 2 / 3, Step: 1079 / 2250 Loss: 0.3163\n",
      "Epoch: 2 / 3, Step: 1080 / 2250 Loss: 0.1123\n",
      "Epoch: 2 / 3, Step: 1081 / 2250 Loss: 0.1636\n",
      "Epoch: 2 / 3, Step: 1082 / 2250 Loss: 0.0312\n",
      "Epoch: 2 / 3, Step: 1083 / 2250 Loss: 0.2984\n",
      "Epoch: 2 / 3, Step: 1084 / 2250 Loss: 0.2026\n",
      "Epoch: 2 / 3, Step: 1085 / 2250 Loss: 0.0780\n",
      "Epoch: 2 / 3, Step: 1086 / 2250 Loss: 0.0742\n",
      "Epoch: 2 / 3, Step: 1087 / 2250 Loss: 0.0333\n",
      "Epoch: 2 / 3, Step: 1088 / 2250 Loss: 0.0532\n",
      "Epoch: 2 / 3, Step: 1089 / 2250 Loss: 0.3161\n",
      "Epoch: 2 / 3, Step: 1090 / 2250 Loss: 0.0546\n",
      "Epoch: 2 / 3, Step: 1091 / 2250 Loss: 0.0866\n",
      "Epoch: 2 / 3, Step: 1092 / 2250 Loss: 0.1837\n",
      "Epoch: 2 / 3, Step: 1093 / 2250 Loss: 0.0336\n",
      "Epoch: 2 / 3, Step: 1094 / 2250 Loss: 0.0472\n",
      "Epoch: 2 / 3, Step: 1095 / 2250 Loss: 0.1221\n",
      "Epoch: 2 / 3, Step: 1096 / 2250 Loss: 0.2485\n",
      "Epoch: 2 / 3, Step: 1097 / 2250 Loss: 0.1513\n",
      "Epoch: 2 / 3, Step: 1098 / 2250 Loss: 0.0966\n",
      "Epoch: 2 / 3, Step: 1099 / 2250 Loss: 0.3798\n",
      "Epoch: 2 / 3, Step: 1100 / 2250 Loss: 0.0492\n",
      "Epoch: 2 / 3, Step: 1101 / 2250 Loss: 0.1361\n",
      "Epoch: 2 / 3, Step: 1102 / 2250 Loss: 0.0639\n",
      "Epoch: 2 / 3, Step: 1103 / 2250 Loss: 0.0736\n",
      "Epoch: 2 / 3, Step: 1104 / 2250 Loss: 0.0346\n",
      "Epoch: 2 / 3, Step: 1105 / 2250 Loss: 0.0805\n",
      "Epoch: 2 / 3, Step: 1106 / 2250 Loss: 0.1298\n",
      "Epoch: 2 / 3, Step: 1107 / 2250 Loss: 0.1732\n",
      "Epoch: 2 / 3, Step: 1108 / 2250 Loss: 0.1448\n",
      "Epoch: 2 / 3, Step: 1109 / 2250 Loss: 0.1363\n",
      "Epoch: 2 / 3, Step: 1110 / 2250 Loss: 0.1075\n",
      "Epoch: 2 / 3, Step: 1111 / 2250 Loss: 0.1057\n",
      "Epoch: 2 / 3, Step: 1112 / 2250 Loss: 0.0395\n",
      "Epoch: 2 / 3, Step: 1113 / 2250 Loss: 0.0471\n",
      "Epoch: 2 / 3, Step: 1114 / 2250 Loss: 0.0898\n",
      "Epoch: 2 / 3, Step: 1115 / 2250 Loss: 0.2308\n",
      "Epoch: 2 / 3, Step: 1116 / 2250 Loss: 0.1218\n",
      "Epoch: 2 / 3, Step: 1117 / 2250 Loss: 0.0944\n",
      "Epoch: 2 / 3, Step: 1118 / 2250 Loss: 0.0385\n",
      "Epoch: 2 / 3, Step: 1119 / 2250 Loss: 0.0228\n",
      "Epoch: 2 / 3, Step: 1120 / 2250 Loss: 0.0356\n",
      "Epoch: 2 / 3, Step: 1121 / 2250 Loss: 0.0433\n",
      "Epoch: 2 / 3, Step: 1122 / 2250 Loss: 0.2062\n",
      "Epoch: 2 / 3, Step: 1123 / 2250 Loss: 0.2258\n",
      "Epoch: 2 / 3, Step: 1124 / 2250 Loss: 0.2450\n",
      "Epoch: 2 / 3, Step: 1125 / 2250 Loss: 0.1641\n",
      "Epoch: 2 / 3, Step: 1126 / 2250 Loss: 0.0635\n",
      "Epoch: 2 / 3, Step: 1127 / 2250 Loss: 0.2684\n",
      "Epoch: 2 / 3, Step: 1128 / 2250 Loss: 0.1279\n",
      "Epoch: 2 / 3, Step: 1129 / 2250 Loss: 0.0316\n",
      "Epoch: 2 / 3, Step: 1130 / 2250 Loss: 0.0272\n",
      "Epoch: 2 / 3, Step: 1131 / 2250 Loss: 0.2085\n",
      "Epoch: 2 / 3, Step: 1132 / 2250 Loss: 0.0389\n",
      "Epoch: 2 / 3, Step: 1133 / 2250 Loss: 0.1938\n",
      "Epoch: 2 / 3, Step: 1134 / 2250 Loss: 0.0731\n",
      "Epoch: 2 / 3, Step: 1135 / 2250 Loss: 0.2906\n",
      "Epoch: 2 / 3, Step: 1136 / 2250 Loss: 0.0681\n",
      "Epoch: 2 / 3, Step: 1137 / 2250 Loss: 0.2285\n",
      "Epoch: 2 / 3, Step: 1138 / 2250 Loss: 0.0730\n",
      "Epoch: 2 / 3, Step: 1139 / 2250 Loss: 0.0524\n",
      "Epoch: 2 / 3, Step: 1140 / 2250 Loss: 0.3730\n",
      "Epoch: 2 / 3, Step: 1141 / 2250 Loss: 0.0952\n",
      "Epoch: 2 / 3, Step: 1142 / 2250 Loss: 0.2199\n",
      "Epoch: 2 / 3, Step: 1143 / 2250 Loss: 0.2402\n",
      "Epoch: 2 / 3, Step: 1144 / 2250 Loss: 0.0507\n",
      "Epoch: 2 / 3, Step: 1145 / 2250 Loss: 0.1838\n",
      "Epoch: 2 / 3, Step: 1146 / 2250 Loss: 0.0823\n",
      "Epoch: 2 / 3, Step: 1147 / 2250 Loss: 0.1127\n",
      "Epoch: 2 / 3, Step: 1148 / 2250 Loss: 0.0293\n",
      "Epoch: 2 / 3, Step: 1149 / 2250 Loss: 0.2463\n",
      "Epoch: 2 / 3, Step: 1150 / 2250 Loss: 0.0613\n",
      "Epoch: 2 / 3, Step: 1151 / 2250 Loss: 0.0846\n",
      "Epoch: 2 / 3, Step: 1152 / 2250 Loss: 0.1600\n",
      "Epoch: 2 / 3, Step: 1153 / 2250 Loss: 0.0580\n",
      "Epoch: 2 / 3, Step: 1154 / 2250 Loss: 0.1475\n",
      "Epoch: 2 / 3, Step: 1155 / 2250 Loss: 0.0374\n",
      "Epoch: 2 / 3, Step: 1156 / 2250 Loss: 0.0540\n",
      "Epoch: 2 / 3, Step: 1157 / 2250 Loss: 0.1854\n",
      "Epoch: 2 / 3, Step: 1158 / 2250 Loss: 0.0782\n",
      "Epoch: 2 / 3, Step: 1159 / 2250 Loss: 0.0734\n",
      "Epoch: 2 / 3, Step: 1160 / 2250 Loss: 0.0791\n",
      "Epoch: 2 / 3, Step: 1161 / 2250 Loss: 0.0979\n",
      "Epoch: 2 / 3, Step: 1162 / 2250 Loss: 0.3045\n",
      "Epoch: 2 / 3, Step: 1163 / 2250 Loss: 0.2112\n",
      "Epoch: 2 / 3, Step: 1164 / 2250 Loss: 0.1622\n",
      "Epoch: 2 / 3, Step: 1165 / 2250 Loss: 0.2473\n",
      "Epoch: 2 / 3, Step: 1166 / 2250 Loss: 0.0428\n",
      "Epoch: 2 / 3, Step: 1167 / 2250 Loss: 0.1592\n",
      "Epoch: 2 / 3, Step: 1168 / 2250 Loss: 0.0671\n",
      "Epoch: 2 / 3, Step: 1169 / 2250 Loss: 0.3492\n",
      "Epoch: 2 / 3, Step: 1170 / 2250 Loss: 0.1145\n",
      "Epoch: 2 / 3, Step: 1171 / 2250 Loss: 0.0386\n",
      "Epoch: 2 / 3, Step: 1172 / 2250 Loss: 0.0692\n",
      "Epoch: 2 / 3, Step: 1173 / 2250 Loss: 0.1651\n",
      "Epoch: 2 / 3, Step: 1174 / 2250 Loss: 0.0305\n",
      "Epoch: 2 / 3, Step: 1175 / 2250 Loss: 0.1788\n",
      "Epoch: 2 / 3, Step: 1176 / 2250 Loss: 0.1272\n",
      "Epoch: 2 / 3, Step: 1177 / 2250 Loss: 0.3162\n",
      "Epoch: 2 / 3, Step: 1178 / 2250 Loss: 0.0243\n",
      "Epoch: 2 / 3, Step: 1179 / 2250 Loss: 0.0604\n",
      "Epoch: 2 / 3, Step: 1180 / 2250 Loss: 0.0242\n",
      "Epoch: 2 / 3, Step: 1181 / 2250 Loss: 0.3233\n",
      "Epoch: 2 / 3, Step: 1182 / 2250 Loss: 0.0603\n",
      "Epoch: 2 / 3, Step: 1183 / 2250 Loss: 0.1293\n",
      "Epoch: 2 / 3, Step: 1184 / 2250 Loss: 0.1954\n",
      "Epoch: 2 / 3, Step: 1185 / 2250 Loss: 0.0623\n",
      "Epoch: 2 / 3, Step: 1186 / 2250 Loss: 0.1795\n",
      "Epoch: 2 / 3, Step: 1187 / 2250 Loss: 0.0338\n",
      "Epoch: 2 / 3, Step: 1188 / 2250 Loss: 0.0700\n",
      "Epoch: 2 / 3, Step: 1189 / 2250 Loss: 0.1135\n",
      "Epoch: 2 / 3, Step: 1190 / 2250 Loss: 0.1490\n",
      "Epoch: 2 / 3, Step: 1191 / 2250 Loss: 0.1027\n",
      "Epoch: 2 / 3, Step: 1192 / 2250 Loss: 0.0992\n",
      "Epoch: 2 / 3, Step: 1193 / 2250 Loss: 0.0583\n",
      "Epoch: 2 / 3, Step: 1194 / 2250 Loss: 0.0425\n",
      "Epoch: 2 / 3, Step: 1195 / 2250 Loss: 0.0626\n",
      "Epoch: 2 / 3, Step: 1196 / 2250 Loss: 0.2693\n",
      "Epoch: 2 / 3, Step: 1197 / 2250 Loss: 0.1121\n",
      "Epoch: 2 / 3, Step: 1198 / 2250 Loss: 0.2443\n",
      "Epoch: 2 / 3, Step: 1199 / 2250 Loss: 0.1220\n",
      "Epoch: 2 / 3, Step: 1200 / 2250 Loss: 0.0432\n",
      "Epoch: 2 / 3, Step: 1201 / 2250 Loss: 0.3213\n",
      "Epoch: 2 / 3, Step: 1202 / 2250 Loss: 0.1172\n",
      "Epoch: 2 / 3, Step: 1203 / 2250 Loss: 0.0835\n",
      "Epoch: 2 / 3, Step: 1204 / 2250 Loss: 0.2268\n",
      "Epoch: 2 / 3, Step: 1205 / 2250 Loss: 0.0799\n",
      "Epoch: 2 / 3, Step: 1206 / 2250 Loss: 0.1363\n",
      "Epoch: 2 / 3, Step: 1207 / 2250 Loss: 0.2406\n",
      "Epoch: 2 / 3, Step: 1208 / 2250 Loss: 0.2508\n",
      "Epoch: 2 / 3, Step: 1209 / 2250 Loss: 0.1792\n",
      "Epoch: 2 / 3, Step: 1210 / 2250 Loss: 0.1055\n",
      "Epoch: 2 / 3, Step: 1211 / 2250 Loss: 0.1643\n",
      "Epoch: 2 / 3, Step: 1212 / 2250 Loss: 0.0252\n",
      "Epoch: 2 / 3, Step: 1213 / 2250 Loss: 0.0389\n",
      "Epoch: 2 / 3, Step: 1214 / 2250 Loss: 0.4169\n",
      "Epoch: 2 / 3, Step: 1215 / 2250 Loss: 0.0176\n",
      "Epoch: 2 / 3, Step: 1216 / 2250 Loss: 0.0970\n",
      "Epoch: 2 / 3, Step: 1217 / 2250 Loss: 0.1889\n",
      "Epoch: 2 / 3, Step: 1218 / 2250 Loss: 0.1756\n",
      "Epoch: 2 / 3, Step: 1219 / 2250 Loss: 0.0323\n",
      "Epoch: 2 / 3, Step: 1220 / 2250 Loss: 0.1135\n",
      "Epoch: 2 / 3, Step: 1221 / 2250 Loss: 0.0906\n",
      "Epoch: 2 / 3, Step: 1222 / 2250 Loss: 0.0592\n",
      "Epoch: 2 / 3, Step: 1223 / 2250 Loss: 0.1611\n",
      "Epoch: 2 / 3, Step: 1224 / 2250 Loss: 0.1048\n",
      "Epoch: 2 / 3, Step: 1225 / 2250 Loss: 0.0895\n",
      "Epoch: 2 / 3, Step: 1226 / 2250 Loss: 0.1440\n",
      "Epoch: 2 / 3, Step: 1227 / 2250 Loss: 0.1583\n",
      "Epoch: 2 / 3, Step: 1228 / 2250 Loss: 0.1124\n",
      "Epoch: 2 / 3, Step: 1229 / 2250 Loss: 0.2622\n",
      "Epoch: 2 / 3, Step: 1230 / 2250 Loss: 0.2977\n",
      "Epoch: 2 / 3, Step: 1231 / 2250 Loss: 0.0299\n",
      "Epoch: 2 / 3, Step: 1232 / 2250 Loss: 0.0284\n",
      "Epoch: 2 / 3, Step: 1233 / 2250 Loss: 0.0563\n",
      "Epoch: 2 / 3, Step: 1234 / 2250 Loss: 0.1260\n",
      "Epoch: 2 / 3, Step: 1235 / 2250 Loss: 0.0720\n",
      "Epoch: 2 / 3, Step: 1236 / 2250 Loss: 0.0829\n",
      "Epoch: 2 / 3, Step: 1237 / 2250 Loss: 0.0805\n",
      "Epoch: 2 / 3, Step: 1238 / 2250 Loss: 0.1444\n",
      "Epoch: 2 / 3, Step: 1239 / 2250 Loss: 0.1314\n",
      "Epoch: 2 / 3, Step: 1240 / 2250 Loss: 0.0247\n",
      "Epoch: 2 / 3, Step: 1241 / 2250 Loss: 0.0227\n",
      "Epoch: 2 / 3, Step: 1242 / 2250 Loss: 0.1594\n",
      "Epoch: 2 / 3, Step: 1243 / 2250 Loss: 0.0224\n",
      "Epoch: 2 / 3, Step: 1244 / 2250 Loss: 0.1467\n",
      "Epoch: 2 / 3, Step: 1245 / 2250 Loss: 0.0354\n",
      "Epoch: 2 / 3, Step: 1246 / 2250 Loss: 0.0452\n",
      "Epoch: 2 / 3, Step: 1247 / 2250 Loss: 0.0589\n",
      "Epoch: 2 / 3, Step: 1248 / 2250 Loss: 0.1417\n",
      "Epoch: 2 / 3, Step: 1249 / 2250 Loss: 0.2134\n",
      "Epoch: 2 / 3, Step: 1250 / 2250 Loss: 0.0627\n",
      "Epoch: 2 / 3, Step: 1251 / 2250 Loss: 0.1385\n",
      "Epoch: 2 / 3, Step: 1252 / 2250 Loss: 0.1186\n",
      "Epoch: 2 / 3, Step: 1253 / 2250 Loss: 0.0370\n",
      "Epoch: 2 / 3, Step: 1254 / 2250 Loss: 0.0826\n",
      "Epoch: 2 / 3, Step: 1255 / 2250 Loss: 0.2205\n",
      "Epoch: 2 / 3, Step: 1256 / 2250 Loss: 0.1220\n",
      "Epoch: 2 / 3, Step: 1257 / 2250 Loss: 0.1299\n",
      "Epoch: 2 / 3, Step: 1258 / 2250 Loss: 0.0864\n",
      "Epoch: 2 / 3, Step: 1259 / 2250 Loss: 0.0560\n",
      "Epoch: 2 / 3, Step: 1260 / 2250 Loss: 0.1908\n",
      "Epoch: 2 / 3, Step: 1261 / 2250 Loss: 0.0964\n",
      "Epoch: 2 / 3, Step: 1262 / 2250 Loss: 0.2506\n",
      "Epoch: 2 / 3, Step: 1263 / 2250 Loss: 0.0811\n",
      "Epoch: 2 / 3, Step: 1264 / 2250 Loss: 0.1135\n",
      "Epoch: 2 / 3, Step: 1265 / 2250 Loss: 0.0394\n",
      "Epoch: 2 / 3, Step: 1266 / 2250 Loss: 0.0527\n",
      "Epoch: 2 / 3, Step: 1267 / 2250 Loss: 0.1873\n",
      "Epoch: 2 / 3, Step: 1268 / 2250 Loss: 0.1185\n",
      "Epoch: 2 / 3, Step: 1269 / 2250 Loss: 0.2346\n",
      "Epoch: 2 / 3, Step: 1270 / 2250 Loss: 0.0254\n",
      "Epoch: 2 / 3, Step: 1271 / 2250 Loss: 0.0165\n",
      "Epoch: 2 / 3, Step: 1272 / 2250 Loss: 0.0744\n",
      "Epoch: 2 / 3, Step: 1273 / 2250 Loss: 0.0244\n",
      "Epoch: 2 / 3, Step: 1274 / 2250 Loss: 0.2144\n",
      "Epoch: 2 / 3, Step: 1275 / 2250 Loss: 0.4861\n",
      "Epoch: 2 / 3, Step: 1276 / 2250 Loss: 0.1730\n",
      "Epoch: 2 / 3, Step: 1277 / 2250 Loss: 0.0886\n",
      "Epoch: 2 / 3, Step: 1278 / 2250 Loss: 0.1707\n",
      "Epoch: 2 / 3, Step: 1279 / 2250 Loss: 0.0307\n",
      "Epoch: 2 / 3, Step: 1280 / 2250 Loss: 0.0470\n",
      "Epoch: 2 / 3, Step: 1281 / 2250 Loss: 0.0761\n",
      "Epoch: 2 / 3, Step: 1282 / 2250 Loss: 0.0467\n",
      "Epoch: 2 / 3, Step: 1283 / 2250 Loss: 0.0865\n",
      "Epoch: 2 / 3, Step: 1284 / 2250 Loss: 0.0925\n",
      "Epoch: 2 / 3, Step: 1285 / 2250 Loss: 0.1507\n",
      "Epoch: 2 / 3, Step: 1286 / 2250 Loss: 0.0801\n",
      "Epoch: 2 / 3, Step: 1287 / 2250 Loss: 0.1868\n",
      "Epoch: 2 / 3, Step: 1288 / 2250 Loss: 0.3402\n",
      "Epoch: 2 / 3, Step: 1289 / 2250 Loss: 0.1254\n",
      "Epoch: 2 / 3, Step: 1290 / 2250 Loss: 0.0418\n",
      "Epoch: 2 / 3, Step: 1291 / 2250 Loss: 0.1181\n",
      "Epoch: 2 / 3, Step: 1292 / 2250 Loss: 0.1731\n",
      "Epoch: 2 / 3, Step: 1293 / 2250 Loss: 0.1121\n",
      "Epoch: 2 / 3, Step: 1294 / 2250 Loss: 0.1443\n",
      "Epoch: 2 / 3, Step: 1295 / 2250 Loss: 0.1235\n",
      "Epoch: 2 / 3, Step: 1296 / 2250 Loss: 0.0985\n",
      "Epoch: 2 / 3, Step: 1297 / 2250 Loss: 0.0464\n",
      "Epoch: 2 / 3, Step: 1298 / 2250 Loss: 0.0393\n",
      "Epoch: 2 / 3, Step: 1299 / 2250 Loss: 0.1666\n",
      "Epoch: 2 / 3, Step: 1300 / 2250 Loss: 0.4507\n",
      "Epoch: 2 / 3, Step: 1301 / 2250 Loss: 0.3073\n",
      "Epoch: 2 / 3, Step: 1302 / 2250 Loss: 0.3888\n",
      "Epoch: 2 / 3, Step: 1303 / 2250 Loss: 0.0608\n",
      "Epoch: 2 / 3, Step: 1304 / 2250 Loss: 0.1061\n",
      "Epoch: 2 / 3, Step: 1305 / 2250 Loss: 0.3564\n",
      "Epoch: 2 / 3, Step: 1306 / 2250 Loss: 0.0416\n",
      "Epoch: 2 / 3, Step: 1307 / 2250 Loss: 0.2480\n",
      "Epoch: 2 / 3, Step: 1308 / 2250 Loss: 0.2199\n",
      "Epoch: 2 / 3, Step: 1309 / 2250 Loss: 0.0996\n",
      "Epoch: 2 / 3, Step: 1310 / 2250 Loss: 0.3452\n",
      "Epoch: 2 / 3, Step: 1311 / 2250 Loss: 0.2499\n",
      "Epoch: 2 / 3, Step: 1312 / 2250 Loss: 0.1677\n",
      "Epoch: 2 / 3, Step: 1313 / 2250 Loss: 0.1006\n",
      "Epoch: 2 / 3, Step: 1314 / 2250 Loss: 0.1077\n",
      "Epoch: 2 / 3, Step: 1315 / 2250 Loss: 0.2465\n",
      "Epoch: 2 / 3, Step: 1316 / 2250 Loss: 0.0926\n",
      "Epoch: 2 / 3, Step: 1317 / 2250 Loss: 0.1346\n",
      "Epoch: 2 / 3, Step: 1318 / 2250 Loss: 0.0435\n",
      "Epoch: 2 / 3, Step: 1319 / 2250 Loss: 0.2561\n",
      "Epoch: 2 / 3, Step: 1320 / 2250 Loss: 0.1254\n",
      "Epoch: 2 / 3, Step: 1321 / 2250 Loss: 0.0626\n",
      "Epoch: 2 / 3, Step: 1322 / 2250 Loss: 0.0639\n",
      "Epoch: 2 / 3, Step: 1323 / 2250 Loss: 0.0516\n",
      "Epoch: 2 / 3, Step: 1324 / 2250 Loss: 0.5454\n",
      "Epoch: 2 / 3, Step: 1325 / 2250 Loss: 0.2007\n",
      "Epoch: 2 / 3, Step: 1326 / 2250 Loss: 0.2022\n",
      "Epoch: 2 / 3, Step: 1327 / 2250 Loss: 0.2349\n",
      "Epoch: 2 / 3, Step: 1328 / 2250 Loss: 0.4125\n",
      "Epoch: 2 / 3, Step: 1329 / 2250 Loss: 0.0390\n",
      "Epoch: 2 / 3, Step: 1330 / 2250 Loss: 0.0229\n",
      "Epoch: 2 / 3, Step: 1331 / 2250 Loss: 0.0719\n",
      "Epoch: 2 / 3, Step: 1332 / 2250 Loss: 0.0319\n",
      "Epoch: 2 / 3, Step: 1333 / 2250 Loss: 0.5116\n",
      "Epoch: 2 / 3, Step: 1334 / 2250 Loss: 0.0669\n",
      "Epoch: 2 / 3, Step: 1335 / 2250 Loss: 0.1389\n",
      "Epoch: 2 / 3, Step: 1336 / 2250 Loss: 0.1462\n",
      "Epoch: 2 / 3, Step: 1337 / 2250 Loss: 0.1280\n",
      "Epoch: 2 / 3, Step: 1338 / 2250 Loss: 0.2254\n",
      "Epoch: 2 / 3, Step: 1339 / 2250 Loss: 0.0679\n",
      "Epoch: 2 / 3, Step: 1340 / 2250 Loss: 0.2019\n",
      "Epoch: 2 / 3, Step: 1341 / 2250 Loss: 0.0408\n",
      "Epoch: 2 / 3, Step: 1342 / 2250 Loss: 0.1598\n",
      "Epoch: 2 / 3, Step: 1343 / 2250 Loss: 0.1913\n",
      "Epoch: 2 / 3, Step: 1344 / 2250 Loss: 0.1389\n",
      "Epoch: 2 / 3, Step: 1345 / 2250 Loss: 0.0601\n",
      "Epoch: 2 / 3, Step: 1346 / 2250 Loss: 0.0222\n",
      "Epoch: 2 / 3, Step: 1347 / 2250 Loss: 0.3155\n",
      "Epoch: 2 / 3, Step: 1348 / 2250 Loss: 0.0556\n",
      "Epoch: 2 / 3, Step: 1349 / 2250 Loss: 0.1089\n",
      "Epoch: 2 / 3, Step: 1350 / 2250 Loss: 0.0381\n",
      "Epoch: 2 / 3, Step: 1351 / 2250 Loss: 0.1486\n",
      "Epoch: 2 / 3, Step: 1352 / 2250 Loss: 0.1503\n",
      "Epoch: 2 / 3, Step: 1353 / 2250 Loss: 0.0761\n",
      "Epoch: 2 / 3, Step: 1354 / 2250 Loss: 0.0797\n",
      "Epoch: 2 / 3, Step: 1355 / 2250 Loss: 0.0739\n",
      "Epoch: 2 / 3, Step: 1356 / 2250 Loss: 0.1274\n",
      "Epoch: 2 / 3, Step: 1357 / 2250 Loss: 0.1342\n",
      "Epoch: 2 / 3, Step: 1358 / 2250 Loss: 0.0862\n",
      "Epoch: 2 / 3, Step: 1359 / 2250 Loss: 0.1090\n",
      "Epoch: 2 / 3, Step: 1360 / 2250 Loss: 0.4632\n",
      "Epoch: 2 / 3, Step: 1361 / 2250 Loss: 0.1510\n",
      "Epoch: 2 / 3, Step: 1362 / 2250 Loss: 0.0528\n",
      "Epoch: 2 / 3, Step: 1363 / 2250 Loss: 0.0184\n",
      "Epoch: 2 / 3, Step: 1364 / 2250 Loss: 0.1073\n",
      "Epoch: 2 / 3, Step: 1365 / 2250 Loss: 0.2551\n",
      "Epoch: 2 / 3, Step: 1366 / 2250 Loss: 0.0931\n",
      "Epoch: 2 / 3, Step: 1367 / 2250 Loss: 0.1894\n",
      "Epoch: 2 / 3, Step: 1368 / 2250 Loss: 0.1802\n",
      "Epoch: 2 / 3, Step: 1369 / 2250 Loss: 0.0314\n",
      "Epoch: 2 / 3, Step: 1370 / 2250 Loss: 0.2793\n",
      "Epoch: 2 / 3, Step: 1371 / 2250 Loss: 0.0854\n",
      "Epoch: 2 / 3, Step: 1372 / 2250 Loss: 0.0745\n",
      "Epoch: 2 / 3, Step: 1373 / 2250 Loss: 0.1305\n",
      "Epoch: 2 / 3, Step: 1374 / 2250 Loss: 0.1261\n",
      "Epoch: 2 / 3, Step: 1375 / 2250 Loss: 0.0814\n",
      "Epoch: 2 / 3, Step: 1376 / 2250 Loss: 0.1169\n",
      "Epoch: 2 / 3, Step: 1377 / 2250 Loss: 0.0273\n",
      "Epoch: 2 / 3, Step: 1378 / 2250 Loss: 0.0543\n",
      "Epoch: 2 / 3, Step: 1379 / 2250 Loss: 0.0428\n",
      "Epoch: 2 / 3, Step: 1380 / 2250 Loss: 0.0665\n",
      "Epoch: 2 / 3, Step: 1381 / 2250 Loss: 0.3680\n",
      "Epoch: 2 / 3, Step: 1382 / 2250 Loss: 0.3273\n",
      "Epoch: 2 / 3, Step: 1383 / 2250 Loss: 0.1965\n",
      "Epoch: 2 / 3, Step: 1384 / 2250 Loss: 0.1457\n",
      "Epoch: 2 / 3, Step: 1385 / 2250 Loss: 0.0500\n",
      "Epoch: 2 / 3, Step: 1386 / 2250 Loss: 0.0499\n",
      "Epoch: 2 / 3, Step: 1387 / 2250 Loss: 0.1925\n",
      "Epoch: 2 / 3, Step: 1388 / 2250 Loss: 0.2272\n",
      "Epoch: 2 / 3, Step: 1389 / 2250 Loss: 0.0909\n",
      "Epoch: 2 / 3, Step: 1390 / 2250 Loss: 0.1368\n",
      "Epoch: 2 / 3, Step: 1391 / 2250 Loss: 0.0738\n",
      "Epoch: 2 / 3, Step: 1392 / 2250 Loss: 0.2215\n",
      "Epoch: 2 / 3, Step: 1393 / 2250 Loss: 0.2266\n",
      "Epoch: 2 / 3, Step: 1394 / 2250 Loss: 0.0694\n",
      "Epoch: 2 / 3, Step: 1395 / 2250 Loss: 0.0207\n",
      "Epoch: 2 / 3, Step: 1396 / 2250 Loss: 0.1333\n",
      "Epoch: 2 / 3, Step: 1397 / 2250 Loss: 0.1706\n",
      "Epoch: 2 / 3, Step: 1398 / 2250 Loss: 0.1882\n",
      "Epoch: 2 / 3, Step: 1399 / 2250 Loss: 0.0794\n",
      "Epoch: 2 / 3, Step: 1400 / 2250 Loss: 0.0796\n",
      "Epoch: 2 / 3, Step: 1401 / 2250 Loss: 0.1021\n",
      "Epoch: 2 / 3, Step: 1402 / 2250 Loss: 0.0369\n",
      "Epoch: 2 / 3, Step: 1403 / 2250 Loss: 0.1251\n",
      "Epoch: 2 / 3, Step: 1404 / 2250 Loss: 0.1331\n",
      "Epoch: 2 / 3, Step: 1405 / 2250 Loss: 0.2799\n",
      "Epoch: 2 / 3, Step: 1406 / 2250 Loss: 0.1143\n",
      "Epoch: 2 / 3, Step: 1407 / 2250 Loss: 0.1759\n",
      "Epoch: 2 / 3, Step: 1408 / 2250 Loss: 0.2034\n",
      "Epoch: 2 / 3, Step: 1409 / 2250 Loss: 0.0485\n",
      "Epoch: 2 / 3, Step: 1410 / 2250 Loss: 0.0985\n",
      "Epoch: 2 / 3, Step: 1411 / 2250 Loss: 0.2248\n",
      "Epoch: 2 / 3, Step: 1412 / 2250 Loss: 0.2195\n",
      "Epoch: 2 / 3, Step: 1413 / 2250 Loss: 0.2320\n",
      "Epoch: 2 / 3, Step: 1414 / 2250 Loss: 0.0817\n",
      "Epoch: 2 / 3, Step: 1415 / 2250 Loss: 0.1670\n",
      "Epoch: 2 / 3, Step: 1416 / 2250 Loss: 0.1427\n",
      "Epoch: 2 / 3, Step: 1417 / 2250 Loss: 0.2574\n",
      "Epoch: 2 / 3, Step: 1418 / 2250 Loss: 0.2400\n",
      "Epoch: 2 / 3, Step: 1419 / 2250 Loss: 0.1404\n",
      "Epoch: 2 / 3, Step: 1420 / 2250 Loss: 0.0485\n",
      "Epoch: 2 / 3, Step: 1421 / 2250 Loss: 0.1802\n",
      "Epoch: 2 / 3, Step: 1422 / 2250 Loss: 0.2650\n",
      "Epoch: 2 / 3, Step: 1423 / 2250 Loss: 0.1223\n",
      "Epoch: 2 / 3, Step: 1424 / 2250 Loss: 0.1795\n",
      "Epoch: 2 / 3, Step: 1425 / 2250 Loss: 0.0390\n",
      "Epoch: 2 / 3, Step: 1426 / 2250 Loss: 0.2055\n",
      "Epoch: 2 / 3, Step: 1427 / 2250 Loss: 0.0284\n",
      "Epoch: 2 / 3, Step: 1428 / 2250 Loss: 0.2078\n",
      "Epoch: 2 / 3, Step: 1429 / 2250 Loss: 0.1800\n",
      "Epoch: 2 / 3, Step: 1430 / 2250 Loss: 0.1141\n",
      "Epoch: 2 / 3, Step: 1431 / 2250 Loss: 0.1194\n",
      "Epoch: 2 / 3, Step: 1432 / 2250 Loss: 0.0477\n",
      "Epoch: 2 / 3, Step: 1433 / 2250 Loss: 0.0420\n",
      "Epoch: 2 / 3, Step: 1434 / 2250 Loss: 0.1963\n",
      "Epoch: 2 / 3, Step: 1435 / 2250 Loss: 0.0349\n",
      "Epoch: 2 / 3, Step: 1436 / 2250 Loss: 0.1615\n",
      "Epoch: 2 / 3, Step: 1437 / 2250 Loss: 0.1196\n",
      "Epoch: 2 / 3, Step: 1438 / 2250 Loss: 0.0445\n",
      "Epoch: 2 / 3, Step: 1439 / 2250 Loss: 0.0594\n",
      "Epoch: 2 / 3, Step: 1440 / 2250 Loss: 0.0177\n",
      "Epoch: 2 / 3, Step: 1441 / 2250 Loss: 0.0372\n",
      "Epoch: 2 / 3, Step: 1442 / 2250 Loss: 0.0631\n",
      "Epoch: 2 / 3, Step: 1443 / 2250 Loss: 0.2806\n",
      "Epoch: 2 / 3, Step: 1444 / 2250 Loss: 0.1385\n",
      "Epoch: 2 / 3, Step: 1445 / 2250 Loss: 0.1180\n",
      "Epoch: 2 / 3, Step: 1446 / 2250 Loss: 0.0348\n",
      "Epoch: 2 / 3, Step: 1447 / 2250 Loss: 0.0114\n",
      "Epoch: 2 / 3, Step: 1448 / 2250 Loss: 0.1381\n",
      "Epoch: 2 / 3, Step: 1449 / 2250 Loss: 0.3438\n",
      "Epoch: 2 / 3, Step: 1450 / 2250 Loss: 0.0552\n",
      "Epoch: 2 / 3, Step: 1451 / 2250 Loss: 0.0908\n",
      "Epoch: 2 / 3, Step: 1452 / 2250 Loss: 0.0116\n",
      "Epoch: 2 / 3, Step: 1453 / 2250 Loss: 0.0182\n",
      "Epoch: 2 / 3, Step: 1454 / 2250 Loss: 0.2900\n",
      "Epoch: 2 / 3, Step: 1455 / 2250 Loss: 0.3350\n",
      "Epoch: 2 / 3, Step: 1456 / 2250 Loss: 0.1736\n",
      "Epoch: 2 / 3, Step: 1457 / 2250 Loss: 0.1575\n",
      "Epoch: 2 / 3, Step: 1458 / 2250 Loss: 0.0171\n",
      "Epoch: 2 / 3, Step: 1459 / 2250 Loss: 0.1096\n",
      "Epoch: 2 / 3, Step: 1460 / 2250 Loss: 0.0203\n",
      "Epoch: 2 / 3, Step: 1461 / 2250 Loss: 0.2994\n",
      "Epoch: 2 / 3, Step: 1462 / 2250 Loss: 0.2163\n",
      "Epoch: 2 / 3, Step: 1463 / 2250 Loss: 0.0474\n",
      "Epoch: 2 / 3, Step: 1464 / 2250 Loss: 0.1363\n",
      "Epoch: 2 / 3, Step: 1465 / 2250 Loss: 0.1553\n",
      "Epoch: 2 / 3, Step: 1466 / 2250 Loss: 0.0538\n",
      "Epoch: 2 / 3, Step: 1467 / 2250 Loss: 0.2000\n",
      "Epoch: 2 / 3, Step: 1468 / 2250 Loss: 0.1243\n",
      "Epoch: 2 / 3, Step: 1469 / 2250 Loss: 0.0327\n",
      "Epoch: 2 / 3, Step: 1470 / 2250 Loss: 0.0388\n",
      "Epoch: 2 / 3, Step: 1471 / 2250 Loss: 0.1255\n",
      "Epoch: 2 / 3, Step: 1472 / 2250 Loss: 0.3726\n",
      "Epoch: 2 / 3, Step: 1473 / 2250 Loss: 0.1236\n",
      "Epoch: 2 / 3, Step: 1474 / 2250 Loss: 0.0414\n",
      "Epoch: 2 / 3, Step: 1475 / 2250 Loss: 0.0528\n",
      "Epoch: 2 / 3, Step: 1476 / 2250 Loss: 0.0715\n",
      "Epoch: 2 / 3, Step: 1477 / 2250 Loss: 0.0956\n",
      "Epoch: 2 / 3, Step: 1478 / 2250 Loss: 0.1476\n",
      "Epoch: 2 / 3, Step: 1479 / 2250 Loss: 0.0850\n",
      "Epoch: 2 / 3, Step: 1480 / 2250 Loss: 0.0218\n",
      "Epoch: 2 / 3, Step: 1481 / 2250 Loss: 0.1350\n",
      "Epoch: 2 / 3, Step: 1482 / 2250 Loss: 0.2535\n",
      "Epoch: 2 / 3, Step: 1483 / 2250 Loss: 0.1800\n",
      "Epoch: 2 / 3, Step: 1484 / 2250 Loss: 0.0427\n",
      "Epoch: 2 / 3, Step: 1485 / 2250 Loss: 0.1877\n",
      "Epoch: 2 / 3, Step: 1486 / 2250 Loss: 0.0405\n",
      "Epoch: 2 / 3, Step: 1487 / 2250 Loss: 0.1626\n",
      "Epoch: 2 / 3, Step: 1488 / 2250 Loss: 0.0418\n",
      "Epoch: 2 / 3, Step: 1489 / 2250 Loss: 0.1799\n",
      "Epoch: 2 / 3, Step: 1490 / 2250 Loss: 0.3498\n",
      "Epoch: 2 / 3, Step: 1491 / 2250 Loss: 0.1967\n",
      "Epoch: 2 / 3, Step: 1492 / 2250 Loss: 0.1412\n",
      "Epoch: 2 / 3, Step: 1493 / 2250 Loss: 0.2388\n",
      "Epoch: 2 / 3, Step: 1494 / 2250 Loss: 0.2956\n",
      "Epoch: 2 / 3, Step: 1495 / 2250 Loss: 0.1968\n",
      "Epoch: 2 / 3, Step: 1496 / 2250 Loss: 0.0230\n",
      "Epoch: 2 / 3, Step: 1497 / 2250 Loss: 0.0515\n",
      "Epoch: 2 / 3, Step: 1498 / 2250 Loss: 0.0536\n",
      "Epoch: 2 / 3, Step: 1499 / 2250 Loss: 0.1158\n",
      "Epoch: 2 / 3, Step: 1500 / 2250 Loss: 0.2047\n",
      "Epoch: 2 / 3, Step: 1501 / 2250 Loss: 0.1632\n",
      "Epoch: 2 / 3, Step: 1502 / 2250 Loss: 0.0357\n",
      "Epoch: 2 / 3, Step: 1503 / 2250 Loss: 0.2633\n",
      "Epoch: 2 / 3, Step: 1504 / 2250 Loss: 0.1609\n",
      "Epoch: 2 / 3, Step: 1505 / 2250 Loss: 0.0294\n",
      "Epoch: 2 / 3, Step: 1506 / 2250 Loss: 0.1659\n",
      "Epoch: 2 / 3, Step: 1507 / 2250 Loss: 0.6158\n",
      "Epoch: 2 / 3, Step: 1508 / 2250 Loss: 0.0530\n",
      "Epoch: 2 / 3, Step: 1509 / 2250 Loss: 0.0995\n",
      "Epoch: 2 / 3, Step: 1510 / 2250 Loss: 0.1424\n",
      "Epoch: 2 / 3, Step: 1511 / 2250 Loss: 0.1583\n",
      "Epoch: 2 / 3, Step: 1512 / 2250 Loss: 0.0768\n",
      "Epoch: 2 / 3, Step: 1513 / 2250 Loss: 0.0401\n",
      "Epoch: 2 / 3, Step: 1514 / 2250 Loss: 0.1966\n",
      "Epoch: 2 / 3, Step: 1515 / 2250 Loss: 0.0847\n",
      "Epoch: 2 / 3, Step: 1516 / 2250 Loss: 0.2524\n",
      "Epoch: 2 / 3, Step: 1517 / 2250 Loss: 0.2191\n",
      "Epoch: 2 / 3, Step: 1518 / 2250 Loss: 0.1017\n",
      "Epoch: 2 / 3, Step: 1519 / 2250 Loss: 0.1233\n",
      "Epoch: 2 / 3, Step: 1520 / 2250 Loss: 0.1304\n",
      "Epoch: 2 / 3, Step: 1521 / 2250 Loss: 0.0815\n",
      "Epoch: 2 / 3, Step: 1522 / 2250 Loss: 0.2214\n",
      "Epoch: 2 / 3, Step: 1523 / 2250 Loss: 0.0798\n",
      "Epoch: 2 / 3, Step: 1524 / 2250 Loss: 0.1864\n",
      "Epoch: 2 / 3, Step: 1525 / 2250 Loss: 0.0414\n",
      "Epoch: 2 / 3, Step: 1526 / 2250 Loss: 0.2962\n",
      "Epoch: 2 / 3, Step: 1527 / 2250 Loss: 0.0276\n",
      "Epoch: 2 / 3, Step: 1528 / 2250 Loss: 0.2327\n",
      "Epoch: 2 / 3, Step: 1529 / 2250 Loss: 0.2393\n",
      "Epoch: 2 / 3, Step: 1530 / 2250 Loss: 0.2366\n",
      "Epoch: 2 / 3, Step: 1531 / 2250 Loss: 0.3284\n",
      "Epoch: 2 / 3, Step: 1532 / 2250 Loss: 0.1490\n",
      "Epoch: 2 / 3, Step: 1533 / 2250 Loss: 0.1553\n",
      "Epoch: 2 / 3, Step: 1534 / 2250 Loss: 0.0210\n",
      "Epoch: 2 / 3, Step: 1535 / 2250 Loss: 0.0373\n",
      "Epoch: 2 / 3, Step: 1536 / 2250 Loss: 0.0560\n",
      "Epoch: 2 / 3, Step: 1537 / 2250 Loss: 0.0502\n",
      "Epoch: 2 / 3, Step: 1538 / 2250 Loss: 0.0978\n",
      "Epoch: 2 / 3, Step: 1539 / 2250 Loss: 0.2842\n",
      "Epoch: 2 / 3, Step: 1540 / 2250 Loss: 0.1659\n",
      "Epoch: 2 / 3, Step: 1541 / 2250 Loss: 0.2336\n",
      "Epoch: 2 / 3, Step: 1542 / 2250 Loss: 0.0714\n",
      "Epoch: 2 / 3, Step: 1543 / 2250 Loss: 0.0244\n",
      "Epoch: 2 / 3, Step: 1544 / 2250 Loss: 0.0566\n",
      "Epoch: 2 / 3, Step: 1545 / 2250 Loss: 0.1032\n",
      "Epoch: 2 / 3, Step: 1546 / 2250 Loss: 0.1578\n",
      "Epoch: 2 / 3, Step: 1547 / 2250 Loss: 0.0914\n",
      "Epoch: 2 / 3, Step: 1548 / 2250 Loss: 0.2126\n",
      "Epoch: 2 / 3, Step: 1549 / 2250 Loss: 0.1990\n",
      "Epoch: 2 / 3, Step: 1550 / 2250 Loss: 0.0367\n",
      "Epoch: 2 / 3, Step: 1551 / 2250 Loss: 0.0880\n",
      "Epoch: 2 / 3, Step: 1552 / 2250 Loss: 0.1284\n",
      "Epoch: 2 / 3, Step: 1553 / 2250 Loss: 0.1539\n",
      "Epoch: 2 / 3, Step: 1554 / 2250 Loss: 0.2265\n",
      "Epoch: 2 / 3, Step: 1555 / 2250 Loss: 0.0219\n",
      "Epoch: 2 / 3, Step: 1556 / 2250 Loss: 0.1169\n",
      "Epoch: 2 / 3, Step: 1557 / 2250 Loss: 0.1025\n",
      "Epoch: 2 / 3, Step: 1558 / 2250 Loss: 0.0691\n",
      "Epoch: 2 / 3, Step: 1559 / 2250 Loss: 0.1082\n",
      "Epoch: 2 / 3, Step: 1560 / 2250 Loss: 0.0677\n",
      "Epoch: 2 / 3, Step: 1561 / 2250 Loss: 0.0737\n",
      "Epoch: 2 / 3, Step: 1562 / 2250 Loss: 0.2477\n",
      "Epoch: 2 / 3, Step: 1563 / 2250 Loss: 0.1007\n",
      "Epoch: 2 / 3, Step: 1564 / 2250 Loss: 0.0440\n",
      "Epoch: 2 / 3, Step: 1565 / 2250 Loss: 0.1225\n",
      "Epoch: 2 / 3, Step: 1566 / 2250 Loss: 0.2161\n",
      "Epoch: 2 / 3, Step: 1567 / 2250 Loss: 0.0600\n",
      "Epoch: 2 / 3, Step: 1568 / 2250 Loss: 0.0971\n",
      "Epoch: 2 / 3, Step: 1569 / 2250 Loss: 0.2889\n",
      "Epoch: 2 / 3, Step: 1570 / 2250 Loss: 0.0410\n",
      "Epoch: 2 / 3, Step: 1571 / 2250 Loss: 0.0621\n",
      "Epoch: 2 / 3, Step: 1572 / 2250 Loss: 0.0281\n",
      "Epoch: 2 / 3, Step: 1573 / 2250 Loss: 0.0305\n",
      "Epoch: 2 / 3, Step: 1574 / 2250 Loss: 0.1561\n",
      "Epoch: 2 / 3, Step: 1575 / 2250 Loss: 0.1919\n",
      "Epoch: 2 / 3, Step: 1576 / 2250 Loss: 0.3868\n",
      "Epoch: 2 / 3, Step: 1577 / 2250 Loss: 0.0636\n",
      "Epoch: 2 / 3, Step: 1578 / 2250 Loss: 0.1201\n",
      "Epoch: 2 / 3, Step: 1579 / 2250 Loss: 0.0676\n",
      "Epoch: 2 / 3, Step: 1580 / 2250 Loss: 0.0508\n",
      "Epoch: 2 / 3, Step: 1581 / 2250 Loss: 0.2577\n",
      "Epoch: 2 / 3, Step: 1582 / 2250 Loss: 0.0286\n",
      "Epoch: 2 / 3, Step: 1583 / 2250 Loss: 0.2533\n",
      "Epoch: 2 / 3, Step: 1584 / 2250 Loss: 0.0163\n",
      "Epoch: 2 / 3, Step: 1585 / 2250 Loss: 0.1191\n",
      "Epoch: 2 / 3, Step: 1586 / 2250 Loss: 0.1225\n",
      "Epoch: 2 / 3, Step: 1587 / 2250 Loss: 0.0178\n",
      "Epoch: 2 / 3, Step: 1588 / 2250 Loss: 0.2661\n",
      "Epoch: 2 / 3, Step: 1589 / 2250 Loss: 0.1774\n",
      "Epoch: 2 / 3, Step: 1590 / 2250 Loss: 0.0307\n",
      "Epoch: 2 / 3, Step: 1591 / 2250 Loss: 0.0338\n",
      "Epoch: 2 / 3, Step: 1592 / 2250 Loss: 0.2761\n",
      "Epoch: 2 / 3, Step: 1593 / 2250 Loss: 0.2285\n",
      "Epoch: 2 / 3, Step: 1594 / 2250 Loss: 0.3101\n",
      "Epoch: 2 / 3, Step: 1595 / 2250 Loss: 0.2792\n",
      "Epoch: 2 / 3, Step: 1596 / 2250 Loss: 0.1871\n",
      "Epoch: 2 / 3, Step: 1597 / 2250 Loss: 0.4027\n",
      "Epoch: 2 / 3, Step: 1598 / 2250 Loss: 0.0117\n",
      "Epoch: 2 / 3, Step: 1599 / 2250 Loss: 0.3909\n",
      "Epoch: 2 / 3, Step: 1600 / 2250 Loss: 0.2394\n",
      "Epoch: 2 / 3, Step: 1601 / 2250 Loss: 0.3467\n",
      "Epoch: 2 / 3, Step: 1602 / 2250 Loss: 0.1753\n",
      "Epoch: 2 / 3, Step: 1603 / 2250 Loss: 0.0151\n",
      "Epoch: 2 / 3, Step: 1604 / 2250 Loss: 0.0641\n",
      "Epoch: 2 / 3, Step: 1605 / 2250 Loss: 0.1205\n",
      "Epoch: 2 / 3, Step: 1606 / 2250 Loss: 0.0586\n",
      "Epoch: 2 / 3, Step: 1607 / 2250 Loss: 0.1495\n",
      "Epoch: 2 / 3, Step: 1608 / 2250 Loss: 0.1727\n",
      "Epoch: 2 / 3, Step: 1609 / 2250 Loss: 0.0981\n",
      "Epoch: 2 / 3, Step: 1610 / 2250 Loss: 0.0623\n",
      "Epoch: 2 / 3, Step: 1611 / 2250 Loss: 0.1762\n",
      "Epoch: 2 / 3, Step: 1612 / 2250 Loss: 0.0439\n",
      "Epoch: 2 / 3, Step: 1613 / 2250 Loss: 0.1225\n",
      "Epoch: 2 / 3, Step: 1614 / 2250 Loss: 0.0580\n",
      "Epoch: 2 / 3, Step: 1615 / 2250 Loss: 0.1309\n",
      "Epoch: 2 / 3, Step: 1616 / 2250 Loss: 0.0298\n",
      "Epoch: 2 / 3, Step: 1617 / 2250 Loss: 0.1572\n",
      "Epoch: 2 / 3, Step: 1618 / 2250 Loss: 0.1688\n",
      "Epoch: 2 / 3, Step: 1619 / 2250 Loss: 0.0610\n",
      "Epoch: 2 / 3, Step: 1620 / 2250 Loss: 0.1564\n",
      "Epoch: 2 / 3, Step: 1621 / 2250 Loss: 0.0456\n",
      "Epoch: 2 / 3, Step: 1622 / 2250 Loss: 0.0956\n",
      "Epoch: 2 / 3, Step: 1623 / 2250 Loss: 0.3133\n",
      "Epoch: 2 / 3, Step: 1624 / 2250 Loss: 0.2486\n",
      "Epoch: 2 / 3, Step: 1625 / 2250 Loss: 0.1570\n",
      "Epoch: 2 / 3, Step: 1626 / 2250 Loss: 0.0733\n",
      "Epoch: 2 / 3, Step: 1627 / 2250 Loss: 0.0374\n",
      "Epoch: 2 / 3, Step: 1628 / 2250 Loss: 0.1801\n",
      "Epoch: 2 / 3, Step: 1629 / 2250 Loss: 0.1001\n",
      "Epoch: 2 / 3, Step: 1630 / 2250 Loss: 0.2522\n",
      "Epoch: 2 / 3, Step: 1631 / 2250 Loss: 0.1285\n",
      "Epoch: 2 / 3, Step: 1632 / 2250 Loss: 0.2071\n",
      "Epoch: 2 / 3, Step: 1633 / 2250 Loss: 0.2323\n",
      "Epoch: 2 / 3, Step: 1634 / 2250 Loss: 0.0728\n",
      "Epoch: 2 / 3, Step: 1635 / 2250 Loss: 0.0776\n",
      "Epoch: 2 / 3, Step: 1636 / 2250 Loss: 0.1086\n",
      "Epoch: 2 / 3, Step: 1637 / 2250 Loss: 0.2087\n",
      "Epoch: 2 / 3, Step: 1638 / 2250 Loss: 0.1016\n",
      "Epoch: 2 / 3, Step: 1639 / 2250 Loss: 0.0645\n",
      "Epoch: 2 / 3, Step: 1640 / 2250 Loss: 0.1624\n",
      "Epoch: 2 / 3, Step: 1641 / 2250 Loss: 0.2472\n",
      "Epoch: 2 / 3, Step: 1642 / 2250 Loss: 0.1617\n",
      "Epoch: 2 / 3, Step: 1643 / 2250 Loss: 0.0998\n",
      "Epoch: 2 / 3, Step: 1644 / 2250 Loss: 0.1005\n",
      "Epoch: 2 / 3, Step: 1645 / 2250 Loss: 0.0487\n",
      "Epoch: 2 / 3, Step: 1646 / 2250 Loss: 0.1883\n",
      "Epoch: 2 / 3, Step: 1647 / 2250 Loss: 0.3952\n",
      "Epoch: 2 / 3, Step: 1648 / 2250 Loss: 0.3498\n",
      "Epoch: 2 / 3, Step: 1649 / 2250 Loss: 0.0223\n",
      "Epoch: 2 / 3, Step: 1650 / 2250 Loss: 0.0421\n",
      "Epoch: 2 / 3, Step: 1651 / 2250 Loss: 0.1317\n",
      "Epoch: 2 / 3, Step: 1652 / 2250 Loss: 0.1507\n",
      "Epoch: 2 / 3, Step: 1653 / 2250 Loss: 0.0317\n",
      "Epoch: 2 / 3, Step: 1654 / 2250 Loss: 0.1189\n",
      "Epoch: 2 / 3, Step: 1655 / 2250 Loss: 0.2647\n",
      "Epoch: 2 / 3, Step: 1656 / 2250 Loss: 0.1374\n",
      "Epoch: 2 / 3, Step: 1657 / 2250 Loss: 0.1177\n",
      "Epoch: 2 / 3, Step: 1658 / 2250 Loss: 0.0280\n",
      "Epoch: 2 / 3, Step: 1659 / 2250 Loss: 0.2475\n",
      "Epoch: 2 / 3, Step: 1660 / 2250 Loss: 0.2874\n",
      "Epoch: 2 / 3, Step: 1661 / 2250 Loss: 0.1804\n",
      "Epoch: 2 / 3, Step: 1662 / 2250 Loss: 0.1067\n",
      "Epoch: 2 / 3, Step: 1663 / 2250 Loss: 0.0236\n",
      "Epoch: 2 / 3, Step: 1664 / 2250 Loss: 0.0334\n",
      "Epoch: 2 / 3, Step: 1665 / 2250 Loss: 0.1002\n",
      "Epoch: 2 / 3, Step: 1666 / 2250 Loss: 0.0695\n",
      "Epoch: 2 / 3, Step: 1667 / 2250 Loss: 0.0649\n",
      "Epoch: 2 / 3, Step: 1668 / 2250 Loss: 0.1960\n",
      "Epoch: 2 / 3, Step: 1669 / 2250 Loss: 0.2257\n",
      "Epoch: 2 / 3, Step: 1670 / 2250 Loss: 0.1180\n",
      "Epoch: 2 / 3, Step: 1671 / 2250 Loss: 0.0878\n",
      "Epoch: 2 / 3, Step: 1672 / 2250 Loss: 0.1611\n",
      "Epoch: 2 / 3, Step: 1673 / 2250 Loss: 0.0307\n",
      "Epoch: 2 / 3, Step: 1674 / 2250 Loss: 0.1584\n",
      "Epoch: 2 / 3, Step: 1675 / 2250 Loss: 0.3801\n",
      "Epoch: 2 / 3, Step: 1676 / 2250 Loss: 0.1608\n",
      "Epoch: 2 / 3, Step: 1677 / 2250 Loss: 0.0573\n",
      "Epoch: 2 / 3, Step: 1678 / 2250 Loss: 0.3068\n",
      "Epoch: 2 / 3, Step: 1679 / 2250 Loss: 0.0362\n",
      "Epoch: 2 / 3, Step: 1680 / 2250 Loss: 0.0252\n",
      "Epoch: 2 / 3, Step: 1681 / 2250 Loss: 0.3798\n",
      "Epoch: 2 / 3, Step: 1682 / 2250 Loss: 0.1114\n",
      "Epoch: 2 / 3, Step: 1683 / 2250 Loss: 0.2181\n",
      "Epoch: 2 / 3, Step: 1684 / 2250 Loss: 0.2472\n",
      "Epoch: 2 / 3, Step: 1685 / 2250 Loss: 0.0143\n",
      "Epoch: 2 / 3, Step: 1686 / 2250 Loss: 0.1103\n",
      "Epoch: 2 / 3, Step: 1687 / 2250 Loss: 0.2716\n",
      "Epoch: 2 / 3, Step: 1688 / 2250 Loss: 0.1359\n",
      "Epoch: 2 / 3, Step: 1689 / 2250 Loss: 0.0338\n",
      "Epoch: 2 / 3, Step: 1690 / 2250 Loss: 0.2389\n",
      "Epoch: 2 / 3, Step: 1691 / 2250 Loss: 0.0160\n",
      "Epoch: 2 / 3, Step: 1692 / 2250 Loss: 0.1252\n",
      "Epoch: 2 / 3, Step: 1693 / 2250 Loss: 0.0335\n",
      "Epoch: 2 / 3, Step: 1694 / 2250 Loss: 0.2935\n",
      "Epoch: 2 / 3, Step: 1695 / 2250 Loss: 0.0332\n",
      "Epoch: 2 / 3, Step: 1696 / 2250 Loss: 0.4157\n",
      "Epoch: 2 / 3, Step: 1697 / 2250 Loss: 0.0866\n",
      "Epoch: 2 / 3, Step: 1698 / 2250 Loss: 0.1750\n",
      "Epoch: 2 / 3, Step: 1699 / 2250 Loss: 0.2466\n",
      "Epoch: 2 / 3, Step: 1700 / 2250 Loss: 0.0213\n",
      "Epoch: 2 / 3, Step: 1701 / 2250 Loss: 0.0383\n",
      "Epoch: 2 / 3, Step: 1702 / 2250 Loss: 0.1263\n",
      "Epoch: 2 / 3, Step: 1703 / 2250 Loss: 0.0583\n",
      "Epoch: 2 / 3, Step: 1704 / 2250 Loss: 0.3151\n",
      "Epoch: 2 / 3, Step: 1705 / 2250 Loss: 0.0694\n",
      "Epoch: 2 / 3, Step: 1706 / 2250 Loss: 0.1774\n",
      "Epoch: 2 / 3, Step: 1707 / 2250 Loss: 0.1940\n",
      "Epoch: 2 / 3, Step: 1708 / 2250 Loss: 0.2108\n",
      "Epoch: 2 / 3, Step: 1709 / 2250 Loss: 0.3736\n",
      "Epoch: 2 / 3, Step: 1710 / 2250 Loss: 0.1812\n",
      "Epoch: 2 / 3, Step: 1711 / 2250 Loss: 0.1012\n",
      "Epoch: 2 / 3, Step: 1712 / 2250 Loss: 0.0592\n",
      "Epoch: 2 / 3, Step: 1713 / 2250 Loss: 0.0801\n",
      "Epoch: 2 / 3, Step: 1714 / 2250 Loss: 0.0667\n",
      "Epoch: 2 / 3, Step: 1715 / 2250 Loss: 0.0516\n",
      "Epoch: 2 / 3, Step: 1716 / 2250 Loss: 0.0557\n",
      "Epoch: 2 / 3, Step: 1717 / 2250 Loss: 0.0315\n",
      "Epoch: 2 / 3, Step: 1718 / 2250 Loss: 0.0559\n",
      "Epoch: 2 / 3, Step: 1719 / 2250 Loss: 0.0966\n",
      "Epoch: 2 / 3, Step: 1720 / 2250 Loss: 0.1273\n",
      "Epoch: 2 / 3, Step: 1721 / 2250 Loss: 0.1197\n",
      "Epoch: 2 / 3, Step: 1722 / 2250 Loss: 0.2226\n",
      "Epoch: 2 / 3, Step: 1723 / 2250 Loss: 0.1766\n",
      "Epoch: 2 / 3, Step: 1724 / 2250 Loss: 0.0275\n",
      "Epoch: 2 / 3, Step: 1725 / 2250 Loss: 0.1237\n",
      "Epoch: 2 / 3, Step: 1726 / 2250 Loss: 0.1129\n",
      "Epoch: 2 / 3, Step: 1727 / 2250 Loss: 0.2599\n",
      "Epoch: 2 / 3, Step: 1728 / 2250 Loss: 0.2383\n",
      "Epoch: 2 / 3, Step: 1729 / 2250 Loss: 0.0529\n",
      "Epoch: 2 / 3, Step: 1730 / 2250 Loss: 0.2660\n",
      "Epoch: 2 / 3, Step: 1731 / 2250 Loss: 0.1318\n",
      "Epoch: 2 / 3, Step: 1732 / 2250 Loss: 0.4652\n",
      "Epoch: 2 / 3, Step: 1733 / 2250 Loss: 0.2095\n",
      "Epoch: 2 / 3, Step: 1734 / 2250 Loss: 0.2860\n",
      "Epoch: 2 / 3, Step: 1735 / 2250 Loss: 0.0912\n",
      "Epoch: 2 / 3, Step: 1736 / 2250 Loss: 0.0465\n",
      "Epoch: 2 / 3, Step: 1737 / 2250 Loss: 0.0754\n",
      "Epoch: 2 / 3, Step: 1738 / 2250 Loss: 0.0814\n",
      "Epoch: 2 / 3, Step: 1739 / 2250 Loss: 0.1373\n",
      "Epoch: 2 / 3, Step: 1740 / 2250 Loss: 0.2201\n",
      "Epoch: 2 / 3, Step: 1741 / 2250 Loss: 0.1646\n",
      "Epoch: 2 / 3, Step: 1742 / 2250 Loss: 0.0778\n",
      "Epoch: 2 / 3, Step: 1743 / 2250 Loss: 0.1762\n",
      "Epoch: 2 / 3, Step: 1744 / 2250 Loss: 0.3136\n",
      "Epoch: 2 / 3, Step: 1745 / 2250 Loss: 0.1062\n",
      "Epoch: 2 / 3, Step: 1746 / 2250 Loss: 0.1856\n",
      "Epoch: 2 / 3, Step: 1747 / 2250 Loss: 0.0303\n",
      "Epoch: 2 / 3, Step: 1748 / 2250 Loss: 0.3098\n",
      "Epoch: 2 / 3, Step: 1749 / 2250 Loss: 0.3122\n",
      "Epoch: 2 / 3, Step: 1750 / 2250 Loss: 0.0520\n",
      "Epoch: 2 / 3, Step: 1751 / 2250 Loss: 0.2036\n",
      "Epoch: 2 / 3, Step: 1752 / 2250 Loss: 0.1151\n",
      "Epoch: 2 / 3, Step: 1753 / 2250 Loss: 0.1769\n",
      "Epoch: 2 / 3, Step: 1754 / 2250 Loss: 0.0614\n",
      "Epoch: 2 / 3, Step: 1755 / 2250 Loss: 0.2411\n",
      "Epoch: 2 / 3, Step: 1756 / 2250 Loss: 0.0995\n",
      "Epoch: 2 / 3, Step: 1757 / 2250 Loss: 0.3743\n",
      "Epoch: 2 / 3, Step: 1758 / 2250 Loss: 0.0784\n",
      "Epoch: 2 / 3, Step: 1759 / 2250 Loss: 0.0810\n",
      "Epoch: 2 / 3, Step: 1760 / 2250 Loss: 0.0855\n",
      "Epoch: 2 / 3, Step: 1761 / 2250 Loss: 0.0411\n",
      "Epoch: 2 / 3, Step: 1762 / 2250 Loss: 0.0461\n",
      "Epoch: 2 / 3, Step: 1763 / 2250 Loss: 0.1349\n",
      "Epoch: 2 / 3, Step: 1764 / 2250 Loss: 0.1558\n",
      "Epoch: 2 / 3, Step: 1765 / 2250 Loss: 0.1282\n",
      "Epoch: 2 / 3, Step: 1766 / 2250 Loss: 0.0763\n",
      "Epoch: 2 / 3, Step: 1767 / 2250 Loss: 0.0249\n",
      "Epoch: 2 / 3, Step: 1768 / 2250 Loss: 0.1423\n",
      "Epoch: 2 / 3, Step: 1769 / 2250 Loss: 0.2342\n",
      "Epoch: 2 / 3, Step: 1770 / 2250 Loss: 0.2447\n",
      "Epoch: 2 / 3, Step: 1771 / 2250 Loss: 0.1467\n",
      "Epoch: 2 / 3, Step: 1772 / 2250 Loss: 0.2307\n",
      "Epoch: 2 / 3, Step: 1773 / 2250 Loss: 0.2481\n",
      "Epoch: 2 / 3, Step: 1774 / 2250 Loss: 0.0681\n",
      "Epoch: 2 / 3, Step: 1775 / 2250 Loss: 0.1813\n",
      "Epoch: 2 / 3, Step: 1776 / 2250 Loss: 0.0513\n",
      "Epoch: 2 / 3, Step: 1777 / 2250 Loss: 0.1742\n",
      "Epoch: 2 / 3, Step: 1778 / 2250 Loss: 0.1420\n",
      "Epoch: 2 / 3, Step: 1779 / 2250 Loss: 0.0756\n",
      "Epoch: 2 / 3, Step: 1780 / 2250 Loss: 0.0816\n",
      "Epoch: 2 / 3, Step: 1781 / 2250 Loss: 0.2822\n",
      "Epoch: 2 / 3, Step: 1782 / 2250 Loss: 0.2517\n",
      "Epoch: 2 / 3, Step: 1783 / 2250 Loss: 0.0820\n",
      "Epoch: 2 / 3, Step: 1784 / 2250 Loss: 0.0432\n",
      "Epoch: 2 / 3, Step: 1785 / 2250 Loss: 0.1520\n",
      "Epoch: 2 / 3, Step: 1786 / 2250 Loss: 0.0295\n",
      "Epoch: 2 / 3, Step: 1787 / 2250 Loss: 0.0432\n",
      "Epoch: 2 / 3, Step: 1788 / 2250 Loss: 0.1173\n",
      "Epoch: 2 / 3, Step: 1789 / 2250 Loss: 0.1081\n",
      "Epoch: 2 / 3, Step: 1790 / 2250 Loss: 0.1681\n",
      "Epoch: 2 / 3, Step: 1791 / 2250 Loss: 0.0442\n",
      "Epoch: 2 / 3, Step: 1792 / 2250 Loss: 0.1783\n",
      "Epoch: 2 / 3, Step: 1793 / 2250 Loss: 0.0684\n",
      "Epoch: 2 / 3, Step: 1794 / 2250 Loss: 0.0734\n",
      "Epoch: 2 / 3, Step: 1795 / 2250 Loss: 0.0718\n",
      "Epoch: 2 / 3, Step: 1796 / 2250 Loss: 0.0411\n",
      "Epoch: 2 / 3, Step: 1797 / 2250 Loss: 0.0251\n",
      "Epoch: 2 / 3, Step: 1798 / 2250 Loss: 0.0229\n",
      "Epoch: 2 / 3, Step: 1799 / 2250 Loss: 0.0390\n",
      "Epoch: 2 / 3, Step: 1800 / 2250 Loss: 0.3013\n",
      "Epoch: 2 / 3, Step: 1801 / 2250 Loss: 0.2295\n",
      "Epoch: 2 / 3, Step: 1802 / 2250 Loss: 0.2910\n",
      "Epoch: 2 / 3, Step: 1803 / 2250 Loss: 0.0326\n",
      "Epoch: 2 / 3, Step: 1804 / 2250 Loss: 0.0256\n",
      "Epoch: 2 / 3, Step: 1805 / 2250 Loss: 0.1279\n",
      "Epoch: 2 / 3, Step: 1806 / 2250 Loss: 0.2424\n",
      "Epoch: 2 / 3, Step: 1807 / 2250 Loss: 0.1389\n",
      "Epoch: 2 / 3, Step: 1808 / 2250 Loss: 0.1344\n",
      "Epoch: 2 / 3, Step: 1809 / 2250 Loss: 0.0195\n",
      "Epoch: 2 / 3, Step: 1810 / 2250 Loss: 0.1627\n",
      "Epoch: 2 / 3, Step: 1811 / 2250 Loss: 0.0245\n",
      "Epoch: 2 / 3, Step: 1812 / 2250 Loss: 0.0548\n",
      "Epoch: 2 / 3, Step: 1813 / 2250 Loss: 0.0451\n",
      "Epoch: 2 / 3, Step: 1814 / 2250 Loss: 0.1451\n",
      "Epoch: 2 / 3, Step: 1815 / 2250 Loss: 0.0615\n",
      "Epoch: 2 / 3, Step: 1816 / 2250 Loss: 0.0231\n",
      "Epoch: 2 / 3, Step: 1817 / 2250 Loss: 0.1109\n",
      "Epoch: 2 / 3, Step: 1818 / 2250 Loss: 0.1645\n",
      "Epoch: 2 / 3, Step: 1819 / 2250 Loss: 0.2505\n",
      "Epoch: 2 / 3, Step: 1820 / 2250 Loss: 0.1120\n",
      "Epoch: 2 / 3, Step: 1821 / 2250 Loss: 0.0278\n",
      "Epoch: 2 / 3, Step: 1822 / 2250 Loss: 0.0362\n",
      "Epoch: 2 / 3, Step: 1823 / 2250 Loss: 0.1601\n",
      "Epoch: 2 / 3, Step: 1824 / 2250 Loss: 0.0328\n",
      "Epoch: 2 / 3, Step: 1825 / 2250 Loss: 0.1719\n",
      "Epoch: 2 / 3, Step: 1826 / 2250 Loss: 0.1130\n",
      "Epoch: 2 / 3, Step: 1827 / 2250 Loss: 0.0966\n",
      "Epoch: 2 / 3, Step: 1828 / 2250 Loss: 0.3725\n",
      "Epoch: 2 / 3, Step: 1829 / 2250 Loss: 0.2938\n",
      "Epoch: 2 / 3, Step: 1830 / 2250 Loss: 0.0235\n",
      "Epoch: 2 / 3, Step: 1831 / 2250 Loss: 0.1454\n",
      "Epoch: 2 / 3, Step: 1832 / 2250 Loss: 0.0981\n",
      "Epoch: 2 / 3, Step: 1833 / 2250 Loss: 0.0453\n",
      "Epoch: 2 / 3, Step: 1834 / 2250 Loss: 0.1595\n",
      "Epoch: 2 / 3, Step: 1835 / 2250 Loss: 0.2370\n",
      "Epoch: 2 / 3, Step: 1836 / 2250 Loss: 0.0359\n",
      "Epoch: 2 / 3, Step: 1837 / 2250 Loss: 0.1289\n",
      "Epoch: 2 / 3, Step: 1838 / 2250 Loss: 0.1233\n",
      "Epoch: 2 / 3, Step: 1839 / 2250 Loss: 0.2214\n",
      "Epoch: 2 / 3, Step: 1840 / 2250 Loss: 0.1146\n",
      "Epoch: 2 / 3, Step: 1841 / 2250 Loss: 0.2130\n",
      "Epoch: 2 / 3, Step: 1842 / 2250 Loss: 0.0358\n",
      "Epoch: 2 / 3, Step: 1843 / 2250 Loss: 0.0946\n",
      "Epoch: 2 / 3, Step: 1844 / 2250 Loss: 0.1138\n",
      "Epoch: 2 / 3, Step: 1845 / 2250 Loss: 0.0430\n",
      "Epoch: 2 / 3, Step: 1846 / 2250 Loss: 0.0224\n",
      "Epoch: 2 / 3, Step: 1847 / 2250 Loss: 0.0256\n",
      "Epoch: 2 / 3, Step: 1848 / 2250 Loss: 0.0383\n",
      "Epoch: 2 / 3, Step: 1849 / 2250 Loss: 0.3065\n",
      "Epoch: 2 / 3, Step: 1850 / 2250 Loss: 0.0257\n",
      "Epoch: 2 / 3, Step: 1851 / 2250 Loss: 0.1012\n",
      "Epoch: 2 / 3, Step: 1852 / 2250 Loss: 0.0159\n",
      "Epoch: 2 / 3, Step: 1853 / 2250 Loss: 0.0330\n",
      "Epoch: 2 / 3, Step: 1854 / 2250 Loss: 0.0283\n",
      "Epoch: 2 / 3, Step: 1855 / 2250 Loss: 0.2541\n",
      "Epoch: 2 / 3, Step: 1856 / 2250 Loss: 0.0995\n",
      "Epoch: 2 / 3, Step: 1857 / 2250 Loss: 0.0428\n",
      "Epoch: 2 / 3, Step: 1858 / 2250 Loss: 0.0721\n",
      "Epoch: 2 / 3, Step: 1859 / 2250 Loss: 0.2555\n",
      "Epoch: 2 / 3, Step: 1860 / 2250 Loss: 0.2120\n",
      "Epoch: 2 / 3, Step: 1861 / 2250 Loss: 0.2163\n",
      "Epoch: 2 / 3, Step: 1862 / 2250 Loss: 0.1304\n",
      "Epoch: 2 / 3, Step: 1863 / 2250 Loss: 0.0603\n",
      "Epoch: 2 / 3, Step: 1864 / 2250 Loss: 0.0819\n",
      "Epoch: 2 / 3, Step: 1865 / 2250 Loss: 0.0219\n",
      "Epoch: 2 / 3, Step: 1866 / 2250 Loss: 0.0210\n",
      "Epoch: 2 / 3, Step: 1867 / 2250 Loss: 0.1463\n",
      "Epoch: 2 / 3, Step: 1868 / 2250 Loss: 0.0400\n",
      "Epoch: 2 / 3, Step: 1869 / 2250 Loss: 0.0383\n",
      "Epoch: 2 / 3, Step: 1870 / 2250 Loss: 0.3857\n",
      "Epoch: 2 / 3, Step: 1871 / 2250 Loss: 0.1470\n",
      "Epoch: 2 / 3, Step: 1872 / 2250 Loss: 0.4181\n",
      "Epoch: 2 / 3, Step: 1873 / 2250 Loss: 0.1045\n",
      "Epoch: 2 / 3, Step: 1874 / 2250 Loss: 0.1508\n",
      "Epoch: 2 / 3, Step: 1875 / 2250 Loss: 0.1703\n",
      "Epoch: 2 / 3, Step: 1876 / 2250 Loss: 0.0587\n",
      "Epoch: 2 / 3, Step: 1877 / 2250 Loss: 0.0838\n",
      "Epoch: 2 / 3, Step: 1878 / 2250 Loss: 0.0642\n",
      "Epoch: 2 / 3, Step: 1879 / 2250 Loss: 0.0784\n",
      "Epoch: 2 / 3, Step: 1880 / 2250 Loss: 0.1095\n",
      "Epoch: 2 / 3, Step: 1881 / 2250 Loss: 0.2958\n",
      "Epoch: 2 / 3, Step: 1882 / 2250 Loss: 0.1525\n",
      "Epoch: 2 / 3, Step: 1883 / 2250 Loss: 0.2048\n",
      "Epoch: 2 / 3, Step: 1884 / 2250 Loss: 0.1791\n",
      "Epoch: 2 / 3, Step: 1885 / 2250 Loss: 0.1759\n",
      "Epoch: 2 / 3, Step: 1886 / 2250 Loss: 0.0877\n",
      "Epoch: 2 / 3, Step: 1887 / 2250 Loss: 0.0371\n",
      "Epoch: 2 / 3, Step: 1888 / 2250 Loss: 0.0401\n",
      "Epoch: 2 / 3, Step: 1889 / 2250 Loss: 0.0535\n",
      "Epoch: 2 / 3, Step: 1890 / 2250 Loss: 0.1479\n",
      "Epoch: 2 / 3, Step: 1891 / 2250 Loss: 0.0995\n",
      "Epoch: 2 / 3, Step: 1892 / 2250 Loss: 0.3861\n",
      "Epoch: 2 / 3, Step: 1893 / 2250 Loss: 0.2622\n",
      "Epoch: 2 / 3, Step: 1894 / 2250 Loss: 0.0355\n",
      "Epoch: 2 / 3, Step: 1895 / 2250 Loss: 0.0344\n",
      "Epoch: 2 / 3, Step: 1896 / 2250 Loss: 0.1432\n",
      "Epoch: 2 / 3, Step: 1897 / 2250 Loss: 0.0961\n",
      "Epoch: 2 / 3, Step: 1898 / 2250 Loss: 0.0703\n",
      "Epoch: 2 / 3, Step: 1899 / 2250 Loss: 0.0901\n",
      "Epoch: 2 / 3, Step: 1900 / 2250 Loss: 0.0420\n",
      "Epoch: 2 / 3, Step: 1901 / 2250 Loss: 0.1491\n",
      "Epoch: 2 / 3, Step: 1902 / 2250 Loss: 0.1139\n",
      "Epoch: 2 / 3, Step: 1903 / 2250 Loss: 0.1089\n",
      "Epoch: 2 / 3, Step: 1904 / 2250 Loss: 0.0163\n",
      "Epoch: 2 / 3, Step: 1905 / 2250 Loss: 0.1824\n",
      "Epoch: 2 / 3, Step: 1906 / 2250 Loss: 0.1463\n",
      "Epoch: 2 / 3, Step: 1907 / 2250 Loss: 0.0196\n",
      "Epoch: 2 / 3, Step: 1908 / 2250 Loss: 0.0336\n",
      "Epoch: 2 / 3, Step: 1909 / 2250 Loss: 0.0480\n",
      "Epoch: 2 / 3, Step: 1910 / 2250 Loss: 0.1808\n",
      "Epoch: 2 / 3, Step: 1911 / 2250 Loss: 0.3766\n",
      "Epoch: 2 / 3, Step: 1912 / 2250 Loss: 0.2319\n",
      "Epoch: 2 / 3, Step: 1913 / 2250 Loss: 0.0215\n",
      "Epoch: 2 / 3, Step: 1914 / 2250 Loss: 0.0502\n",
      "Epoch: 2 / 3, Step: 1915 / 2250 Loss: 0.0611\n",
      "Epoch: 2 / 3, Step: 1916 / 2250 Loss: 0.1461\n",
      "Epoch: 2 / 3, Step: 1917 / 2250 Loss: 0.1959\n",
      "Epoch: 2 / 3, Step: 1918 / 2250 Loss: 0.0779\n",
      "Epoch: 2 / 3, Step: 1919 / 2250 Loss: 0.0817\n",
      "Epoch: 2 / 3, Step: 1920 / 2250 Loss: 0.0697\n",
      "Epoch: 2 / 3, Step: 1921 / 2250 Loss: 0.2142\n",
      "Epoch: 2 / 3, Step: 1922 / 2250 Loss: 0.1992\n",
      "Epoch: 2 / 3, Step: 1923 / 2250 Loss: 0.3197\n",
      "Epoch: 2 / 3, Step: 1924 / 2250 Loss: 0.0791\n",
      "Epoch: 2 / 3, Step: 1925 / 2250 Loss: 0.0667\n",
      "Epoch: 2 / 3, Step: 1926 / 2250 Loss: 0.3184\n",
      "Epoch: 2 / 3, Step: 1927 / 2250 Loss: 0.0508\n",
      "Epoch: 2 / 3, Step: 1928 / 2250 Loss: 0.0687\n",
      "Epoch: 2 / 3, Step: 1929 / 2250 Loss: 0.1700\n",
      "Epoch: 2 / 3, Step: 1930 / 2250 Loss: 0.1626\n",
      "Epoch: 2 / 3, Step: 1931 / 2250 Loss: 0.0747\n",
      "Epoch: 2 / 3, Step: 1932 / 2250 Loss: 0.0674\n",
      "Epoch: 2 / 3, Step: 1933 / 2250 Loss: 0.1082\n",
      "Epoch: 2 / 3, Step: 1934 / 2250 Loss: 0.1671\n",
      "Epoch: 2 / 3, Step: 1935 / 2250 Loss: 0.0292\n",
      "Epoch: 2 / 3, Step: 1936 / 2250 Loss: 0.1178\n",
      "Epoch: 2 / 3, Step: 1937 / 2250 Loss: 0.2423\n",
      "Epoch: 2 / 3, Step: 1938 / 2250 Loss: 0.0331\n",
      "Epoch: 2 / 3, Step: 1939 / 2250 Loss: 0.3367\n",
      "Epoch: 2 / 3, Step: 1940 / 2250 Loss: 0.0859\n",
      "Epoch: 2 / 3, Step: 1941 / 2250 Loss: 0.0532\n",
      "Epoch: 2 / 3, Step: 1942 / 2250 Loss: 0.3018\n",
      "Epoch: 2 / 3, Step: 1943 / 2250 Loss: 0.1106\n",
      "Epoch: 2 / 3, Step: 1944 / 2250 Loss: 0.2056\n",
      "Epoch: 2 / 3, Step: 1945 / 2250 Loss: 0.3992\n",
      "Epoch: 2 / 3, Step: 1946 / 2250 Loss: 0.0470\n",
      "Epoch: 2 / 3, Step: 1947 / 2250 Loss: 0.2641\n",
      "Epoch: 2 / 3, Step: 1948 / 2250 Loss: 0.1806\n",
      "Epoch: 2 / 3, Step: 1949 / 2250 Loss: 0.0646\n",
      "Epoch: 2 / 3, Step: 1950 / 2250 Loss: 0.0433\n",
      "Epoch: 2 / 3, Step: 1951 / 2250 Loss: 0.4036\n",
      "Epoch: 2 / 3, Step: 1952 / 2250 Loss: 0.0947\n",
      "Epoch: 2 / 3, Step: 1953 / 2250 Loss: 0.0872\n",
      "Epoch: 2 / 3, Step: 1954 / 2250 Loss: 0.2094\n",
      "Epoch: 2 / 3, Step: 1955 / 2250 Loss: 0.3526\n",
      "Epoch: 2 / 3, Step: 1956 / 2250 Loss: 0.0863\n",
      "Epoch: 2 / 3, Step: 1957 / 2250 Loss: 0.1031\n",
      "Epoch: 2 / 3, Step: 1958 / 2250 Loss: 0.0691\n",
      "Epoch: 2 / 3, Step: 1959 / 2250 Loss: 0.0970\n",
      "Epoch: 2 / 3, Step: 1960 / 2250 Loss: 0.1777\n",
      "Epoch: 2 / 3, Step: 1961 / 2250 Loss: 0.1082\n",
      "Epoch: 2 / 3, Step: 1962 / 2250 Loss: 0.0765\n",
      "Epoch: 2 / 3, Step: 1963 / 2250 Loss: 0.1070\n",
      "Epoch: 2 / 3, Step: 1964 / 2250 Loss: 0.3389\n",
      "Epoch: 2 / 3, Step: 1965 / 2250 Loss: 0.1988\n",
      "Epoch: 2 / 3, Step: 1966 / 2250 Loss: 0.1678\n",
      "Epoch: 2 / 3, Step: 1967 / 2250 Loss: 0.0931\n",
      "Epoch: 2 / 3, Step: 1968 / 2250 Loss: 0.1579\n",
      "Epoch: 2 / 3, Step: 1969 / 2250 Loss: 0.2034\n",
      "Epoch: 2 / 3, Step: 1970 / 2250 Loss: 0.0756\n",
      "Epoch: 2 / 3, Step: 1971 / 2250 Loss: 0.0564\n",
      "Epoch: 2 / 3, Step: 1972 / 2250 Loss: 0.0232\n",
      "Epoch: 2 / 3, Step: 1973 / 2250 Loss: 0.0448\n",
      "Epoch: 2 / 3, Step: 1974 / 2250 Loss: 0.0841\n",
      "Epoch: 2 / 3, Step: 1975 / 2250 Loss: 0.2214\n",
      "Epoch: 2 / 3, Step: 1976 / 2250 Loss: 0.0553\n",
      "Epoch: 2 / 3, Step: 1977 / 2250 Loss: 0.0837\n",
      "Epoch: 2 / 3, Step: 1978 / 2250 Loss: 0.0461\n",
      "Epoch: 2 / 3, Step: 1979 / 2250 Loss: 0.0470\n",
      "Epoch: 2 / 3, Step: 1980 / 2250 Loss: 0.1134\n",
      "Epoch: 2 / 3, Step: 1981 / 2250 Loss: 0.1485\n",
      "Epoch: 2 / 3, Step: 1982 / 2250 Loss: 0.0199\n",
      "Epoch: 2 / 3, Step: 1983 / 2250 Loss: 0.2114\n",
      "Epoch: 2 / 3, Step: 1984 / 2250 Loss: 0.0856\n",
      "Epoch: 2 / 3, Step: 1985 / 2250 Loss: 0.0476\n",
      "Epoch: 2 / 3, Step: 1986 / 2250 Loss: 0.0444\n",
      "Epoch: 2 / 3, Step: 1987 / 2250 Loss: 0.0356\n",
      "Epoch: 2 / 3, Step: 1988 / 2250 Loss: 0.1322\n",
      "Epoch: 2 / 3, Step: 1989 / 2250 Loss: 0.0170\n",
      "Epoch: 2 / 3, Step: 1990 / 2250 Loss: 0.0369\n",
      "Epoch: 2 / 3, Step: 1991 / 2250 Loss: 0.2630\n",
      "Epoch: 2 / 3, Step: 1992 / 2250 Loss: 0.0388\n",
      "Epoch: 2 / 3, Step: 1993 / 2250 Loss: 0.0290\n",
      "Epoch: 2 / 3, Step: 1994 / 2250 Loss: 0.0876\n",
      "Epoch: 2 / 3, Step: 1995 / 2250 Loss: 0.0625\n",
      "Epoch: 2 / 3, Step: 1996 / 2250 Loss: 0.0254\n",
      "Epoch: 2 / 3, Step: 1997 / 2250 Loss: 0.2318\n",
      "Epoch: 2 / 3, Step: 1998 / 2250 Loss: 0.1402\n",
      "Epoch: 2 / 3, Step: 1999 / 2250 Loss: 0.2758\n",
      "Epoch: 2 / 3, Step: 2000 / 2250 Loss: 0.1380\n",
      "Epoch: 2 / 3, Step: 2001 / 2250 Loss: 0.0142\n",
      "Epoch: 2 / 3, Step: 2002 / 2250 Loss: 0.1442\n",
      "Epoch: 2 / 3, Step: 2003 / 2250 Loss: 0.1265\n",
      "Epoch: 2 / 3, Step: 2004 / 2250 Loss: 0.1659\n",
      "Epoch: 2 / 3, Step: 2005 / 2250 Loss: 0.1451\n",
      "Epoch: 2 / 3, Step: 2006 / 2250 Loss: 0.0751\n",
      "Epoch: 2 / 3, Step: 2007 / 2250 Loss: 0.1359\n",
      "Epoch: 2 / 3, Step: 2008 / 2250 Loss: 0.1847\n",
      "Epoch: 2 / 3, Step: 2009 / 2250 Loss: 0.1238\n",
      "Epoch: 2 / 3, Step: 2010 / 2250 Loss: 0.1509\n",
      "Epoch: 2 / 3, Step: 2011 / 2250 Loss: 0.0419\n",
      "Epoch: 2 / 3, Step: 2012 / 2250 Loss: 0.1118\n",
      "Epoch: 2 / 3, Step: 2013 / 2250 Loss: 0.0398\n",
      "Epoch: 2 / 3, Step: 2014 / 2250 Loss: 0.1397\n",
      "Epoch: 2 / 3, Step: 2015 / 2250 Loss: 0.0328\n",
      "Epoch: 2 / 3, Step: 2016 / 2250 Loss: 0.1596\n",
      "Epoch: 2 / 3, Step: 2017 / 2250 Loss: 0.2284\n",
      "Epoch: 2 / 3, Step: 2018 / 2250 Loss: 0.1139\n",
      "Epoch: 2 / 3, Step: 2019 / 2250 Loss: 0.3977\n",
      "Epoch: 2 / 3, Step: 2020 / 2250 Loss: 0.0299\n",
      "Epoch: 2 / 3, Step: 2021 / 2250 Loss: 0.0871\n",
      "Epoch: 2 / 3, Step: 2022 / 2250 Loss: 0.1542\n",
      "Epoch: 2 / 3, Step: 2023 / 2250 Loss: 0.0281\n",
      "Epoch: 2 / 3, Step: 2024 / 2250 Loss: 0.1214\n",
      "Epoch: 2 / 3, Step: 2025 / 2250 Loss: 0.0413\n",
      "Epoch: 2 / 3, Step: 2026 / 2250 Loss: 0.1194\n",
      "Epoch: 2 / 3, Step: 2027 / 2250 Loss: 0.2409\n",
      "Epoch: 2 / 3, Step: 2028 / 2250 Loss: 0.2307\n",
      "Epoch: 2 / 3, Step: 2029 / 2250 Loss: 0.0131\n",
      "Epoch: 2 / 3, Step: 2030 / 2250 Loss: 0.1285\n",
      "Epoch: 2 / 3, Step: 2031 / 2250 Loss: 0.0408\n",
      "Epoch: 2 / 3, Step: 2032 / 2250 Loss: 0.0820\n",
      "Epoch: 2 / 3, Step: 2033 / 2250 Loss: 0.2594\n",
      "Epoch: 2 / 3, Step: 2034 / 2250 Loss: 0.0461\n",
      "Epoch: 2 / 3, Step: 2035 / 2250 Loss: 0.0995\n",
      "Epoch: 2 / 3, Step: 2036 / 2250 Loss: 0.0406\n",
      "Epoch: 2 / 3, Step: 2037 / 2250 Loss: 0.0555\n",
      "Epoch: 2 / 3, Step: 2038 / 2250 Loss: 0.1650\n",
      "Epoch: 2 / 3, Step: 2039 / 2250 Loss: 0.3460\n",
      "Epoch: 2 / 3, Step: 2040 / 2250 Loss: 0.4988\n",
      "Epoch: 2 / 3, Step: 2041 / 2250 Loss: 0.0307\n",
      "Epoch: 2 / 3, Step: 2042 / 2250 Loss: 0.0835\n",
      "Epoch: 2 / 3, Step: 2043 / 2250 Loss: 0.0693\n",
      "Epoch: 2 / 3, Step: 2044 / 2250 Loss: 0.0987\n",
      "Epoch: 2 / 3, Step: 2045 / 2250 Loss: 0.0333\n",
      "Epoch: 2 / 3, Step: 2046 / 2250 Loss: 0.0308\n",
      "Epoch: 2 / 3, Step: 2047 / 2250 Loss: 0.2082\n",
      "Epoch: 2 / 3, Step: 2048 / 2250 Loss: 0.0196\n",
      "Epoch: 2 / 3, Step: 2049 / 2250 Loss: 0.1180\n",
      "Epoch: 2 / 3, Step: 2050 / 2250 Loss: 0.0187\n",
      "Epoch: 2 / 3, Step: 2051 / 2250 Loss: 0.0527\n",
      "Epoch: 2 / 3, Step: 2052 / 2250 Loss: 0.1039\n",
      "Epoch: 2 / 3, Step: 2053 / 2250 Loss: 0.1474\n",
      "Epoch: 2 / 3, Step: 2054 / 2250 Loss: 0.0767\n",
      "Epoch: 2 / 3, Step: 2055 / 2250 Loss: 0.2231\n",
      "Epoch: 2 / 3, Step: 2056 / 2250 Loss: 0.0945\n",
      "Epoch: 2 / 3, Step: 2057 / 2250 Loss: 0.0648\n",
      "Epoch: 2 / 3, Step: 2058 / 2250 Loss: 0.0232\n",
      "Epoch: 2 / 3, Step: 2059 / 2250 Loss: 0.2135\n",
      "Epoch: 2 / 3, Step: 2060 / 2250 Loss: 0.1453\n",
      "Epoch: 2 / 3, Step: 2061 / 2250 Loss: 0.0434\n",
      "Epoch: 2 / 3, Step: 2062 / 2250 Loss: 0.0189\n",
      "Epoch: 2 / 3, Step: 2063 / 2250 Loss: 0.0321\n",
      "Epoch: 2 / 3, Step: 2064 / 2250 Loss: 0.0366\n",
      "Epoch: 2 / 3, Step: 2065 / 2250 Loss: 0.1882\n",
      "Epoch: 2 / 3, Step: 2066 / 2250 Loss: 0.0375\n",
      "Epoch: 2 / 3, Step: 2067 / 2250 Loss: 0.2576\n",
      "Epoch: 2 / 3, Step: 2068 / 2250 Loss: 0.1898\n",
      "Epoch: 2 / 3, Step: 2069 / 2250 Loss: 0.0354\n",
      "Epoch: 2 / 3, Step: 2070 / 2250 Loss: 0.1557\n",
      "Epoch: 2 / 3, Step: 2071 / 2250 Loss: 0.2793\n",
      "Epoch: 2 / 3, Step: 2072 / 2250 Loss: 0.1332\n",
      "Epoch: 2 / 3, Step: 2073 / 2250 Loss: 0.0098\n",
      "Epoch: 2 / 3, Step: 2074 / 2250 Loss: 0.1892\n",
      "Epoch: 2 / 3, Step: 2075 / 2250 Loss: 0.0564\n",
      "Epoch: 2 / 3, Step: 2076 / 2250 Loss: 0.0156\n",
      "Epoch: 2 / 3, Step: 2077 / 2250 Loss: 0.2055\n",
      "Epoch: 2 / 3, Step: 2078 / 2250 Loss: 0.2529\n",
      "Epoch: 2 / 3, Step: 2079 / 2250 Loss: 0.0368\n",
      "Epoch: 2 / 3, Step: 2080 / 2250 Loss: 0.3291\n",
      "Epoch: 2 / 3, Step: 2081 / 2250 Loss: 0.2832\n",
      "Epoch: 2 / 3, Step: 2082 / 2250 Loss: 0.0636\n",
      "Epoch: 2 / 3, Step: 2083 / 2250 Loss: 0.1857\n",
      "Epoch: 2 / 3, Step: 2084 / 2250 Loss: 0.0283\n",
      "Epoch: 2 / 3, Step: 2085 / 2250 Loss: 0.0517\n",
      "Epoch: 2 / 3, Step: 2086 / 2250 Loss: 0.0950\n",
      "Epoch: 2 / 3, Step: 2087 / 2250 Loss: 0.0926\n",
      "Epoch: 2 / 3, Step: 2088 / 2250 Loss: 0.1220\n",
      "Epoch: 2 / 3, Step: 2089 / 2250 Loss: 0.0534\n",
      "Epoch: 2 / 3, Step: 2090 / 2250 Loss: 0.0753\n",
      "Epoch: 2 / 3, Step: 2091 / 2250 Loss: 0.0265\n",
      "Epoch: 2 / 3, Step: 2092 / 2250 Loss: 0.0230\n",
      "Epoch: 2 / 3, Step: 2093 / 2250 Loss: 0.2798\n",
      "Epoch: 2 / 3, Step: 2094 / 2250 Loss: 0.2274\n",
      "Epoch: 2 / 3, Step: 2095 / 2250 Loss: 0.1874\n",
      "Epoch: 2 / 3, Step: 2096 / 2250 Loss: 0.0268\n",
      "Epoch: 2 / 3, Step: 2097 / 2250 Loss: 0.2529\n",
      "Epoch: 2 / 3, Step: 2098 / 2250 Loss: 0.0661\n",
      "Epoch: 2 / 3, Step: 2099 / 2250 Loss: 0.0756\n",
      "Epoch: 2 / 3, Step: 2100 / 2250 Loss: 0.0887\n",
      "Epoch: 2 / 3, Step: 2101 / 2250 Loss: 0.3227\n",
      "Epoch: 2 / 3, Step: 2102 / 2250 Loss: 0.0969\n",
      "Epoch: 2 / 3, Step: 2103 / 2250 Loss: 0.0179\n",
      "Epoch: 2 / 3, Step: 2104 / 2250 Loss: 0.0152\n",
      "Epoch: 2 / 3, Step: 2105 / 2250 Loss: 0.0726\n",
      "Epoch: 2 / 3, Step: 2106 / 2250 Loss: 0.0357\n",
      "Epoch: 2 / 3, Step: 2107 / 2250 Loss: 0.2229\n",
      "Epoch: 2 / 3, Step: 2108 / 2250 Loss: 0.0391\n",
      "Epoch: 2 / 3, Step: 2109 / 2250 Loss: 0.0113\n",
      "Epoch: 2 / 3, Step: 2110 / 2250 Loss: 0.0513\n",
      "Epoch: 2 / 3, Step: 2111 / 2250 Loss: 0.0354\n",
      "Epoch: 2 / 3, Step: 2112 / 2250 Loss: 0.0123\n",
      "Epoch: 2 / 3, Step: 2113 / 2250 Loss: 0.0082\n",
      "Epoch: 2 / 3, Step: 2114 / 2250 Loss: 0.0389\n",
      "Epoch: 2 / 3, Step: 2115 / 2250 Loss: 0.0229\n",
      "Epoch: 2 / 3, Step: 2116 / 2250 Loss: 0.1977\n",
      "Epoch: 2 / 3, Step: 2117 / 2250 Loss: 0.0319\n",
      "Epoch: 2 / 3, Step: 2118 / 2250 Loss: 0.2535\n",
      "Epoch: 2 / 3, Step: 2119 / 2250 Loss: 0.0350\n",
      "Epoch: 2 / 3, Step: 2120 / 2250 Loss: 0.0484\n",
      "Epoch: 2 / 3, Step: 2121 / 2250 Loss: 0.1303\n",
      "Epoch: 2 / 3, Step: 2122 / 2250 Loss: 0.1599\n",
      "Epoch: 2 / 3, Step: 2123 / 2250 Loss: 0.0323\n",
      "Epoch: 2 / 3, Step: 2124 / 2250 Loss: 0.0091\n",
      "Epoch: 2 / 3, Step: 2125 / 2250 Loss: 0.0317\n",
      "Epoch: 2 / 3, Step: 2126 / 2250 Loss: 0.2134\n",
      "Epoch: 2 / 3, Step: 2127 / 2250 Loss: 0.0327\n",
      "Epoch: 2 / 3, Step: 2128 / 2250 Loss: 0.0805\n",
      "Epoch: 2 / 3, Step: 2129 / 2250 Loss: 0.0066\n",
      "Epoch: 2 / 3, Step: 2130 / 2250 Loss: 0.2719\n",
      "Epoch: 2 / 3, Step: 2131 / 2250 Loss: 0.0263\n",
      "Epoch: 2 / 3, Step: 2132 / 2250 Loss: 0.0277\n",
      "Epoch: 2 / 3, Step: 2133 / 2250 Loss: 0.0144\n",
      "Epoch: 2 / 3, Step: 2134 / 2250 Loss: 0.0305\n",
      "Epoch: 2 / 3, Step: 2135 / 2250 Loss: 0.1921\n",
      "Epoch: 2 / 3, Step: 2136 / 2250 Loss: 0.0101\n",
      "Epoch: 2 / 3, Step: 2137 / 2250 Loss: 0.0431\n",
      "Epoch: 2 / 3, Step: 2138 / 2250 Loss: 0.1957\n",
      "Epoch: 2 / 3, Step: 2139 / 2250 Loss: 0.1267\n",
      "Epoch: 2 / 3, Step: 2140 / 2250 Loss: 0.3210\n",
      "Epoch: 2 / 3, Step: 2141 / 2250 Loss: 0.1903\n",
      "Epoch: 2 / 3, Step: 2142 / 2250 Loss: 0.0393\n",
      "Epoch: 2 / 3, Step: 2143 / 2250 Loss: 0.0911\n",
      "Epoch: 2 / 3, Step: 2144 / 2250 Loss: 0.0108\n",
      "Epoch: 2 / 3, Step: 2145 / 2250 Loss: 0.0251\n",
      "Epoch: 2 / 3, Step: 2146 / 2250 Loss: 0.0797\n",
      "Epoch: 2 / 3, Step: 2147 / 2250 Loss: 0.1422\n",
      "Epoch: 2 / 3, Step: 2148 / 2250 Loss: 0.1934\n",
      "Epoch: 2 / 3, Step: 2149 / 2250 Loss: 0.0328\n",
      "Epoch: 2 / 3, Step: 2150 / 2250 Loss: 0.1135\n",
      "Epoch: 2 / 3, Step: 2151 / 2250 Loss: 0.0890\n",
      "Epoch: 2 / 3, Step: 2152 / 2250 Loss: 0.0974\n",
      "Epoch: 2 / 3, Step: 2153 / 2250 Loss: 0.1027\n",
      "Epoch: 2 / 3, Step: 2154 / 2250 Loss: 0.1469\n",
      "Epoch: 2 / 3, Step: 2155 / 2250 Loss: 0.2075\n",
      "Epoch: 2 / 3, Step: 2156 / 2250 Loss: 0.2346\n",
      "Epoch: 2 / 3, Step: 2157 / 2250 Loss: 0.1337\n",
      "Epoch: 2 / 3, Step: 2158 / 2250 Loss: 0.0192\n",
      "Epoch: 2 / 3, Step: 2159 / 2250 Loss: 0.0576\n",
      "Epoch: 2 / 3, Step: 2160 / 2250 Loss: 0.0481\n",
      "Epoch: 2 / 3, Step: 2161 / 2250 Loss: 0.0210\n",
      "Epoch: 2 / 3, Step: 2162 / 2250 Loss: 0.2280\n",
      "Epoch: 2 / 3, Step: 2163 / 2250 Loss: 0.1084\n",
      "Epoch: 2 / 3, Step: 2164 / 2250 Loss: 0.0617\n",
      "Epoch: 2 / 3, Step: 2165 / 2250 Loss: 0.0409\n",
      "Epoch: 2 / 3, Step: 2166 / 2250 Loss: 0.1009\n",
      "Epoch: 2 / 3, Step: 2167 / 2250 Loss: 0.0819\n",
      "Epoch: 2 / 3, Step: 2168 / 2250 Loss: 0.0479\n",
      "Epoch: 2 / 3, Step: 2169 / 2250 Loss: 0.1423\n",
      "Epoch: 2 / 3, Step: 2170 / 2250 Loss: 0.2086\n",
      "Epoch: 2 / 3, Step: 2171 / 2250 Loss: 0.0548\n",
      "Epoch: 2 / 3, Step: 2172 / 2250 Loss: 0.3748\n",
      "Epoch: 2 / 3, Step: 2173 / 2250 Loss: 0.2946\n",
      "Epoch: 2 / 3, Step: 2174 / 2250 Loss: 0.1911\n",
      "Epoch: 2 / 3, Step: 2175 / 2250 Loss: 0.0617\n",
      "Epoch: 2 / 3, Step: 2176 / 2250 Loss: 0.1739\n",
      "Epoch: 2 / 3, Step: 2177 / 2250 Loss: 0.0630\n",
      "Epoch: 2 / 3, Step: 2178 / 2250 Loss: 0.0451\n",
      "Epoch: 2 / 3, Step: 2179 / 2250 Loss: 0.1648\n",
      "Epoch: 2 / 3, Step: 2180 / 2250 Loss: 0.1052\n",
      "Epoch: 2 / 3, Step: 2181 / 2250 Loss: 0.1424\n",
      "Epoch: 2 / 3, Step: 2182 / 2250 Loss: 0.1244\n",
      "Epoch: 2 / 3, Step: 2183 / 2250 Loss: 0.5817\n",
      "Epoch: 2 / 3, Step: 2184 / 2250 Loss: 0.0483\n",
      "Epoch: 2 / 3, Step: 2185 / 2250 Loss: 0.2557\n",
      "Epoch: 2 / 3, Step: 2186 / 2250 Loss: 0.1822\n",
      "Epoch: 2 / 3, Step: 2187 / 2250 Loss: 0.1885\n",
      "Epoch: 2 / 3, Step: 2188 / 2250 Loss: 0.2104\n",
      "Epoch: 2 / 3, Step: 2189 / 2250 Loss: 0.0596\n",
      "Epoch: 2 / 3, Step: 2190 / 2250 Loss: 0.1286\n",
      "Epoch: 2 / 3, Step: 2191 / 2250 Loss: 0.1211\n",
      "Epoch: 2 / 3, Step: 2192 / 2250 Loss: 0.1758\n",
      "Epoch: 2 / 3, Step: 2193 / 2250 Loss: 0.0191\n",
      "Epoch: 2 / 3, Step: 2194 / 2250 Loss: 0.1016\n",
      "Epoch: 2 / 3, Step: 2195 / 2250 Loss: 0.0347\n",
      "Epoch: 2 / 3, Step: 2196 / 2250 Loss: 0.0683\n",
      "Epoch: 2 / 3, Step: 2197 / 2250 Loss: 0.1029\n",
      "Epoch: 2 / 3, Step: 2198 / 2250 Loss: 0.2511\n",
      "Epoch: 2 / 3, Step: 2199 / 2250 Loss: 0.0524\n",
      "Epoch: 2 / 3, Step: 2200 / 2250 Loss: 0.1933\n",
      "Epoch: 2 / 3, Step: 2201 / 2250 Loss: 0.0727\n",
      "Epoch: 2 / 3, Step: 2202 / 2250 Loss: 0.0726\n",
      "Epoch: 2 / 3, Step: 2203 / 2250 Loss: 0.0586\n",
      "Epoch: 2 / 3, Step: 2204 / 2250 Loss: 0.2435\n",
      "Epoch: 2 / 3, Step: 2205 / 2250 Loss: 0.0182\n",
      "Epoch: 2 / 3, Step: 2206 / 2250 Loss: 0.0750\n",
      "Epoch: 2 / 3, Step: 2207 / 2250 Loss: 0.0927\n",
      "Epoch: 2 / 3, Step: 2208 / 2250 Loss: 0.1132\n",
      "Epoch: 2 / 3, Step: 2209 / 2250 Loss: 0.0616\n",
      "Epoch: 2 / 3, Step: 2210 / 2250 Loss: 0.0818\n",
      "Epoch: 2 / 3, Step: 2211 / 2250 Loss: 0.1902\n",
      "Epoch: 2 / 3, Step: 2212 / 2250 Loss: 0.0812\n",
      "Epoch: 2 / 3, Step: 2213 / 2250 Loss: 0.0940\n",
      "Epoch: 2 / 3, Step: 2214 / 2250 Loss: 0.0468\n",
      "Epoch: 2 / 3, Step: 2215 / 2250 Loss: 0.2716\n",
      "Epoch: 2 / 3, Step: 2216 / 2250 Loss: 0.1862\n",
      "Epoch: 2 / 3, Step: 2217 / 2250 Loss: 0.1510\n",
      "Epoch: 2 / 3, Step: 2218 / 2250 Loss: 0.1324\n",
      "Epoch: 2 / 3, Step: 2219 / 2250 Loss: 0.1900\n",
      "Epoch: 2 / 3, Step: 2220 / 2250 Loss: 0.2028\n",
      "Epoch: 2 / 3, Step: 2221 / 2250 Loss: 0.0169\n",
      "Epoch: 2 / 3, Step: 2222 / 2250 Loss: 0.3207\n",
      "Epoch: 2 / 3, Step: 2223 / 2250 Loss: 0.1068\n",
      "Epoch: 2 / 3, Step: 2224 / 2250 Loss: 0.1742\n",
      "Epoch: 2 / 3, Step: 2225 / 2250 Loss: 0.1830\n",
      "Epoch: 2 / 3, Step: 2226 / 2250 Loss: 0.0919\n",
      "Epoch: 2 / 3, Step: 2227 / 2250 Loss: 0.3714\n",
      "Epoch: 2 / 3, Step: 2228 / 2250 Loss: 0.0237\n",
      "Epoch: 2 / 3, Step: 2229 / 2250 Loss: 0.0919\n",
      "Epoch: 2 / 3, Step: 2230 / 2250 Loss: 0.1956\n",
      "Epoch: 2 / 3, Step: 2231 / 2250 Loss: 0.0567\n",
      "Epoch: 2 / 3, Step: 2232 / 2250 Loss: 0.0683\n",
      "Epoch: 2 / 3, Step: 2233 / 2250 Loss: 0.1009\n",
      "Epoch: 2 / 3, Step: 2234 / 2250 Loss: 0.1802\n",
      "Epoch: 2 / 3, Step: 2235 / 2250 Loss: 0.1633\n",
      "Epoch: 2 / 3, Step: 2236 / 2250 Loss: 0.1441\n",
      "Epoch: 2 / 3, Step: 2237 / 2250 Loss: 0.0682\n",
      "Epoch: 2 / 3, Step: 2238 / 2250 Loss: 0.2259\n",
      "Epoch: 2 / 3, Step: 2239 / 2250 Loss: 0.0935\n",
      "Epoch: 2 / 3, Step: 2240 / 2250 Loss: 0.0826\n",
      "Epoch: 2 / 3, Step: 2241 / 2250 Loss: 0.0297\n",
      "Epoch: 2 / 3, Step: 2242 / 2250 Loss: 0.0490\n",
      "Epoch: 2 / 3, Step: 2243 / 2250 Loss: 0.0485\n",
      "Epoch: 2 / 3, Step: 2244 / 2250 Loss: 0.1133\n",
      "Epoch: 2 / 3, Step: 2245 / 2250 Loss: 0.0303\n",
      "Epoch: 2 / 3, Step: 2246 / 2250 Loss: 0.0978\n",
      "Epoch: 2 / 3, Step: 2247 / 2250 Loss: 0.0785\n",
      "Epoch: 2 / 3, Step: 2248 / 2250 Loss: 0.0204\n",
      "Epoch: 2 / 3, Step: 2249 / 2250 Loss: 0.0229\n",
      "Epoch: 3 / 3, Step: 0 / 2250 Loss: 0.0676\n",
      "Epoch: 3 / 3, Step: 1 / 2250 Loss: 0.2232\n",
      "Epoch: 3 / 3, Step: 2 / 2250 Loss: 0.0244\n",
      "Epoch: 3 / 3, Step: 3 / 2250 Loss: 0.1639\n",
      "Epoch: 3 / 3, Step: 4 / 2250 Loss: 0.0161\n",
      "Epoch: 3 / 3, Step: 5 / 2250 Loss: 0.0439\n",
      "Epoch: 3 / 3, Step: 6 / 2250 Loss: 0.1320\n",
      "Epoch: 3 / 3, Step: 7 / 2250 Loss: 0.1843\n",
      "Epoch: 3 / 3, Step: 8 / 2250 Loss: 0.0190\n",
      "Epoch: 3 / 3, Step: 9 / 2250 Loss: 0.1453\n",
      "Epoch: 3 / 3, Step: 10 / 2250 Loss: 0.1677\n",
      "Epoch: 3 / 3, Step: 11 / 2250 Loss: 0.0234\n",
      "Epoch: 3 / 3, Step: 12 / 2250 Loss: 0.0680\n",
      "Epoch: 3 / 3, Step: 13 / 2250 Loss: 0.1476\n",
      "Epoch: 3 / 3, Step: 14 / 2250 Loss: 0.0322\n",
      "Epoch: 3 / 3, Step: 15 / 2250 Loss: 0.2552\n",
      "Epoch: 3 / 3, Step: 16 / 2250 Loss: 0.1043\n",
      "Epoch: 3 / 3, Step: 17 / 2250 Loss: 0.0097\n",
      "Epoch: 3 / 3, Step: 18 / 2250 Loss: 0.2755\n",
      "Epoch: 3 / 3, Step: 19 / 2250 Loss: 0.1397\n",
      "Epoch: 3 / 3, Step: 20 / 2250 Loss: 0.0594\n",
      "Epoch: 3 / 3, Step: 21 / 2250 Loss: 0.0835\n",
      "Epoch: 3 / 3, Step: 22 / 2250 Loss: 0.0977\n",
      "Epoch: 3 / 3, Step: 23 / 2250 Loss: 0.1766\n",
      "Epoch: 3 / 3, Step: 24 / 2250 Loss: 0.0310\n",
      "Epoch: 3 / 3, Step: 25 / 2250 Loss: 0.0468\n",
      "Epoch: 3 / 3, Step: 26 / 2250 Loss: 0.1444\n",
      "Epoch: 3 / 3, Step: 27 / 2250 Loss: 0.1738\n",
      "Epoch: 3 / 3, Step: 28 / 2250 Loss: 0.2190\n",
      "Epoch: 3 / 3, Step: 29 / 2250 Loss: 0.1769\n",
      "Epoch: 3 / 3, Step: 30 / 2250 Loss: 0.0557\n",
      "Epoch: 3 / 3, Step: 31 / 2250 Loss: 0.2627\n",
      "Epoch: 3 / 3, Step: 32 / 2250 Loss: 0.0825\n",
      "Epoch: 3 / 3, Step: 33 / 2250 Loss: 0.0473\n",
      "Epoch: 3 / 3, Step: 34 / 2250 Loss: 0.0426\n",
      "Epoch: 3 / 3, Step: 35 / 2250 Loss: 0.2747\n",
      "Epoch: 3 / 3, Step: 36 / 2250 Loss: 0.0242\n",
      "Epoch: 3 / 3, Step: 37 / 2250 Loss: 0.2348\n",
      "Epoch: 3 / 3, Step: 38 / 2250 Loss: 0.0307\n",
      "Epoch: 3 / 3, Step: 39 / 2250 Loss: 0.0504\n",
      "Epoch: 3 / 3, Step: 40 / 2250 Loss: 0.0990\n",
      "Epoch: 3 / 3, Step: 41 / 2250 Loss: 0.1435\n",
      "Epoch: 3 / 3, Step: 42 / 2250 Loss: 0.1090\n",
      "Epoch: 3 / 3, Step: 43 / 2250 Loss: 0.1454\n",
      "Epoch: 3 / 3, Step: 44 / 2250 Loss: 0.1143\n",
      "Epoch: 3 / 3, Step: 45 / 2250 Loss: 0.0764\n",
      "Epoch: 3 / 3, Step: 46 / 2250 Loss: 0.1166\n",
      "Epoch: 3 / 3, Step: 47 / 2250 Loss: 0.1278\n",
      "Epoch: 3 / 3, Step: 48 / 2250 Loss: 0.0247\n",
      "Epoch: 3 / 3, Step: 49 / 2250 Loss: 0.0189\n",
      "Epoch: 3 / 3, Step: 50 / 2250 Loss: 0.0465\n",
      "Epoch: 3 / 3, Step: 51 / 2250 Loss: 0.0265\n",
      "Epoch: 3 / 3, Step: 52 / 2250 Loss: 0.0454\n",
      "Epoch: 3 / 3, Step: 53 / 2250 Loss: 0.0580\n",
      "Epoch: 3 / 3, Step: 54 / 2250 Loss: 0.0214\n",
      "Epoch: 3 / 3, Step: 55 / 2250 Loss: 0.0192\n",
      "Epoch: 3 / 3, Step: 56 / 2250 Loss: 0.1837\n",
      "Epoch: 3 / 3, Step: 57 / 2250 Loss: 0.1453\n",
      "Epoch: 3 / 3, Step: 58 / 2250 Loss: 0.1270\n",
      "Epoch: 3 / 3, Step: 59 / 2250 Loss: 0.1196\n",
      "Epoch: 3 / 3, Step: 60 / 2250 Loss: 0.0304\n",
      "Epoch: 3 / 3, Step: 61 / 2250 Loss: 0.1590\n",
      "Epoch: 3 / 3, Step: 62 / 2250 Loss: 0.0134\n",
      "Epoch: 3 / 3, Step: 63 / 2250 Loss: 0.0173\n",
      "Epoch: 3 / 3, Step: 64 / 2250 Loss: 0.0148\n",
      "Epoch: 3 / 3, Step: 65 / 2250 Loss: 0.0169\n",
      "Epoch: 3 / 3, Step: 66 / 2250 Loss: 0.1852\n",
      "Epoch: 3 / 3, Step: 67 / 2250 Loss: 0.1089\n",
      "Epoch: 3 / 3, Step: 68 / 2250 Loss: 0.0679\n",
      "Epoch: 3 / 3, Step: 69 / 2250 Loss: 0.0177\n",
      "Epoch: 3 / 3, Step: 70 / 2250 Loss: 0.1204\n",
      "Epoch: 3 / 3, Step: 71 / 2250 Loss: 0.0346\n",
      "Epoch: 3 / 3, Step: 72 / 2250 Loss: 0.1227\n",
      "Epoch: 3 / 3, Step: 73 / 2250 Loss: 0.0102\n",
      "Epoch: 3 / 3, Step: 74 / 2250 Loss: 0.0411\n",
      "Epoch: 3 / 3, Step: 75 / 2250 Loss: 0.0400\n",
      "Epoch: 3 / 3, Step: 76 / 2250 Loss: 0.3044\n",
      "Epoch: 3 / 3, Step: 77 / 2250 Loss: 0.0911\n",
      "Epoch: 3 / 3, Step: 78 / 2250 Loss: 0.0617\n",
      "Epoch: 3 / 3, Step: 79 / 2250 Loss: 0.0098\n",
      "Epoch: 3 / 3, Step: 80 / 2250 Loss: 0.0486\n",
      "Epoch: 3 / 3, Step: 81 / 2250 Loss: 0.0440\n",
      "Epoch: 3 / 3, Step: 82 / 2250 Loss: 0.1089\n",
      "Epoch: 3 / 3, Step: 83 / 2250 Loss: 0.5897\n",
      "Epoch: 3 / 3, Step: 84 / 2250 Loss: 0.0464\n",
      "Epoch: 3 / 3, Step: 85 / 2250 Loss: 0.0276\n",
      "Epoch: 3 / 3, Step: 86 / 2250 Loss: 0.0119\n",
      "Epoch: 3 / 3, Step: 87 / 2250 Loss: 0.1199\n",
      "Epoch: 3 / 3, Step: 88 / 2250 Loss: 0.1035\n",
      "Epoch: 3 / 3, Step: 89 / 2250 Loss: 0.0472\n",
      "Epoch: 3 / 3, Step: 90 / 2250 Loss: 0.0205\n",
      "Epoch: 3 / 3, Step: 91 / 2250 Loss: 0.0179\n",
      "Epoch: 3 / 3, Step: 92 / 2250 Loss: 0.0784\n",
      "Epoch: 3 / 3, Step: 93 / 2250 Loss: 0.2726\n",
      "Epoch: 3 / 3, Step: 94 / 2250 Loss: 0.0513\n",
      "Epoch: 3 / 3, Step: 95 / 2250 Loss: 0.0199\n",
      "Epoch: 3 / 3, Step: 96 / 2250 Loss: 0.0345\n",
      "Epoch: 3 / 3, Step: 97 / 2250 Loss: 0.0233\n",
      "Epoch: 3 / 3, Step: 98 / 2250 Loss: 0.0854\n",
      "Epoch: 3 / 3, Step: 99 / 2250 Loss: 0.0534\n",
      "Epoch: 3 / 3, Step: 100 / 2250 Loss: 0.0350\n",
      "Epoch: 3 / 3, Step: 101 / 2250 Loss: 0.0217\n",
      "Epoch: 3 / 3, Step: 102 / 2250 Loss: 0.0341\n",
      "Epoch: 3 / 3, Step: 103 / 2250 Loss: 0.4294\n",
      "Epoch: 3 / 3, Step: 104 / 2250 Loss: 0.0290\n",
      "Epoch: 3 / 3, Step: 105 / 2250 Loss: 0.0351\n",
      "Epoch: 3 / 3, Step: 106 / 2250 Loss: 0.0235\n",
      "Epoch: 3 / 3, Step: 107 / 2250 Loss: 0.0233\n",
      "Epoch: 3 / 3, Step: 108 / 2250 Loss: 0.0348\n",
      "Epoch: 3 / 3, Step: 109 / 2250 Loss: 0.0497\n",
      "Epoch: 3 / 3, Step: 110 / 2250 Loss: 0.0109\n",
      "Epoch: 3 / 3, Step: 111 / 2250 Loss: 0.0628\n",
      "Epoch: 3 / 3, Step: 112 / 2250 Loss: 0.0992\n",
      "Epoch: 3 / 3, Step: 113 / 2250 Loss: 0.0140\n",
      "Epoch: 3 / 3, Step: 114 / 2250 Loss: 0.0588\n",
      "Epoch: 3 / 3, Step: 115 / 2250 Loss: 0.2396\n",
      "Epoch: 3 / 3, Step: 116 / 2250 Loss: 0.1030\n",
      "Epoch: 3 / 3, Step: 117 / 2250 Loss: 0.0327\n",
      "Epoch: 3 / 3, Step: 118 / 2250 Loss: 0.2749\n",
      "Epoch: 3 / 3, Step: 119 / 2250 Loss: 0.1490\n",
      "Epoch: 3 / 3, Step: 120 / 2250 Loss: 0.1310\n",
      "Epoch: 3 / 3, Step: 121 / 2250 Loss: 0.0195\n",
      "Epoch: 3 / 3, Step: 122 / 2250 Loss: 0.0457\n",
      "Epoch: 3 / 3, Step: 123 / 2250 Loss: 0.1108\n",
      "Epoch: 3 / 3, Step: 124 / 2250 Loss: 0.0123\n",
      "Epoch: 3 / 3, Step: 125 / 2250 Loss: 0.0371\n",
      "Epoch: 3 / 3, Step: 126 / 2250 Loss: 0.0145\n",
      "Epoch: 3 / 3, Step: 127 / 2250 Loss: 0.1190\n",
      "Epoch: 3 / 3, Step: 128 / 2250 Loss: 0.0518\n",
      "Epoch: 3 / 3, Step: 129 / 2250 Loss: 0.1043\n",
      "Epoch: 3 / 3, Step: 130 / 2250 Loss: 0.0123\n",
      "Epoch: 3 / 3, Step: 131 / 2250 Loss: 0.0233\n",
      "Epoch: 3 / 3, Step: 132 / 2250 Loss: 0.1262\n",
      "Epoch: 3 / 3, Step: 133 / 2250 Loss: 0.2380\n",
      "Epoch: 3 / 3, Step: 134 / 2250 Loss: 0.0354\n",
      "Epoch: 3 / 3, Step: 135 / 2250 Loss: 0.0816\n",
      "Epoch: 3 / 3, Step: 136 / 2250 Loss: 0.0351\n",
      "Epoch: 3 / 3, Step: 137 / 2250 Loss: 0.0204\n",
      "Epoch: 3 / 3, Step: 138 / 2250 Loss: 0.1245\n",
      "Epoch: 3 / 3, Step: 139 / 2250 Loss: 0.0363\n",
      "Epoch: 3 / 3, Step: 140 / 2250 Loss: 0.0234\n",
      "Epoch: 3 / 3, Step: 141 / 2250 Loss: 0.0865\n",
      "Epoch: 3 / 3, Step: 142 / 2250 Loss: 0.0157\n",
      "Epoch: 3 / 3, Step: 143 / 2250 Loss: 0.1310\n",
      "Epoch: 3 / 3, Step: 144 / 2250 Loss: 0.4317\n",
      "Epoch: 3 / 3, Step: 145 / 2250 Loss: 0.0392\n",
      "Epoch: 3 / 3, Step: 146 / 2250 Loss: 0.0293\n",
      "Epoch: 3 / 3, Step: 147 / 2250 Loss: 0.0218\n",
      "Epoch: 3 / 3, Step: 148 / 2250 Loss: 0.0594\n",
      "Epoch: 3 / 3, Step: 149 / 2250 Loss: 0.1355\n",
      "Epoch: 3 / 3, Step: 150 / 2250 Loss: 0.0284\n",
      "Epoch: 3 / 3, Step: 151 / 2250 Loss: 0.0127\n",
      "Epoch: 3 / 3, Step: 152 / 2250 Loss: 0.0191\n",
      "Epoch: 3 / 3, Step: 153 / 2250 Loss: 0.1357\n",
      "Epoch: 3 / 3, Step: 154 / 2250 Loss: 0.2927\n",
      "Epoch: 3 / 3, Step: 155 / 2250 Loss: 0.0199\n",
      "Epoch: 3 / 3, Step: 156 / 2250 Loss: 0.0207\n",
      "Epoch: 3 / 3, Step: 157 / 2250 Loss: 0.0141\n",
      "Epoch: 3 / 3, Step: 158 / 2250 Loss: 0.0154\n",
      "Epoch: 3 / 3, Step: 159 / 2250 Loss: 0.1408\n",
      "Epoch: 3 / 3, Step: 160 / 2250 Loss: 0.0208\n",
      "Epoch: 3 / 3, Step: 161 / 2250 Loss: 0.0122\n",
      "Epoch: 3 / 3, Step: 162 / 2250 Loss: 0.0676\n",
      "Epoch: 3 / 3, Step: 163 / 2250 Loss: 0.1680\n",
      "Epoch: 3 / 3, Step: 164 / 2250 Loss: 0.0379\n",
      "Epoch: 3 / 3, Step: 165 / 2250 Loss: 0.0228\n",
      "Epoch: 3 / 3, Step: 166 / 2250 Loss: 0.0240\n",
      "Epoch: 3 / 3, Step: 167 / 2250 Loss: 0.0944\n",
      "Epoch: 3 / 3, Step: 168 / 2250 Loss: 0.1772\n",
      "Epoch: 3 / 3, Step: 169 / 2250 Loss: 0.1270\n",
      "Epoch: 3 / 3, Step: 170 / 2250 Loss: 0.0108\n",
      "Epoch: 3 / 3, Step: 171 / 2250 Loss: 0.1895\n",
      "Epoch: 3 / 3, Step: 172 / 2250 Loss: 0.0088\n",
      "Epoch: 3 / 3, Step: 173 / 2250 Loss: 0.0106\n",
      "Epoch: 3 / 3, Step: 174 / 2250 Loss: 0.0673\n",
      "Epoch: 3 / 3, Step: 175 / 2250 Loss: 0.0607\n",
      "Epoch: 3 / 3, Step: 176 / 2250 Loss: 0.1559\n",
      "Epoch: 3 / 3, Step: 177 / 2250 Loss: 0.1031\n",
      "Epoch: 3 / 3, Step: 178 / 2250 Loss: 0.0382\n",
      "Epoch: 3 / 3, Step: 179 / 2250 Loss: 0.0763\n",
      "Epoch: 3 / 3, Step: 180 / 2250 Loss: 0.0726\n",
      "Epoch: 3 / 3, Step: 181 / 2250 Loss: 0.0349\n",
      "Epoch: 3 / 3, Step: 182 / 2250 Loss: 0.1939\n",
      "Epoch: 3 / 3, Step: 183 / 2250 Loss: 0.2545\n",
      "Epoch: 3 / 3, Step: 184 / 2250 Loss: 0.1629\n",
      "Epoch: 3 / 3, Step: 185 / 2250 Loss: 0.0894\n",
      "Epoch: 3 / 3, Step: 186 / 2250 Loss: 0.1415\n",
      "Epoch: 3 / 3, Step: 187 / 2250 Loss: 0.0656\n",
      "Epoch: 3 / 3, Step: 188 / 2250 Loss: 0.0356\n",
      "Epoch: 3 / 3, Step: 189 / 2250 Loss: 0.0663\n",
      "Epoch: 3 / 3, Step: 190 / 2250 Loss: 0.0125\n",
      "Epoch: 3 / 3, Step: 191 / 2250 Loss: 0.0984\n",
      "Epoch: 3 / 3, Step: 192 / 2250 Loss: 0.0896\n",
      "Epoch: 3 / 3, Step: 193 / 2250 Loss: 0.1361\n",
      "Epoch: 3 / 3, Step: 194 / 2250 Loss: 0.0414\n",
      "Epoch: 3 / 3, Step: 195 / 2250 Loss: 0.0243\n",
      "Epoch: 3 / 3, Step: 196 / 2250 Loss: 0.0542\n",
      "Epoch: 3 / 3, Step: 197 / 2250 Loss: 0.0287\n",
      "Epoch: 3 / 3, Step: 198 / 2250 Loss: 0.0082\n",
      "Epoch: 3 / 3, Step: 199 / 2250 Loss: 0.0142\n",
      "Epoch: 3 / 3, Step: 200 / 2250 Loss: 0.1026\n",
      "Epoch: 3 / 3, Step: 201 / 2250 Loss: 0.0641\n",
      "Epoch: 3 / 3, Step: 202 / 2250 Loss: 0.0382\n",
      "Epoch: 3 / 3, Step: 203 / 2250 Loss: 0.2999\n",
      "Epoch: 3 / 3, Step: 204 / 2250 Loss: 0.1630\n",
      "Epoch: 3 / 3, Step: 205 / 2250 Loss: 0.1483\n",
      "Epoch: 3 / 3, Step: 206 / 2250 Loss: 0.1277\n",
      "Epoch: 3 / 3, Step: 207 / 2250 Loss: 0.0536\n",
      "Epoch: 3 / 3, Step: 208 / 2250 Loss: 0.3237\n",
      "Epoch: 3 / 3, Step: 209 / 2250 Loss: 0.0748\n",
      "Epoch: 3 / 3, Step: 210 / 2250 Loss: 0.0233\n",
      "Epoch: 3 / 3, Step: 211 / 2250 Loss: 0.0173\n",
      "Epoch: 3 / 3, Step: 212 / 2250 Loss: 0.0566\n",
      "Epoch: 3 / 3, Step: 213 / 2250 Loss: 0.0128\n",
      "Epoch: 3 / 3, Step: 214 / 2250 Loss: 0.3809\n",
      "Epoch: 3 / 3, Step: 215 / 2250 Loss: 0.0279\n",
      "Epoch: 3 / 3, Step: 216 / 2250 Loss: 0.0979\n",
      "Epoch: 3 / 3, Step: 217 / 2250 Loss: 0.2014\n",
      "Epoch: 3 / 3, Step: 218 / 2250 Loss: 0.2238\n",
      "Epoch: 3 / 3, Step: 219 / 2250 Loss: 0.0801\n",
      "Epoch: 3 / 3, Step: 220 / 2250 Loss: 0.0444\n",
      "Epoch: 3 / 3, Step: 221 / 2250 Loss: 0.0221\n",
      "Epoch: 3 / 3, Step: 222 / 2250 Loss: 0.0414\n",
      "Epoch: 3 / 3, Step: 223 / 2250 Loss: 0.1694\n",
      "Epoch: 3 / 3, Step: 224 / 2250 Loss: 0.0121\n",
      "Epoch: 3 / 3, Step: 225 / 2250 Loss: 0.0520\n",
      "Epoch: 3 / 3, Step: 226 / 2250 Loss: 0.0312\n",
      "Epoch: 3 / 3, Step: 227 / 2250 Loss: 0.2129\n",
      "Epoch: 3 / 3, Step: 228 / 2250 Loss: 0.1739\n",
      "Epoch: 3 / 3, Step: 229 / 2250 Loss: 0.0526\n",
      "Epoch: 3 / 3, Step: 230 / 2250 Loss: 0.0842\n",
      "Epoch: 3 / 3, Step: 231 / 2250 Loss: 0.0184\n",
      "Epoch: 3 / 3, Step: 232 / 2250 Loss: 0.0122\n",
      "Epoch: 3 / 3, Step: 233 / 2250 Loss: 0.0089\n",
      "Epoch: 3 / 3, Step: 234 / 2250 Loss: 0.1770\n",
      "Epoch: 3 / 3, Step: 235 / 2250 Loss: 0.0371\n",
      "Epoch: 3 / 3, Step: 236 / 2250 Loss: 0.3290\n",
      "Epoch: 3 / 3, Step: 237 / 2250 Loss: 0.0325\n",
      "Epoch: 3 / 3, Step: 238 / 2250 Loss: 0.0079\n",
      "Epoch: 3 / 3, Step: 239 / 2250 Loss: 0.0148\n",
      "Epoch: 3 / 3, Step: 240 / 2250 Loss: 0.1455\n",
      "Epoch: 3 / 3, Step: 241 / 2250 Loss: 0.0383\n",
      "Epoch: 3 / 3, Step: 242 / 2250 Loss: 0.0232\n",
      "Epoch: 3 / 3, Step: 243 / 2250 Loss: 0.0949\n",
      "Epoch: 3 / 3, Step: 244 / 2250 Loss: 0.0103\n",
      "Epoch: 3 / 3, Step: 245 / 2250 Loss: 0.1223\n",
      "Epoch: 3 / 3, Step: 246 / 2250 Loss: 0.0147\n",
      "Epoch: 3 / 3, Step: 247 / 2250 Loss: 0.0568\n",
      "Epoch: 3 / 3, Step: 248 / 2250 Loss: 0.1317\n",
      "Epoch: 3 / 3, Step: 249 / 2250 Loss: 0.0167\n",
      "Epoch: 3 / 3, Step: 250 / 2250 Loss: 0.0684\n",
      "Epoch: 3 / 3, Step: 251 / 2250 Loss: 0.0186\n",
      "Epoch: 3 / 3, Step: 252 / 2250 Loss: 0.0732\n",
      "Epoch: 3 / 3, Step: 253 / 2250 Loss: 0.0298\n",
      "Epoch: 3 / 3, Step: 254 / 2250 Loss: 0.0113\n",
      "Epoch: 3 / 3, Step: 255 / 2250 Loss: 0.1408\n",
      "Epoch: 3 / 3, Step: 256 / 2250 Loss: 0.0410\n",
      "Epoch: 3 / 3, Step: 257 / 2250 Loss: 0.0557\n",
      "Epoch: 3 / 3, Step: 258 / 2250 Loss: 0.1523\n",
      "Epoch: 3 / 3, Step: 259 / 2250 Loss: 0.0458\n",
      "Epoch: 3 / 3, Step: 260 / 2250 Loss: 0.0299\n",
      "Epoch: 3 / 3, Step: 261 / 2250 Loss: 0.0855\n",
      "Epoch: 3 / 3, Step: 262 / 2250 Loss: 0.1063\n",
      "Epoch: 3 / 3, Step: 263 / 2250 Loss: 0.0859\n",
      "Epoch: 3 / 3, Step: 264 / 2250 Loss: 0.0574\n",
      "Epoch: 3 / 3, Step: 265 / 2250 Loss: 0.0453\n",
      "Epoch: 3 / 3, Step: 266 / 2250 Loss: 0.0643\n",
      "Epoch: 3 / 3, Step: 267 / 2250 Loss: 0.1108\n",
      "Epoch: 3 / 3, Step: 268 / 2250 Loss: 0.0557\n",
      "Epoch: 3 / 3, Step: 269 / 2250 Loss: 0.0301\n",
      "Epoch: 3 / 3, Step: 270 / 2250 Loss: 0.0866\n",
      "Epoch: 3 / 3, Step: 271 / 2250 Loss: 0.0463\n",
      "Epoch: 3 / 3, Step: 272 / 2250 Loss: 0.2388\n",
      "Epoch: 3 / 3, Step: 273 / 2250 Loss: 0.0951\n",
      "Epoch: 3 / 3, Step: 274 / 2250 Loss: 0.1602\n",
      "Epoch: 3 / 3, Step: 275 / 2250 Loss: 0.1483\n",
      "Epoch: 3 / 3, Step: 276 / 2250 Loss: 0.0057\n",
      "Epoch: 3 / 3, Step: 277 / 2250 Loss: 0.1254\n",
      "Epoch: 3 / 3, Step: 278 / 2250 Loss: 0.1588\n",
      "Epoch: 3 / 3, Step: 279 / 2250 Loss: 0.0313\n",
      "Epoch: 3 / 3, Step: 280 / 2250 Loss: 0.0581\n",
      "Epoch: 3 / 3, Step: 281 / 2250 Loss: 0.0210\n",
      "Epoch: 3 / 3, Step: 282 / 2250 Loss: 0.1890\n",
      "Epoch: 3 / 3, Step: 283 / 2250 Loss: 0.0641\n",
      "Epoch: 3 / 3, Step: 284 / 2250 Loss: 0.0094\n",
      "Epoch: 3 / 3, Step: 285 / 2250 Loss: 0.0492\n",
      "Epoch: 3 / 3, Step: 286 / 2250 Loss: 0.1639\n",
      "Epoch: 3 / 3, Step: 287 / 2250 Loss: 0.1672\n",
      "Epoch: 3 / 3, Step: 288 / 2250 Loss: 0.0418\n",
      "Epoch: 3 / 3, Step: 289 / 2250 Loss: 0.2763\n",
      "Epoch: 3 / 3, Step: 290 / 2250 Loss: 0.0314\n",
      "Epoch: 3 / 3, Step: 291 / 2250 Loss: 0.1163\n",
      "Epoch: 3 / 3, Step: 292 / 2250 Loss: 0.0731\n",
      "Epoch: 3 / 3, Step: 293 / 2250 Loss: 0.1552\n",
      "Epoch: 3 / 3, Step: 294 / 2250 Loss: 0.0143\n",
      "Epoch: 3 / 3, Step: 295 / 2250 Loss: 0.0095\n",
      "Epoch: 3 / 3, Step: 296 / 2250 Loss: 0.1927\n",
      "Epoch: 3 / 3, Step: 297 / 2250 Loss: 0.1182\n",
      "Epoch: 3 / 3, Step: 298 / 2250 Loss: 0.0514\n",
      "Epoch: 3 / 3, Step: 299 / 2250 Loss: 0.0619\n",
      "Epoch: 3 / 3, Step: 300 / 2250 Loss: 0.1758\n",
      "Epoch: 3 / 3, Step: 301 / 2250 Loss: 0.0052\n",
      "Epoch: 3 / 3, Step: 302 / 2250 Loss: 0.1080\n",
      "Epoch: 3 / 3, Step: 303 / 2250 Loss: 0.0474\n",
      "Epoch: 3 / 3, Step: 304 / 2250 Loss: 0.0506\n",
      "Epoch: 3 / 3, Step: 305 / 2250 Loss: 0.0172\n",
      "Epoch: 3 / 3, Step: 306 / 2250 Loss: 0.0285\n",
      "Epoch: 3 / 3, Step: 307 / 2250 Loss: 0.0468\n",
      "Epoch: 3 / 3, Step: 308 / 2250 Loss: 0.0125\n",
      "Epoch: 3 / 3, Step: 309 / 2250 Loss: 0.0649\n",
      "Epoch: 3 / 3, Step: 310 / 2250 Loss: 0.1886\n",
      "Epoch: 3 / 3, Step: 311 / 2250 Loss: 0.0437\n",
      "Epoch: 3 / 3, Step: 312 / 2250 Loss: 0.1065\n",
      "Epoch: 3 / 3, Step: 313 / 2250 Loss: 0.2022\n",
      "Epoch: 3 / 3, Step: 314 / 2250 Loss: 0.0428\n",
      "Epoch: 3 / 3, Step: 315 / 2250 Loss: 0.0208\n",
      "Epoch: 3 / 3, Step: 316 / 2250 Loss: 0.0395\n",
      "Epoch: 3 / 3, Step: 317 / 2250 Loss: 0.0667\n",
      "Epoch: 3 / 3, Step: 318 / 2250 Loss: 0.0227\n",
      "Epoch: 3 / 3, Step: 319 / 2250 Loss: 0.0215\n",
      "Epoch: 3 / 3, Step: 320 / 2250 Loss: 0.0895\n",
      "Epoch: 3 / 3, Step: 321 / 2250 Loss: 0.0578\n",
      "Epoch: 3 / 3, Step: 322 / 2250 Loss: 0.0401\n",
      "Epoch: 3 / 3, Step: 323 / 2250 Loss: 0.1740\n",
      "Epoch: 3 / 3, Step: 324 / 2250 Loss: 0.0286\n",
      "Epoch: 3 / 3, Step: 325 / 2250 Loss: 0.0720\n",
      "Epoch: 3 / 3, Step: 326 / 2250 Loss: 0.0278\n",
      "Epoch: 3 / 3, Step: 327 / 2250 Loss: 0.0974\n",
      "Epoch: 3 / 3, Step: 328 / 2250 Loss: 0.0197\n",
      "Epoch: 3 / 3, Step: 329 / 2250 Loss: 0.1152\n",
      "Epoch: 3 / 3, Step: 330 / 2250 Loss: 0.0849\n",
      "Epoch: 3 / 3, Step: 331 / 2250 Loss: 0.0333\n",
      "Epoch: 3 / 3, Step: 332 / 2250 Loss: 0.0324\n",
      "Epoch: 3 / 3, Step: 333 / 2250 Loss: 0.0907\n",
      "Epoch: 3 / 3, Step: 334 / 2250 Loss: 0.0141\n",
      "Epoch: 3 / 3, Step: 335 / 2250 Loss: 0.2524\n",
      "Epoch: 3 / 3, Step: 336 / 2250 Loss: 0.1084\n",
      "Epoch: 3 / 3, Step: 337 / 2250 Loss: 0.1020\n",
      "Epoch: 3 / 3, Step: 338 / 2250 Loss: 0.0470\n",
      "Epoch: 3 / 3, Step: 339 / 2250 Loss: 0.0829\n",
      "Epoch: 3 / 3, Step: 340 / 2250 Loss: 0.1765\n",
      "Epoch: 3 / 3, Step: 341 / 2250 Loss: 0.1968\n",
      "Epoch: 3 / 3, Step: 342 / 2250 Loss: 0.0131\n",
      "Epoch: 3 / 3, Step: 343 / 2250 Loss: 0.0364\n",
      "Epoch: 3 / 3, Step: 344 / 2250 Loss: 0.1444\n",
      "Epoch: 3 / 3, Step: 345 / 2250 Loss: 0.0200\n",
      "Epoch: 3 / 3, Step: 346 / 2250 Loss: 0.1022\n",
      "Epoch: 3 / 3, Step: 347 / 2250 Loss: 0.0506\n",
      "Epoch: 3 / 3, Step: 348 / 2250 Loss: 0.0098\n",
      "Epoch: 3 / 3, Step: 349 / 2250 Loss: 0.0807\n",
      "Epoch: 3 / 3, Step: 350 / 2250 Loss: 0.2295\n",
      "Epoch: 3 / 3, Step: 351 / 2250 Loss: 0.1462\n",
      "Epoch: 3 / 3, Step: 352 / 2250 Loss: 0.0103\n",
      "Epoch: 3 / 3, Step: 353 / 2250 Loss: 0.1355\n",
      "Epoch: 3 / 3, Step: 354 / 2250 Loss: 0.0209\n",
      "Epoch: 3 / 3, Step: 355 / 2250 Loss: 0.0148\n",
      "Epoch: 3 / 3, Step: 356 / 2250 Loss: 0.2359\n",
      "Epoch: 3 / 3, Step: 357 / 2250 Loss: 0.0487\n",
      "Epoch: 3 / 3, Step: 358 / 2250 Loss: 0.1387\n",
      "Epoch: 3 / 3, Step: 359 / 2250 Loss: 0.1195\n",
      "Epoch: 3 / 3, Step: 360 / 2250 Loss: 0.0413\n",
      "Epoch: 3 / 3, Step: 361 / 2250 Loss: 0.2387\n",
      "Epoch: 3 / 3, Step: 362 / 2250 Loss: 0.1316\n",
      "Epoch: 3 / 3, Step: 363 / 2250 Loss: 0.0214\n",
      "Epoch: 3 / 3, Step: 364 / 2250 Loss: 0.0949\n",
      "Epoch: 3 / 3, Step: 365 / 2250 Loss: 0.0255\n",
      "Epoch: 3 / 3, Step: 366 / 2250 Loss: 0.0119\n",
      "Epoch: 3 / 3, Step: 367 / 2250 Loss: 0.0409\n",
      "Epoch: 3 / 3, Step: 368 / 2250 Loss: 0.1947\n",
      "Epoch: 3 / 3, Step: 369 / 2250 Loss: 0.1194\n",
      "Epoch: 3 / 3, Step: 370 / 2250 Loss: 0.0152\n",
      "Epoch: 3 / 3, Step: 371 / 2250 Loss: 0.0190\n",
      "Epoch: 3 / 3, Step: 372 / 2250 Loss: 0.1806\n",
      "Epoch: 3 / 3, Step: 373 / 2250 Loss: 0.1011\n",
      "Epoch: 3 / 3, Step: 374 / 2250 Loss: 0.0328\n",
      "Epoch: 3 / 3, Step: 375 / 2250 Loss: 0.2672\n",
      "Epoch: 3 / 3, Step: 376 / 2250 Loss: 0.1731\n",
      "Epoch: 3 / 3, Step: 377 / 2250 Loss: 0.0484\n",
      "Epoch: 3 / 3, Step: 378 / 2250 Loss: 0.1638\n",
      "Epoch: 3 / 3, Step: 379 / 2250 Loss: 0.0217\n",
      "Epoch: 3 / 3, Step: 380 / 2250 Loss: 0.0502\n",
      "Epoch: 3 / 3, Step: 381 / 2250 Loss: 0.2983\n",
      "Epoch: 3 / 3, Step: 382 / 2250 Loss: 0.0824\n",
      "Epoch: 3 / 3, Step: 383 / 2250 Loss: 0.0571\n",
      "Epoch: 3 / 3, Step: 384 / 2250 Loss: 0.1249\n",
      "Epoch: 3 / 3, Step: 385 / 2250 Loss: 0.0859\n",
      "Epoch: 3 / 3, Step: 386 / 2250 Loss: 0.0925\n",
      "Epoch: 3 / 3, Step: 387 / 2250 Loss: 0.2726\n",
      "Epoch: 3 / 3, Step: 388 / 2250 Loss: 0.2144\n",
      "Epoch: 3 / 3, Step: 389 / 2250 Loss: 0.1146\n",
      "Epoch: 3 / 3, Step: 390 / 2250 Loss: 0.2008\n",
      "Epoch: 3 / 3, Step: 391 / 2250 Loss: 0.0895\n",
      "Epoch: 3 / 3, Step: 392 / 2250 Loss: 0.1475\n",
      "Epoch: 3 / 3, Step: 393 / 2250 Loss: 0.0150\n",
      "Epoch: 3 / 3, Step: 394 / 2250 Loss: 0.0506\n",
      "Epoch: 3 / 3, Step: 395 / 2250 Loss: 0.0217\n",
      "Epoch: 3 / 3, Step: 396 / 2250 Loss: 0.0270\n",
      "Epoch: 3 / 3, Step: 397 / 2250 Loss: 0.1123\n",
      "Epoch: 3 / 3, Step: 398 / 2250 Loss: 0.0175\n",
      "Epoch: 3 / 3, Step: 399 / 2250 Loss: 0.2113\n",
      "Epoch: 3 / 3, Step: 400 / 2250 Loss: 0.0591\n",
      "Epoch: 3 / 3, Step: 401 / 2250 Loss: 0.1734\n",
      "Epoch: 3 / 3, Step: 402 / 2250 Loss: 0.0710\n",
      "Epoch: 3 / 3, Step: 403 / 2250 Loss: 0.0969\n",
      "Epoch: 3 / 3, Step: 404 / 2250 Loss: 0.2150\n",
      "Epoch: 3 / 3, Step: 405 / 2250 Loss: 0.0222\n",
      "Epoch: 3 / 3, Step: 406 / 2250 Loss: 0.2224\n",
      "Epoch: 3 / 3, Step: 407 / 2250 Loss: 0.0180\n",
      "Epoch: 3 / 3, Step: 408 / 2250 Loss: 0.0110\n",
      "Epoch: 3 / 3, Step: 409 / 2250 Loss: 0.0387\n",
      "Epoch: 3 / 3, Step: 410 / 2250 Loss: 0.1024\n",
      "Epoch: 3 / 3, Step: 411 / 2250 Loss: 0.0827\n",
      "Epoch: 3 / 3, Step: 412 / 2250 Loss: 0.0908\n",
      "Epoch: 3 / 3, Step: 413 / 2250 Loss: 0.0533\n",
      "Epoch: 3 / 3, Step: 414 / 2250 Loss: 0.0200\n",
      "Epoch: 3 / 3, Step: 415 / 2250 Loss: 0.0652\n",
      "Epoch: 3 / 3, Step: 416 / 2250 Loss: 0.1666\n",
      "Epoch: 3 / 3, Step: 417 / 2250 Loss: 0.0367\n",
      "Epoch: 3 / 3, Step: 418 / 2250 Loss: 0.0224\n",
      "Epoch: 3 / 3, Step: 419 / 2250 Loss: 0.0261\n",
      "Epoch: 3 / 3, Step: 420 / 2250 Loss: 0.0406\n",
      "Epoch: 3 / 3, Step: 421 / 2250 Loss: 0.0669\n",
      "Epoch: 3 / 3, Step: 422 / 2250 Loss: 0.0717\n",
      "Epoch: 3 / 3, Step: 423 / 2250 Loss: 0.0649\n",
      "Epoch: 3 / 3, Step: 424 / 2250 Loss: 0.0669\n",
      "Epoch: 3 / 3, Step: 425 / 2250 Loss: 0.0352\n",
      "Epoch: 3 / 3, Step: 426 / 2250 Loss: 0.2426\n",
      "Epoch: 3 / 3, Step: 427 / 2250 Loss: 0.1612\n",
      "Epoch: 3 / 3, Step: 428 / 2250 Loss: 0.0116\n",
      "Epoch: 3 / 3, Step: 429 / 2250 Loss: 0.0292\n",
      "Epoch: 3 / 3, Step: 430 / 2250 Loss: 0.0353\n",
      "Epoch: 3 / 3, Step: 431 / 2250 Loss: 0.0296\n",
      "Epoch: 3 / 3, Step: 432 / 2250 Loss: 0.0303\n",
      "Epoch: 3 / 3, Step: 433 / 2250 Loss: 0.0086\n",
      "Epoch: 3 / 3, Step: 434 / 2250 Loss: 0.0150\n",
      "Epoch: 3 / 3, Step: 435 / 2250 Loss: 0.0493\n",
      "Epoch: 3 / 3, Step: 436 / 2250 Loss: 0.2322\n",
      "Epoch: 3 / 3, Step: 437 / 2250 Loss: 0.0309\n",
      "Epoch: 3 / 3, Step: 438 / 2250 Loss: 0.2146\n",
      "Epoch: 3 / 3, Step: 439 / 2250 Loss: 0.2494\n",
      "Epoch: 3 / 3, Step: 440 / 2250 Loss: 0.1279\n",
      "Epoch: 3 / 3, Step: 441 / 2250 Loss: 0.0083\n",
      "Epoch: 3 / 3, Step: 442 / 2250 Loss: 0.1778\n",
      "Epoch: 3 / 3, Step: 443 / 2250 Loss: 0.0166\n",
      "Epoch: 3 / 3, Step: 444 / 2250 Loss: 0.0316\n",
      "Epoch: 3 / 3, Step: 445 / 2250 Loss: 0.0571\n",
      "Epoch: 3 / 3, Step: 446 / 2250 Loss: 0.2323\n",
      "Epoch: 3 / 3, Step: 447 / 2250 Loss: 0.0755\n",
      "Epoch: 3 / 3, Step: 448 / 2250 Loss: 0.1043\n",
      "Epoch: 3 / 3, Step: 449 / 2250 Loss: 0.2757\n",
      "Epoch: 3 / 3, Step: 450 / 2250 Loss: 0.0747\n",
      "Epoch: 3 / 3, Step: 451 / 2250 Loss: 0.0295\n",
      "Epoch: 3 / 3, Step: 452 / 2250 Loss: 0.0180\n",
      "Epoch: 3 / 3, Step: 453 / 2250 Loss: 0.0319\n",
      "Epoch: 3 / 3, Step: 454 / 2250 Loss: 0.0722\n",
      "Epoch: 3 / 3, Step: 455 / 2250 Loss: 0.1349\n",
      "Epoch: 3 / 3, Step: 456 / 2250 Loss: 0.0618\n",
      "Epoch: 3 / 3, Step: 457 / 2250 Loss: 0.0929\n",
      "Epoch: 3 / 3, Step: 458 / 2250 Loss: 0.0174\n",
      "Epoch: 3 / 3, Step: 459 / 2250 Loss: 0.0370\n",
      "Epoch: 3 / 3, Step: 460 / 2250 Loss: 0.0557\n",
      "Epoch: 3 / 3, Step: 461 / 2250 Loss: 0.0458\n",
      "Epoch: 3 / 3, Step: 462 / 2250 Loss: 0.0216\n",
      "Epoch: 3 / 3, Step: 463 / 2250 Loss: 0.0226\n",
      "Epoch: 3 / 3, Step: 464 / 2250 Loss: 0.0416\n",
      "Epoch: 3 / 3, Step: 465 / 2250 Loss: 0.0647\n",
      "Epoch: 3 / 3, Step: 466 / 2250 Loss: 0.1559\n",
      "Epoch: 3 / 3, Step: 467 / 2250 Loss: 0.0176\n",
      "Epoch: 3 / 3, Step: 468 / 2250 Loss: 0.0288\n",
      "Epoch: 3 / 3, Step: 469 / 2250 Loss: 0.0994\n",
      "Epoch: 3 / 3, Step: 470 / 2250 Loss: 0.1657\n",
      "Epoch: 3 / 3, Step: 471 / 2250 Loss: 0.1076\n",
      "Epoch: 3 / 3, Step: 472 / 2250 Loss: 0.1830\n",
      "Epoch: 3 / 3, Step: 473 / 2250 Loss: 0.0380\n",
      "Epoch: 3 / 3, Step: 474 / 2250 Loss: 0.0236\n",
      "Epoch: 3 / 3, Step: 475 / 2250 Loss: 0.0110\n",
      "Epoch: 3 / 3, Step: 476 / 2250 Loss: 0.0093\n",
      "Epoch: 3 / 3, Step: 477 / 2250 Loss: 0.2044\n",
      "Epoch: 3 / 3, Step: 478 / 2250 Loss: 0.2755\n",
      "Epoch: 3 / 3, Step: 479 / 2250 Loss: 0.0389\n",
      "Epoch: 3 / 3, Step: 480 / 2250 Loss: 0.0470\n",
      "Epoch: 3 / 3, Step: 481 / 2250 Loss: 0.0489\n",
      "Epoch: 3 / 3, Step: 482 / 2250 Loss: 0.0233\n",
      "Epoch: 3 / 3, Step: 483 / 2250 Loss: 0.0422\n",
      "Epoch: 3 / 3, Step: 484 / 2250 Loss: 0.1728\n",
      "Epoch: 3 / 3, Step: 485 / 2250 Loss: 0.0789\n",
      "Epoch: 3 / 3, Step: 486 / 2250 Loss: 0.0604\n",
      "Epoch: 3 / 3, Step: 487 / 2250 Loss: 0.0291\n",
      "Epoch: 3 / 3, Step: 488 / 2250 Loss: 0.1451\n",
      "Epoch: 3 / 3, Step: 489 / 2250 Loss: 0.1566\n",
      "Epoch: 3 / 3, Step: 490 / 2250 Loss: 0.3376\n",
      "Epoch: 3 / 3, Step: 491 / 2250 Loss: 0.0670\n",
      "Epoch: 3 / 3, Step: 492 / 2250 Loss: 0.2826\n",
      "Epoch: 3 / 3, Step: 493 / 2250 Loss: 0.0684\n",
      "Epoch: 3 / 3, Step: 494 / 2250 Loss: 0.0333\n",
      "Epoch: 3 / 3, Step: 495 / 2250 Loss: 0.0304\n",
      "Epoch: 3 / 3, Step: 496 / 2250 Loss: 0.0190\n",
      "Epoch: 3 / 3, Step: 497 / 2250 Loss: 0.1424\n",
      "Epoch: 3 / 3, Step: 498 / 2250 Loss: 0.1640\n",
      "Epoch: 3 / 3, Step: 499 / 2250 Loss: 0.0290\n",
      "Epoch: 3 / 3, Step: 500 / 2250 Loss: 0.0959\n",
      "Epoch: 3 / 3, Step: 501 / 2250 Loss: 0.0325\n",
      "Epoch: 3 / 3, Step: 502 / 2250 Loss: 0.1468\n",
      "Epoch: 3 / 3, Step: 503 / 2250 Loss: 0.0468\n",
      "Epoch: 3 / 3, Step: 504 / 2250 Loss: 0.0083\n",
      "Epoch: 3 / 3, Step: 505 / 2250 Loss: 0.2418\n",
      "Epoch: 3 / 3, Step: 506 / 2250 Loss: 0.0200\n",
      "Epoch: 3 / 3, Step: 507 / 2250 Loss: 0.0251\n",
      "Epoch: 3 / 3, Step: 508 / 2250 Loss: 0.1877\n",
      "Epoch: 3 / 3, Step: 509 / 2250 Loss: 0.0197\n",
      "Epoch: 3 / 3, Step: 510 / 2250 Loss: 0.1371\n",
      "Epoch: 3 / 3, Step: 511 / 2250 Loss: 0.2097\n",
      "Epoch: 3 / 3, Step: 512 / 2250 Loss: 0.0304\n",
      "Epoch: 3 / 3, Step: 513 / 2250 Loss: 0.0977\n",
      "Epoch: 3 / 3, Step: 514 / 2250 Loss: 0.1203\n",
      "Epoch: 3 / 3, Step: 515 / 2250 Loss: 0.0738\n",
      "Epoch: 3 / 3, Step: 516 / 2250 Loss: 0.0128\n",
      "Epoch: 3 / 3, Step: 517 / 2250 Loss: 0.0711\n",
      "Epoch: 3 / 3, Step: 518 / 2250 Loss: 0.1935\n",
      "Epoch: 3 / 3, Step: 519 / 2250 Loss: 0.0157\n",
      "Epoch: 3 / 3, Step: 520 / 2250 Loss: 0.0543\n",
      "Epoch: 3 / 3, Step: 521 / 2250 Loss: 0.0168\n",
      "Epoch: 3 / 3, Step: 522 / 2250 Loss: 0.0360\n",
      "Epoch: 3 / 3, Step: 523 / 2250 Loss: 0.0351\n",
      "Epoch: 3 / 3, Step: 524 / 2250 Loss: 0.0215\n",
      "Epoch: 3 / 3, Step: 525 / 2250 Loss: 0.1229\n",
      "Epoch: 3 / 3, Step: 526 / 2250 Loss: 0.1324\n",
      "Epoch: 3 / 3, Step: 527 / 2250 Loss: 0.1479\n",
      "Epoch: 3 / 3, Step: 528 / 2250 Loss: 0.0130\n",
      "Epoch: 3 / 3, Step: 529 / 2250 Loss: 0.0212\n",
      "Epoch: 3 / 3, Step: 530 / 2250 Loss: 0.0237\n",
      "Epoch: 3 / 3, Step: 531 / 2250 Loss: 0.2190\n",
      "Epoch: 3 / 3, Step: 532 / 2250 Loss: 0.0961\n",
      "Epoch: 3 / 3, Step: 533 / 2250 Loss: 0.1143\n",
      "Epoch: 3 / 3, Step: 534 / 2250 Loss: 0.0949\n",
      "Epoch: 3 / 3, Step: 535 / 2250 Loss: 0.0292\n",
      "Epoch: 3 / 3, Step: 536 / 2250 Loss: 0.1785\n",
      "Epoch: 3 / 3, Step: 537 / 2250 Loss: 0.1092\n",
      "Epoch: 3 / 3, Step: 538 / 2250 Loss: 0.1547\n",
      "Epoch: 3 / 3, Step: 539 / 2250 Loss: 0.2088\n",
      "Epoch: 3 / 3, Step: 540 / 2250 Loss: 0.0525\n",
      "Epoch: 3 / 3, Step: 541 / 2250 Loss: 0.5572\n",
      "Epoch: 3 / 3, Step: 542 / 2250 Loss: 0.0348\n",
      "Epoch: 3 / 3, Step: 543 / 2250 Loss: 0.0782\n",
      "Epoch: 3 / 3, Step: 544 / 2250 Loss: 0.0211\n",
      "Epoch: 3 / 3, Step: 545 / 2250 Loss: 0.3797\n",
      "Epoch: 3 / 3, Step: 546 / 2250 Loss: 0.0594\n",
      "Epoch: 3 / 3, Step: 547 / 2250 Loss: 0.1420\n",
      "Epoch: 3 / 3, Step: 548 / 2250 Loss: 0.0088\n",
      "Epoch: 3 / 3, Step: 549 / 2250 Loss: 0.0194\n",
      "Epoch: 3 / 3, Step: 550 / 2250 Loss: 0.0671\n",
      "Epoch: 3 / 3, Step: 551 / 2250 Loss: 0.0697\n",
      "Epoch: 3 / 3, Step: 552 / 2250 Loss: 0.0273\n",
      "Epoch: 3 / 3, Step: 553 / 2250 Loss: 0.2265\n",
      "Epoch: 3 / 3, Step: 554 / 2250 Loss: 0.0259\n",
      "Epoch: 3 / 3, Step: 555 / 2250 Loss: 0.0850\n",
      "Epoch: 3 / 3, Step: 556 / 2250 Loss: 0.3478\n",
      "Epoch: 3 / 3, Step: 557 / 2250 Loss: 0.0667\n",
      "Epoch: 3 / 3, Step: 558 / 2250 Loss: 0.0589\n",
      "Epoch: 3 / 3, Step: 559 / 2250 Loss: 0.0414\n",
      "Epoch: 3 / 3, Step: 560 / 2250 Loss: 0.0446\n",
      "Epoch: 3 / 3, Step: 561 / 2250 Loss: 0.0284\n",
      "Epoch: 3 / 3, Step: 562 / 2250 Loss: 0.3163\n",
      "Epoch: 3 / 3, Step: 563 / 2250 Loss: 0.1331\n",
      "Epoch: 3 / 3, Step: 564 / 2250 Loss: 0.0250\n",
      "Epoch: 3 / 3, Step: 565 / 2250 Loss: 0.0260\n",
      "Epoch: 3 / 3, Step: 566 / 2250 Loss: 0.1543\n",
      "Epoch: 3 / 3, Step: 567 / 2250 Loss: 0.1103\n",
      "Epoch: 3 / 3, Step: 568 / 2250 Loss: 0.0449\n",
      "Epoch: 3 / 3, Step: 569 / 2250 Loss: 0.0428\n",
      "Epoch: 3 / 3, Step: 570 / 2250 Loss: 0.0135\n",
      "Epoch: 3 / 3, Step: 571 / 2250 Loss: 0.1072\n",
      "Epoch: 3 / 3, Step: 572 / 2250 Loss: 0.0190\n",
      "Epoch: 3 / 3, Step: 573 / 2250 Loss: 0.1875\n",
      "Epoch: 3 / 3, Step: 574 / 2250 Loss: 0.0603\n",
      "Epoch: 3 / 3, Step: 575 / 2250 Loss: 0.0954\n",
      "Epoch: 3 / 3, Step: 576 / 2250 Loss: 0.1653\n",
      "Epoch: 3 / 3, Step: 577 / 2250 Loss: 0.0197\n",
      "Epoch: 3 / 3, Step: 578 / 2250 Loss: 0.0133\n",
      "Epoch: 3 / 3, Step: 579 / 2250 Loss: 0.0333\n",
      "Epoch: 3 / 3, Step: 580 / 2250 Loss: 0.1003\n",
      "Epoch: 3 / 3, Step: 581 / 2250 Loss: 0.0151\n",
      "Epoch: 3 / 3, Step: 582 / 2250 Loss: 0.0118\n",
      "Epoch: 3 / 3, Step: 583 / 2250 Loss: 0.1480\n",
      "Epoch: 3 / 3, Step: 584 / 2250 Loss: 0.1252\n",
      "Epoch: 3 / 3, Step: 585 / 2250 Loss: 0.0585\n",
      "Epoch: 3 / 3, Step: 586 / 2250 Loss: 0.0486\n",
      "Epoch: 3 / 3, Step: 587 / 2250 Loss: 0.2738\n",
      "Epoch: 3 / 3, Step: 588 / 2250 Loss: 0.0896\n",
      "Epoch: 3 / 3, Step: 589 / 2250 Loss: 0.1777\n",
      "Epoch: 3 / 3, Step: 590 / 2250 Loss: 0.1367\n",
      "Epoch: 3 / 3, Step: 591 / 2250 Loss: 0.0210\n",
      "Epoch: 3 / 3, Step: 592 / 2250 Loss: 0.0853\n",
      "Epoch: 3 / 3, Step: 593 / 2250 Loss: 0.1135\n",
      "Epoch: 3 / 3, Step: 594 / 2250 Loss: 0.2417\n",
      "Epoch: 3 / 3, Step: 595 / 2250 Loss: 0.0492\n",
      "Epoch: 3 / 3, Step: 596 / 2250 Loss: 0.0304\n",
      "Epoch: 3 / 3, Step: 597 / 2250 Loss: 0.0134\n",
      "Epoch: 3 / 3, Step: 598 / 2250 Loss: 0.0417\n",
      "Epoch: 3 / 3, Step: 599 / 2250 Loss: 0.0101\n",
      "Epoch: 3 / 3, Step: 600 / 2250 Loss: 0.0305\n",
      "Epoch: 3 / 3, Step: 601 / 2250 Loss: 0.1902\n",
      "Epoch: 3 / 3, Step: 602 / 2250 Loss: 0.1974\n",
      "Epoch: 3 / 3, Step: 603 / 2250 Loss: 0.2518\n",
      "Epoch: 3 / 3, Step: 604 / 2250 Loss: 0.1986\n",
      "Epoch: 3 / 3, Step: 605 / 2250 Loss: 0.0260\n",
      "Epoch: 3 / 3, Step: 606 / 2250 Loss: 0.0292\n",
      "Epoch: 3 / 3, Step: 607 / 2250 Loss: 0.1760\n",
      "Epoch: 3 / 3, Step: 608 / 2250 Loss: 0.1116\n",
      "Epoch: 3 / 3, Step: 609 / 2250 Loss: 0.0836\n",
      "Epoch: 3 / 3, Step: 610 / 2250 Loss: 0.0441\n",
      "Epoch: 3 / 3, Step: 611 / 2250 Loss: 0.0327\n",
      "Epoch: 3 / 3, Step: 612 / 2250 Loss: 0.0322\n",
      "Epoch: 3 / 3, Step: 613 / 2250 Loss: 0.2162\n",
      "Epoch: 3 / 3, Step: 614 / 2250 Loss: 0.3531\n",
      "Epoch: 3 / 3, Step: 615 / 2250 Loss: 0.0281\n",
      "Epoch: 3 / 3, Step: 616 / 2250 Loss: 0.0360\n",
      "Epoch: 3 / 3, Step: 617 / 2250 Loss: 0.0491\n",
      "Epoch: 3 / 3, Step: 618 / 2250 Loss: 0.0073\n",
      "Epoch: 3 / 3, Step: 619 / 2250 Loss: 0.0848\n",
      "Epoch: 3 / 3, Step: 620 / 2250 Loss: 0.0233\n",
      "Epoch: 3 / 3, Step: 621 / 2250 Loss: 0.0349\n",
      "Epoch: 3 / 3, Step: 622 / 2250 Loss: 0.2828\n",
      "Epoch: 3 / 3, Step: 623 / 2250 Loss: 0.0673\n",
      "Epoch: 3 / 3, Step: 624 / 2250 Loss: 0.0213\n",
      "Epoch: 3 / 3, Step: 625 / 2250 Loss: 0.0477\n",
      "Epoch: 3 / 3, Step: 626 / 2250 Loss: 0.0592\n",
      "Epoch: 3 / 3, Step: 627 / 2250 Loss: 0.0909\n",
      "Epoch: 3 / 3, Step: 628 / 2250 Loss: 0.0630\n",
      "Epoch: 3 / 3, Step: 629 / 2250 Loss: 0.0685\n",
      "Epoch: 3 / 3, Step: 630 / 2250 Loss: 0.2288\n",
      "Epoch: 3 / 3, Step: 631 / 2250 Loss: 0.1435\n",
      "Epoch: 3 / 3, Step: 632 / 2250 Loss: 0.1359\n",
      "Epoch: 3 / 3, Step: 633 / 2250 Loss: 0.0659\n",
      "Epoch: 3 / 3, Step: 634 / 2250 Loss: 0.0597\n",
      "Epoch: 3 / 3, Step: 635 / 2250 Loss: 0.1179\n",
      "Epoch: 3 / 3, Step: 636 / 2250 Loss: 0.1433\n",
      "Epoch: 3 / 3, Step: 637 / 2250 Loss: 0.0122\n",
      "Epoch: 3 / 3, Step: 638 / 2250 Loss: 0.0226\n",
      "Epoch: 3 / 3, Step: 639 / 2250 Loss: 0.0827\n",
      "Epoch: 3 / 3, Step: 640 / 2250 Loss: 0.0755\n",
      "Epoch: 3 / 3, Step: 641 / 2250 Loss: 0.0152\n",
      "Epoch: 3 / 3, Step: 642 / 2250 Loss: 0.1173\n",
      "Epoch: 3 / 3, Step: 643 / 2250 Loss: 0.1921\n",
      "Epoch: 3 / 3, Step: 644 / 2250 Loss: 0.0983\n",
      "Epoch: 3 / 3, Step: 645 / 2250 Loss: 0.0094\n",
      "Epoch: 3 / 3, Step: 646 / 2250 Loss: 0.0442\n",
      "Epoch: 3 / 3, Step: 647 / 2250 Loss: 0.0110\n",
      "Epoch: 3 / 3, Step: 648 / 2250 Loss: 0.1686\n",
      "Epoch: 3 / 3, Step: 649 / 2250 Loss: 0.2470\n",
      "Epoch: 3 / 3, Step: 650 / 2250 Loss: 0.0569\n",
      "Epoch: 3 / 3, Step: 651 / 2250 Loss: 0.2822\n",
      "Epoch: 3 / 3, Step: 652 / 2250 Loss: 0.0276\n",
      "Epoch: 3 / 3, Step: 653 / 2250 Loss: 0.0408\n",
      "Epoch: 3 / 3, Step: 654 / 2250 Loss: 0.1209\n",
      "Epoch: 3 / 3, Step: 655 / 2250 Loss: 0.0545\n",
      "Epoch: 3 / 3, Step: 656 / 2250 Loss: 0.0326\n",
      "Epoch: 3 / 3, Step: 657 / 2250 Loss: 0.0429\n",
      "Epoch: 3 / 3, Step: 658 / 2250 Loss: 0.0254\n",
      "Epoch: 3 / 3, Step: 659 / 2250 Loss: 0.0463\n",
      "Epoch: 3 / 3, Step: 660 / 2250 Loss: 0.0363\n",
      "Epoch: 3 / 3, Step: 661 / 2250 Loss: 0.1374\n",
      "Epoch: 3 / 3, Step: 662 / 2250 Loss: 0.0789\n",
      "Epoch: 3 / 3, Step: 663 / 2250 Loss: 0.0227\n",
      "Epoch: 3 / 3, Step: 664 / 2250 Loss: 0.0479\n",
      "Epoch: 3 / 3, Step: 665 / 2250 Loss: 0.0638\n",
      "Epoch: 3 / 3, Step: 666 / 2250 Loss: 0.1384\n",
      "Epoch: 3 / 3, Step: 667 / 2250 Loss: 0.0112\n",
      "Epoch: 3 / 3, Step: 668 / 2250 Loss: 0.1394\n",
      "Epoch: 3 / 3, Step: 669 / 2250 Loss: 0.0543\n",
      "Epoch: 3 / 3, Step: 670 / 2250 Loss: 0.0119\n",
      "Epoch: 3 / 3, Step: 671 / 2250 Loss: 0.3056\n",
      "Epoch: 3 / 3, Step: 672 / 2250 Loss: 0.1383\n",
      "Epoch: 3 / 3, Step: 673 / 2250 Loss: 0.0079\n",
      "Epoch: 3 / 3, Step: 674 / 2250 Loss: 0.0167\n",
      "Epoch: 3 / 3, Step: 675 / 2250 Loss: 0.1065\n",
      "Epoch: 3 / 3, Step: 676 / 2250 Loss: 0.0348\n",
      "Epoch: 3 / 3, Step: 677 / 2250 Loss: 0.0148\n",
      "Epoch: 3 / 3, Step: 678 / 2250 Loss: 0.0397\n",
      "Epoch: 3 / 3, Step: 679 / 2250 Loss: 0.1408\n",
      "Epoch: 3 / 3, Step: 680 / 2250 Loss: 0.0289\n",
      "Epoch: 3 / 3, Step: 681 / 2250 Loss: 0.0783\n",
      "Epoch: 3 / 3, Step: 682 / 2250 Loss: 0.0611\n",
      "Epoch: 3 / 3, Step: 683 / 2250 Loss: 0.0176\n",
      "Epoch: 3 / 3, Step: 684 / 2250 Loss: 0.1328\n",
      "Epoch: 3 / 3, Step: 685 / 2250 Loss: 0.0960\n",
      "Epoch: 3 / 3, Step: 686 / 2250 Loss: 0.0537\n",
      "Epoch: 3 / 3, Step: 687 / 2250 Loss: 0.0786\n",
      "Epoch: 3 / 3, Step: 688 / 2250 Loss: 0.1255\n",
      "Epoch: 3 / 3, Step: 689 / 2250 Loss: 0.0981\n",
      "Epoch: 3 / 3, Step: 690 / 2250 Loss: 0.0868\n",
      "Epoch: 3 / 3, Step: 691 / 2250 Loss: 0.2861\n",
      "Epoch: 3 / 3, Step: 692 / 2250 Loss: 0.0884\n",
      "Epoch: 3 / 3, Step: 693 / 2250 Loss: 0.4066\n",
      "Epoch: 3 / 3, Step: 694 / 2250 Loss: 0.1958\n",
      "Epoch: 3 / 3, Step: 695 / 2250 Loss: 0.0282\n",
      "Epoch: 3 / 3, Step: 696 / 2250 Loss: 0.1729\n",
      "Epoch: 3 / 3, Step: 697 / 2250 Loss: 0.0947\n",
      "Epoch: 3 / 3, Step: 698 / 2250 Loss: 0.0461\n",
      "Epoch: 3 / 3, Step: 699 / 2250 Loss: 0.0203\n",
      "Epoch: 3 / 3, Step: 700 / 2250 Loss: 0.0643\n",
      "Epoch: 3 / 3, Step: 701 / 2250 Loss: 0.0533\n",
      "Epoch: 3 / 3, Step: 702 / 2250 Loss: 0.1823\n",
      "Epoch: 3 / 3, Step: 703 / 2250 Loss: 0.1250\n",
      "Epoch: 3 / 3, Step: 704 / 2250 Loss: 0.0194\n",
      "Epoch: 3 / 3, Step: 705 / 2250 Loss: 0.1042\n",
      "Epoch: 3 / 3, Step: 706 / 2250 Loss: 0.1165\n",
      "Epoch: 3 / 3, Step: 707 / 2250 Loss: 0.2185\n",
      "Epoch: 3 / 3, Step: 708 / 2250 Loss: 0.0104\n",
      "Epoch: 3 / 3, Step: 709 / 2250 Loss: 0.0174\n",
      "Epoch: 3 / 3, Step: 710 / 2250 Loss: 0.0329\n",
      "Epoch: 3 / 3, Step: 711 / 2250 Loss: 0.0593\n",
      "Epoch: 3 / 3, Step: 712 / 2250 Loss: 0.0562\n",
      "Epoch: 3 / 3, Step: 713 / 2250 Loss: 0.1999\n",
      "Epoch: 3 / 3, Step: 714 / 2250 Loss: 0.0136\n",
      "Epoch: 3 / 3, Step: 715 / 2250 Loss: 0.0234\n",
      "Epoch: 3 / 3, Step: 716 / 2250 Loss: 0.0857\n",
      "Epoch: 3 / 3, Step: 717 / 2250 Loss: 0.0811\n",
      "Epoch: 3 / 3, Step: 718 / 2250 Loss: 0.0618\n",
      "Epoch: 3 / 3, Step: 719 / 2250 Loss: 0.1133\n",
      "Epoch: 3 / 3, Step: 720 / 2250 Loss: 0.0948\n",
      "Epoch: 3 / 3, Step: 721 / 2250 Loss: 0.0893\n",
      "Epoch: 3 / 3, Step: 722 / 2250 Loss: 0.0518\n",
      "Epoch: 3 / 3, Step: 723 / 2250 Loss: 0.3037\n",
      "Epoch: 3 / 3, Step: 724 / 2250 Loss: 0.0155\n",
      "Epoch: 3 / 3, Step: 725 / 2250 Loss: 0.0222\n",
      "Epoch: 3 / 3, Step: 726 / 2250 Loss: 0.0223\n",
      "Epoch: 3 / 3, Step: 727 / 2250 Loss: 0.0408\n",
      "Epoch: 3 / 3, Step: 728 / 2250 Loss: 0.0416\n",
      "Epoch: 3 / 3, Step: 729 / 2250 Loss: 0.0428\n",
      "Epoch: 3 / 3, Step: 730 / 2250 Loss: 0.2107\n",
      "Epoch: 3 / 3, Step: 731 / 2250 Loss: 0.1474\n",
      "Epoch: 3 / 3, Step: 732 / 2250 Loss: 0.2120\n",
      "Epoch: 3 / 3, Step: 733 / 2250 Loss: 0.0882\n",
      "Epoch: 3 / 3, Step: 734 / 2250 Loss: 0.0413\n",
      "Epoch: 3 / 3, Step: 735 / 2250 Loss: 0.1442\n",
      "Epoch: 3 / 3, Step: 736 / 2250 Loss: 0.0107\n",
      "Epoch: 3 / 3, Step: 737 / 2250 Loss: 0.0117\n",
      "Epoch: 3 / 3, Step: 738 / 2250 Loss: 0.2623\n",
      "Epoch: 3 / 3, Step: 739 / 2250 Loss: 0.0125\n",
      "Epoch: 3 / 3, Step: 740 / 2250 Loss: 0.0554\n",
      "Epoch: 3 / 3, Step: 741 / 2250 Loss: 0.0086\n",
      "Epoch: 3 / 3, Step: 742 / 2250 Loss: 0.1005\n",
      "Epoch: 3 / 3, Step: 743 / 2250 Loss: 0.0926\n",
      "Epoch: 3 / 3, Step: 744 / 2250 Loss: 0.0316\n",
      "Epoch: 3 / 3, Step: 745 / 2250 Loss: 0.0585\n",
      "Epoch: 3 / 3, Step: 746 / 2250 Loss: 0.0265\n",
      "Epoch: 3 / 3, Step: 747 / 2250 Loss: 0.1146\n",
      "Epoch: 3 / 3, Step: 748 / 2250 Loss: 0.0757\n",
      "Epoch: 3 / 3, Step: 749 / 2250 Loss: 0.0497\n",
      "Epoch: 3 / 3, Step: 750 / 2250 Loss: 0.0176\n",
      "Epoch: 3 / 3, Step: 751 / 2250 Loss: 0.0308\n",
      "Epoch: 3 / 3, Step: 752 / 2250 Loss: 0.3251\n",
      "Epoch: 3 / 3, Step: 753 / 2250 Loss: 0.0105\n",
      "Epoch: 3 / 3, Step: 754 / 2250 Loss: 0.1800\n",
      "Epoch: 3 / 3, Step: 755 / 2250 Loss: 0.0097\n",
      "Epoch: 3 / 3, Step: 756 / 2250 Loss: 0.0164\n",
      "Epoch: 3 / 3, Step: 757 / 2250 Loss: 0.0357\n",
      "Epoch: 3 / 3, Step: 758 / 2250 Loss: 0.0071\n",
      "Epoch: 3 / 3, Step: 759 / 2250 Loss: 0.0275\n",
      "Epoch: 3 / 3, Step: 760 / 2250 Loss: 0.0953\n",
      "Epoch: 3 / 3, Step: 761 / 2250 Loss: 0.1689\n",
      "Epoch: 3 / 3, Step: 762 / 2250 Loss: 0.0181\n",
      "Epoch: 3 / 3, Step: 763 / 2250 Loss: 0.3698\n",
      "Epoch: 3 / 3, Step: 764 / 2250 Loss: 0.1308\n",
      "Epoch: 3 / 3, Step: 765 / 2250 Loss: 0.0670\n",
      "Epoch: 3 / 3, Step: 766 / 2250 Loss: 0.0592\n",
      "Epoch: 3 / 3, Step: 767 / 2250 Loss: 0.0072\n",
      "Epoch: 3 / 3, Step: 768 / 2250 Loss: 0.0583\n",
      "Epoch: 3 / 3, Step: 769 / 2250 Loss: 0.0983\n",
      "Epoch: 3 / 3, Step: 770 / 2250 Loss: 0.0249\n",
      "Epoch: 3 / 3, Step: 771 / 2250 Loss: 0.1457\n",
      "Epoch: 3 / 3, Step: 772 / 2250 Loss: 0.0202\n",
      "Epoch: 3 / 3, Step: 773 / 2250 Loss: 0.0099\n",
      "Epoch: 3 / 3, Step: 774 / 2250 Loss: 0.0255\n",
      "Epoch: 3 / 3, Step: 775 / 2250 Loss: 0.0146\n",
      "Epoch: 3 / 3, Step: 776 / 2250 Loss: 0.0275\n",
      "Epoch: 3 / 3, Step: 777 / 2250 Loss: 0.2127\n",
      "Epoch: 3 / 3, Step: 778 / 2250 Loss: 0.0367\n",
      "Epoch: 3 / 3, Step: 779 / 2250 Loss: 0.1543\n",
      "Epoch: 3 / 3, Step: 780 / 2250 Loss: 0.0651\n",
      "Epoch: 3 / 3, Step: 781 / 2250 Loss: 0.0526\n",
      "Epoch: 3 / 3, Step: 782 / 2250 Loss: 0.0268\n",
      "Epoch: 3 / 3, Step: 783 / 2250 Loss: 0.0963\n",
      "Epoch: 3 / 3, Step: 784 / 2250 Loss: 0.0330\n",
      "Epoch: 3 / 3, Step: 785 / 2250 Loss: 0.0102\n",
      "Epoch: 3 / 3, Step: 786 / 2250 Loss: 0.0884\n",
      "Epoch: 3 / 3, Step: 787 / 2250 Loss: 0.0177\n",
      "Epoch: 3 / 3, Step: 788 / 2250 Loss: 0.1455\n",
      "Epoch: 3 / 3, Step: 789 / 2250 Loss: 0.2101\n",
      "Epoch: 3 / 3, Step: 790 / 2250 Loss: 0.1591\n",
      "Epoch: 3 / 3, Step: 791 / 2250 Loss: 0.0277\n",
      "Epoch: 3 / 3, Step: 792 / 2250 Loss: 0.0374\n",
      "Epoch: 3 / 3, Step: 793 / 2250 Loss: 0.0222\n",
      "Epoch: 3 / 3, Step: 794 / 2250 Loss: 0.0391\n",
      "Epoch: 3 / 3, Step: 795 / 2250 Loss: 0.2360\n",
      "Epoch: 3 / 3, Step: 796 / 2250 Loss: 0.1559\n",
      "Epoch: 3 / 3, Step: 797 / 2250 Loss: 0.0713\n",
      "Epoch: 3 / 3, Step: 798 / 2250 Loss: 0.0184\n",
      "Epoch: 3 / 3, Step: 799 / 2250 Loss: 0.1638\n",
      "Epoch: 3 / 3, Step: 800 / 2250 Loss: 0.2528\n",
      "Epoch: 3 / 3, Step: 801 / 2250 Loss: 0.1124\n",
      "Epoch: 3 / 3, Step: 802 / 2250 Loss: 0.1217\n",
      "Epoch: 3 / 3, Step: 803 / 2250 Loss: 0.1081\n",
      "Epoch: 3 / 3, Step: 804 / 2250 Loss: 0.1046\n",
      "Epoch: 3 / 3, Step: 805 / 2250 Loss: 0.0560\n",
      "Epoch: 3 / 3, Step: 806 / 2250 Loss: 0.0510\n",
      "Epoch: 3 / 3, Step: 807 / 2250 Loss: 0.0390\n",
      "Epoch: 3 / 3, Step: 808 / 2250 Loss: 0.0928\n",
      "Epoch: 3 / 3, Step: 809 / 2250 Loss: 0.0461\n",
      "Epoch: 3 / 3, Step: 810 / 2250 Loss: 0.0911\n",
      "Epoch: 3 / 3, Step: 811 / 2250 Loss: 0.1121\n",
      "Epoch: 3 / 3, Step: 812 / 2250 Loss: 0.2214\n",
      "Epoch: 3 / 3, Step: 813 / 2250 Loss: 0.1023\n",
      "Epoch: 3 / 3, Step: 814 / 2250 Loss: 0.0280\n",
      "Epoch: 3 / 3, Step: 815 / 2250 Loss: 0.1868\n",
      "Epoch: 3 / 3, Step: 816 / 2250 Loss: 0.1898\n",
      "Epoch: 3 / 3, Step: 817 / 2250 Loss: 0.0920\n",
      "Epoch: 3 / 3, Step: 818 / 2250 Loss: 0.2195\n",
      "Epoch: 3 / 3, Step: 819 / 2250 Loss: 0.1398\n",
      "Epoch: 3 / 3, Step: 820 / 2250 Loss: 0.0284\n",
      "Epoch: 3 / 3, Step: 821 / 2250 Loss: 0.2546\n",
      "Epoch: 3 / 3, Step: 822 / 2250 Loss: 0.0213\n",
      "Epoch: 3 / 3, Step: 823 / 2250 Loss: 0.0746\n",
      "Epoch: 3 / 3, Step: 824 / 2250 Loss: 0.3492\n",
      "Epoch: 3 / 3, Step: 825 / 2250 Loss: 0.1139\n",
      "Epoch: 3 / 3, Step: 826 / 2250 Loss: 0.3294\n",
      "Epoch: 3 / 3, Step: 827 / 2250 Loss: 0.3597\n",
      "Epoch: 3 / 3, Step: 828 / 2250 Loss: 0.0321\n",
      "Epoch: 3 / 3, Step: 829 / 2250 Loss: 0.3160\n",
      "Epoch: 3 / 3, Step: 830 / 2250 Loss: 0.1074\n",
      "Epoch: 3 / 3, Step: 831 / 2250 Loss: 0.0247\n",
      "Epoch: 3 / 3, Step: 832 / 2250 Loss: 0.0453\n",
      "Epoch: 3 / 3, Step: 833 / 2250 Loss: 0.0299\n",
      "Epoch: 3 / 3, Step: 834 / 2250 Loss: 0.1448\n",
      "Epoch: 3 / 3, Step: 835 / 2250 Loss: 0.0439\n",
      "Epoch: 3 / 3, Step: 836 / 2250 Loss: 0.1036\n",
      "Epoch: 3 / 3, Step: 837 / 2250 Loss: 0.0274\n",
      "Epoch: 3 / 3, Step: 838 / 2250 Loss: 0.0449\n",
      "Epoch: 3 / 3, Step: 839 / 2250 Loss: 0.1061\n",
      "Epoch: 3 / 3, Step: 840 / 2250 Loss: 0.3399\n",
      "Epoch: 3 / 3, Step: 841 / 2250 Loss: 0.1548\n",
      "Epoch: 3 / 3, Step: 842 / 2250 Loss: 0.0646\n",
      "Epoch: 3 / 3, Step: 843 / 2250 Loss: 0.0491\n",
      "Epoch: 3 / 3, Step: 844 / 2250 Loss: 0.0872\n",
      "Epoch: 3 / 3, Step: 845 / 2250 Loss: 0.0278\n",
      "Epoch: 3 / 3, Step: 846 / 2250 Loss: 0.0523\n",
      "Epoch: 3 / 3, Step: 847 / 2250 Loss: 0.0885\n",
      "Epoch: 3 / 3, Step: 848 / 2250 Loss: 0.1525\n",
      "Epoch: 3 / 3, Step: 849 / 2250 Loss: 0.1400\n",
      "Epoch: 3 / 3, Step: 850 / 2250 Loss: 0.1183\n",
      "Epoch: 3 / 3, Step: 851 / 2250 Loss: 0.0626\n",
      "Epoch: 3 / 3, Step: 852 / 2250 Loss: 0.1945\n",
      "Epoch: 3 / 3, Step: 853 / 2250 Loss: 0.0532\n",
      "Epoch: 3 / 3, Step: 854 / 2250 Loss: 0.0339\n",
      "Epoch: 3 / 3, Step: 855 / 2250 Loss: 0.0183\n",
      "Epoch: 3 / 3, Step: 856 / 2250 Loss: 0.0811\n",
      "Epoch: 3 / 3, Step: 857 / 2250 Loss: 0.2452\n",
      "Epoch: 3 / 3, Step: 858 / 2250 Loss: 0.0340\n",
      "Epoch: 3 / 3, Step: 859 / 2250 Loss: 0.2503\n",
      "Epoch: 3 / 3, Step: 860 / 2250 Loss: 0.0253\n",
      "Epoch: 3 / 3, Step: 861 / 2250 Loss: 0.0316\n",
      "Epoch: 3 / 3, Step: 862 / 2250 Loss: 0.0453\n",
      "Epoch: 3 / 3, Step: 863 / 2250 Loss: 0.0266\n",
      "Epoch: 3 / 3, Step: 864 / 2250 Loss: 0.3035\n",
      "Epoch: 3 / 3, Step: 865 / 2250 Loss: 0.0372\n",
      "Epoch: 3 / 3, Step: 866 / 2250 Loss: 0.0384\n",
      "Epoch: 3 / 3, Step: 867 / 2250 Loss: 0.1327\n",
      "Epoch: 3 / 3, Step: 868 / 2250 Loss: 0.3457\n",
      "Epoch: 3 / 3, Step: 869 / 2250 Loss: 0.2665\n",
      "Epoch: 3 / 3, Step: 870 / 2250 Loss: 0.1287\n",
      "Epoch: 3 / 3, Step: 871 / 2250 Loss: 0.2570\n",
      "Epoch: 3 / 3, Step: 872 / 2250 Loss: 0.0932\n",
      "Epoch: 3 / 3, Step: 873 / 2250 Loss: 0.0435\n",
      "Epoch: 3 / 3, Step: 874 / 2250 Loss: 0.0156\n",
      "Epoch: 3 / 3, Step: 875 / 2250 Loss: 0.1787\n",
      "Epoch: 3 / 3, Step: 876 / 2250 Loss: 0.0373\n",
      "Epoch: 3 / 3, Step: 877 / 2250 Loss: 0.0131\n",
      "Epoch: 3 / 3, Step: 878 / 2250 Loss: 0.0400\n",
      "Epoch: 3 / 3, Step: 879 / 2250 Loss: 0.2123\n",
      "Epoch: 3 / 3, Step: 880 / 2250 Loss: 0.0124\n",
      "Epoch: 3 / 3, Step: 881 / 2250 Loss: 0.2823\n",
      "Epoch: 3 / 3, Step: 882 / 2250 Loss: 0.0648\n",
      "Epoch: 3 / 3, Step: 883 / 2250 Loss: 0.0354\n",
      "Epoch: 3 / 3, Step: 884 / 2250 Loss: 0.0925\n",
      "Epoch: 3 / 3, Step: 885 / 2250 Loss: 0.0666\n",
      "Epoch: 3 / 3, Step: 886 / 2250 Loss: 0.2503\n",
      "Epoch: 3 / 3, Step: 887 / 2250 Loss: 0.2226\n",
      "Epoch: 3 / 3, Step: 888 / 2250 Loss: 0.0329\n",
      "Epoch: 3 / 3, Step: 889 / 2250 Loss: 0.0338\n",
      "Epoch: 3 / 3, Step: 890 / 2250 Loss: 0.0349\n",
      "Epoch: 3 / 3, Step: 891 / 2250 Loss: 0.0395\n",
      "Epoch: 3 / 3, Step: 892 / 2250 Loss: 0.0589\n",
      "Epoch: 3 / 3, Step: 893 / 2250 Loss: 0.2155\n",
      "Epoch: 3 / 3, Step: 894 / 2250 Loss: 0.2354\n",
      "Epoch: 3 / 3, Step: 895 / 2250 Loss: 0.2635\n",
      "Epoch: 3 / 3, Step: 896 / 2250 Loss: 0.0539\n",
      "Epoch: 3 / 3, Step: 897 / 2250 Loss: 0.0775\n",
      "Epoch: 3 / 3, Step: 898 / 2250 Loss: 0.0829\n",
      "Epoch: 3 / 3, Step: 899 / 2250 Loss: 0.0975\n",
      "Epoch: 3 / 3, Step: 900 / 2250 Loss: 0.0186\n",
      "Epoch: 3 / 3, Step: 901 / 2250 Loss: 0.1069\n",
      "Epoch: 3 / 3, Step: 902 / 2250 Loss: 0.2071\n",
      "Epoch: 3 / 3, Step: 903 / 2250 Loss: 0.1330\n",
      "Epoch: 3 / 3, Step: 904 / 2250 Loss: 0.2201\n",
      "Epoch: 3 / 3, Step: 905 / 2250 Loss: 0.2364\n",
      "Epoch: 3 / 3, Step: 906 / 2250 Loss: 0.3533\n",
      "Epoch: 3 / 3, Step: 907 / 2250 Loss: 0.0157\n",
      "Epoch: 3 / 3, Step: 908 / 2250 Loss: 0.0322\n",
      "Epoch: 3 / 3, Step: 909 / 2250 Loss: 0.3884\n",
      "Epoch: 3 / 3, Step: 910 / 2250 Loss: 0.1340\n",
      "Epoch: 3 / 3, Step: 911 / 2250 Loss: 0.0198\n",
      "Epoch: 3 / 3, Step: 912 / 2250 Loss: 0.0998\n",
      "Epoch: 3 / 3, Step: 913 / 2250 Loss: 0.0661\n",
      "Epoch: 3 / 3, Step: 914 / 2250 Loss: 0.1251\n",
      "Epoch: 3 / 3, Step: 915 / 2250 Loss: 0.0776\n",
      "Epoch: 3 / 3, Step: 916 / 2250 Loss: 0.0718\n",
      "Epoch: 3 / 3, Step: 917 / 2250 Loss: 0.2076\n",
      "Epoch: 3 / 3, Step: 918 / 2250 Loss: 0.1365\n",
      "Epoch: 3 / 3, Step: 919 / 2250 Loss: 0.0411\n",
      "Epoch: 3 / 3, Step: 920 / 2250 Loss: 0.0820\n",
      "Epoch: 3 / 3, Step: 921 / 2250 Loss: 0.2289\n",
      "Epoch: 3 / 3, Step: 922 / 2250 Loss: 0.0605\n",
      "Epoch: 3 / 3, Step: 923 / 2250 Loss: 0.0656\n",
      "Epoch: 3 / 3, Step: 924 / 2250 Loss: 0.1555\n",
      "Epoch: 3 / 3, Step: 925 / 2250 Loss: 0.2913\n",
      "Epoch: 3 / 3, Step: 926 / 2250 Loss: 0.0164\n",
      "Epoch: 3 / 3, Step: 927 / 2250 Loss: 0.0319\n",
      "Epoch: 3 / 3, Step: 928 / 2250 Loss: 0.2869\n",
      "Epoch: 3 / 3, Step: 929 / 2250 Loss: 0.0755\n",
      "Epoch: 3 / 3, Step: 930 / 2250 Loss: 0.1954\n",
      "Epoch: 3 / 3, Step: 931 / 2250 Loss: 0.1490\n",
      "Epoch: 3 / 3, Step: 932 / 2250 Loss: 0.0205\n",
      "Epoch: 3 / 3, Step: 933 / 2250 Loss: 0.1075\n",
      "Epoch: 3 / 3, Step: 934 / 2250 Loss: 0.1570\n",
      "Epoch: 3 / 3, Step: 935 / 2250 Loss: 0.0815\n",
      "Epoch: 3 / 3, Step: 936 / 2250 Loss: 0.1459\n",
      "Epoch: 3 / 3, Step: 937 / 2250 Loss: 0.0253\n",
      "Epoch: 3 / 3, Step: 938 / 2250 Loss: 0.0866\n",
      "Epoch: 3 / 3, Step: 939 / 2250 Loss: 0.1011\n",
      "Epoch: 3 / 3, Step: 940 / 2250 Loss: 0.0443\n",
      "Epoch: 3 / 3, Step: 941 / 2250 Loss: 0.1557\n",
      "Epoch: 3 / 3, Step: 942 / 2250 Loss: 0.0243\n",
      "Epoch: 3 / 3, Step: 943 / 2250 Loss: 0.1895\n",
      "Epoch: 3 / 3, Step: 944 / 2250 Loss: 0.0537\n",
      "Epoch: 3 / 3, Step: 945 / 2250 Loss: 0.0314\n",
      "Epoch: 3 / 3, Step: 946 / 2250 Loss: 0.1169\n",
      "Epoch: 3 / 3, Step: 947 / 2250 Loss: 0.0149\n",
      "Epoch: 3 / 3, Step: 948 / 2250 Loss: 0.0540\n",
      "Epoch: 3 / 3, Step: 949 / 2250 Loss: 0.1627\n",
      "Epoch: 3 / 3, Step: 950 / 2250 Loss: 0.1472\n",
      "Epoch: 3 / 3, Step: 951 / 2250 Loss: 0.0761\n",
      "Epoch: 3 / 3, Step: 952 / 2250 Loss: 0.0187\n",
      "Epoch: 3 / 3, Step: 953 / 2250 Loss: 0.0557\n",
      "Epoch: 3 / 3, Step: 954 / 2250 Loss: 0.0220\n",
      "Epoch: 3 / 3, Step: 955 / 2250 Loss: 0.1394\n",
      "Epoch: 3 / 3, Step: 956 / 2250 Loss: 0.1675\n",
      "Epoch: 3 / 3, Step: 957 / 2250 Loss: 0.0147\n",
      "Epoch: 3 / 3, Step: 958 / 2250 Loss: 0.0695\n",
      "Epoch: 3 / 3, Step: 959 / 2250 Loss: 0.3181\n",
      "Epoch: 3 / 3, Step: 960 / 2250 Loss: 0.0712\n",
      "Epoch: 3 / 3, Step: 961 / 2250 Loss: 0.0639\n",
      "Epoch: 3 / 3, Step: 962 / 2250 Loss: 0.1848\n",
      "Epoch: 3 / 3, Step: 963 / 2250 Loss: 0.0561\n",
      "Epoch: 3 / 3, Step: 964 / 2250 Loss: 0.0317\n",
      "Epoch: 3 / 3, Step: 965 / 2250 Loss: 0.0505\n",
      "Epoch: 3 / 3, Step: 966 / 2250 Loss: 0.1540\n",
      "Epoch: 3 / 3, Step: 967 / 2250 Loss: 0.0450\n",
      "Epoch: 3 / 3, Step: 968 / 2250 Loss: 0.0095\n",
      "Epoch: 3 / 3, Step: 969 / 2250 Loss: 0.1027\n",
      "Epoch: 3 / 3, Step: 970 / 2250 Loss: 0.0356\n",
      "Epoch: 3 / 3, Step: 971 / 2250 Loss: 0.0159\n",
      "Epoch: 3 / 3, Step: 972 / 2250 Loss: 0.0770\n",
      "Epoch: 3 / 3, Step: 973 / 2250 Loss: 0.0218\n",
      "Epoch: 3 / 3, Step: 974 / 2250 Loss: 0.1014\n",
      "Epoch: 3 / 3, Step: 975 / 2250 Loss: 0.0482\n",
      "Epoch: 3 / 3, Step: 976 / 2250 Loss: 0.0073\n",
      "Epoch: 3 / 3, Step: 977 / 2250 Loss: 0.2798\n",
      "Epoch: 3 / 3, Step: 978 / 2250 Loss: 0.0230\n",
      "Epoch: 3 / 3, Step: 979 / 2250 Loss: 0.0253\n",
      "Epoch: 3 / 3, Step: 980 / 2250 Loss: 0.0720\n",
      "Epoch: 3 / 3, Step: 981 / 2250 Loss: 0.1248\n",
      "Epoch: 3 / 3, Step: 982 / 2250 Loss: 0.0162\n",
      "Epoch: 3 / 3, Step: 983 / 2250 Loss: 0.0419\n",
      "Epoch: 3 / 3, Step: 984 / 2250 Loss: 0.5295\n",
      "Epoch: 3 / 3, Step: 985 / 2250 Loss: 0.0780\n",
      "Epoch: 3 / 3, Step: 986 / 2250 Loss: 0.0201\n",
      "Epoch: 3 / 3, Step: 987 / 2250 Loss: 0.0139\n",
      "Epoch: 3 / 3, Step: 988 / 2250 Loss: 0.0231\n",
      "Epoch: 3 / 3, Step: 989 / 2250 Loss: 0.0502\n",
      "Epoch: 3 / 3, Step: 990 / 2250 Loss: 0.0429\n",
      "Epoch: 3 / 3, Step: 991 / 2250 Loss: 0.0138\n",
      "Epoch: 3 / 3, Step: 992 / 2250 Loss: 0.0423\n",
      "Epoch: 3 / 3, Step: 993 / 2250 Loss: 0.0242\n",
      "Epoch: 3 / 3, Step: 994 / 2250 Loss: 0.0772\n",
      "Epoch: 3 / 3, Step: 995 / 2250 Loss: 0.2267\n",
      "Epoch: 3 / 3, Step: 996 / 2250 Loss: 0.0827\n",
      "Epoch: 3 / 3, Step: 997 / 2250 Loss: 0.0705\n",
      "Epoch: 3 / 3, Step: 998 / 2250 Loss: 0.0429\n",
      "Epoch: 3 / 3, Step: 999 / 2250 Loss: 0.0173\n",
      "Epoch: 3 / 3, Step: 1000 / 2250 Loss: 0.0356\n",
      "Epoch: 3 / 3, Step: 1001 / 2250 Loss: 0.0350\n",
      "Epoch: 3 / 3, Step: 1002 / 2250 Loss: 0.0086\n",
      "Epoch: 3 / 3, Step: 1003 / 2250 Loss: 0.0618\n",
      "Epoch: 3 / 3, Step: 1004 / 2250 Loss: 0.0051\n",
      "Epoch: 3 / 3, Step: 1005 / 2250 Loss: 0.1111\n",
      "Epoch: 3 / 3, Step: 1006 / 2250 Loss: 0.0298\n",
      "Epoch: 3 / 3, Step: 1007 / 2250 Loss: 0.0208\n",
      "Epoch: 3 / 3, Step: 1008 / 2250 Loss: 0.0313\n",
      "Epoch: 3 / 3, Step: 1009 / 2250 Loss: 0.1507\n",
      "Epoch: 3 / 3, Step: 1010 / 2250 Loss: 0.1721\n",
      "Epoch: 3 / 3, Step: 1011 / 2250 Loss: 0.1921\n",
      "Epoch: 3 / 3, Step: 1012 / 2250 Loss: 0.0925\n",
      "Epoch: 3 / 3, Step: 1013 / 2250 Loss: 0.0360\n",
      "Epoch: 3 / 3, Step: 1014 / 2250 Loss: 0.0150\n",
      "Epoch: 3 / 3, Step: 1015 / 2250 Loss: 0.0065\n",
      "Epoch: 3 / 3, Step: 1016 / 2250 Loss: 0.0417\n",
      "Epoch: 3 / 3, Step: 1017 / 2250 Loss: 0.0445\n",
      "Epoch: 3 / 3, Step: 1018 / 2250 Loss: 0.0284\n",
      "Epoch: 3 / 3, Step: 1019 / 2250 Loss: 0.2156\n",
      "Epoch: 3 / 3, Step: 1020 / 2250 Loss: 0.0175\n",
      "Epoch: 3 / 3, Step: 1021 / 2250 Loss: 0.1317\n",
      "Epoch: 3 / 3, Step: 1022 / 2250 Loss: 0.0092\n",
      "Epoch: 3 / 3, Step: 1023 / 2250 Loss: 0.0664\n",
      "Epoch: 3 / 3, Step: 1024 / 2250 Loss: 0.0301\n",
      "Epoch: 3 / 3, Step: 1025 / 2250 Loss: 0.0742\n",
      "Epoch: 3 / 3, Step: 1026 / 2250 Loss: 0.0437\n",
      "Epoch: 3 / 3, Step: 1027 / 2250 Loss: 0.1051\n",
      "Epoch: 3 / 3, Step: 1028 / 2250 Loss: 0.1065\n",
      "Epoch: 3 / 3, Step: 1029 / 2250 Loss: 0.1442\n",
      "Epoch: 3 / 3, Step: 1030 / 2250 Loss: 0.3097\n",
      "Epoch: 3 / 3, Step: 1031 / 2250 Loss: 0.0153\n",
      "Epoch: 3 / 3, Step: 1032 / 2250 Loss: 0.0194\n",
      "Epoch: 3 / 3, Step: 1033 / 2250 Loss: 0.0668\n",
      "Epoch: 3 / 3, Step: 1034 / 2250 Loss: 0.0205\n",
      "Epoch: 3 / 3, Step: 1035 / 2250 Loss: 0.0121\n",
      "Epoch: 3 / 3, Step: 1036 / 2250 Loss: 0.1590\n",
      "Epoch: 3 / 3, Step: 1037 / 2250 Loss: 0.0308\n",
      "Epoch: 3 / 3, Step: 1038 / 2250 Loss: 0.0869\n",
      "Epoch: 3 / 3, Step: 1039 / 2250 Loss: 0.1396\n",
      "Epoch: 3 / 3, Step: 1040 / 2250 Loss: 0.0654\n",
      "Epoch: 3 / 3, Step: 1041 / 2250 Loss: 0.0210\n",
      "Epoch: 3 / 3, Step: 1042 / 2250 Loss: 0.0955\n",
      "Epoch: 3 / 3, Step: 1043 / 2250 Loss: 0.4912\n",
      "Epoch: 3 / 3, Step: 1044 / 2250 Loss: 0.1472\n",
      "Epoch: 3 / 3, Step: 1045 / 2250 Loss: 0.0109\n",
      "Epoch: 3 / 3, Step: 1046 / 2250 Loss: 0.0516\n",
      "Epoch: 3 / 3, Step: 1047 / 2250 Loss: 0.1379\n",
      "Epoch: 3 / 3, Step: 1048 / 2250 Loss: 0.1497\n",
      "Epoch: 3 / 3, Step: 1049 / 2250 Loss: 0.0444\n",
      "Epoch: 3 / 3, Step: 1050 / 2250 Loss: 0.1563\n",
      "Epoch: 3 / 3, Step: 1051 / 2250 Loss: 0.0772\n",
      "Epoch: 3 / 3, Step: 1052 / 2250 Loss: 0.0311\n",
      "Epoch: 3 / 3, Step: 1053 / 2250 Loss: 0.0896\n",
      "Epoch: 3 / 3, Step: 1054 / 2250 Loss: 0.0287\n",
      "Epoch: 3 / 3, Step: 1055 / 2250 Loss: 0.0944\n",
      "Epoch: 3 / 3, Step: 1056 / 2250 Loss: 0.2021\n",
      "Epoch: 3 / 3, Step: 1057 / 2250 Loss: 0.4489\n",
      "Epoch: 3 / 3, Step: 1058 / 2250 Loss: 0.0220\n",
      "Epoch: 3 / 3, Step: 1059 / 2250 Loss: 0.1306\n",
      "Epoch: 3 / 3, Step: 1060 / 2250 Loss: 0.0150\n",
      "Epoch: 3 / 3, Step: 1061 / 2250 Loss: 0.0619\n",
      "Epoch: 3 / 3, Step: 1062 / 2250 Loss: 0.0316\n",
      "Epoch: 3 / 3, Step: 1063 / 2250 Loss: 0.1976\n",
      "Epoch: 3 / 3, Step: 1064 / 2250 Loss: 0.0468\n",
      "Epoch: 3 / 3, Step: 1065 / 2250 Loss: 0.0425\n",
      "Epoch: 3 / 3, Step: 1066 / 2250 Loss: 0.0574\n",
      "Epoch: 3 / 3, Step: 1067 / 2250 Loss: 0.0963\n",
      "Epoch: 3 / 3, Step: 1068 / 2250 Loss: 0.2167\n",
      "Epoch: 3 / 3, Step: 1069 / 2250 Loss: 0.1086\n",
      "Epoch: 3 / 3, Step: 1070 / 2250 Loss: 0.0161\n",
      "Epoch: 3 / 3, Step: 1071 / 2250 Loss: 0.0669\n",
      "Epoch: 3 / 3, Step: 1072 / 2250 Loss: 0.1156\n",
      "Epoch: 3 / 3, Step: 1073 / 2250 Loss: 0.0096\n",
      "Epoch: 3 / 3, Step: 1074 / 2250 Loss: 0.0310\n",
      "Epoch: 3 / 3, Step: 1075 / 2250 Loss: 0.0572\n",
      "Epoch: 3 / 3, Step: 1076 / 2250 Loss: 0.0116\n",
      "Epoch: 3 / 3, Step: 1077 / 2250 Loss: 0.1840\n",
      "Epoch: 3 / 3, Step: 1078 / 2250 Loss: 0.0430\n",
      "Epoch: 3 / 3, Step: 1079 / 2250 Loss: 0.1896\n",
      "Epoch: 3 / 3, Step: 1080 / 2250 Loss: 0.0436\n",
      "Epoch: 3 / 3, Step: 1081 / 2250 Loss: 0.1348\n",
      "Epoch: 3 / 3, Step: 1082 / 2250 Loss: 0.0580\n",
      "Epoch: 3 / 3, Step: 1083 / 2250 Loss: 0.0399\n",
      "Epoch: 3 / 3, Step: 1084 / 2250 Loss: 0.1092\n",
      "Epoch: 3 / 3, Step: 1085 / 2250 Loss: 0.1278\n",
      "Epoch: 3 / 3, Step: 1086 / 2250 Loss: 0.0731\n",
      "Epoch: 3 / 3, Step: 1087 / 2250 Loss: 0.0590\n",
      "Epoch: 3 / 3, Step: 1088 / 2250 Loss: 0.0326\n",
      "Epoch: 3 / 3, Step: 1089 / 2250 Loss: 0.2246\n",
      "Epoch: 3 / 3, Step: 1090 / 2250 Loss: 0.0484\n",
      "Epoch: 3 / 3, Step: 1091 / 2250 Loss: 0.0240\n",
      "Epoch: 3 / 3, Step: 1092 / 2250 Loss: 0.1126\n",
      "Epoch: 3 / 3, Step: 1093 / 2250 Loss: 0.0116\n",
      "Epoch: 3 / 3, Step: 1094 / 2250 Loss: 0.0230\n",
      "Epoch: 3 / 3, Step: 1095 / 2250 Loss: 0.2562\n",
      "Epoch: 3 / 3, Step: 1096 / 2250 Loss: 0.1052\n",
      "Epoch: 3 / 3, Step: 1097 / 2250 Loss: 0.0431\n",
      "Epoch: 3 / 3, Step: 1098 / 2250 Loss: 0.1739\n",
      "Epoch: 3 / 3, Step: 1099 / 2250 Loss: 0.0887\n",
      "Epoch: 3 / 3, Step: 1100 / 2250 Loss: 0.0639\n",
      "Epoch: 3 / 3, Step: 1101 / 2250 Loss: 0.0149\n",
      "Epoch: 3 / 3, Step: 1102 / 2250 Loss: 0.2142\n",
      "Epoch: 3 / 3, Step: 1103 / 2250 Loss: 0.0133\n",
      "Epoch: 3 / 3, Step: 1104 / 2250 Loss: 0.0138\n",
      "Epoch: 3 / 3, Step: 1105 / 2250 Loss: 0.2186\n",
      "Epoch: 3 / 3, Step: 1106 / 2250 Loss: 0.2613\n",
      "Epoch: 3 / 3, Step: 1107 / 2250 Loss: 0.0595\n",
      "Epoch: 3 / 3, Step: 1108 / 2250 Loss: 0.2040\n",
      "Epoch: 3 / 3, Step: 1109 / 2250 Loss: 0.2070\n",
      "Epoch: 3 / 3, Step: 1110 / 2250 Loss: 0.0147\n",
      "Epoch: 3 / 3, Step: 1111 / 2250 Loss: 0.0142\n",
      "Epoch: 3 / 3, Step: 1112 / 2250 Loss: 0.0334\n",
      "Epoch: 3 / 3, Step: 1113 / 2250 Loss: 0.1444\n",
      "Epoch: 3 / 3, Step: 1114 / 2250 Loss: 0.0250\n",
      "Epoch: 3 / 3, Step: 1115 / 2250 Loss: 0.0167\n",
      "Epoch: 3 / 3, Step: 1116 / 2250 Loss: 0.0504\n",
      "Epoch: 3 / 3, Step: 1117 / 2250 Loss: 0.0234\n",
      "Epoch: 3 / 3, Step: 1118 / 2250 Loss: 0.0506\n",
      "Epoch: 3 / 3, Step: 1119 / 2250 Loss: 0.0200\n",
      "Epoch: 3 / 3, Step: 1120 / 2250 Loss: 0.0291\n",
      "Epoch: 3 / 3, Step: 1121 / 2250 Loss: 0.0689\n",
      "Epoch: 3 / 3, Step: 1122 / 2250 Loss: 0.1751\n",
      "Epoch: 3 / 3, Step: 1123 / 2250 Loss: 0.1230\n",
      "Epoch: 3 / 3, Step: 1124 / 2250 Loss: 0.0585\n",
      "Epoch: 3 / 3, Step: 1125 / 2250 Loss: 0.1631\n",
      "Epoch: 3 / 3, Step: 1126 / 2250 Loss: 0.0170\n",
      "Epoch: 3 / 3, Step: 1127 / 2250 Loss: 0.3946\n",
      "Epoch: 3 / 3, Step: 1128 / 2250 Loss: 0.1189\n",
      "Epoch: 3 / 3, Step: 1129 / 2250 Loss: 0.0298\n",
      "Epoch: 3 / 3, Step: 1130 / 2250 Loss: 0.1029\n",
      "Epoch: 3 / 3, Step: 1131 / 2250 Loss: 0.0402\n",
      "Epoch: 3 / 3, Step: 1132 / 2250 Loss: 0.0162\n",
      "Epoch: 3 / 3, Step: 1133 / 2250 Loss: 0.0618\n",
      "Epoch: 3 / 3, Step: 1134 / 2250 Loss: 0.0117\n",
      "Epoch: 3 / 3, Step: 1135 / 2250 Loss: 0.0318\n",
      "Epoch: 3 / 3, Step: 1136 / 2250 Loss: 0.0652\n",
      "Epoch: 3 / 3, Step: 1137 / 2250 Loss: 0.0250\n",
      "Epoch: 3 / 3, Step: 1138 / 2250 Loss: 0.1545\n",
      "Epoch: 3 / 3, Step: 1139 / 2250 Loss: 0.2668\n",
      "Epoch: 3 / 3, Step: 1140 / 2250 Loss: 0.0261\n",
      "Epoch: 3 / 3, Step: 1141 / 2250 Loss: 0.1209\n",
      "Epoch: 3 / 3, Step: 1142 / 2250 Loss: 0.1177\n",
      "Epoch: 3 / 3, Step: 1143 / 2250 Loss: 0.1661\n",
      "Epoch: 3 / 3, Step: 1144 / 2250 Loss: 0.0539\n",
      "Epoch: 3 / 3, Step: 1145 / 2250 Loss: 0.3035\n",
      "Epoch: 3 / 3, Step: 1146 / 2250 Loss: 0.1500\n",
      "Epoch: 3 / 3, Step: 1147 / 2250 Loss: 0.1670\n",
      "Epoch: 3 / 3, Step: 1148 / 2250 Loss: 0.0544\n",
      "Epoch: 3 / 3, Step: 1149 / 2250 Loss: 0.0608\n",
      "Epoch: 3 / 3, Step: 1150 / 2250 Loss: 0.1446\n",
      "Epoch: 3 / 3, Step: 1151 / 2250 Loss: 0.0098\n",
      "Epoch: 3 / 3, Step: 1152 / 2250 Loss: 0.3569\n",
      "Epoch: 3 / 3, Step: 1153 / 2250 Loss: 0.1041\n",
      "Epoch: 3 / 3, Step: 1154 / 2250 Loss: 0.0378\n",
      "Epoch: 3 / 3, Step: 1155 / 2250 Loss: 0.0135\n",
      "Epoch: 3 / 3, Step: 1156 / 2250 Loss: 0.1932\n",
      "Epoch: 3 / 3, Step: 1157 / 2250 Loss: 0.2073\n",
      "Epoch: 3 / 3, Step: 1158 / 2250 Loss: 0.0247\n",
      "Epoch: 3 / 3, Step: 1159 / 2250 Loss: 0.2149\n",
      "Epoch: 3 / 3, Step: 1160 / 2250 Loss: 0.0547\n",
      "Epoch: 3 / 3, Step: 1161 / 2250 Loss: 0.0128\n",
      "Epoch: 3 / 3, Step: 1162 / 2250 Loss: 0.0490\n",
      "Epoch: 3 / 3, Step: 1163 / 2250 Loss: 0.0161\n",
      "Epoch: 3 / 3, Step: 1164 / 2250 Loss: 0.2844\n",
      "Epoch: 3 / 3, Step: 1165 / 2250 Loss: 0.2523\n",
      "Epoch: 3 / 3, Step: 1166 / 2250 Loss: 0.1797\n",
      "Epoch: 3 / 3, Step: 1167 / 2250 Loss: 0.0189\n",
      "Epoch: 3 / 3, Step: 1168 / 2250 Loss: 0.1028\n",
      "Epoch: 3 / 3, Step: 1169 / 2250 Loss: 0.0451\n",
      "Epoch: 3 / 3, Step: 1170 / 2250 Loss: 0.1465\n",
      "Epoch: 3 / 3, Step: 1171 / 2250 Loss: 0.0485\n",
      "Epoch: 3 / 3, Step: 1172 / 2250 Loss: 0.0620\n",
      "Epoch: 3 / 3, Step: 1173 / 2250 Loss: 0.2464\n",
      "Epoch: 3 / 3, Step: 1174 / 2250 Loss: 0.1183\n",
      "Epoch: 3 / 3, Step: 1175 / 2250 Loss: 0.0275\n",
      "Epoch: 3 / 3, Step: 1176 / 2250 Loss: 0.0850\n",
      "Epoch: 3 / 3, Step: 1177 / 2250 Loss: 0.0151\n",
      "Epoch: 3 / 3, Step: 1178 / 2250 Loss: 0.0413\n",
      "Epoch: 3 / 3, Step: 1179 / 2250 Loss: 0.1166\n",
      "Epoch: 3 / 3, Step: 1180 / 2250 Loss: 0.0673\n",
      "Epoch: 3 / 3, Step: 1181 / 2250 Loss: 0.0138\n",
      "Epoch: 3 / 3, Step: 1182 / 2250 Loss: 0.0141\n",
      "Epoch: 3 / 3, Step: 1183 / 2250 Loss: 0.1732\n",
      "Epoch: 3 / 3, Step: 1184 / 2250 Loss: 0.2023\n",
      "Epoch: 3 / 3, Step: 1185 / 2250 Loss: 0.0657\n",
      "Epoch: 3 / 3, Step: 1186 / 2250 Loss: 0.0169\n",
      "Epoch: 3 / 3, Step: 1187 / 2250 Loss: 0.0657\n",
      "Epoch: 3 / 3, Step: 1188 / 2250 Loss: 0.1220\n",
      "Epoch: 3 / 3, Step: 1189 / 2250 Loss: 0.0609\n",
      "Epoch: 3 / 3, Step: 1190 / 2250 Loss: 0.0698\n",
      "Epoch: 3 / 3, Step: 1191 / 2250 Loss: 0.1939\n",
      "Epoch: 3 / 3, Step: 1192 / 2250 Loss: 0.2417\n",
      "Epoch: 3 / 3, Step: 1193 / 2250 Loss: 0.1739\n",
      "Epoch: 3 / 3, Step: 1194 / 2250 Loss: 0.0306\n",
      "Epoch: 3 / 3, Step: 1195 / 2250 Loss: 0.0331\n",
      "Epoch: 3 / 3, Step: 1196 / 2250 Loss: 0.0806\n",
      "Epoch: 3 / 3, Step: 1197 / 2250 Loss: 0.0091\n",
      "Epoch: 3 / 3, Step: 1198 / 2250 Loss: 0.2720\n",
      "Epoch: 3 / 3, Step: 1199 / 2250 Loss: 0.2079\n",
      "Epoch: 3 / 3, Step: 1200 / 2250 Loss: 0.0568\n",
      "Epoch: 3 / 3, Step: 1201 / 2250 Loss: 0.0150\n",
      "Epoch: 3 / 3, Step: 1202 / 2250 Loss: 0.0952\n",
      "Epoch: 3 / 3, Step: 1203 / 2250 Loss: 0.0543\n",
      "Epoch: 3 / 3, Step: 1204 / 2250 Loss: 0.0419\n",
      "Epoch: 3 / 3, Step: 1205 / 2250 Loss: 0.1941\n",
      "Epoch: 3 / 3, Step: 1206 / 2250 Loss: 0.0118\n",
      "Epoch: 3 / 3, Step: 1207 / 2250 Loss: 0.0841\n",
      "Epoch: 3 / 3, Step: 1208 / 2250 Loss: 0.0323\n",
      "Epoch: 3 / 3, Step: 1209 / 2250 Loss: 0.0141\n",
      "Epoch: 3 / 3, Step: 1210 / 2250 Loss: 0.0273\n",
      "Epoch: 3 / 3, Step: 1211 / 2250 Loss: 0.0711\n",
      "Epoch: 3 / 3, Step: 1212 / 2250 Loss: 0.0714\n",
      "Epoch: 3 / 3, Step: 1213 / 2250 Loss: 0.0197\n",
      "Epoch: 3 / 3, Step: 1214 / 2250 Loss: 0.0539\n",
      "Epoch: 3 / 3, Step: 1215 / 2250 Loss: 0.1060\n",
      "Epoch: 3 / 3, Step: 1216 / 2250 Loss: 0.0184\n",
      "Epoch: 3 / 3, Step: 1217 / 2250 Loss: 0.0129\n",
      "Epoch: 3 / 3, Step: 1218 / 2250 Loss: 0.0214\n",
      "Epoch: 3 / 3, Step: 1219 / 2250 Loss: 0.0069\n",
      "Epoch: 3 / 3, Step: 1220 / 2250 Loss: 0.0898\n",
      "Epoch: 3 / 3, Step: 1221 / 2250 Loss: 0.0262\n",
      "Epoch: 3 / 3, Step: 1222 / 2250 Loss: 0.0798\n",
      "Epoch: 3 / 3, Step: 1223 / 2250 Loss: 0.0115\n",
      "Epoch: 3 / 3, Step: 1224 / 2250 Loss: 0.0850\n",
      "Epoch: 3 / 3, Step: 1225 / 2250 Loss: 0.2300\n",
      "Epoch: 3 / 3, Step: 1226 / 2250 Loss: 0.0166\n",
      "Epoch: 3 / 3, Step: 1227 / 2250 Loss: 0.0709\n",
      "Epoch: 3 / 3, Step: 1228 / 2250 Loss: 0.1570\n",
      "Epoch: 3 / 3, Step: 1229 / 2250 Loss: 0.0734\n",
      "Epoch: 3 / 3, Step: 1230 / 2250 Loss: 0.0782\n",
      "Epoch: 3 / 3, Step: 1231 / 2250 Loss: 0.1430\n",
      "Epoch: 3 / 3, Step: 1232 / 2250 Loss: 0.0145\n",
      "Epoch: 3 / 3, Step: 1233 / 2250 Loss: 0.0694\n",
      "Epoch: 3 / 3, Step: 1234 / 2250 Loss: 0.0279\n",
      "Epoch: 3 / 3, Step: 1235 / 2250 Loss: 0.0728\n",
      "Epoch: 3 / 3, Step: 1236 / 2250 Loss: 0.0127\n",
      "Epoch: 3 / 3, Step: 1237 / 2250 Loss: 0.0144\n",
      "Epoch: 3 / 3, Step: 1238 / 2250 Loss: 0.1681\n",
      "Epoch: 3 / 3, Step: 1239 / 2250 Loss: 0.0340\n",
      "Epoch: 3 / 3, Step: 1240 / 2250 Loss: 0.1342\n",
      "Epoch: 3 / 3, Step: 1241 / 2250 Loss: 0.0244\n",
      "Epoch: 3 / 3, Step: 1242 / 2250 Loss: 0.0225\n",
      "Epoch: 3 / 3, Step: 1243 / 2250 Loss: 0.0448\n",
      "Epoch: 3 / 3, Step: 1244 / 2250 Loss: 0.0219\n",
      "Epoch: 3 / 3, Step: 1245 / 2250 Loss: 0.1855\n",
      "Epoch: 3 / 3, Step: 1246 / 2250 Loss: 0.0327\n",
      "Epoch: 3 / 3, Step: 1247 / 2250 Loss: 0.0868\n",
      "Epoch: 3 / 3, Step: 1248 / 2250 Loss: 0.1318\n",
      "Epoch: 3 / 3, Step: 1249 / 2250 Loss: 0.0176\n",
      "Epoch: 3 / 3, Step: 1250 / 2250 Loss: 0.0180\n",
      "Epoch: 3 / 3, Step: 1251 / 2250 Loss: 0.0917\n",
      "Epoch: 3 / 3, Step: 1252 / 2250 Loss: 0.0241\n",
      "Epoch: 3 / 3, Step: 1253 / 2250 Loss: 0.0129\n",
      "Epoch: 3 / 3, Step: 1254 / 2250 Loss: 0.0329\n",
      "Epoch: 3 / 3, Step: 1255 / 2250 Loss: 0.0087\n",
      "Epoch: 3 / 3, Step: 1256 / 2250 Loss: 0.0123\n",
      "Epoch: 3 / 3, Step: 1257 / 2250 Loss: 0.2170\n",
      "Epoch: 3 / 3, Step: 1258 / 2250 Loss: 0.0070\n",
      "Epoch: 3 / 3, Step: 1259 / 2250 Loss: 0.0377\n",
      "Epoch: 3 / 3, Step: 1260 / 2250 Loss: 0.0891\n",
      "Epoch: 3 / 3, Step: 1261 / 2250 Loss: 0.0510\n",
      "Epoch: 3 / 3, Step: 1262 / 2250 Loss: 0.0585\n",
      "Epoch: 3 / 3, Step: 1263 / 2250 Loss: 0.0515\n",
      "Epoch: 3 / 3, Step: 1264 / 2250 Loss: 0.1671\n",
      "Epoch: 3 / 3, Step: 1265 / 2250 Loss: 0.1525\n",
      "Epoch: 3 / 3, Step: 1266 / 2250 Loss: 0.0176\n",
      "Epoch: 3 / 3, Step: 1267 / 2250 Loss: 0.0095\n",
      "Epoch: 3 / 3, Step: 1268 / 2250 Loss: 0.0242\n",
      "Epoch: 3 / 3, Step: 1269 / 2250 Loss: 0.0996\n",
      "Epoch: 3 / 3, Step: 1270 / 2250 Loss: 0.0094\n",
      "Epoch: 3 / 3, Step: 1271 / 2250 Loss: 0.1007\n",
      "Epoch: 3 / 3, Step: 1272 / 2250 Loss: 0.1998\n",
      "Epoch: 3 / 3, Step: 1273 / 2250 Loss: 0.0708\n",
      "Epoch: 3 / 3, Step: 1274 / 2250 Loss: 0.0135\n",
      "Epoch: 3 / 3, Step: 1275 / 2250 Loss: 0.0201\n",
      "Epoch: 3 / 3, Step: 1276 / 2250 Loss: 0.0169\n",
      "Epoch: 3 / 3, Step: 1277 / 2250 Loss: 0.0349\n",
      "Epoch: 3 / 3, Step: 1278 / 2250 Loss: 0.3726\n",
      "Epoch: 3 / 3, Step: 1279 / 2250 Loss: 0.1950\n",
      "Epoch: 3 / 3, Step: 1280 / 2250 Loss: 0.0112\n",
      "Epoch: 3 / 3, Step: 1281 / 2250 Loss: 0.0043\n",
      "Epoch: 3 / 3, Step: 1282 / 2250 Loss: 0.3112\n",
      "Epoch: 3 / 3, Step: 1283 / 2250 Loss: 0.0151\n",
      "Epoch: 3 / 3, Step: 1284 / 2250 Loss: 0.0166\n",
      "Epoch: 3 / 3, Step: 1285 / 2250 Loss: 0.0111\n",
      "Epoch: 3 / 3, Step: 1286 / 2250 Loss: 0.3713\n",
      "Epoch: 3 / 3, Step: 1287 / 2250 Loss: 0.0605\n",
      "Epoch: 3 / 3, Step: 1288 / 2250 Loss: 0.0187\n",
      "Epoch: 3 / 3, Step: 1289 / 2250 Loss: 0.0092\n",
      "Epoch: 3 / 3, Step: 1290 / 2250 Loss: 0.0498\n",
      "Epoch: 3 / 3, Step: 1291 / 2250 Loss: 0.0667\n",
      "Epoch: 3 / 3, Step: 1292 / 2250 Loss: 0.4120\n",
      "Epoch: 3 / 3, Step: 1293 / 2250 Loss: 0.1873\n",
      "Epoch: 3 / 3, Step: 1294 / 2250 Loss: 0.2103\n",
      "Epoch: 3 / 3, Step: 1295 / 2250 Loss: 0.0654\n",
      "Epoch: 3 / 3, Step: 1296 / 2250 Loss: 0.0192\n",
      "Epoch: 3 / 3, Step: 1297 / 2250 Loss: 0.1907\n",
      "Epoch: 3 / 3, Step: 1298 / 2250 Loss: 0.0195\n",
      "Epoch: 3 / 3, Step: 1299 / 2250 Loss: 0.0252\n",
      "Epoch: 3 / 3, Step: 1300 / 2250 Loss: 0.1885\n",
      "Epoch: 3 / 3, Step: 1301 / 2250 Loss: 0.0272\n",
      "Epoch: 3 / 3, Step: 1302 / 2250 Loss: 0.0184\n",
      "Epoch: 3 / 3, Step: 1303 / 2250 Loss: 0.1455\n",
      "Epoch: 3 / 3, Step: 1304 / 2250 Loss: 0.1277\n",
      "Epoch: 3 / 3, Step: 1305 / 2250 Loss: 0.0408\n",
      "Epoch: 3 / 3, Step: 1306 / 2250 Loss: 0.0612\n",
      "Epoch: 3 / 3, Step: 1307 / 2250 Loss: 0.1113\n",
      "Epoch: 3 / 3, Step: 1308 / 2250 Loss: 0.0364\n",
      "Epoch: 3 / 3, Step: 1309 / 2250 Loss: 0.2727\n",
      "Epoch: 3 / 3, Step: 1310 / 2250 Loss: 0.2060\n",
      "Epoch: 3 / 3, Step: 1311 / 2250 Loss: 0.0256\n",
      "Epoch: 3 / 3, Step: 1312 / 2250 Loss: 0.0812\n",
      "Epoch: 3 / 3, Step: 1313 / 2250 Loss: 0.1704\n",
      "Epoch: 3 / 3, Step: 1314 / 2250 Loss: 0.0147\n",
      "Epoch: 3 / 3, Step: 1315 / 2250 Loss: 0.0555\n",
      "Epoch: 3 / 3, Step: 1316 / 2250 Loss: 0.0741\n",
      "Epoch: 3 / 3, Step: 1317 / 2250 Loss: 0.1490\n",
      "Epoch: 3 / 3, Step: 1318 / 2250 Loss: 0.0584\n",
      "Epoch: 3 / 3, Step: 1319 / 2250 Loss: 0.1458\n",
      "Epoch: 3 / 3, Step: 1320 / 2250 Loss: 0.0229\n",
      "Epoch: 3 / 3, Step: 1321 / 2250 Loss: 0.2410\n",
      "Epoch: 3 / 3, Step: 1322 / 2250 Loss: 0.0435\n",
      "Epoch: 3 / 3, Step: 1323 / 2250 Loss: 0.0082\n",
      "Epoch: 3 / 3, Step: 1324 / 2250 Loss: 0.2913\n",
      "Epoch: 3 / 3, Step: 1325 / 2250 Loss: 0.0336\n",
      "Epoch: 3 / 3, Step: 1326 / 2250 Loss: 0.3365\n",
      "Epoch: 3 / 3, Step: 1327 / 2250 Loss: 0.0442\n",
      "Epoch: 3 / 3, Step: 1328 / 2250 Loss: 0.0101\n",
      "Epoch: 3 / 3, Step: 1329 / 2250 Loss: 0.0419\n",
      "Epoch: 3 / 3, Step: 1330 / 2250 Loss: 0.0393\n",
      "Epoch: 3 / 3, Step: 1331 / 2250 Loss: 0.0313\n",
      "Epoch: 3 / 3, Step: 1332 / 2250 Loss: 0.0151\n",
      "Epoch: 3 / 3, Step: 1333 / 2250 Loss: 0.0126\n",
      "Epoch: 3 / 3, Step: 1334 / 2250 Loss: 0.1295\n",
      "Epoch: 3 / 3, Step: 1335 / 2250 Loss: 0.0643\n",
      "Epoch: 3 / 3, Step: 1336 / 2250 Loss: 0.0079\n",
      "Epoch: 3 / 3, Step: 1337 / 2250 Loss: 0.0099\n",
      "Epoch: 3 / 3, Step: 1338 / 2250 Loss: 0.0181\n",
      "Epoch: 3 / 3, Step: 1339 / 2250 Loss: 0.0059\n",
      "Epoch: 3 / 3, Step: 1340 / 2250 Loss: 0.1933\n",
      "Epoch: 3 / 3, Step: 1341 / 2250 Loss: 0.0629\n",
      "Epoch: 3 / 3, Step: 1342 / 2250 Loss: 0.0123\n",
      "Epoch: 3 / 3, Step: 1343 / 2250 Loss: 0.0784\n",
      "Epoch: 3 / 3, Step: 1344 / 2250 Loss: 0.0760\n",
      "Epoch: 3 / 3, Step: 1345 / 2250 Loss: 0.0130\n",
      "Epoch: 3 / 3, Step: 1346 / 2250 Loss: 0.3159\n",
      "Epoch: 3 / 3, Step: 1347 / 2250 Loss: 0.0082\n",
      "Epoch: 3 / 3, Step: 1348 / 2250 Loss: 0.0573\n",
      "Epoch: 3 / 3, Step: 1349 / 2250 Loss: 0.0384\n",
      "Epoch: 3 / 3, Step: 1350 / 2250 Loss: 0.0865\n",
      "Epoch: 3 / 3, Step: 1351 / 2250 Loss: 0.0197\n",
      "Epoch: 3 / 3, Step: 1352 / 2250 Loss: 0.0381\n",
      "Epoch: 3 / 3, Step: 1353 / 2250 Loss: 0.1476\n",
      "Epoch: 3 / 3, Step: 1354 / 2250 Loss: 0.0236\n",
      "Epoch: 3 / 3, Step: 1355 / 2250 Loss: 0.0719\n",
      "Epoch: 3 / 3, Step: 1356 / 2250 Loss: 0.1129\n",
      "Epoch: 3 / 3, Step: 1357 / 2250 Loss: 0.0919\n",
      "Epoch: 3 / 3, Step: 1358 / 2250 Loss: 0.0208\n",
      "Epoch: 3 / 3, Step: 1359 / 2250 Loss: 0.1048\n",
      "Epoch: 3 / 3, Step: 1360 / 2250 Loss: 0.0121\n",
      "Epoch: 3 / 3, Step: 1361 / 2250 Loss: 0.0444\n",
      "Epoch: 3 / 3, Step: 1362 / 2250 Loss: 0.0220\n",
      "Epoch: 3 / 3, Step: 1363 / 2250 Loss: 0.0961\n",
      "Epoch: 3 / 3, Step: 1364 / 2250 Loss: 0.1847\n",
      "Epoch: 3 / 3, Step: 1365 / 2250 Loss: 0.0478\n",
      "Epoch: 3 / 3, Step: 1366 / 2250 Loss: 0.0220\n",
      "Epoch: 3 / 3, Step: 1367 / 2250 Loss: 0.0093\n",
      "Epoch: 3 / 3, Step: 1368 / 2250 Loss: 0.0173\n",
      "Epoch: 3 / 3, Step: 1369 / 2250 Loss: 0.2140\n",
      "Epoch: 3 / 3, Step: 1370 / 2250 Loss: 0.0130\n",
      "Epoch: 3 / 3, Step: 1371 / 2250 Loss: 0.0155\n",
      "Epoch: 3 / 3, Step: 1372 / 2250 Loss: 0.0152\n",
      "Epoch: 3 / 3, Step: 1373 / 2250 Loss: 0.2877\n",
      "Epoch: 3 / 3, Step: 1374 / 2250 Loss: 0.0330\n",
      "Epoch: 3 / 3, Step: 1375 / 2250 Loss: 0.0106\n",
      "Epoch: 3 / 3, Step: 1376 / 2250 Loss: 0.0802\n",
      "Epoch: 3 / 3, Step: 1377 / 2250 Loss: 0.1193\n",
      "Epoch: 3 / 3, Step: 1378 / 2250 Loss: 0.0840\n",
      "Epoch: 3 / 3, Step: 1379 / 2250 Loss: 0.1722\n",
      "Epoch: 3 / 3, Step: 1380 / 2250 Loss: 0.0125\n",
      "Epoch: 3 / 3, Step: 1381 / 2250 Loss: 0.0240\n",
      "Epoch: 3 / 3, Step: 1382 / 2250 Loss: 0.2119\n",
      "Epoch: 3 / 3, Step: 1383 / 2250 Loss: 0.1020\n",
      "Epoch: 3 / 3, Step: 1384 / 2250 Loss: 0.0175\n",
      "Epoch: 3 / 3, Step: 1385 / 2250 Loss: 0.2132\n",
      "Epoch: 3 / 3, Step: 1386 / 2250 Loss: 0.1377\n",
      "Epoch: 3 / 3, Step: 1387 / 2250 Loss: 0.0306\n",
      "Epoch: 3 / 3, Step: 1388 / 2250 Loss: 0.0149\n",
      "Epoch: 3 / 3, Step: 1389 / 2250 Loss: 0.0165\n",
      "Epoch: 3 / 3, Step: 1390 / 2250 Loss: 0.1172\n",
      "Epoch: 3 / 3, Step: 1391 / 2250 Loss: 0.0152\n",
      "Epoch: 3 / 3, Step: 1392 / 2250 Loss: 0.0990\n",
      "Epoch: 3 / 3, Step: 1393 / 2250 Loss: 0.2590\n",
      "Epoch: 3 / 3, Step: 1394 / 2250 Loss: 0.0222\n",
      "Epoch: 3 / 3, Step: 1395 / 2250 Loss: 0.0234\n",
      "Epoch: 3 / 3, Step: 1396 / 2250 Loss: 0.1785\n",
      "Epoch: 3 / 3, Step: 1397 / 2250 Loss: 0.1065\n",
      "Epoch: 3 / 3, Step: 1398 / 2250 Loss: 0.0145\n",
      "Epoch: 3 / 3, Step: 1399 / 2250 Loss: 0.0443\n",
      "Epoch: 3 / 3, Step: 1400 / 2250 Loss: 0.0478\n",
      "Epoch: 3 / 3, Step: 1401 / 2250 Loss: 0.0539\n",
      "Epoch: 3 / 3, Step: 1402 / 2250 Loss: 0.0295\n",
      "Epoch: 3 / 3, Step: 1403 / 2250 Loss: 0.0656\n",
      "Epoch: 3 / 3, Step: 1404 / 2250 Loss: 0.0322\n",
      "Epoch: 3 / 3, Step: 1405 / 2250 Loss: 0.0413\n",
      "Epoch: 3 / 3, Step: 1406 / 2250 Loss: 0.0428\n",
      "Epoch: 3 / 3, Step: 1407 / 2250 Loss: 0.0849\n",
      "Epoch: 3 / 3, Step: 1408 / 2250 Loss: 0.1326\n",
      "Epoch: 3 / 3, Step: 1409 / 2250 Loss: 0.1160\n",
      "Epoch: 3 / 3, Step: 1410 / 2250 Loss: 0.0283\n",
      "Epoch: 3 / 3, Step: 1411 / 2250 Loss: 0.0198\n",
      "Epoch: 3 / 3, Step: 1412 / 2250 Loss: 0.1051\n",
      "Epoch: 3 / 3, Step: 1413 / 2250 Loss: 0.0230\n",
      "Epoch: 3 / 3, Step: 1414 / 2250 Loss: 0.0722\n",
      "Epoch: 3 / 3, Step: 1415 / 2250 Loss: 0.1088\n",
      "Epoch: 3 / 3, Step: 1416 / 2250 Loss: 0.1182\n",
      "Epoch: 3 / 3, Step: 1417 / 2250 Loss: 0.2605\n",
      "Epoch: 3 / 3, Step: 1418 / 2250 Loss: 0.0138\n",
      "Epoch: 3 / 3, Step: 1419 / 2250 Loss: 0.0732\n",
      "Epoch: 3 / 3, Step: 1420 / 2250 Loss: 0.0175\n",
      "Epoch: 3 / 3, Step: 1421 / 2250 Loss: 0.0315\n",
      "Epoch: 3 / 3, Step: 1422 / 2250 Loss: 0.0527\n",
      "Epoch: 3 / 3, Step: 1423 / 2250 Loss: 0.0133\n",
      "Epoch: 3 / 3, Step: 1424 / 2250 Loss: 0.0497\n",
      "Epoch: 3 / 3, Step: 1425 / 2250 Loss: 0.1043\n",
      "Epoch: 3 / 3, Step: 1426 / 2250 Loss: 0.0298\n",
      "Epoch: 3 / 3, Step: 1427 / 2250 Loss: 0.0077\n",
      "Epoch: 3 / 3, Step: 1428 / 2250 Loss: 0.0085\n",
      "Epoch: 3 / 3, Step: 1429 / 2250 Loss: 0.0494\n",
      "Epoch: 3 / 3, Step: 1430 / 2250 Loss: 0.0187\n",
      "Epoch: 3 / 3, Step: 1431 / 2250 Loss: 0.0185\n",
      "Epoch: 3 / 3, Step: 1432 / 2250 Loss: 0.1254\n",
      "Epoch: 3 / 3, Step: 1433 / 2250 Loss: 0.1179\n",
      "Epoch: 3 / 3, Step: 1434 / 2250 Loss: 0.0560\n",
      "Epoch: 3 / 3, Step: 1435 / 2250 Loss: 0.0095\n",
      "Epoch: 3 / 3, Step: 1436 / 2250 Loss: 0.1953\n",
      "Epoch: 3 / 3, Step: 1437 / 2250 Loss: 0.0498\n",
      "Epoch: 3 / 3, Step: 1438 / 2250 Loss: 0.1418\n",
      "Epoch: 3 / 3, Step: 1439 / 2250 Loss: 0.0700\n",
      "Epoch: 3 / 3, Step: 1440 / 2250 Loss: 0.1469\n",
      "Epoch: 3 / 3, Step: 1441 / 2250 Loss: 0.0162\n",
      "Epoch: 3 / 3, Step: 1442 / 2250 Loss: 0.1167\n",
      "Epoch: 3 / 3, Step: 1443 / 2250 Loss: 0.0153\n",
      "Epoch: 3 / 3, Step: 1444 / 2250 Loss: 0.4189\n",
      "Epoch: 3 / 3, Step: 1445 / 2250 Loss: 0.1643\n",
      "Epoch: 3 / 3, Step: 1446 / 2250 Loss: 0.0456\n",
      "Epoch: 3 / 3, Step: 1447 / 2250 Loss: 0.3546\n",
      "Epoch: 3 / 3, Step: 1448 / 2250 Loss: 0.1397\n",
      "Epoch: 3 / 3, Step: 1449 / 2250 Loss: 0.1093\n",
      "Epoch: 3 / 3, Step: 1450 / 2250 Loss: 0.0148\n",
      "Epoch: 3 / 3, Step: 1451 / 2250 Loss: 0.1210\n",
      "Epoch: 3 / 3, Step: 1452 / 2250 Loss: 0.0097\n",
      "Epoch: 3 / 3, Step: 1453 / 2250 Loss: 0.0950\n",
      "Epoch: 3 / 3, Step: 1454 / 2250 Loss: 0.0681\n",
      "Epoch: 3 / 3, Step: 1455 / 2250 Loss: 0.0766\n",
      "Epoch: 3 / 3, Step: 1456 / 2250 Loss: 0.1161\n",
      "Epoch: 3 / 3, Step: 1457 / 2250 Loss: 0.0271\n",
      "Epoch: 3 / 3, Step: 1458 / 2250 Loss: 0.1967\n",
      "Epoch: 3 / 3, Step: 1459 / 2250 Loss: 0.0501\n",
      "Epoch: 3 / 3, Step: 1460 / 2250 Loss: 0.0374\n",
      "Epoch: 3 / 3, Step: 1461 / 2250 Loss: 0.0582\n",
      "Epoch: 3 / 3, Step: 1462 / 2250 Loss: 0.0434\n",
      "Epoch: 3 / 3, Step: 1463 / 2250 Loss: 0.0457\n",
      "Epoch: 3 / 3, Step: 1464 / 2250 Loss: 0.0648\n",
      "Epoch: 3 / 3, Step: 1465 / 2250 Loss: 0.1309\n",
      "Epoch: 3 / 3, Step: 1466 / 2250 Loss: 0.1154\n",
      "Epoch: 3 / 3, Step: 1467 / 2250 Loss: 0.0164\n",
      "Epoch: 3 / 3, Step: 1468 / 2250 Loss: 0.1042\n",
      "Epoch: 3 / 3, Step: 1469 / 2250 Loss: 0.0350\n",
      "Epoch: 3 / 3, Step: 1470 / 2250 Loss: 0.0220\n",
      "Epoch: 3 / 3, Step: 1471 / 2250 Loss: 0.0308\n",
      "Epoch: 3 / 3, Step: 1472 / 2250 Loss: 0.0823\n",
      "Epoch: 3 / 3, Step: 1473 / 2250 Loss: 0.1367\n",
      "Epoch: 3 / 3, Step: 1474 / 2250 Loss: 0.0165\n",
      "Epoch: 3 / 3, Step: 1475 / 2250 Loss: 0.1120\n",
      "Epoch: 3 / 3, Step: 1476 / 2250 Loss: 0.0395\n",
      "Epoch: 3 / 3, Step: 1477 / 2250 Loss: 0.0672\n",
      "Epoch: 3 / 3, Step: 1478 / 2250 Loss: 0.0664\n",
      "Epoch: 3 / 3, Step: 1479 / 2250 Loss: 0.0817\n",
      "Epoch: 3 / 3, Step: 1480 / 2250 Loss: 0.1801\n",
      "Epoch: 3 / 3, Step: 1481 / 2250 Loss: 0.0468\n",
      "Epoch: 3 / 3, Step: 1482 / 2250 Loss: 0.0724\n",
      "Epoch: 3 / 3, Step: 1483 / 2250 Loss: 0.3440\n",
      "Epoch: 3 / 3, Step: 1484 / 2250 Loss: 0.0204\n",
      "Epoch: 3 / 3, Step: 1485 / 2250 Loss: 0.0308\n",
      "Epoch: 3 / 3, Step: 1486 / 2250 Loss: 0.0532\n",
      "Epoch: 3 / 3, Step: 1487 / 2250 Loss: 0.2132\n",
      "Epoch: 3 / 3, Step: 1488 / 2250 Loss: 0.1902\n",
      "Epoch: 3 / 3, Step: 1489 / 2250 Loss: 0.0816\n",
      "Epoch: 3 / 3, Step: 1490 / 2250 Loss: 0.1161\n",
      "Epoch: 3 / 3, Step: 1491 / 2250 Loss: 0.0321\n",
      "Epoch: 3 / 3, Step: 1492 / 2250 Loss: 0.0645\n",
      "Epoch: 3 / 3, Step: 1493 / 2250 Loss: 0.1740\n",
      "Epoch: 3 / 3, Step: 1494 / 2250 Loss: 0.0486\n",
      "Epoch: 3 / 3, Step: 1495 / 2250 Loss: 0.0219\n",
      "Epoch: 3 / 3, Step: 1496 / 2250 Loss: 0.1603\n",
      "Epoch: 3 / 3, Step: 1497 / 2250 Loss: 0.0322\n",
      "Epoch: 3 / 3, Step: 1498 / 2250 Loss: 0.0322\n",
      "Epoch: 3 / 3, Step: 1499 / 2250 Loss: 0.0326\n",
      "Epoch: 3 / 3, Step: 1500 / 2250 Loss: 0.0195\n",
      "Epoch: 3 / 3, Step: 1501 / 2250 Loss: 0.0635\n",
      "Epoch: 3 / 3, Step: 1502 / 2250 Loss: 0.1284\n",
      "Epoch: 3 / 3, Step: 1503 / 2250 Loss: 0.2810\n",
      "Epoch: 3 / 3, Step: 1504 / 2250 Loss: 0.1750\n",
      "Epoch: 3 / 3, Step: 1505 / 2250 Loss: 0.0091\n",
      "Epoch: 3 / 3, Step: 1506 / 2250 Loss: 0.0271\n",
      "Epoch: 3 / 3, Step: 1507 / 2250 Loss: 0.0432\n",
      "Epoch: 3 / 3, Step: 1508 / 2250 Loss: 0.0184\n",
      "Epoch: 3 / 3, Step: 1509 / 2250 Loss: 0.0259\n",
      "Epoch: 3 / 3, Step: 1510 / 2250 Loss: 0.2082\n",
      "Epoch: 3 / 3, Step: 1511 / 2250 Loss: 0.1358\n",
      "Epoch: 3 / 3, Step: 1512 / 2250 Loss: 0.2046\n",
      "Epoch: 3 / 3, Step: 1513 / 2250 Loss: 0.2477\n",
      "Epoch: 3 / 3, Step: 1514 / 2250 Loss: 0.0118\n",
      "Epoch: 3 / 3, Step: 1515 / 2250 Loss: 0.0450\n",
      "Epoch: 3 / 3, Step: 1516 / 2250 Loss: 0.0640\n",
      "Epoch: 3 / 3, Step: 1517 / 2250 Loss: 0.0513\n",
      "Epoch: 3 / 3, Step: 1518 / 2250 Loss: 0.0595\n",
      "Epoch: 3 / 3, Step: 1519 / 2250 Loss: 0.0423\n",
      "Epoch: 3 / 3, Step: 1520 / 2250 Loss: 0.0508\n",
      "Epoch: 3 / 3, Step: 1521 / 2250 Loss: 0.0534\n",
      "Epoch: 3 / 3, Step: 1522 / 2250 Loss: 0.0815\n",
      "Epoch: 3 / 3, Step: 1523 / 2250 Loss: 0.0259\n",
      "Epoch: 3 / 3, Step: 1524 / 2250 Loss: 0.0310\n",
      "Epoch: 3 / 3, Step: 1525 / 2250 Loss: 0.1340\n",
      "Epoch: 3 / 3, Step: 1526 / 2250 Loss: 0.0082\n",
      "Epoch: 3 / 3, Step: 1527 / 2250 Loss: 0.0584\n",
      "Epoch: 3 / 3, Step: 1528 / 2250 Loss: 0.0198\n",
      "Epoch: 3 / 3, Step: 1529 / 2250 Loss: 0.1175\n",
      "Epoch: 3 / 3, Step: 1530 / 2250 Loss: 0.3079\n",
      "Epoch: 3 / 3, Step: 1531 / 2250 Loss: 0.0141\n",
      "Epoch: 3 / 3, Step: 1532 / 2250 Loss: 0.0049\n",
      "Epoch: 3 / 3, Step: 1533 / 2250 Loss: 0.0512\n",
      "Epoch: 3 / 3, Step: 1534 / 2250 Loss: 0.1019\n",
      "Epoch: 3 / 3, Step: 1535 / 2250 Loss: 0.0429\n",
      "Epoch: 3 / 3, Step: 1536 / 2250 Loss: 0.0107\n",
      "Epoch: 3 / 3, Step: 1537 / 2250 Loss: 0.1199\n",
      "Epoch: 3 / 3, Step: 1538 / 2250 Loss: 0.0186\n",
      "Epoch: 3 / 3, Step: 1539 / 2250 Loss: 0.1260\n",
      "Epoch: 3 / 3, Step: 1540 / 2250 Loss: 0.0395\n",
      "Epoch: 3 / 3, Step: 1541 / 2250 Loss: 0.0033\n",
      "Epoch: 3 / 3, Step: 1542 / 2250 Loss: 0.1030\n",
      "Epoch: 3 / 3, Step: 1543 / 2250 Loss: 0.1221\n",
      "Epoch: 3 / 3, Step: 1544 / 2250 Loss: 0.1029\n",
      "Epoch: 3 / 3, Step: 1545 / 2250 Loss: 0.0337\n",
      "Epoch: 3 / 3, Step: 1546 / 2250 Loss: 0.2570\n",
      "Epoch: 3 / 3, Step: 1547 / 2250 Loss: 0.1903\n",
      "Epoch: 3 / 3, Step: 1548 / 2250 Loss: 0.0438\n",
      "Epoch: 3 / 3, Step: 1549 / 2250 Loss: 0.0456\n",
      "Epoch: 3 / 3, Step: 1550 / 2250 Loss: 0.0383\n",
      "Epoch: 3 / 3, Step: 1551 / 2250 Loss: 0.0122\n",
      "Epoch: 3 / 3, Step: 1552 / 2250 Loss: 0.0490\n",
      "Epoch: 3 / 3, Step: 1553 / 2250 Loss: 0.0075\n",
      "Epoch: 3 / 3, Step: 1554 / 2250 Loss: 0.1166\n",
      "Epoch: 3 / 3, Step: 1555 / 2250 Loss: 0.2113\n",
      "Epoch: 3 / 3, Step: 1556 / 2250 Loss: 0.0149\n",
      "Epoch: 3 / 3, Step: 1557 / 2250 Loss: 0.0208\n",
      "Epoch: 3 / 3, Step: 1558 / 2250 Loss: 0.0225\n",
      "Epoch: 3 / 3, Step: 1559 / 2250 Loss: 0.1226\n",
      "Epoch: 3 / 3, Step: 1560 / 2250 Loss: 0.0327\n",
      "Epoch: 3 / 3, Step: 1561 / 2250 Loss: 0.1829\n",
      "Epoch: 3 / 3, Step: 1562 / 2250 Loss: 0.0908\n",
      "Epoch: 3 / 3, Step: 1563 / 2250 Loss: 0.0234\n",
      "Epoch: 3 / 3, Step: 1564 / 2250 Loss: 0.1428\n",
      "Epoch: 3 / 3, Step: 1565 / 2250 Loss: 0.2031\n",
      "Epoch: 3 / 3, Step: 1566 / 2250 Loss: 0.2430\n",
      "Epoch: 3 / 3, Step: 1567 / 2250 Loss: 0.0540\n",
      "Epoch: 3 / 3, Step: 1568 / 2250 Loss: 0.0524\n",
      "Epoch: 3 / 3, Step: 1569 / 2250 Loss: 0.0405\n",
      "Epoch: 3 / 3, Step: 1570 / 2250 Loss: 0.0384\n",
      "Epoch: 3 / 3, Step: 1571 / 2250 Loss: 0.0218\n",
      "Epoch: 3 / 3, Step: 1572 / 2250 Loss: 0.0851\n",
      "Epoch: 3 / 3, Step: 1573 / 2250 Loss: 0.1710\n",
      "Epoch: 3 / 3, Step: 1574 / 2250 Loss: 0.0823\n",
      "Epoch: 3 / 3, Step: 1575 / 2250 Loss: 0.0414\n",
      "Epoch: 3 / 3, Step: 1576 / 2250 Loss: 0.0122\n",
      "Epoch: 3 / 3, Step: 1577 / 2250 Loss: 0.0560\n",
      "Epoch: 3 / 3, Step: 1578 / 2250 Loss: 0.0261\n",
      "Epoch: 3 / 3, Step: 1579 / 2250 Loss: 0.0569\n",
      "Epoch: 3 / 3, Step: 1580 / 2250 Loss: 0.0629\n",
      "Epoch: 3 / 3, Step: 1581 / 2250 Loss: 0.0793\n",
      "Epoch: 3 / 3, Step: 1582 / 2250 Loss: 0.1086\n",
      "Epoch: 3 / 3, Step: 1583 / 2250 Loss: 0.1973\n",
      "Epoch: 3 / 3, Step: 1584 / 2250 Loss: 0.0139\n",
      "Epoch: 3 / 3, Step: 1585 / 2250 Loss: 0.2615\n",
      "Epoch: 3 / 3, Step: 1586 / 2250 Loss: 0.0685\n",
      "Epoch: 3 / 3, Step: 1587 / 2250 Loss: 0.0197\n",
      "Epoch: 3 / 3, Step: 1588 / 2250 Loss: 0.0440\n",
      "Epoch: 3 / 3, Step: 1589 / 2250 Loss: 0.0676\n",
      "Epoch: 3 / 3, Step: 1590 / 2250 Loss: 0.2319\n",
      "Epoch: 3 / 3, Step: 1591 / 2250 Loss: 0.0332\n",
      "Epoch: 3 / 3, Step: 1592 / 2250 Loss: 0.2022\n",
      "Epoch: 3 / 3, Step: 1593 / 2250 Loss: 0.0573\n",
      "Epoch: 3 / 3, Step: 1594 / 2250 Loss: 0.1048\n",
      "Epoch: 3 / 3, Step: 1595 / 2250 Loss: 0.0683\n",
      "Epoch: 3 / 3, Step: 1596 / 2250 Loss: 0.0293\n",
      "Epoch: 3 / 3, Step: 1597 / 2250 Loss: 0.1130\n",
      "Epoch: 3 / 3, Step: 1598 / 2250 Loss: 0.1168\n",
      "Epoch: 3 / 3, Step: 1599 / 2250 Loss: 0.0051\n",
      "Epoch: 3 / 3, Step: 1600 / 2250 Loss: 0.0500\n",
      "Epoch: 3 / 3, Step: 1601 / 2250 Loss: 0.0101\n",
      "Epoch: 3 / 3, Step: 1602 / 2250 Loss: 0.0878\n",
      "Epoch: 3 / 3, Step: 1603 / 2250 Loss: 0.0922\n",
      "Epoch: 3 / 3, Step: 1604 / 2250 Loss: 0.0409\n",
      "Epoch: 3 / 3, Step: 1605 / 2250 Loss: 0.0180\n",
      "Epoch: 3 / 3, Step: 1606 / 2250 Loss: 0.1582\n",
      "Epoch: 3 / 3, Step: 1607 / 2250 Loss: 0.0361\n",
      "Epoch: 3 / 3, Step: 1608 / 2250 Loss: 0.4124\n",
      "Epoch: 3 / 3, Step: 1609 / 2250 Loss: 0.0140\n",
      "Epoch: 3 / 3, Step: 1610 / 2250 Loss: 0.0153\n",
      "Epoch: 3 / 3, Step: 1611 / 2250 Loss: 0.0132\n",
      "Epoch: 3 / 3, Step: 1612 / 2250 Loss: 0.0782\n",
      "Epoch: 3 / 3, Step: 1613 / 2250 Loss: 0.0219\n",
      "Epoch: 3 / 3, Step: 1614 / 2250 Loss: 0.0535\n",
      "Epoch: 3 / 3, Step: 1615 / 2250 Loss: 0.0154\n",
      "Epoch: 3 / 3, Step: 1616 / 2250 Loss: 0.1557\n",
      "Epoch: 3 / 3, Step: 1617 / 2250 Loss: 0.0331\n",
      "Epoch: 3 / 3, Step: 1618 / 2250 Loss: 0.0565\n",
      "Epoch: 3 / 3, Step: 1619 / 2250 Loss: 0.1013\n",
      "Epoch: 3 / 3, Step: 1620 / 2250 Loss: 0.0481\n",
      "Epoch: 3 / 3, Step: 1621 / 2250 Loss: 0.0203\n",
      "Epoch: 3 / 3, Step: 1622 / 2250 Loss: 0.0333\n",
      "Epoch: 3 / 3, Step: 1623 / 2250 Loss: 0.0609\n",
      "Epoch: 3 / 3, Step: 1624 / 2250 Loss: 0.0213\n",
      "Epoch: 3 / 3, Step: 1625 / 2250 Loss: 0.0969\n",
      "Epoch: 3 / 3, Step: 1626 / 2250 Loss: 0.2163\n",
      "Epoch: 3 / 3, Step: 1627 / 2250 Loss: 0.0218\n",
      "Epoch: 3 / 3, Step: 1628 / 2250 Loss: 0.0357\n",
      "Epoch: 3 / 3, Step: 1629 / 2250 Loss: 0.0193\n",
      "Epoch: 3 / 3, Step: 1630 / 2250 Loss: 0.2436\n",
      "Epoch: 3 / 3, Step: 1631 / 2250 Loss: 0.0281\n",
      "Epoch: 3 / 3, Step: 1632 / 2250 Loss: 0.0566\n",
      "Epoch: 3 / 3, Step: 1633 / 2250 Loss: 0.0065\n",
      "Epoch: 3 / 3, Step: 1634 / 2250 Loss: 0.1143\n",
      "Epoch: 3 / 3, Step: 1635 / 2250 Loss: 0.0367\n",
      "Epoch: 3 / 3, Step: 1636 / 2250 Loss: 0.1357\n",
      "Epoch: 3 / 3, Step: 1637 / 2250 Loss: 0.0218\n",
      "Epoch: 3 / 3, Step: 1638 / 2250 Loss: 0.0786\n",
      "Epoch: 3 / 3, Step: 1639 / 2250 Loss: 0.0087\n",
      "Epoch: 3 / 3, Step: 1640 / 2250 Loss: 0.0523\n",
      "Epoch: 3 / 3, Step: 1641 / 2250 Loss: 0.0219\n",
      "Epoch: 3 / 3, Step: 1642 / 2250 Loss: 0.0491\n",
      "Epoch: 3 / 3, Step: 1643 / 2250 Loss: 0.0271\n",
      "Epoch: 3 / 3, Step: 1644 / 2250 Loss: 0.0191\n",
      "Epoch: 3 / 3, Step: 1645 / 2250 Loss: 0.1469\n",
      "Epoch: 3 / 3, Step: 1646 / 2250 Loss: 0.0483\n",
      "Epoch: 3 / 3, Step: 1647 / 2250 Loss: 0.0099\n",
      "Epoch: 3 / 3, Step: 1648 / 2250 Loss: 0.0296\n",
      "Epoch: 3 / 3, Step: 1649 / 2250 Loss: 0.3430\n",
      "Epoch: 3 / 3, Step: 1650 / 2250 Loss: 0.0170\n",
      "Epoch: 3 / 3, Step: 1651 / 2250 Loss: 0.0238\n",
      "Epoch: 3 / 3, Step: 1652 / 2250 Loss: 0.0229\n",
      "Epoch: 3 / 3, Step: 1653 / 2250 Loss: 0.0515\n",
      "Epoch: 3 / 3, Step: 1654 / 2250 Loss: 0.0425\n",
      "Epoch: 3 / 3, Step: 1655 / 2250 Loss: 0.0348\n",
      "Epoch: 3 / 3, Step: 1656 / 2250 Loss: 0.1823\n",
      "Epoch: 3 / 3, Step: 1657 / 2250 Loss: 0.0084\n",
      "Epoch: 3 / 3, Step: 1658 / 2250 Loss: 0.2454\n",
      "Epoch: 3 / 3, Step: 1659 / 2250 Loss: 0.0330\n",
      "Epoch: 3 / 3, Step: 1660 / 2250 Loss: 0.0363\n",
      "Epoch: 3 / 3, Step: 1661 / 2250 Loss: 0.0215\n",
      "Epoch: 3 / 3, Step: 1662 / 2250 Loss: 0.1120\n",
      "Epoch: 3 / 3, Step: 1663 / 2250 Loss: 0.0275\n",
      "Epoch: 3 / 3, Step: 1664 / 2250 Loss: 0.0489\n",
      "Epoch: 3 / 3, Step: 1665 / 2250 Loss: 0.0200\n",
      "Epoch: 3 / 3, Step: 1666 / 2250 Loss: 0.0303\n",
      "Epoch: 3 / 3, Step: 1667 / 2250 Loss: 0.0175\n",
      "Epoch: 3 / 3, Step: 1668 / 2250 Loss: 0.0130\n",
      "Epoch: 3 / 3, Step: 1669 / 2250 Loss: 0.2045\n",
      "Epoch: 3 / 3, Step: 1670 / 2250 Loss: 0.0205\n",
      "Epoch: 3 / 3, Step: 1671 / 2250 Loss: 0.1289\n",
      "Epoch: 3 / 3, Step: 1672 / 2250 Loss: 0.0232\n",
      "Epoch: 3 / 3, Step: 1673 / 2250 Loss: 0.0879\n",
      "Epoch: 3 / 3, Step: 1674 / 2250 Loss: 0.0093\n",
      "Epoch: 3 / 3, Step: 1675 / 2250 Loss: 0.0293\n",
      "Epoch: 3 / 3, Step: 1676 / 2250 Loss: 0.0129\n",
      "Epoch: 3 / 3, Step: 1677 / 2250 Loss: 0.0817\n",
      "Epoch: 3 / 3, Step: 1678 / 2250 Loss: 0.1629\n",
      "Epoch: 3 / 3, Step: 1679 / 2250 Loss: 0.1914\n",
      "Epoch: 3 / 3, Step: 1680 / 2250 Loss: 0.0337\n",
      "Epoch: 3 / 3, Step: 1681 / 2250 Loss: 0.1863\n",
      "Epoch: 3 / 3, Step: 1682 / 2250 Loss: 0.0048\n",
      "Epoch: 3 / 3, Step: 1683 / 2250 Loss: 0.0072\n",
      "Epoch: 3 / 3, Step: 1684 / 2250 Loss: 0.0163\n",
      "Epoch: 3 / 3, Step: 1685 / 2250 Loss: 0.2969\n",
      "Epoch: 3 / 3, Step: 1686 / 2250 Loss: 0.1070\n",
      "Epoch: 3 / 3, Step: 1687 / 2250 Loss: 0.2028\n",
      "Epoch: 3 / 3, Step: 1688 / 2250 Loss: 0.0184\n",
      "Epoch: 3 / 3, Step: 1689 / 2250 Loss: 0.1886\n",
      "Epoch: 3 / 3, Step: 1690 / 2250 Loss: 0.0374\n",
      "Epoch: 3 / 3, Step: 1691 / 2250 Loss: 0.0268\n",
      "Epoch: 3 / 3, Step: 1692 / 2250 Loss: 0.0431\n",
      "Epoch: 3 / 3, Step: 1693 / 2250 Loss: 0.0379\n",
      "Epoch: 3 / 3, Step: 1694 / 2250 Loss: 0.2456\n",
      "Epoch: 3 / 3, Step: 1695 / 2250 Loss: 0.0336\n",
      "Epoch: 3 / 3, Step: 1696 / 2250 Loss: 0.1414\n",
      "Epoch: 3 / 3, Step: 1697 / 2250 Loss: 0.1040\n",
      "Epoch: 3 / 3, Step: 1698 / 2250 Loss: 0.1109\n",
      "Epoch: 3 / 3, Step: 1699 / 2250 Loss: 0.0264\n",
      "Epoch: 3 / 3, Step: 1700 / 2250 Loss: 0.1939\n",
      "Epoch: 3 / 3, Step: 1701 / 2250 Loss: 0.2099\n",
      "Epoch: 3 / 3, Step: 1702 / 2250 Loss: 0.0215\n",
      "Epoch: 3 / 3, Step: 1703 / 2250 Loss: 0.0800\n",
      "Epoch: 3 / 3, Step: 1704 / 2250 Loss: 0.0469\n",
      "Epoch: 3 / 3, Step: 1705 / 2250 Loss: 0.0110\n",
      "Epoch: 3 / 3, Step: 1706 / 2250 Loss: 0.2215\n",
      "Epoch: 3 / 3, Step: 1707 / 2250 Loss: 0.0539\n",
      "Epoch: 3 / 3, Step: 1708 / 2250 Loss: 0.2473\n",
      "Epoch: 3 / 3, Step: 1709 / 2250 Loss: 0.0496\n",
      "Epoch: 3 / 3, Step: 1710 / 2250 Loss: 0.0650\n",
      "Epoch: 3 / 3, Step: 1711 / 2250 Loss: 0.0300\n",
      "Epoch: 3 / 3, Step: 1712 / 2250 Loss: 0.2507\n",
      "Epoch: 3 / 3, Step: 1713 / 2250 Loss: 0.0461\n",
      "Epoch: 3 / 3, Step: 1714 / 2250 Loss: 0.0995\n",
      "Epoch: 3 / 3, Step: 1715 / 2250 Loss: 0.0253\n",
      "Epoch: 3 / 3, Step: 1716 / 2250 Loss: 0.0551\n",
      "Epoch: 3 / 3, Step: 1717 / 2250 Loss: 0.0926\n",
      "Epoch: 3 / 3, Step: 1718 / 2250 Loss: 0.0395\n",
      "Epoch: 3 / 3, Step: 1719 / 2250 Loss: 0.0371\n",
      "Epoch: 3 / 3, Step: 1720 / 2250 Loss: 0.0406\n",
      "Epoch: 3 / 3, Step: 1721 / 2250 Loss: 0.1318\n",
      "Epoch: 3 / 3, Step: 1722 / 2250 Loss: 0.0121\n",
      "Epoch: 3 / 3, Step: 1723 / 2250 Loss: 0.2607\n",
      "Epoch: 3 / 3, Step: 1724 / 2250 Loss: 0.1833\n",
      "Epoch: 3 / 3, Step: 1725 / 2250 Loss: 0.0638\n",
      "Epoch: 3 / 3, Step: 1726 / 2250 Loss: 0.0065\n",
      "Epoch: 3 / 3, Step: 1727 / 2250 Loss: 0.1842\n",
      "Epoch: 3 / 3, Step: 1728 / 2250 Loss: 0.0671\n",
      "Epoch: 3 / 3, Step: 1729 / 2250 Loss: 0.0072\n",
      "Epoch: 3 / 3, Step: 1730 / 2250 Loss: 0.1056\n",
      "Epoch: 3 / 3, Step: 1731 / 2250 Loss: 0.0379\n",
      "Epoch: 3 / 3, Step: 1732 / 2250 Loss: 0.0840\n",
      "Epoch: 3 / 3, Step: 1733 / 2250 Loss: 0.1739\n",
      "Epoch: 3 / 3, Step: 1734 / 2250 Loss: 0.0184\n",
      "Epoch: 3 / 3, Step: 1735 / 2250 Loss: 0.0284\n",
      "Epoch: 3 / 3, Step: 1736 / 2250 Loss: 0.0108\n",
      "Epoch: 3 / 3, Step: 1737 / 2250 Loss: 0.1848\n",
      "Epoch: 3 / 3, Step: 1738 / 2250 Loss: 0.0645\n",
      "Epoch: 3 / 3, Step: 1739 / 2250 Loss: 0.1886\n",
      "Epoch: 3 / 3, Step: 1740 / 2250 Loss: 0.2117\n",
      "Epoch: 3 / 3, Step: 1741 / 2250 Loss: 0.0179\n",
      "Epoch: 3 / 3, Step: 1742 / 2250 Loss: 0.0461\n",
      "Epoch: 3 / 3, Step: 1743 / 2250 Loss: 0.1389\n",
      "Epoch: 3 / 3, Step: 1744 / 2250 Loss: 0.0421\n",
      "Epoch: 3 / 3, Step: 1745 / 2250 Loss: 0.2871\n",
      "Epoch: 3 / 3, Step: 1746 / 2250 Loss: 0.1062\n",
      "Epoch: 3 / 3, Step: 1747 / 2250 Loss: 0.0951\n",
      "Epoch: 3 / 3, Step: 1748 / 2250 Loss: 0.0986\n",
      "Epoch: 3 / 3, Step: 1749 / 2250 Loss: 0.0204\n",
      "Epoch: 3 / 3, Step: 1750 / 2250 Loss: 0.0346\n",
      "Epoch: 3 / 3, Step: 1751 / 2250 Loss: 0.0302\n",
      "Epoch: 3 / 3, Step: 1752 / 2250 Loss: 0.0115\n",
      "Epoch: 3 / 3, Step: 1753 / 2250 Loss: 0.0567\n",
      "Epoch: 3 / 3, Step: 1754 / 2250 Loss: 0.0347\n",
      "Epoch: 3 / 3, Step: 1755 / 2250 Loss: 0.2293\n",
      "Epoch: 3 / 3, Step: 1756 / 2250 Loss: 0.0248\n",
      "Epoch: 3 / 3, Step: 1757 / 2250 Loss: 0.0362\n",
      "Epoch: 3 / 3, Step: 1758 / 2250 Loss: 0.0787\n",
      "Epoch: 3 / 3, Step: 1759 / 2250 Loss: 0.1173\n",
      "Epoch: 3 / 3, Step: 1760 / 2250 Loss: 0.0943\n",
      "Epoch: 3 / 3, Step: 1761 / 2250 Loss: 0.2611\n",
      "Epoch: 3 / 3, Step: 1762 / 2250 Loss: 0.0676\n",
      "Epoch: 3 / 3, Step: 1763 / 2250 Loss: 0.0150\n",
      "Epoch: 3 / 3, Step: 1764 / 2250 Loss: 0.0607\n",
      "Epoch: 3 / 3, Step: 1765 / 2250 Loss: 0.0732\n",
      "Epoch: 3 / 3, Step: 1766 / 2250 Loss: 0.1045\n",
      "Epoch: 3 / 3, Step: 1767 / 2250 Loss: 0.0084\n",
      "Epoch: 3 / 3, Step: 1768 / 2250 Loss: 0.0199\n",
      "Epoch: 3 / 3, Step: 1769 / 2250 Loss: 0.2123\n",
      "Epoch: 3 / 3, Step: 1770 / 2250 Loss: 0.0559\n",
      "Epoch: 3 / 3, Step: 1771 / 2250 Loss: 0.0306\n",
      "Epoch: 3 / 3, Step: 1772 / 2250 Loss: 0.0260\n",
      "Epoch: 3 / 3, Step: 1773 / 2250 Loss: 0.0140\n",
      "Epoch: 3 / 3, Step: 1774 / 2250 Loss: 0.0127\n",
      "Epoch: 3 / 3, Step: 1775 / 2250 Loss: 0.0111\n",
      "Epoch: 3 / 3, Step: 1776 / 2250 Loss: 0.0508\n",
      "Epoch: 3 / 3, Step: 1777 / 2250 Loss: 0.0344\n",
      "Epoch: 3 / 3, Step: 1778 / 2250 Loss: 0.0184\n",
      "Epoch: 3 / 3, Step: 1779 / 2250 Loss: 0.2211\n",
      "Epoch: 3 / 3, Step: 1780 / 2250 Loss: 0.0307\n",
      "Epoch: 3 / 3, Step: 1781 / 2250 Loss: 0.0725\n",
      "Epoch: 3 / 3, Step: 1782 / 2250 Loss: 0.3449\n",
      "Epoch: 3 / 3, Step: 1783 / 2250 Loss: 0.1029\n",
      "Epoch: 3 / 3, Step: 1784 / 2250 Loss: 0.2256\n",
      "Epoch: 3 / 3, Step: 1785 / 2250 Loss: 0.1149\n",
      "Epoch: 3 / 3, Step: 1786 / 2250 Loss: 0.0667\n",
      "Epoch: 3 / 3, Step: 1787 / 2250 Loss: 0.0209\n",
      "Epoch: 3 / 3, Step: 1788 / 2250 Loss: 0.1759\n",
      "Epoch: 3 / 3, Step: 1789 / 2250 Loss: 0.0949\n",
      "Epoch: 3 / 3, Step: 1790 / 2250 Loss: 0.0265\n",
      "Epoch: 3 / 3, Step: 1791 / 2250 Loss: 0.0734\n",
      "Epoch: 3 / 3, Step: 1792 / 2250 Loss: 0.1075\n",
      "Epoch: 3 / 3, Step: 1793 / 2250 Loss: 0.0789\n",
      "Epoch: 3 / 3, Step: 1794 / 2250 Loss: 0.0417\n",
      "Epoch: 3 / 3, Step: 1795 / 2250 Loss: 0.0065\n",
      "Epoch: 3 / 3, Step: 1796 / 2250 Loss: 0.0604\n",
      "Epoch: 3 / 3, Step: 1797 / 2250 Loss: 0.0396\n",
      "Epoch: 3 / 3, Step: 1798 / 2250 Loss: 0.0516\n",
      "Epoch: 3 / 3, Step: 1799 / 2250 Loss: 0.0520\n",
      "Epoch: 3 / 3, Step: 1800 / 2250 Loss: 0.0172\n",
      "Epoch: 3 / 3, Step: 1801 / 2250 Loss: 0.0431\n",
      "Epoch: 3 / 3, Step: 1802 / 2250 Loss: 0.0421\n",
      "Epoch: 3 / 3, Step: 1803 / 2250 Loss: 0.1534\n",
      "Epoch: 3 / 3, Step: 1804 / 2250 Loss: 0.0469\n",
      "Epoch: 3 / 3, Step: 1805 / 2250 Loss: 0.0525\n",
      "Epoch: 3 / 3, Step: 1806 / 2250 Loss: 0.0256\n",
      "Epoch: 3 / 3, Step: 1807 / 2250 Loss: 0.0164\n",
      "Epoch: 3 / 3, Step: 1808 / 2250 Loss: 0.0572\n",
      "Epoch: 3 / 3, Step: 1809 / 2250 Loss: 0.0720\n",
      "Epoch: 3 / 3, Step: 1810 / 2250 Loss: 0.0247\n",
      "Epoch: 3 / 3, Step: 1811 / 2250 Loss: 0.0055\n",
      "Epoch: 3 / 3, Step: 1812 / 2250 Loss: 0.0536\n",
      "Epoch: 3 / 3, Step: 1813 / 2250 Loss: 0.0604\n",
      "Epoch: 3 / 3, Step: 1814 / 2250 Loss: 0.1412\n",
      "Epoch: 3 / 3, Step: 1815 / 2250 Loss: 0.0069\n",
      "Epoch: 3 / 3, Step: 1816 / 2250 Loss: 0.0650\n",
      "Epoch: 3 / 3, Step: 1817 / 2250 Loss: 0.3129\n",
      "Epoch: 3 / 3, Step: 1818 / 2250 Loss: 0.1910\n",
      "Epoch: 3 / 3, Step: 1819 / 2250 Loss: 0.2800\n",
      "Epoch: 3 / 3, Step: 1820 / 2250 Loss: 0.1102\n",
      "Epoch: 3 / 3, Step: 1821 / 2250 Loss: 0.0736\n",
      "Epoch: 3 / 3, Step: 1822 / 2250 Loss: 0.0499\n",
      "Epoch: 3 / 3, Step: 1823 / 2250 Loss: 0.0440\n",
      "Epoch: 3 / 3, Step: 1824 / 2250 Loss: 0.0712\n",
      "Epoch: 3 / 3, Step: 1825 / 2250 Loss: 0.0192\n",
      "Epoch: 3 / 3, Step: 1826 / 2250 Loss: 0.0446\n",
      "Epoch: 3 / 3, Step: 1827 / 2250 Loss: 0.0095\n",
      "Epoch: 3 / 3, Step: 1828 / 2250 Loss: 0.1566\n",
      "Epoch: 3 / 3, Step: 1829 / 2250 Loss: 0.0716\n",
      "Epoch: 3 / 3, Step: 1830 / 2250 Loss: 0.2119\n",
      "Epoch: 3 / 3, Step: 1831 / 2250 Loss: 0.0879\n",
      "Epoch: 3 / 3, Step: 1832 / 2250 Loss: 0.2600\n",
      "Epoch: 3 / 3, Step: 1833 / 2250 Loss: 0.2024\n",
      "Epoch: 3 / 3, Step: 1834 / 2250 Loss: 0.2571\n",
      "Epoch: 3 / 3, Step: 1835 / 2250 Loss: 0.1044\n",
      "Epoch: 3 / 3, Step: 1836 / 2250 Loss: 0.0140\n",
      "Epoch: 3 / 3, Step: 1837 / 2250 Loss: 0.0683\n",
      "Epoch: 3 / 3, Step: 1838 / 2250 Loss: 0.2365\n",
      "Epoch: 3 / 3, Step: 1839 / 2250 Loss: 0.0170\n",
      "Epoch: 3 / 3, Step: 1840 / 2250 Loss: 0.0329\n",
      "Epoch: 3 / 3, Step: 1841 / 2250 Loss: 0.1782\n",
      "Epoch: 3 / 3, Step: 1842 / 2250 Loss: 0.0699\n",
      "Epoch: 3 / 3, Step: 1843 / 2250 Loss: 0.0312\n",
      "Epoch: 3 / 3, Step: 1844 / 2250 Loss: 0.0797\n",
      "Epoch: 3 / 3, Step: 1845 / 2250 Loss: 0.1356\n",
      "Epoch: 3 / 3, Step: 1846 / 2250 Loss: 0.0804\n",
      "Epoch: 3 / 3, Step: 1847 / 2250 Loss: 0.0210\n",
      "Epoch: 3 / 3, Step: 1848 / 2250 Loss: 0.0126\n",
      "Epoch: 3 / 3, Step: 1849 / 2250 Loss: 0.0247\n",
      "Epoch: 3 / 3, Step: 1850 / 2250 Loss: 0.2371\n",
      "Epoch: 3 / 3, Step: 1851 / 2250 Loss: 0.2884\n",
      "Epoch: 3 / 3, Step: 1852 / 2250 Loss: 0.0231\n",
      "Epoch: 3 / 3, Step: 1853 / 2250 Loss: 0.0200\n",
      "Epoch: 3 / 3, Step: 1854 / 2250 Loss: 0.0138\n",
      "Epoch: 3 / 3, Step: 1855 / 2250 Loss: 0.0292\n",
      "Epoch: 3 / 3, Step: 1856 / 2250 Loss: 0.0268\n",
      "Epoch: 3 / 3, Step: 1857 / 2250 Loss: 0.0404\n",
      "Epoch: 3 / 3, Step: 1858 / 2250 Loss: 0.0493\n",
      "Epoch: 3 / 3, Step: 1859 / 2250 Loss: 0.0756\n",
      "Epoch: 3 / 3, Step: 1860 / 2250 Loss: 0.1897\n",
      "Epoch: 3 / 3, Step: 1861 / 2250 Loss: 0.2018\n",
      "Epoch: 3 / 3, Step: 1862 / 2250 Loss: 0.0798\n",
      "Epoch: 3 / 3, Step: 1863 / 2250 Loss: 0.0213\n",
      "Epoch: 3 / 3, Step: 1864 / 2250 Loss: 0.0434\n",
      "Epoch: 3 / 3, Step: 1865 / 2250 Loss: 0.1832\n",
      "Epoch: 3 / 3, Step: 1866 / 2250 Loss: 0.0203\n",
      "Epoch: 3 / 3, Step: 1867 / 2250 Loss: 0.1711\n",
      "Epoch: 3 / 3, Step: 1868 / 2250 Loss: 0.4343\n",
      "Epoch: 3 / 3, Step: 1869 / 2250 Loss: 0.0098\n",
      "Epoch: 3 / 3, Step: 1870 / 2250 Loss: 0.0293\n",
      "Epoch: 3 / 3, Step: 1871 / 2250 Loss: 0.0546\n",
      "Epoch: 3 / 3, Step: 1872 / 2250 Loss: 0.1721\n",
      "Epoch: 3 / 3, Step: 1873 / 2250 Loss: 0.1782\n",
      "Epoch: 3 / 3, Step: 1874 / 2250 Loss: 0.0411\n",
      "Epoch: 3 / 3, Step: 1875 / 2250 Loss: 0.1836\n",
      "Epoch: 3 / 3, Step: 1876 / 2250 Loss: 0.0301\n",
      "Epoch: 3 / 3, Step: 1877 / 2250 Loss: 0.2563\n",
      "Epoch: 3 / 3, Step: 1878 / 2250 Loss: 0.0436\n",
      "Epoch: 3 / 3, Step: 1879 / 2250 Loss: 0.0231\n",
      "Epoch: 3 / 3, Step: 1880 / 2250 Loss: 0.1673\n",
      "Epoch: 3 / 3, Step: 1881 / 2250 Loss: 0.1015\n",
      "Epoch: 3 / 3, Step: 1882 / 2250 Loss: 0.0992\n",
      "Epoch: 3 / 3, Step: 1883 / 2250 Loss: 0.0677\n",
      "Epoch: 3 / 3, Step: 1884 / 2250 Loss: 0.1818\n",
      "Epoch: 3 / 3, Step: 1885 / 2250 Loss: 0.2338\n",
      "Epoch: 3 / 3, Step: 1886 / 2250 Loss: 0.4011\n",
      "Epoch: 3 / 3, Step: 1887 / 2250 Loss: 0.1745\n",
      "Epoch: 3 / 3, Step: 1888 / 2250 Loss: 0.0373\n",
      "Epoch: 3 / 3, Step: 1889 / 2250 Loss: 0.0547\n",
      "Epoch: 3 / 3, Step: 1890 / 2250 Loss: 0.1602\n",
      "Epoch: 3 / 3, Step: 1891 / 2250 Loss: 0.1376\n",
      "Epoch: 3 / 3, Step: 1892 / 2250 Loss: 0.0194\n",
      "Epoch: 3 / 3, Step: 1893 / 2250 Loss: 0.0583\n",
      "Epoch: 3 / 3, Step: 1894 / 2250 Loss: 0.0493\n",
      "Epoch: 3 / 3, Step: 1895 / 2250 Loss: 0.0642\n",
      "Epoch: 3 / 3, Step: 1896 / 2250 Loss: 0.1001\n",
      "Epoch: 3 / 3, Step: 1897 / 2250 Loss: 0.0602\n",
      "Epoch: 3 / 3, Step: 1898 / 2250 Loss: 0.0370\n",
      "Epoch: 3 / 3, Step: 1899 / 2250 Loss: 0.0297\n",
      "Epoch: 3 / 3, Step: 1900 / 2250 Loss: 0.1421\n",
      "Epoch: 3 / 3, Step: 1901 / 2250 Loss: 0.0546\n",
      "Epoch: 3 / 3, Step: 1902 / 2250 Loss: 0.1909\n",
      "Epoch: 3 / 3, Step: 1903 / 2250 Loss: 0.0677\n",
      "Epoch: 3 / 3, Step: 1904 / 2250 Loss: 0.0137\n",
      "Epoch: 3 / 3, Step: 1905 / 2250 Loss: 0.0692\n",
      "Epoch: 3 / 3, Step: 1906 / 2250 Loss: 0.0461\n",
      "Epoch: 3 / 3, Step: 1907 / 2250 Loss: 0.0287\n",
      "Epoch: 3 / 3, Step: 1908 / 2250 Loss: 0.0258\n",
      "Epoch: 3 / 3, Step: 1909 / 2250 Loss: 0.0107\n",
      "Epoch: 3 / 3, Step: 1910 / 2250 Loss: 0.1706\n",
      "Epoch: 3 / 3, Step: 1911 / 2250 Loss: 0.0761\n",
      "Epoch: 3 / 3, Step: 1912 / 2250 Loss: 0.0223\n",
      "Epoch: 3 / 3, Step: 1913 / 2250 Loss: 0.2280\n",
      "Epoch: 3 / 3, Step: 1914 / 2250 Loss: 0.2961\n",
      "Epoch: 3 / 3, Step: 1915 / 2250 Loss: 0.0066\n",
      "Epoch: 3 / 3, Step: 1916 / 2250 Loss: 0.0668\n",
      "Epoch: 3 / 3, Step: 1917 / 2250 Loss: 0.2128\n",
      "Epoch: 3 / 3, Step: 1918 / 2250 Loss: 0.0494\n",
      "Epoch: 3 / 3, Step: 1919 / 2250 Loss: 0.1177\n",
      "Epoch: 3 / 3, Step: 1920 / 2250 Loss: 0.0680\n",
      "Epoch: 3 / 3, Step: 1921 / 2250 Loss: 0.0088\n",
      "Epoch: 3 / 3, Step: 1922 / 2250 Loss: 0.1720\n",
      "Epoch: 3 / 3, Step: 1923 / 2250 Loss: 0.0711\n",
      "Epoch: 3 / 3, Step: 1924 / 2250 Loss: 0.2925\n",
      "Epoch: 3 / 3, Step: 1925 / 2250 Loss: 0.0432\n",
      "Epoch: 3 / 3, Step: 1926 / 2250 Loss: 0.0805\n",
      "Epoch: 3 / 3, Step: 1927 / 2250 Loss: 0.1758\n",
      "Epoch: 3 / 3, Step: 1928 / 2250 Loss: 0.0630\n",
      "Epoch: 3 / 3, Step: 1929 / 2250 Loss: 0.1179\n",
      "Epoch: 3 / 3, Step: 1930 / 2250 Loss: 0.0103\n",
      "Epoch: 3 / 3, Step: 1931 / 2250 Loss: 0.0570\n",
      "Epoch: 3 / 3, Step: 1932 / 2250 Loss: 0.0574\n",
      "Epoch: 3 / 3, Step: 1933 / 2250 Loss: 0.3245\n",
      "Epoch: 3 / 3, Step: 1934 / 2250 Loss: 0.0688\n",
      "Epoch: 3 / 3, Step: 1935 / 2250 Loss: 0.2013\n",
      "Epoch: 3 / 3, Step: 1936 / 2250 Loss: 0.1331\n",
      "Epoch: 3 / 3, Step: 1937 / 2250 Loss: 0.0357\n",
      "Epoch: 3 / 3, Step: 1938 / 2250 Loss: 0.0496\n",
      "Epoch: 3 / 3, Step: 1939 / 2250 Loss: 0.2756\n",
      "Epoch: 3 / 3, Step: 1940 / 2250 Loss: 0.0749\n",
      "Epoch: 3 / 3, Step: 1941 / 2250 Loss: 0.2377\n",
      "Epoch: 3 / 3, Step: 1942 / 2250 Loss: 0.1290\n",
      "Epoch: 3 / 3, Step: 1943 / 2250 Loss: 0.0327\n",
      "Epoch: 3 / 3, Step: 1944 / 2250 Loss: 0.1409\n",
      "Epoch: 3 / 3, Step: 1945 / 2250 Loss: 0.0064\n",
      "Epoch: 3 / 3, Step: 1946 / 2250 Loss: 0.0378\n",
      "Epoch: 3 / 3, Step: 1947 / 2250 Loss: 0.1029\n",
      "Epoch: 3 / 3, Step: 1948 / 2250 Loss: 0.1622\n",
      "Epoch: 3 / 3, Step: 1949 / 2250 Loss: 0.0433\n",
      "Epoch: 3 / 3, Step: 1950 / 2250 Loss: 0.0725\n",
      "Epoch: 3 / 3, Step: 1951 / 2250 Loss: 0.2383\n",
      "Epoch: 3 / 3, Step: 1952 / 2250 Loss: 0.0616\n",
      "Epoch: 3 / 3, Step: 1953 / 2250 Loss: 0.0987\n",
      "Epoch: 3 / 3, Step: 1954 / 2250 Loss: 0.0140\n",
      "Epoch: 3 / 3, Step: 1955 / 2250 Loss: 0.2253\n",
      "Epoch: 3 / 3, Step: 1956 / 2250 Loss: 0.0121\n",
      "Epoch: 3 / 3, Step: 1957 / 2250 Loss: 0.2664\n",
      "Epoch: 3 / 3, Step: 1958 / 2250 Loss: 0.0121\n",
      "Epoch: 3 / 3, Step: 1959 / 2250 Loss: 0.0437\n",
      "Epoch: 3 / 3, Step: 1960 / 2250 Loss: 0.0638\n",
      "Epoch: 3 / 3, Step: 1961 / 2250 Loss: 0.0284\n",
      "Epoch: 3 / 3, Step: 1962 / 2250 Loss: 0.0532\n",
      "Epoch: 3 / 3, Step: 1963 / 2250 Loss: 0.0057\n",
      "Epoch: 3 / 3, Step: 1964 / 2250 Loss: 0.0255\n",
      "Epoch: 3 / 3, Step: 1965 / 2250 Loss: 0.0425\n",
      "Epoch: 3 / 3, Step: 1966 / 2250 Loss: 0.0455\n",
      "Epoch: 3 / 3, Step: 1967 / 2250 Loss: 0.1193\n",
      "Epoch: 3 / 3, Step: 1968 / 2250 Loss: 0.1054\n",
      "Epoch: 3 / 3, Step: 1969 / 2250 Loss: 0.0527\n",
      "Epoch: 3 / 3, Step: 1970 / 2250 Loss: 0.0499\n",
      "Epoch: 3 / 3, Step: 1971 / 2250 Loss: 0.0632\n",
      "Epoch: 3 / 3, Step: 1972 / 2250 Loss: 0.0403\n",
      "Epoch: 3 / 3, Step: 1973 / 2250 Loss: 0.0951\n",
      "Epoch: 3 / 3, Step: 1974 / 2250 Loss: 0.0385\n",
      "Epoch: 3 / 3, Step: 1975 / 2250 Loss: 0.0283\n",
      "Epoch: 3 / 3, Step: 1976 / 2250 Loss: 0.0487\n",
      "Epoch: 3 / 3, Step: 1977 / 2250 Loss: 0.0116\n",
      "Epoch: 3 / 3, Step: 1978 / 2250 Loss: 0.0064\n",
      "Epoch: 3 / 3, Step: 1979 / 2250 Loss: 0.0700\n",
      "Epoch: 3 / 3, Step: 1980 / 2250 Loss: 0.0920\n",
      "Epoch: 3 / 3, Step: 1981 / 2250 Loss: 0.1279\n",
      "Epoch: 3 / 3, Step: 1982 / 2250 Loss: 0.0837\n",
      "Epoch: 3 / 3, Step: 1983 / 2250 Loss: 0.0194\n",
      "Epoch: 3 / 3, Step: 1984 / 2250 Loss: 0.3909\n",
      "Epoch: 3 / 3, Step: 1985 / 2250 Loss: 0.3005\n",
      "Epoch: 3 / 3, Step: 1986 / 2250 Loss: 0.0763\n",
      "Epoch: 3 / 3, Step: 1987 / 2250 Loss: 0.0099\n",
      "Epoch: 3 / 3, Step: 1988 / 2250 Loss: 0.0882\n",
      "Epoch: 3 / 3, Step: 1989 / 2250 Loss: 0.0137\n",
      "Epoch: 3 / 3, Step: 1990 / 2250 Loss: 0.0128\n",
      "Epoch: 3 / 3, Step: 1991 / 2250 Loss: 0.0116\n",
      "Epoch: 3 / 3, Step: 1992 / 2250 Loss: 0.0803\n",
      "Epoch: 3 / 3, Step: 1993 / 2250 Loss: 0.0477\n",
      "Epoch: 3 / 3, Step: 1994 / 2250 Loss: 0.1678\n",
      "Epoch: 3 / 3, Step: 1995 / 2250 Loss: 0.0171\n",
      "Epoch: 3 / 3, Step: 1996 / 2250 Loss: 0.0489\n",
      "Epoch: 3 / 3, Step: 1997 / 2250 Loss: 0.2624\n",
      "Epoch: 3 / 3, Step: 1998 / 2250 Loss: 0.1142\n",
      "Epoch: 3 / 3, Step: 1999 / 2250 Loss: 0.0236\n",
      "Epoch: 3 / 3, Step: 2000 / 2250 Loss: 0.0464\n",
      "Epoch: 3 / 3, Step: 2001 / 2250 Loss: 0.1169\n",
      "Epoch: 3 / 3, Step: 2002 / 2250 Loss: 0.0117\n",
      "Epoch: 3 / 3, Step: 2003 / 2250 Loss: 0.0114\n",
      "Epoch: 3 / 3, Step: 2004 / 2250 Loss: 0.0094\n",
      "Epoch: 3 / 3, Step: 2005 / 2250 Loss: 0.0208\n",
      "Epoch: 3 / 3, Step: 2006 / 2250 Loss: 0.0248\n",
      "Epoch: 3 / 3, Step: 2007 / 2250 Loss: 0.0217\n",
      "Epoch: 3 / 3, Step: 2008 / 2250 Loss: 0.2353\n",
      "Epoch: 3 / 3, Step: 2009 / 2250 Loss: 0.0107\n",
      "Epoch: 3 / 3, Step: 2010 / 2250 Loss: 0.0238\n",
      "Epoch: 3 / 3, Step: 2011 / 2250 Loss: 0.0072\n",
      "Epoch: 3 / 3, Step: 2012 / 2250 Loss: 0.0148\n",
      "Epoch: 3 / 3, Step: 2013 / 2250 Loss: 0.0804\n",
      "Epoch: 3 / 3, Step: 2014 / 2250 Loss: 0.1455\n",
      "Epoch: 3 / 3, Step: 2015 / 2250 Loss: 0.0086\n",
      "Epoch: 3 / 3, Step: 2016 / 2250 Loss: 0.0299\n",
      "Epoch: 3 / 3, Step: 2017 / 2250 Loss: 0.0226\n",
      "Epoch: 3 / 3, Step: 2018 / 2250 Loss: 0.2828\n",
      "Epoch: 3 / 3, Step: 2019 / 2250 Loss: 0.0347\n",
      "Epoch: 3 / 3, Step: 2020 / 2250 Loss: 0.4661\n",
      "Epoch: 3 / 3, Step: 2021 / 2250 Loss: 0.0087\n",
      "Epoch: 3 / 3, Step: 2022 / 2250 Loss: 0.0105\n",
      "Epoch: 3 / 3, Step: 2023 / 2250 Loss: 0.0777\n",
      "Epoch: 3 / 3, Step: 2024 / 2250 Loss: 0.1063\n",
      "Epoch: 3 / 3, Step: 2025 / 2250 Loss: 0.2089\n",
      "Epoch: 3 / 3, Step: 2026 / 2250 Loss: 0.2724\n",
      "Epoch: 3 / 3, Step: 2027 / 2250 Loss: 0.0240\n",
      "Epoch: 3 / 3, Step: 2028 / 2250 Loss: 0.0229\n",
      "Epoch: 3 / 3, Step: 2029 / 2250 Loss: 0.0224\n",
      "Epoch: 3 / 3, Step: 2030 / 2250 Loss: 0.0516\n",
      "Epoch: 3 / 3, Step: 2031 / 2250 Loss: 0.1070\n",
      "Epoch: 3 / 3, Step: 2032 / 2250 Loss: 0.2085\n",
      "Epoch: 3 / 3, Step: 2033 / 2250 Loss: 0.0119\n",
      "Epoch: 3 / 3, Step: 2034 / 2250 Loss: 0.0163\n",
      "Epoch: 3 / 3, Step: 2035 / 2250 Loss: 0.1594\n",
      "Epoch: 3 / 3, Step: 2036 / 2250 Loss: 0.0844\n",
      "Epoch: 3 / 3, Step: 2037 / 2250 Loss: 0.0386\n",
      "Epoch: 3 / 3, Step: 2038 / 2250 Loss: 0.0427\n",
      "Epoch: 3 / 3, Step: 2039 / 2250 Loss: 0.0191\n",
      "Epoch: 3 / 3, Step: 2040 / 2250 Loss: 0.0839\n",
      "Epoch: 3 / 3, Step: 2041 / 2250 Loss: 0.0487\n",
      "Epoch: 3 / 3, Step: 2042 / 2250 Loss: 0.1001\n",
      "Epoch: 3 / 3, Step: 2043 / 2250 Loss: 0.0595\n",
      "Epoch: 3 / 3, Step: 2044 / 2250 Loss: 0.0760\n",
      "Epoch: 3 / 3, Step: 2045 / 2250 Loss: 0.0815\n",
      "Epoch: 3 / 3, Step: 2046 / 2250 Loss: 0.0818\n",
      "Epoch: 3 / 3, Step: 2047 / 2250 Loss: 0.0109\n",
      "Epoch: 3 / 3, Step: 2048 / 2250 Loss: 0.0285\n",
      "Epoch: 3 / 3, Step: 2049 / 2250 Loss: 0.0244\n",
      "Epoch: 3 / 3, Step: 2050 / 2250 Loss: 0.0210\n",
      "Epoch: 3 / 3, Step: 2051 / 2250 Loss: 0.0947\n",
      "Epoch: 3 / 3, Step: 2052 / 2250 Loss: 0.0929\n",
      "Epoch: 3 / 3, Step: 2053 / 2250 Loss: 0.0183\n",
      "Epoch: 3 / 3, Step: 2054 / 2250 Loss: 0.3355\n",
      "Epoch: 3 / 3, Step: 2055 / 2250 Loss: 0.0305\n",
      "Epoch: 3 / 3, Step: 2056 / 2250 Loss: 0.0889\n",
      "Epoch: 3 / 3, Step: 2057 / 2250 Loss: 0.0106\n",
      "Epoch: 3 / 3, Step: 2058 / 2250 Loss: 0.1023\n",
      "Epoch: 3 / 3, Step: 2059 / 2250 Loss: 0.1456\n",
      "Epoch: 3 / 3, Step: 2060 / 2250 Loss: 0.0324\n",
      "Epoch: 3 / 3, Step: 2061 / 2250 Loss: 0.0217\n",
      "Epoch: 3 / 3, Step: 2062 / 2250 Loss: 0.0213\n",
      "Epoch: 3 / 3, Step: 2063 / 2250 Loss: 0.0625\n",
      "Epoch: 3 / 3, Step: 2064 / 2250 Loss: 0.0159\n",
      "Epoch: 3 / 3, Step: 2065 / 2250 Loss: 0.1877\n",
      "Epoch: 3 / 3, Step: 2066 / 2250 Loss: 0.1491\n",
      "Epoch: 3 / 3, Step: 2067 / 2250 Loss: 0.0116\n",
      "Epoch: 3 / 3, Step: 2068 / 2250 Loss: 0.1812\n",
      "Epoch: 3 / 3, Step: 2069 / 2250 Loss: 0.0765\n",
      "Epoch: 3 / 3, Step: 2070 / 2250 Loss: 0.2005\n",
      "Epoch: 3 / 3, Step: 2071 / 2250 Loss: 0.0563\n",
      "Epoch: 3 / 3, Step: 2072 / 2250 Loss: 0.0282\n",
      "Epoch: 3 / 3, Step: 2073 / 2250 Loss: 0.0687\n",
      "Epoch: 3 / 3, Step: 2074 / 2250 Loss: 0.2676\n",
      "Epoch: 3 / 3, Step: 2075 / 2250 Loss: 0.0365\n",
      "Epoch: 3 / 3, Step: 2076 / 2250 Loss: 0.1551\n",
      "Epoch: 3 / 3, Step: 2077 / 2250 Loss: 0.0289\n",
      "Epoch: 3 / 3, Step: 2078 / 2250 Loss: 0.1506\n",
      "Epoch: 3 / 3, Step: 2079 / 2250 Loss: 0.4630\n",
      "Epoch: 3 / 3, Step: 2080 / 2250 Loss: 0.2224\n",
      "Epoch: 3 / 3, Step: 2081 / 2250 Loss: 0.0191\n",
      "Epoch: 3 / 3, Step: 2082 / 2250 Loss: 0.1596\n",
      "Epoch: 3 / 3, Step: 2083 / 2250 Loss: 0.0752\n",
      "Epoch: 3 / 3, Step: 2084 / 2250 Loss: 0.1730\n",
      "Epoch: 3 / 3, Step: 2085 / 2250 Loss: 0.0106\n",
      "Epoch: 3 / 3, Step: 2086 / 2250 Loss: 0.0163\n",
      "Epoch: 3 / 3, Step: 2087 / 2250 Loss: 0.0238\n",
      "Epoch: 3 / 3, Step: 2088 / 2250 Loss: 0.0194\n",
      "Epoch: 3 / 3, Step: 2089 / 2250 Loss: 0.1891\n",
      "Epoch: 3 / 3, Step: 2090 / 2250 Loss: 0.0185\n",
      "Epoch: 3 / 3, Step: 2091 / 2250 Loss: 0.1204\n",
      "Epoch: 3 / 3, Step: 2092 / 2250 Loss: 0.0900\n",
      "Epoch: 3 / 3, Step: 2093 / 2250 Loss: 0.0962\n",
      "Epoch: 3 / 3, Step: 2094 / 2250 Loss: 0.0142\n",
      "Epoch: 3 / 3, Step: 2095 / 2250 Loss: 0.0824\n",
      "Epoch: 3 / 3, Step: 2096 / 2250 Loss: 0.0241\n",
      "Epoch: 3 / 3, Step: 2097 / 2250 Loss: 0.1235\n",
      "Epoch: 3 / 3, Step: 2098 / 2250 Loss: 0.0311\n",
      "Epoch: 3 / 3, Step: 2099 / 2250 Loss: 0.2299\n",
      "Epoch: 3 / 3, Step: 2100 / 2250 Loss: 0.1457\n",
      "Epoch: 3 / 3, Step: 2101 / 2250 Loss: 0.0529\n",
      "Epoch: 3 / 3, Step: 2102 / 2250 Loss: 0.0641\n",
      "Epoch: 3 / 3, Step: 2103 / 2250 Loss: 0.0345\n",
      "Epoch: 3 / 3, Step: 2104 / 2250 Loss: 0.1644\n",
      "Epoch: 3 / 3, Step: 2105 / 2250 Loss: 0.0691\n",
      "Epoch: 3 / 3, Step: 2106 / 2250 Loss: 0.1829\n",
      "Epoch: 3 / 3, Step: 2107 / 2250 Loss: 0.0661\n",
      "Epoch: 3 / 3, Step: 2108 / 2250 Loss: 0.0120\n",
      "Epoch: 3 / 3, Step: 2109 / 2250 Loss: 0.2010\n",
      "Epoch: 3 / 3, Step: 2110 / 2250 Loss: 0.0359\n",
      "Epoch: 3 / 3, Step: 2111 / 2250 Loss: 0.0340\n",
      "Epoch: 3 / 3, Step: 2112 / 2250 Loss: 0.0202\n",
      "Epoch: 3 / 3, Step: 2113 / 2250 Loss: 0.1930\n",
      "Epoch: 3 / 3, Step: 2114 / 2250 Loss: 0.1041\n",
      "Epoch: 3 / 3, Step: 2115 / 2250 Loss: 0.0233\n",
      "Epoch: 3 / 3, Step: 2116 / 2250 Loss: 0.0368\n",
      "Epoch: 3 / 3, Step: 2117 / 2250 Loss: 0.0919\n",
      "Epoch: 3 / 3, Step: 2118 / 2250 Loss: 0.1655\n",
      "Epoch: 3 / 3, Step: 2119 / 2250 Loss: 0.3240\n",
      "Epoch: 3 / 3, Step: 2120 / 2250 Loss: 0.1524\n",
      "Epoch: 3 / 3, Step: 2121 / 2250 Loss: 0.0069\n",
      "Epoch: 3 / 3, Step: 2122 / 2250 Loss: 0.0229\n",
      "Epoch: 3 / 3, Step: 2123 / 2250 Loss: 0.0516\n",
      "Epoch: 3 / 3, Step: 2124 / 2250 Loss: 0.0413\n",
      "Epoch: 3 / 3, Step: 2125 / 2250 Loss: 0.0217\n",
      "Epoch: 3 / 3, Step: 2126 / 2250 Loss: 0.0239\n",
      "Epoch: 3 / 3, Step: 2127 / 2250 Loss: 0.1363\n",
      "Epoch: 3 / 3, Step: 2128 / 2250 Loss: 0.0713\n",
      "Epoch: 3 / 3, Step: 2129 / 2250 Loss: 0.0989\n",
      "Epoch: 3 / 3, Step: 2130 / 2250 Loss: 0.0745\n",
      "Epoch: 3 / 3, Step: 2131 / 2250 Loss: 0.0123\n",
      "Epoch: 3 / 3, Step: 2132 / 2250 Loss: 0.0129\n",
      "Epoch: 3 / 3, Step: 2133 / 2250 Loss: 0.1392\n",
      "Epoch: 3 / 3, Step: 2134 / 2250 Loss: 0.1193\n",
      "Epoch: 3 / 3, Step: 2135 / 2250 Loss: 0.0441\n",
      "Epoch: 3 / 3, Step: 2136 / 2250 Loss: 0.1185\n",
      "Epoch: 3 / 3, Step: 2137 / 2250 Loss: 0.0405\n",
      "Epoch: 3 / 3, Step: 2138 / 2250 Loss: 0.0346\n",
      "Epoch: 3 / 3, Step: 2139 / 2250 Loss: 0.1835\n",
      "Epoch: 3 / 3, Step: 2140 / 2250 Loss: 0.0441\n",
      "Epoch: 3 / 3, Step: 2141 / 2250 Loss: 0.0085\n",
      "Epoch: 3 / 3, Step: 2142 / 2250 Loss: 0.0057\n",
      "Epoch: 3 / 3, Step: 2143 / 2250 Loss: 0.1844\n",
      "Epoch: 3 / 3, Step: 2144 / 2250 Loss: 0.0584\n",
      "Epoch: 3 / 3, Step: 2145 / 2250 Loss: 0.0710\n",
      "Epoch: 3 / 3, Step: 2146 / 2250 Loss: 0.0143\n",
      "Epoch: 3 / 3, Step: 2147 / 2250 Loss: 0.0407\n",
      "Epoch: 3 / 3, Step: 2148 / 2250 Loss: 0.0937\n",
      "Epoch: 3 / 3, Step: 2149 / 2250 Loss: 0.0622\n",
      "Epoch: 3 / 3, Step: 2150 / 2250 Loss: 0.0854\n",
      "Epoch: 3 / 3, Step: 2151 / 2250 Loss: 0.0085\n",
      "Epoch: 3 / 3, Step: 2152 / 2250 Loss: 0.0413\n",
      "Epoch: 3 / 3, Step: 2153 / 2250 Loss: 0.0194\n",
      "Epoch: 3 / 3, Step: 2154 / 2250 Loss: 0.0900\n",
      "Epoch: 3 / 3, Step: 2155 / 2250 Loss: 0.0148\n",
      "Epoch: 3 / 3, Step: 2156 / 2250 Loss: 0.0416\n",
      "Epoch: 3 / 3, Step: 2157 / 2250 Loss: 0.0098\n",
      "Epoch: 3 / 3, Step: 2158 / 2250 Loss: 0.0109\n",
      "Epoch: 3 / 3, Step: 2159 / 2250 Loss: 0.0603\n",
      "Epoch: 3 / 3, Step: 2160 / 2250 Loss: 0.0949\n",
      "Epoch: 3 / 3, Step: 2161 / 2250 Loss: 0.0798\n",
      "Epoch: 3 / 3, Step: 2162 / 2250 Loss: 0.1993\n",
      "Epoch: 3 / 3, Step: 2163 / 2250 Loss: 0.0036\n",
      "Epoch: 3 / 3, Step: 2164 / 2250 Loss: 0.0193\n",
      "Epoch: 3 / 3, Step: 2165 / 2250 Loss: 0.0115\n",
      "Epoch: 3 / 3, Step: 2166 / 2250 Loss: 0.0062\n",
      "Epoch: 3 / 3, Step: 2167 / 2250 Loss: 0.0480\n",
      "Epoch: 3 / 3, Step: 2168 / 2250 Loss: 0.0065\n",
      "Epoch: 3 / 3, Step: 2169 / 2250 Loss: 0.0129\n",
      "Epoch: 3 / 3, Step: 2170 / 2250 Loss: 0.1167\n",
      "Epoch: 3 / 3, Step: 2171 / 2250 Loss: 0.0846\n",
      "Epoch: 3 / 3, Step: 2172 / 2250 Loss: 0.0205\n",
      "Epoch: 3 / 3, Step: 2173 / 2250 Loss: 0.0240\n",
      "Epoch: 3 / 3, Step: 2174 / 2250 Loss: 0.0301\n",
      "Epoch: 3 / 3, Step: 2175 / 2250 Loss: 0.2840\n",
      "Epoch: 3 / 3, Step: 2176 / 2250 Loss: 0.1169\n",
      "Epoch: 3 / 3, Step: 2177 / 2250 Loss: 0.0049\n",
      "Epoch: 3 / 3, Step: 2178 / 2250 Loss: 0.2209\n",
      "Epoch: 3 / 3, Step: 2179 / 2250 Loss: 0.2727\n",
      "Epoch: 3 / 3, Step: 2180 / 2250 Loss: 0.1530\n",
      "Epoch: 3 / 3, Step: 2181 / 2250 Loss: 0.2407\n",
      "Epoch: 3 / 3, Step: 2182 / 2250 Loss: 0.0163\n",
      "Epoch: 3 / 3, Step: 2183 / 2250 Loss: 0.0547\n",
      "Epoch: 3 / 3, Step: 2184 / 2250 Loss: 0.0258\n",
      "Epoch: 3 / 3, Step: 2185 / 2250 Loss: 0.0435\n",
      "Epoch: 3 / 3, Step: 2186 / 2250 Loss: 0.0286\n",
      "Epoch: 3 / 3, Step: 2187 / 2250 Loss: 0.0413\n",
      "Epoch: 3 / 3, Step: 2188 / 2250 Loss: 0.0462\n",
      "Epoch: 3 / 3, Step: 2189 / 2250 Loss: 0.2233\n",
      "Epoch: 3 / 3, Step: 2190 / 2250 Loss: 0.1965\n",
      "Epoch: 3 / 3, Step: 2191 / 2250 Loss: 0.1700\n",
      "Epoch: 3 / 3, Step: 2192 / 2250 Loss: 0.0324\n",
      "Epoch: 3 / 3, Step: 2193 / 2250 Loss: 0.1320\n",
      "Epoch: 3 / 3, Step: 2194 / 2250 Loss: 0.0209\n",
      "Epoch: 3 / 3, Step: 2195 / 2250 Loss: 0.0171\n",
      "Epoch: 3 / 3, Step: 2196 / 2250 Loss: 0.1236\n",
      "Epoch: 3 / 3, Step: 2197 / 2250 Loss: 0.0648\n",
      "Epoch: 3 / 3, Step: 2198 / 2250 Loss: 0.0102\n",
      "Epoch: 3 / 3, Step: 2199 / 2250 Loss: 0.0912\n",
      "Epoch: 3 / 3, Step: 2200 / 2250 Loss: 0.1122\n",
      "Epoch: 3 / 3, Step: 2201 / 2250 Loss: 0.0687\n",
      "Epoch: 3 / 3, Step: 2202 / 2250 Loss: 0.0548\n",
      "Epoch: 3 / 3, Step: 2203 / 2250 Loss: 0.3617\n",
      "Epoch: 3 / 3, Step: 2204 / 2250 Loss: 0.0099\n",
      "Epoch: 3 / 3, Step: 2205 / 2250 Loss: 0.0222\n",
      "Epoch: 3 / 3, Step: 2206 / 2250 Loss: 0.1172\n",
      "Epoch: 3 / 3, Step: 2207 / 2250 Loss: 0.1080\n",
      "Epoch: 3 / 3, Step: 2208 / 2250 Loss: 0.0293\n",
      "Epoch: 3 / 3, Step: 2209 / 2250 Loss: 0.0230\n",
      "Epoch: 3 / 3, Step: 2210 / 2250 Loss: 0.0506\n",
      "Epoch: 3 / 3, Step: 2211 / 2250 Loss: 0.0264\n",
      "Epoch: 3 / 3, Step: 2212 / 2250 Loss: 0.0250\n",
      "Epoch: 3 / 3, Step: 2213 / 2250 Loss: 0.0245\n",
      "Epoch: 3 / 3, Step: 2214 / 2250 Loss: 0.0179\n",
      "Epoch: 3 / 3, Step: 2215 / 2250 Loss: 0.0200\n",
      "Epoch: 3 / 3, Step: 2216 / 2250 Loss: 0.0340\n",
      "Epoch: 3 / 3, Step: 2217 / 2250 Loss: 0.0107\n",
      "Epoch: 3 / 3, Step: 2218 / 2250 Loss: 0.1720\n",
      "Epoch: 3 / 3, Step: 2219 / 2250 Loss: 0.2202\n",
      "Epoch: 3 / 3, Step: 2220 / 2250 Loss: 0.0564\n",
      "Epoch: 3 / 3, Step: 2221 / 2250 Loss: 0.0869\n",
      "Epoch: 3 / 3, Step: 2222 / 2250 Loss: 0.0218\n",
      "Epoch: 3 / 3, Step: 2223 / 2250 Loss: 0.1541\n",
      "Epoch: 3 / 3, Step: 2224 / 2250 Loss: 0.0857\n",
      "Epoch: 3 / 3, Step: 2225 / 2250 Loss: 0.3926\n",
      "Epoch: 3 / 3, Step: 2226 / 2250 Loss: 0.0270\n",
      "Epoch: 3 / 3, Step: 2227 / 2250 Loss: 0.0282\n",
      "Epoch: 3 / 3, Step: 2228 / 2250 Loss: 0.0050\n",
      "Epoch: 3 / 3, Step: 2229 / 2250 Loss: 0.0695\n",
      "Epoch: 3 / 3, Step: 2230 / 2250 Loss: 0.0140\n",
      "Epoch: 3 / 3, Step: 2231 / 2250 Loss: 0.0535\n",
      "Epoch: 3 / 3, Step: 2232 / 2250 Loss: 0.0090\n",
      "Epoch: 3 / 3, Step: 2233 / 2250 Loss: 0.0233\n",
      "Epoch: 3 / 3, Step: 2234 / 2250 Loss: 0.0176\n",
      "Epoch: 3 / 3, Step: 2235 / 2250 Loss: 0.1970\n",
      "Epoch: 3 / 3, Step: 2236 / 2250 Loss: 0.0472\n",
      "Epoch: 3 / 3, Step: 2237 / 2250 Loss: 0.1695\n",
      "Epoch: 3 / 3, Step: 2238 / 2250 Loss: 0.0154\n",
      "Epoch: 3 / 3, Step: 2239 / 2250 Loss: 0.0129\n",
      "Epoch: 3 / 3, Step: 2240 / 2250 Loss: 0.0129\n",
      "Epoch: 3 / 3, Step: 2241 / 2250 Loss: 0.0183\n",
      "Epoch: 3 / 3, Step: 2242 / 2250 Loss: 0.0149\n",
      "Epoch: 3 / 3, Step: 2243 / 2250 Loss: 0.1448\n",
      "Epoch: 3 / 3, Step: 2244 / 2250 Loss: 0.1137\n",
      "Epoch: 3 / 3, Step: 2245 / 2250 Loss: 0.0655\n",
      "Epoch: 3 / 3, Step: 2246 / 2250 Loss: 0.0358\n",
      "Epoch: 3 / 3, Step: 2247 / 2250 Loss: 0.0137\n",
      "Epoch: 3 / 3, Step: 2248 / 2250 Loss: 0.0389\n",
      "Epoch: 3 / 3, Step: 2249 / 2250 Loss: 0.0522\n",
      "Time for training:  2380.8000745773315\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "loss_total = 0\n",
    "model.train()\n",
    "start = time.time()\n",
    "for i in range(3):\n",
    "    for j, data in enumerate(train_dataloader):\n",
    "        inputs = {'input_ids': data[0].cuda(), \n",
    "                      'attention_mask': data[1].cuda(), \n",
    "                      'labels': data[2].cuda()}\n",
    "        output = model(**inputs)\n",
    "        loss = output[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Epoch: {} / {}, Step: {} / {} Loss: {:.4f}\".format(i+1, 3, j, len(train_dataloader),\n",
    "                                                                      loss))\n",
    "print(\"Time for training: \", time.time()-start)\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b5e473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference:  65.6012110710144\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "results = defaultdict(dict)\n",
    "predictions, true_vals = [], []\n",
    "start = time.time()\n",
    "for j, data in enumerate(test_dataloader):\n",
    "    inputs = {'input_ids': data[0].cuda(), \n",
    "              'attention_mask': data[1].cuda(), \n",
    "              'labels': data[2].cuda()}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "    loss = output[0]\n",
    "    logits = output[1]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    labels = inputs['labels'].cpu().numpy()\n",
    "    loss_total += loss.item()\n",
    "    predictions.append(logits)\n",
    "    true_vals.append(labels)\n",
    "print(\"Time for inference: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80836d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_vals = np.concatenate(true_vals, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "262d8c79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.93      0.95      0.94      3267\n",
      "         pos       0.91      0.91      0.91      1256\n",
      "         neu       0.98      0.98      0.98     13477\n",
      "\n",
      "    accuracy                           0.97     18000\n",
      "   macro avg       0.94      0.95      0.94     18000\n",
      "weighted avg       0.97      0.97      0.97     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds_flat = np.argmax(predictions, axis = 1).flatten()\n",
    "labels_flat = true_vals.flatten()\n",
    "target_names = ['neg', 'pos', 'neu']\n",
    "print(classification_report(labels_flat, preds_flat, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5ad2c4",
   "metadata": {},
   "source": [
    "The performance by BERT is excellent with 97% overall accuracy and satisfactory per-class precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68db0df",
   "metadata": {},
   "source": [
    "# DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83dacc3",
   "metadata": {},
   "source": [
    "DistilBert is a lighter version of BERT. This model is built by knowledge distillation technique wherein a small model is trained to reproduce the behavior of a larger model. Accordingly, it has 40% less parameters than BERT and runs 60% faster while retaining 95% of the BERT-base-uncased performance[3]. Here, we are planning to compare the results of BERT with DistilBERT and try to get hands on both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6bb7211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertConfig,DistilBertTokenizer,DistilBertModel\n",
    "distil_berttokenizer = DistilBertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab1409c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_train = distil_berttokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='train'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')\n",
    "\n",
    "encoded_data_test = distil_berttokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='test'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e089054",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(covid_senti[covid_senti.data_type == 'train'].label_num.values)\n",
    "\n",
    "#validation set\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(covid_senti[covid_senti.data_type == 'test'].label_num.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fff63db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.dense.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.10.output.dense.weight', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'cls.predictions.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['transformer.layer.4.attention.q_lin.bias', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.6.attention.q_lin.weight', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.9.attention.out_lin.bias', 'embeddings.LayerNorm.weight', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.7.ffn.lin1.bias', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.8.sa_layer_norm.weight', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.9.attention.out_lin.weight', 'pre_classifier.weight', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.5.sa_layer_norm.weight', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.7.sa_layer_norm.bias', 'pre_classifier.bias', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.6.output_layer_norm.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.11.attention.v_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.9.output_layer_norm.weight', 'embeddings.position_embeddings.weight', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'classifier.weight', 'transformer.layer.10.attention.out_lin.bias', 'classifier.bias', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.5.sa_layer_norm.bias', 'embeddings.word_embeddings.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.10.attention.k_lin.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(class_dict),\n",
    "                                                     output_attentions = False,\n",
    "                                                      output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f2b098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch import nn, optim\n",
    "\n",
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train,labels_train)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test,labels_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0170747",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f18f84c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 3, Step: 0 / 2250 Loss: 1.0783\n",
      "Epoch: 1 / 3, Step: 1 / 2250 Loss: 0.8679\n",
      "Epoch: 1 / 3, Step: 2 / 2250 Loss: 0.6708\n",
      "Epoch: 1 / 3, Step: 3 / 2250 Loss: 0.7652\n",
      "Epoch: 1 / 3, Step: 4 / 2250 Loss: 0.3557\n",
      "Epoch: 1 / 3, Step: 5 / 2250 Loss: 0.7270\n",
      "Epoch: 1 / 3, Step: 6 / 2250 Loss: 0.7649\n",
      "Epoch: 1 / 3, Step: 7 / 2250 Loss: 0.8280\n",
      "Epoch: 1 / 3, Step: 8 / 2250 Loss: 1.0821\n",
      "Epoch: 1 / 3, Step: 9 / 2250 Loss: 0.9098\n",
      "Epoch: 1 / 3, Step: 10 / 2250 Loss: 0.9553\n",
      "Epoch: 1 / 3, Step: 11 / 2250 Loss: 0.4044\n",
      "Epoch: 1 / 3, Step: 12 / 2250 Loss: 0.7077\n",
      "Epoch: 1 / 3, Step: 13 / 2250 Loss: 0.5611\n",
      "Epoch: 1 / 3, Step: 14 / 2250 Loss: 0.6160\n",
      "Epoch: 1 / 3, Step: 15 / 2250 Loss: 0.8396\n",
      "Epoch: 1 / 3, Step: 16 / 2250 Loss: 0.5527\n",
      "Epoch: 1 / 3, Step: 17 / 2250 Loss: 0.6816\n",
      "Epoch: 1 / 3, Step: 18 / 2250 Loss: 0.5158\n",
      "Epoch: 1 / 3, Step: 19 / 2250 Loss: 0.5346\n",
      "Epoch: 1 / 3, Step: 20 / 2250 Loss: 0.5446\n",
      "Epoch: 1 / 3, Step: 21 / 2250 Loss: 0.8661\n",
      "Epoch: 1 / 3, Step: 22 / 2250 Loss: 0.9522\n",
      "Epoch: 1 / 3, Step: 23 / 2250 Loss: 0.4858\n",
      "Epoch: 1 / 3, Step: 24 / 2250 Loss: 0.6585\n",
      "Epoch: 1 / 3, Step: 25 / 2250 Loss: 0.7022\n",
      "Epoch: 1 / 3, Step: 26 / 2250 Loss: 0.6534\n",
      "Epoch: 1 / 3, Step: 27 / 2250 Loss: 0.8769\n",
      "Epoch: 1 / 3, Step: 28 / 2250 Loss: 0.5844\n",
      "Epoch: 1 / 3, Step: 29 / 2250 Loss: 0.7194\n",
      "Epoch: 1 / 3, Step: 30 / 2250 Loss: 0.5776\n",
      "Epoch: 1 / 3, Step: 31 / 2250 Loss: 0.8551\n",
      "Epoch: 1 / 3, Step: 32 / 2250 Loss: 0.5434\n",
      "Epoch: 1 / 3, Step: 33 / 2250 Loss: 0.6463\n",
      "Epoch: 1 / 3, Step: 34 / 2250 Loss: 0.7726\n",
      "Epoch: 1 / 3, Step: 35 / 2250 Loss: 0.4604\n",
      "Epoch: 1 / 3, Step: 36 / 2250 Loss: 0.6856\n",
      "Epoch: 1 / 3, Step: 37 / 2250 Loss: 0.7301\n",
      "Epoch: 1 / 3, Step: 38 / 2250 Loss: 0.8645\n",
      "Epoch: 1 / 3, Step: 39 / 2250 Loss: 0.5204\n",
      "Epoch: 1 / 3, Step: 40 / 2250 Loss: 0.7051\n",
      "Epoch: 1 / 3, Step: 41 / 2250 Loss: 0.8332\n",
      "Epoch: 1 / 3, Step: 42 / 2250 Loss: 0.7155\n",
      "Epoch: 1 / 3, Step: 43 / 2250 Loss: 0.6166\n",
      "Epoch: 1 / 3, Step: 44 / 2250 Loss: 0.6118\n",
      "Epoch: 1 / 3, Step: 45 / 2250 Loss: 0.7044\n",
      "Epoch: 1 / 3, Step: 46 / 2250 Loss: 0.8473\n",
      "Epoch: 1 / 3, Step: 47 / 2250 Loss: 0.7262\n",
      "Epoch: 1 / 3, Step: 48 / 2250 Loss: 0.6583\n",
      "Epoch: 1 / 3, Step: 49 / 2250 Loss: 0.9350\n",
      "Epoch: 1 / 3, Step: 50 / 2250 Loss: 0.6722\n",
      "Epoch: 1 / 3, Step: 51 / 2250 Loss: 0.8146\n",
      "Epoch: 1 / 3, Step: 52 / 2250 Loss: 0.6488\n",
      "Epoch: 1 / 3, Step: 53 / 2250 Loss: 0.6938\n",
      "Epoch: 1 / 3, Step: 54 / 2250 Loss: 0.6962\n",
      "Epoch: 1 / 3, Step: 55 / 2250 Loss: 0.4221\n",
      "Epoch: 1 / 3, Step: 56 / 2250 Loss: 0.7326\n",
      "Epoch: 1 / 3, Step: 57 / 2250 Loss: 0.8177\n",
      "Epoch: 1 / 3, Step: 58 / 2250 Loss: 0.5949\n",
      "Epoch: 1 / 3, Step: 59 / 2250 Loss: 0.7086\n",
      "Epoch: 1 / 3, Step: 60 / 2250 Loss: 0.7208\n",
      "Epoch: 1 / 3, Step: 61 / 2250 Loss: 0.7454\n",
      "Epoch: 1 / 3, Step: 62 / 2250 Loss: 0.6777\n",
      "Epoch: 1 / 3, Step: 63 / 2250 Loss: 0.7928\n",
      "Epoch: 1 / 3, Step: 64 / 2250 Loss: 0.5582\n",
      "Epoch: 1 / 3, Step: 65 / 2250 Loss: 0.6258\n",
      "Epoch: 1 / 3, Step: 66 / 2250 Loss: 0.4841\n",
      "Epoch: 1 / 3, Step: 67 / 2250 Loss: 0.7959\n",
      "Epoch: 1 / 3, Step: 68 / 2250 Loss: 0.7320\n",
      "Epoch: 1 / 3, Step: 69 / 2250 Loss: 0.8636\n",
      "Epoch: 1 / 3, Step: 70 / 2250 Loss: 0.7267\n",
      "Epoch: 1 / 3, Step: 71 / 2250 Loss: 0.6715\n",
      "Epoch: 1 / 3, Step: 72 / 2250 Loss: 0.9086\n",
      "Epoch: 1 / 3, Step: 73 / 2250 Loss: 0.5831\n",
      "Epoch: 1 / 3, Step: 74 / 2250 Loss: 0.5563\n",
      "Epoch: 1 / 3, Step: 75 / 2250 Loss: 0.5994\n",
      "Epoch: 1 / 3, Step: 76 / 2250 Loss: 0.8660\n",
      "Epoch: 1 / 3, Step: 77 / 2250 Loss: 0.6882\n",
      "Epoch: 1 / 3, Step: 78 / 2250 Loss: 0.4480\n",
      "Epoch: 1 / 3, Step: 79 / 2250 Loss: 0.6672\n",
      "Epoch: 1 / 3, Step: 80 / 2250 Loss: 0.7212\n",
      "Epoch: 1 / 3, Step: 81 / 2250 Loss: 0.5771\n",
      "Epoch: 1 / 3, Step: 82 / 2250 Loss: 0.5304\n",
      "Epoch: 1 / 3, Step: 83 / 2250 Loss: 0.7895\n",
      "Epoch: 1 / 3, Step: 84 / 2250 Loss: 0.5680\n",
      "Epoch: 1 / 3, Step: 85 / 2250 Loss: 0.6956\n",
      "Epoch: 1 / 3, Step: 86 / 2250 Loss: 0.7227\n",
      "Epoch: 1 / 3, Step: 87 / 2250 Loss: 0.7811\n",
      "Epoch: 1 / 3, Step: 88 / 2250 Loss: 0.5445\n",
      "Epoch: 1 / 3, Step: 89 / 2250 Loss: 0.9342\n",
      "Epoch: 1 / 3, Step: 90 / 2250 Loss: 0.7017\n",
      "Epoch: 1 / 3, Step: 91 / 2250 Loss: 0.7295\n",
      "Epoch: 1 / 3, Step: 92 / 2250 Loss: 0.7042\n",
      "Epoch: 1 / 3, Step: 93 / 2250 Loss: 0.8838\n",
      "Epoch: 1 / 3, Step: 94 / 2250 Loss: 0.6685\n",
      "Epoch: 1 / 3, Step: 95 / 2250 Loss: 0.7254\n",
      "Epoch: 1 / 3, Step: 96 / 2250 Loss: 0.5826\n",
      "Epoch: 1 / 3, Step: 97 / 2250 Loss: 0.5708\n",
      "Epoch: 1 / 3, Step: 98 / 2250 Loss: 0.8251\n",
      "Epoch: 1 / 3, Step: 99 / 2250 Loss: 0.7651\n",
      "Epoch: 1 / 3, Step: 100 / 2250 Loss: 0.5744\n",
      "Epoch: 1 / 3, Step: 101 / 2250 Loss: 0.6740\n",
      "Epoch: 1 / 3, Step: 102 / 2250 Loss: 0.7731\n",
      "Epoch: 1 / 3, Step: 103 / 2250 Loss: 0.6464\n",
      "Epoch: 1 / 3, Step: 104 / 2250 Loss: 0.5010\n",
      "Epoch: 1 / 3, Step: 105 / 2250 Loss: 0.7995\n",
      "Epoch: 1 / 3, Step: 106 / 2250 Loss: 0.7744\n",
      "Epoch: 1 / 3, Step: 107 / 2250 Loss: 0.5927\n",
      "Epoch: 1 / 3, Step: 108 / 2250 Loss: 0.2753\n",
      "Epoch: 1 / 3, Step: 109 / 2250 Loss: 0.7219\n",
      "Epoch: 1 / 3, Step: 110 / 2250 Loss: 0.8130\n",
      "Epoch: 1 / 3, Step: 111 / 2250 Loss: 0.6498\n",
      "Epoch: 1 / 3, Step: 112 / 2250 Loss: 0.7350\n",
      "Epoch: 1 / 3, Step: 113 / 2250 Loss: 0.4446\n",
      "Epoch: 1 / 3, Step: 114 / 2250 Loss: 0.7781\n",
      "Epoch: 1 / 3, Step: 115 / 2250 Loss: 0.6590\n",
      "Epoch: 1 / 3, Step: 116 / 2250 Loss: 0.6257\n",
      "Epoch: 1 / 3, Step: 117 / 2250 Loss: 0.6118\n",
      "Epoch: 1 / 3, Step: 118 / 2250 Loss: 0.8183\n",
      "Epoch: 1 / 3, Step: 119 / 2250 Loss: 0.7768\n",
      "Epoch: 1 / 3, Step: 120 / 2250 Loss: 0.8024\n",
      "Epoch: 1 / 3, Step: 121 / 2250 Loss: 0.7362\n",
      "Epoch: 1 / 3, Step: 122 / 2250 Loss: 0.3802\n",
      "Epoch: 1 / 3, Step: 123 / 2250 Loss: 0.6678\n",
      "Epoch: 1 / 3, Step: 124 / 2250 Loss: 0.6441\n",
      "Epoch: 1 / 3, Step: 125 / 2250 Loss: 0.5569\n",
      "Epoch: 1 / 3, Step: 126 / 2250 Loss: 0.6547\n",
      "Epoch: 1 / 3, Step: 127 / 2250 Loss: 1.0189\n",
      "Epoch: 1 / 3, Step: 128 / 2250 Loss: 0.7126\n",
      "Epoch: 1 / 3, Step: 129 / 2250 Loss: 0.7778\n",
      "Epoch: 1 / 3, Step: 130 / 2250 Loss: 0.8230\n",
      "Epoch: 1 / 3, Step: 131 / 2250 Loss: 0.9363\n",
      "Epoch: 1 / 3, Step: 132 / 2250 Loss: 0.6588\n",
      "Epoch: 1 / 3, Step: 133 / 2250 Loss: 0.7953\n",
      "Epoch: 1 / 3, Step: 134 / 2250 Loss: 0.4874\n",
      "Epoch: 1 / 3, Step: 135 / 2250 Loss: 0.6588\n",
      "Epoch: 1 / 3, Step: 136 / 2250 Loss: 0.8404\n",
      "Epoch: 1 / 3, Step: 137 / 2250 Loss: 0.7244\n",
      "Epoch: 1 / 3, Step: 138 / 2250 Loss: 0.5861\n",
      "Epoch: 1 / 3, Step: 139 / 2250 Loss: 0.7449\n",
      "Epoch: 1 / 3, Step: 140 / 2250 Loss: 0.8032\n",
      "Epoch: 1 / 3, Step: 141 / 2250 Loss: 0.7869\n",
      "Epoch: 1 / 3, Step: 142 / 2250 Loss: 0.4713\n",
      "Epoch: 1 / 3, Step: 143 / 2250 Loss: 0.8337\n",
      "Epoch: 1 / 3, Step: 144 / 2250 Loss: 0.8281\n",
      "Epoch: 1 / 3, Step: 145 / 2250 Loss: 0.7205\n",
      "Epoch: 1 / 3, Step: 146 / 2250 Loss: 0.7352\n",
      "Epoch: 1 / 3, Step: 147 / 2250 Loss: 0.6667\n",
      "Epoch: 1 / 3, Step: 148 / 2250 Loss: 0.7421\n",
      "Epoch: 1 / 3, Step: 149 / 2250 Loss: 0.7423\n",
      "Epoch: 1 / 3, Step: 150 / 2250 Loss: 0.7107\n",
      "Epoch: 1 / 3, Step: 151 / 2250 Loss: 0.8883\n",
      "Epoch: 1 / 3, Step: 152 / 2250 Loss: 0.6540\n",
      "Epoch: 1 / 3, Step: 153 / 2250 Loss: 0.8631\n",
      "Epoch: 1 / 3, Step: 154 / 2250 Loss: 0.8915\n",
      "Epoch: 1 / 3, Step: 155 / 2250 Loss: 1.0852\n",
      "Epoch: 1 / 3, Step: 156 / 2250 Loss: 0.7561\n",
      "Epoch: 1 / 3, Step: 157 / 2250 Loss: 0.6503\n",
      "Epoch: 1 / 3, Step: 158 / 2250 Loss: 0.8717\n",
      "Epoch: 1 / 3, Step: 159 / 2250 Loss: 0.8425\n",
      "Epoch: 1 / 3, Step: 160 / 2250 Loss: 0.7416\n",
      "Epoch: 1 / 3, Step: 161 / 2250 Loss: 0.7279\n",
      "Epoch: 1 / 3, Step: 162 / 2250 Loss: 0.8651\n",
      "Epoch: 1 / 3, Step: 163 / 2250 Loss: 0.8448\n",
      "Epoch: 1 / 3, Step: 164 / 2250 Loss: 0.9724\n",
      "Epoch: 1 / 3, Step: 165 / 2250 Loss: 0.6964\n",
      "Epoch: 1 / 3, Step: 166 / 2250 Loss: 0.6579\n",
      "Epoch: 1 / 3, Step: 167 / 2250 Loss: 0.7395\n",
      "Epoch: 1 / 3, Step: 168 / 2250 Loss: 0.9220\n",
      "Epoch: 1 / 3, Step: 169 / 2250 Loss: 0.6956\n",
      "Epoch: 1 / 3, Step: 170 / 2250 Loss: 0.8581\n",
      "Epoch: 1 / 3, Step: 171 / 2250 Loss: 0.3923\n",
      "Epoch: 1 / 3, Step: 172 / 2250 Loss: 0.6479\n",
      "Epoch: 1 / 3, Step: 173 / 2250 Loss: 0.8951\n",
      "Epoch: 1 / 3, Step: 174 / 2250 Loss: 0.9605\n",
      "Epoch: 1 / 3, Step: 175 / 2250 Loss: 0.8374\n",
      "Epoch: 1 / 3, Step: 176 / 2250 Loss: 0.8237\n",
      "Epoch: 1 / 3, Step: 177 / 2250 Loss: 0.5897\n",
      "Epoch: 1 / 3, Step: 178 / 2250 Loss: 0.5593\n",
      "Epoch: 1 / 3, Step: 179 / 2250 Loss: 0.7545\n",
      "Epoch: 1 / 3, Step: 180 / 2250 Loss: 0.8011\n",
      "Epoch: 1 / 3, Step: 181 / 2250 Loss: 0.6475\n",
      "Epoch: 1 / 3, Step: 182 / 2250 Loss: 0.6186\n",
      "Epoch: 1 / 3, Step: 183 / 2250 Loss: 0.6701\n",
      "Epoch: 1 / 3, Step: 184 / 2250 Loss: 0.8891\n",
      "Epoch: 1 / 3, Step: 185 / 2250 Loss: 0.8983\n",
      "Epoch: 1 / 3, Step: 186 / 2250 Loss: 0.8123\n",
      "Epoch: 1 / 3, Step: 187 / 2250 Loss: 0.8087\n",
      "Epoch: 1 / 3, Step: 188 / 2250 Loss: 0.7890\n",
      "Epoch: 1 / 3, Step: 189 / 2250 Loss: 0.8868\n",
      "Epoch: 1 / 3, Step: 190 / 2250 Loss: 0.7107\n",
      "Epoch: 1 / 3, Step: 191 / 2250 Loss: 0.6884\n",
      "Epoch: 1 / 3, Step: 192 / 2250 Loss: 0.5373\n",
      "Epoch: 1 / 3, Step: 193 / 2250 Loss: 0.7441\n",
      "Epoch: 1 / 3, Step: 194 / 2250 Loss: 0.8060\n",
      "Epoch: 1 / 3, Step: 195 / 2250 Loss: 0.6629\n",
      "Epoch: 1 / 3, Step: 196 / 2250 Loss: 0.9239\n",
      "Epoch: 1 / 3, Step: 197 / 2250 Loss: 1.1109\n",
      "Epoch: 1 / 3, Step: 198 / 2250 Loss: 0.6222\n",
      "Epoch: 1 / 3, Step: 199 / 2250 Loss: 0.7234\n",
      "Epoch: 1 / 3, Step: 200 / 2250 Loss: 0.7318\n",
      "Epoch: 1 / 3, Step: 201 / 2250 Loss: 0.6649\n",
      "Epoch: 1 / 3, Step: 202 / 2250 Loss: 0.7392\n",
      "Epoch: 1 / 3, Step: 203 / 2250 Loss: 0.7748\n",
      "Epoch: 1 / 3, Step: 204 / 2250 Loss: 0.6669\n",
      "Epoch: 1 / 3, Step: 205 / 2250 Loss: 0.5663\n",
      "Epoch: 1 / 3, Step: 206 / 2250 Loss: 0.8187\n",
      "Epoch: 1 / 3, Step: 207 / 2250 Loss: 0.8292\n",
      "Epoch: 1 / 3, Step: 208 / 2250 Loss: 0.5492\n",
      "Epoch: 1 / 3, Step: 209 / 2250 Loss: 0.6731\n",
      "Epoch: 1 / 3, Step: 210 / 2250 Loss: 0.5169\n",
      "Epoch: 1 / 3, Step: 211 / 2250 Loss: 1.0493\n",
      "Epoch: 1 / 3, Step: 212 / 2250 Loss: 0.7727\n",
      "Epoch: 1 / 3, Step: 213 / 2250 Loss: 0.7566\n",
      "Epoch: 1 / 3, Step: 214 / 2250 Loss: 0.7151\n",
      "Epoch: 1 / 3, Step: 215 / 2250 Loss: 0.8561\n",
      "Epoch: 1 / 3, Step: 216 / 2250 Loss: 0.7774\n",
      "Epoch: 1 / 3, Step: 217 / 2250 Loss: 0.7534\n",
      "Epoch: 1 / 3, Step: 218 / 2250 Loss: 0.7958\n",
      "Epoch: 1 / 3, Step: 219 / 2250 Loss: 0.6498\n",
      "Epoch: 1 / 3, Step: 220 / 2250 Loss: 0.8080\n",
      "Epoch: 1 / 3, Step: 221 / 2250 Loss: 0.5739\n",
      "Epoch: 1 / 3, Step: 222 / 2250 Loss: 0.8814\n",
      "Epoch: 1 / 3, Step: 223 / 2250 Loss: 0.8256\n",
      "Epoch: 1 / 3, Step: 224 / 2250 Loss: 0.6495\n",
      "Epoch: 1 / 3, Step: 225 / 2250 Loss: 0.5915\n",
      "Epoch: 1 / 3, Step: 226 / 2250 Loss: 0.8361\n",
      "Epoch: 1 / 3, Step: 227 / 2250 Loss: 1.0014\n",
      "Epoch: 1 / 3, Step: 228 / 2250 Loss: 0.7111\n",
      "Epoch: 1 / 3, Step: 229 / 2250 Loss: 0.7950\n",
      "Epoch: 1 / 3, Step: 230 / 2250 Loss: 0.7643\n",
      "Epoch: 1 / 3, Step: 231 / 2250 Loss: 1.0295\n",
      "Epoch: 1 / 3, Step: 232 / 2250 Loss: 0.7845\n",
      "Epoch: 1 / 3, Step: 233 / 2250 Loss: 0.8582\n",
      "Epoch: 1 / 3, Step: 234 / 2250 Loss: 0.8483\n",
      "Epoch: 1 / 3, Step: 235 / 2250 Loss: 0.7516\n",
      "Epoch: 1 / 3, Step: 236 / 2250 Loss: 0.6412\n",
      "Epoch: 1 / 3, Step: 237 / 2250 Loss: 0.5044\n",
      "Epoch: 1 / 3, Step: 238 / 2250 Loss: 0.6914\n",
      "Epoch: 1 / 3, Step: 239 / 2250 Loss: 0.8299\n",
      "Epoch: 1 / 3, Step: 240 / 2250 Loss: 0.8178\n",
      "Epoch: 1 / 3, Step: 241 / 2250 Loss: 0.6278\n",
      "Epoch: 1 / 3, Step: 242 / 2250 Loss: 0.7645\n",
      "Epoch: 1 / 3, Step: 243 / 2250 Loss: 0.9988\n",
      "Epoch: 1 / 3, Step: 244 / 2250 Loss: 0.7436\n",
      "Epoch: 1 / 3, Step: 245 / 2250 Loss: 0.6101\n",
      "Epoch: 1 / 3, Step: 246 / 2250 Loss: 0.6175\n",
      "Epoch: 1 / 3, Step: 247 / 2250 Loss: 0.7500\n",
      "Epoch: 1 / 3, Step: 248 / 2250 Loss: 0.6478\n",
      "Epoch: 1 / 3, Step: 249 / 2250 Loss: 0.7642\n",
      "Epoch: 1 / 3, Step: 250 / 2250 Loss: 0.6689\n",
      "Epoch: 1 / 3, Step: 251 / 2250 Loss: 0.8573\n",
      "Epoch: 1 / 3, Step: 252 / 2250 Loss: 0.6037\n",
      "Epoch: 1 / 3, Step: 253 / 2250 Loss: 0.7433\n",
      "Epoch: 1 / 3, Step: 254 / 2250 Loss: 0.8426\n",
      "Epoch: 1 / 3, Step: 255 / 2250 Loss: 0.6306\n",
      "Epoch: 1 / 3, Step: 256 / 2250 Loss: 0.7286\n",
      "Epoch: 1 / 3, Step: 257 / 2250 Loss: 0.9703\n",
      "Epoch: 1 / 3, Step: 258 / 2250 Loss: 0.7975\n",
      "Epoch: 1 / 3, Step: 259 / 2250 Loss: 0.7845\n",
      "Epoch: 1 / 3, Step: 260 / 2250 Loss: 0.5493\n",
      "Epoch: 1 / 3, Step: 261 / 2250 Loss: 0.5725\n",
      "Epoch: 1 / 3, Step: 262 / 2250 Loss: 0.9646\n",
      "Epoch: 1 / 3, Step: 263 / 2250 Loss: 0.7651\n",
      "Epoch: 1 / 3, Step: 264 / 2250 Loss: 0.7634\n",
      "Epoch: 1 / 3, Step: 265 / 2250 Loss: 0.8440\n",
      "Epoch: 1 / 3, Step: 266 / 2250 Loss: 0.7550\n",
      "Epoch: 1 / 3, Step: 267 / 2250 Loss: 0.5772\n",
      "Epoch: 1 / 3, Step: 268 / 2250 Loss: 0.5758\n",
      "Epoch: 1 / 3, Step: 269 / 2250 Loss: 0.5922\n",
      "Epoch: 1 / 3, Step: 270 / 2250 Loss: 0.5436\n",
      "Epoch: 1 / 3, Step: 271 / 2250 Loss: 0.4494\n",
      "Epoch: 1 / 3, Step: 272 / 2250 Loss: 0.4854\n",
      "Epoch: 1 / 3, Step: 273 / 2250 Loss: 0.6097\n",
      "Epoch: 1 / 3, Step: 274 / 2250 Loss: 0.8325\n",
      "Epoch: 1 / 3, Step: 275 / 2250 Loss: 0.8935\n",
      "Epoch: 1 / 3, Step: 276 / 2250 Loss: 0.6821\n",
      "Epoch: 1 / 3, Step: 277 / 2250 Loss: 0.6006\n",
      "Epoch: 1 / 3, Step: 278 / 2250 Loss: 0.8523\n",
      "Epoch: 1 / 3, Step: 279 / 2250 Loss: 0.7632\n",
      "Epoch: 1 / 3, Step: 280 / 2250 Loss: 0.7186\n",
      "Epoch: 1 / 3, Step: 281 / 2250 Loss: 0.6299\n",
      "Epoch: 1 / 3, Step: 282 / 2250 Loss: 0.5350\n",
      "Epoch: 1 / 3, Step: 283 / 2250 Loss: 0.5528\n",
      "Epoch: 1 / 3, Step: 284 / 2250 Loss: 0.7185\n",
      "Epoch: 1 / 3, Step: 285 / 2250 Loss: 0.5922\n",
      "Epoch: 1 / 3, Step: 286 / 2250 Loss: 0.7869\n",
      "Epoch: 1 / 3, Step: 287 / 2250 Loss: 0.7529\n",
      "Epoch: 1 / 3, Step: 288 / 2250 Loss: 0.7842\n",
      "Epoch: 1 / 3, Step: 289 / 2250 Loss: 0.7016\n",
      "Epoch: 1 / 3, Step: 290 / 2250 Loss: 0.6529\n",
      "Epoch: 1 / 3, Step: 291 / 2250 Loss: 0.8653\n",
      "Epoch: 1 / 3, Step: 292 / 2250 Loss: 0.6017\n",
      "Epoch: 1 / 3, Step: 293 / 2250 Loss: 0.6127\n",
      "Epoch: 1 / 3, Step: 294 / 2250 Loss: 0.5411\n",
      "Epoch: 1 / 3, Step: 295 / 2250 Loss: 0.5832\n",
      "Epoch: 1 / 3, Step: 296 / 2250 Loss: 0.4928\n",
      "Epoch: 1 / 3, Step: 297 / 2250 Loss: 0.4784\n",
      "Epoch: 1 / 3, Step: 298 / 2250 Loss: 0.6840\n",
      "Epoch: 1 / 3, Step: 299 / 2250 Loss: 0.7339\n",
      "Epoch: 1 / 3, Step: 300 / 2250 Loss: 0.8837\n",
      "Epoch: 1 / 3, Step: 301 / 2250 Loss: 0.8586\n",
      "Epoch: 1 / 3, Step: 302 / 2250 Loss: 0.6809\n",
      "Epoch: 1 / 3, Step: 303 / 2250 Loss: 0.5726\n",
      "Epoch: 1 / 3, Step: 304 / 2250 Loss: 0.7675\n",
      "Epoch: 1 / 3, Step: 305 / 2250 Loss: 0.6946\n",
      "Epoch: 1 / 3, Step: 306 / 2250 Loss: 0.5146\n",
      "Epoch: 1 / 3, Step: 307 / 2250 Loss: 0.5200\n",
      "Epoch: 1 / 3, Step: 308 / 2250 Loss: 0.6490\n",
      "Epoch: 1 / 3, Step: 309 / 2250 Loss: 0.7344\n",
      "Epoch: 1 / 3, Step: 310 / 2250 Loss: 0.6267\n",
      "Epoch: 1 / 3, Step: 311 / 2250 Loss: 0.7146\n",
      "Epoch: 1 / 3, Step: 312 / 2250 Loss: 0.6776\n",
      "Epoch: 1 / 3, Step: 313 / 2250 Loss: 0.4523\n",
      "Epoch: 1 / 3, Step: 314 / 2250 Loss: 0.5489\n",
      "Epoch: 1 / 3, Step: 315 / 2250 Loss: 0.7503\n",
      "Epoch: 1 / 3, Step: 316 / 2250 Loss: 0.5836\n",
      "Epoch: 1 / 3, Step: 317 / 2250 Loss: 0.6682\n",
      "Epoch: 1 / 3, Step: 318 / 2250 Loss: 0.7618\n",
      "Epoch: 1 / 3, Step: 319 / 2250 Loss: 0.5078\n",
      "Epoch: 1 / 3, Step: 320 / 2250 Loss: 0.5223\n",
      "Epoch: 1 / 3, Step: 321 / 2250 Loss: 0.6744\n",
      "Epoch: 1 / 3, Step: 322 / 2250 Loss: 0.3787\n",
      "Epoch: 1 / 3, Step: 323 / 2250 Loss: 0.9140\n",
      "Epoch: 1 / 3, Step: 324 / 2250 Loss: 0.7744\n",
      "Epoch: 1 / 3, Step: 325 / 2250 Loss: 0.7262\n",
      "Epoch: 1 / 3, Step: 326 / 2250 Loss: 0.6984\n",
      "Epoch: 1 / 3, Step: 327 / 2250 Loss: 0.6871\n",
      "Epoch: 1 / 3, Step: 328 / 2250 Loss: 0.6159\n",
      "Epoch: 1 / 3, Step: 329 / 2250 Loss: 0.6011\n",
      "Epoch: 1 / 3, Step: 330 / 2250 Loss: 0.7003\n",
      "Epoch: 1 / 3, Step: 331 / 2250 Loss: 0.6733\n",
      "Epoch: 1 / 3, Step: 332 / 2250 Loss: 0.6023\n",
      "Epoch: 1 / 3, Step: 333 / 2250 Loss: 0.7465\n",
      "Epoch: 1 / 3, Step: 334 / 2250 Loss: 0.4899\n",
      "Epoch: 1 / 3, Step: 335 / 2250 Loss: 0.7212\n",
      "Epoch: 1 / 3, Step: 336 / 2250 Loss: 0.4277\n",
      "Epoch: 1 / 3, Step: 337 / 2250 Loss: 0.6287\n",
      "Epoch: 1 / 3, Step: 338 / 2250 Loss: 0.6019\n",
      "Epoch: 1 / 3, Step: 339 / 2250 Loss: 0.6770\n",
      "Epoch: 1 / 3, Step: 340 / 2250 Loss: 0.8299\n",
      "Epoch: 1 / 3, Step: 341 / 2250 Loss: 0.6356\n",
      "Epoch: 1 / 3, Step: 342 / 2250 Loss: 0.7574\n",
      "Epoch: 1 / 3, Step: 343 / 2250 Loss: 0.5232\n",
      "Epoch: 1 / 3, Step: 344 / 2250 Loss: 0.8702\n",
      "Epoch: 1 / 3, Step: 345 / 2250 Loss: 0.7664\n",
      "Epoch: 1 / 3, Step: 346 / 2250 Loss: 0.7804\n",
      "Epoch: 1 / 3, Step: 347 / 2250 Loss: 0.7708\n",
      "Epoch: 1 / 3, Step: 348 / 2250 Loss: 0.8158\n",
      "Epoch: 1 / 3, Step: 349 / 2250 Loss: 0.5941\n",
      "Epoch: 1 / 3, Step: 350 / 2250 Loss: 0.5672\n",
      "Epoch: 1 / 3, Step: 351 / 2250 Loss: 0.6199\n",
      "Epoch: 1 / 3, Step: 352 / 2250 Loss: 0.7529\n",
      "Epoch: 1 / 3, Step: 353 / 2250 Loss: 0.8788\n",
      "Epoch: 1 / 3, Step: 354 / 2250 Loss: 0.6932\n",
      "Epoch: 1 / 3, Step: 355 / 2250 Loss: 0.8454\n",
      "Epoch: 1 / 3, Step: 356 / 2250 Loss: 0.9309\n",
      "Epoch: 1 / 3, Step: 357 / 2250 Loss: 0.6004\n",
      "Epoch: 1 / 3, Step: 358 / 2250 Loss: 0.7889\n",
      "Epoch: 1 / 3, Step: 359 / 2250 Loss: 0.8084\n",
      "Epoch: 1 / 3, Step: 360 / 2250 Loss: 0.6276\n",
      "Epoch: 1 / 3, Step: 361 / 2250 Loss: 0.7632\n",
      "Epoch: 1 / 3, Step: 362 / 2250 Loss: 0.6323\n",
      "Epoch: 1 / 3, Step: 363 / 2250 Loss: 0.7383\n",
      "Epoch: 1 / 3, Step: 364 / 2250 Loss: 0.7114\n",
      "Epoch: 1 / 3, Step: 365 / 2250 Loss: 0.7787\n",
      "Epoch: 1 / 3, Step: 366 / 2250 Loss: 0.9595\n",
      "Epoch: 1 / 3, Step: 367 / 2250 Loss: 0.6530\n",
      "Epoch: 1 / 3, Step: 368 / 2250 Loss: 0.7139\n",
      "Epoch: 1 / 3, Step: 369 / 2250 Loss: 0.5727\n",
      "Epoch: 1 / 3, Step: 370 / 2250 Loss: 0.7790\n",
      "Epoch: 1 / 3, Step: 371 / 2250 Loss: 0.5693\n",
      "Epoch: 1 / 3, Step: 372 / 2250 Loss: 0.5739\n",
      "Epoch: 1 / 3, Step: 373 / 2250 Loss: 0.6754\n",
      "Epoch: 1 / 3, Step: 374 / 2250 Loss: 0.7434\n",
      "Epoch: 1 / 3, Step: 375 / 2250 Loss: 0.6680\n",
      "Epoch: 1 / 3, Step: 376 / 2250 Loss: 0.5466\n",
      "Epoch: 1 / 3, Step: 377 / 2250 Loss: 0.7498\n",
      "Epoch: 1 / 3, Step: 378 / 2250 Loss: 0.7791\n",
      "Epoch: 1 / 3, Step: 379 / 2250 Loss: 0.8386\n",
      "Epoch: 1 / 3, Step: 380 / 2250 Loss: 0.7389\n",
      "Epoch: 1 / 3, Step: 381 / 2250 Loss: 0.7013\n",
      "Epoch: 1 / 3, Step: 382 / 2250 Loss: 0.6974\n",
      "Epoch: 1 / 3, Step: 383 / 2250 Loss: 1.1927\n",
      "Epoch: 1 / 3, Step: 384 / 2250 Loss: 0.7764\n",
      "Epoch: 1 / 3, Step: 385 / 2250 Loss: 0.8612\n",
      "Epoch: 1 / 3, Step: 386 / 2250 Loss: 0.6208\n",
      "Epoch: 1 / 3, Step: 387 / 2250 Loss: 0.6819\n",
      "Epoch: 1 / 3, Step: 388 / 2250 Loss: 0.6532\n",
      "Epoch: 1 / 3, Step: 389 / 2250 Loss: 0.7960\n",
      "Epoch: 1 / 3, Step: 390 / 2250 Loss: 0.7641\n",
      "Epoch: 1 / 3, Step: 391 / 2250 Loss: 0.6123\n",
      "Epoch: 1 / 3, Step: 392 / 2250 Loss: 0.7651\n",
      "Epoch: 1 / 3, Step: 393 / 2250 Loss: 0.6258\n",
      "Epoch: 1 / 3, Step: 394 / 2250 Loss: 0.4795\n",
      "Epoch: 1 / 3, Step: 395 / 2250 Loss: 0.4416\n",
      "Epoch: 1 / 3, Step: 396 / 2250 Loss: 0.8750\n",
      "Epoch: 1 / 3, Step: 397 / 2250 Loss: 0.4243\n",
      "Epoch: 1 / 3, Step: 398 / 2250 Loss: 0.5055\n",
      "Epoch: 1 / 3, Step: 399 / 2250 Loss: 0.5614\n",
      "Epoch: 1 / 3, Step: 400 / 2250 Loss: 0.7216\n",
      "Epoch: 1 / 3, Step: 401 / 2250 Loss: 0.7335\n",
      "Epoch: 1 / 3, Step: 402 / 2250 Loss: 0.9571\n",
      "Epoch: 1 / 3, Step: 403 / 2250 Loss: 0.4617\n",
      "Epoch: 1 / 3, Step: 404 / 2250 Loss: 0.7049\n",
      "Epoch: 1 / 3, Step: 405 / 2250 Loss: 0.3785\n",
      "Epoch: 1 / 3, Step: 406 / 2250 Loss: 0.6311\n",
      "Epoch: 1 / 3, Step: 407 / 2250 Loss: 0.8192\n",
      "Epoch: 1 / 3, Step: 408 / 2250 Loss: 1.1398\n",
      "Epoch: 1 / 3, Step: 409 / 2250 Loss: 0.7486\n",
      "Epoch: 1 / 3, Step: 410 / 2250 Loss: 0.7158\n",
      "Epoch: 1 / 3, Step: 411 / 2250 Loss: 0.7123\n",
      "Epoch: 1 / 3, Step: 412 / 2250 Loss: 0.6522\n",
      "Epoch: 1 / 3, Step: 413 / 2250 Loss: 0.8500\n",
      "Epoch: 1 / 3, Step: 414 / 2250 Loss: 0.5036\n",
      "Epoch: 1 / 3, Step: 415 / 2250 Loss: 0.5801\n",
      "Epoch: 1 / 3, Step: 416 / 2250 Loss: 0.7653\n",
      "Epoch: 1 / 3, Step: 417 / 2250 Loss: 0.7832\n",
      "Epoch: 1 / 3, Step: 418 / 2250 Loss: 0.7281\n",
      "Epoch: 1 / 3, Step: 419 / 2250 Loss: 0.5079\n",
      "Epoch: 1 / 3, Step: 420 / 2250 Loss: 0.7679\n",
      "Epoch: 1 / 3, Step: 421 / 2250 Loss: 0.5949\n",
      "Epoch: 1 / 3, Step: 422 / 2250 Loss: 0.5067\n",
      "Epoch: 1 / 3, Step: 423 / 2250 Loss: 0.6800\n",
      "Epoch: 1 / 3, Step: 424 / 2250 Loss: 0.9392\n",
      "Epoch: 1 / 3, Step: 425 / 2250 Loss: 0.6603\n",
      "Epoch: 1 / 3, Step: 426 / 2250 Loss: 0.6261\n",
      "Epoch: 1 / 3, Step: 427 / 2250 Loss: 0.8779\n",
      "Epoch: 1 / 3, Step: 428 / 2250 Loss: 0.4767\n",
      "Epoch: 1 / 3, Step: 429 / 2250 Loss: 0.6767\n",
      "Epoch: 1 / 3, Step: 430 / 2250 Loss: 0.8613\n",
      "Epoch: 1 / 3, Step: 431 / 2250 Loss: 0.8543\n",
      "Epoch: 1 / 3, Step: 432 / 2250 Loss: 0.6351\n",
      "Epoch: 1 / 3, Step: 433 / 2250 Loss: 0.7845\n",
      "Epoch: 1 / 3, Step: 434 / 2250 Loss: 0.6925\n",
      "Epoch: 1 / 3, Step: 435 / 2250 Loss: 0.5246\n",
      "Epoch: 1 / 3, Step: 436 / 2250 Loss: 0.6058\n",
      "Epoch: 1 / 3, Step: 437 / 2250 Loss: 0.8108\n",
      "Epoch: 1 / 3, Step: 438 / 2250 Loss: 0.8569\n",
      "Epoch: 1 / 3, Step: 439 / 2250 Loss: 0.8327\n",
      "Epoch: 1 / 3, Step: 440 / 2250 Loss: 0.7035\n",
      "Epoch: 1 / 3, Step: 441 / 2250 Loss: 0.6941\n",
      "Epoch: 1 / 3, Step: 442 / 2250 Loss: 0.6838\n",
      "Epoch: 1 / 3, Step: 443 / 2250 Loss: 0.5121\n",
      "Epoch: 1 / 3, Step: 444 / 2250 Loss: 0.5875\n",
      "Epoch: 1 / 3, Step: 445 / 2250 Loss: 0.6687\n",
      "Epoch: 1 / 3, Step: 446 / 2250 Loss: 0.5421\n",
      "Epoch: 1 / 3, Step: 447 / 2250 Loss: 0.7246\n",
      "Epoch: 1 / 3, Step: 448 / 2250 Loss: 0.5998\n",
      "Epoch: 1 / 3, Step: 449 / 2250 Loss: 0.6244\n",
      "Epoch: 1 / 3, Step: 450 / 2250 Loss: 0.9482\n",
      "Epoch: 1 / 3, Step: 451 / 2250 Loss: 0.5659\n",
      "Epoch: 1 / 3, Step: 452 / 2250 Loss: 0.8225\n",
      "Epoch: 1 / 3, Step: 453 / 2250 Loss: 0.6552\n",
      "Epoch: 1 / 3, Step: 454 / 2250 Loss: 0.3915\n",
      "Epoch: 1 / 3, Step: 455 / 2250 Loss: 0.8302\n",
      "Epoch: 1 / 3, Step: 456 / 2250 Loss: 0.8832\n",
      "Epoch: 1 / 3, Step: 457 / 2250 Loss: 0.3176\n",
      "Epoch: 1 / 3, Step: 458 / 2250 Loss: 0.6764\n",
      "Epoch: 1 / 3, Step: 459 / 2250 Loss: 0.7384\n",
      "Epoch: 1 / 3, Step: 460 / 2250 Loss: 0.7664\n",
      "Epoch: 1 / 3, Step: 461 / 2250 Loss: 0.5558\n",
      "Epoch: 1 / 3, Step: 462 / 2250 Loss: 0.6376\n",
      "Epoch: 1 / 3, Step: 463 / 2250 Loss: 0.9137\n",
      "Epoch: 1 / 3, Step: 464 / 2250 Loss: 0.9208\n",
      "Epoch: 1 / 3, Step: 465 / 2250 Loss: 0.5048\n",
      "Epoch: 1 / 3, Step: 466 / 2250 Loss: 0.6351\n",
      "Epoch: 1 / 3, Step: 467 / 2250 Loss: 0.6287\n",
      "Epoch: 1 / 3, Step: 468 / 2250 Loss: 0.6773\n",
      "Epoch: 1 / 3, Step: 469 / 2250 Loss: 0.7934\n",
      "Epoch: 1 / 3, Step: 470 / 2250 Loss: 0.6853\n",
      "Epoch: 1 / 3, Step: 471 / 2250 Loss: 0.6738\n",
      "Epoch: 1 / 3, Step: 472 / 2250 Loss: 0.6475\n",
      "Epoch: 1 / 3, Step: 473 / 2250 Loss: 0.7980\n",
      "Epoch: 1 / 3, Step: 474 / 2250 Loss: 0.4393\n",
      "Epoch: 1 / 3, Step: 475 / 2250 Loss: 0.6300\n",
      "Epoch: 1 / 3, Step: 476 / 2250 Loss: 0.5520\n",
      "Epoch: 1 / 3, Step: 477 / 2250 Loss: 0.3838\n",
      "Epoch: 1 / 3, Step: 478 / 2250 Loss: 0.7797\n",
      "Epoch: 1 / 3, Step: 479 / 2250 Loss: 0.7480\n",
      "Epoch: 1 / 3, Step: 480 / 2250 Loss: 0.4463\n",
      "Epoch: 1 / 3, Step: 481 / 2250 Loss: 0.8390\n",
      "Epoch: 1 / 3, Step: 482 / 2250 Loss: 0.7832\n",
      "Epoch: 1 / 3, Step: 483 / 2250 Loss: 0.6332\n",
      "Epoch: 1 / 3, Step: 484 / 2250 Loss: 0.6769\n",
      "Epoch: 1 / 3, Step: 485 / 2250 Loss: 0.5968\n",
      "Epoch: 1 / 3, Step: 486 / 2250 Loss: 0.5514\n",
      "Epoch: 1 / 3, Step: 487 / 2250 Loss: 0.9963\n",
      "Epoch: 1 / 3, Step: 488 / 2250 Loss: 0.6443\n",
      "Epoch: 1 / 3, Step: 489 / 2250 Loss: 0.4830\n",
      "Epoch: 1 / 3, Step: 490 / 2250 Loss: 0.8722\n",
      "Epoch: 1 / 3, Step: 491 / 2250 Loss: 0.6322\n",
      "Epoch: 1 / 3, Step: 492 / 2250 Loss: 0.9835\n",
      "Epoch: 1 / 3, Step: 493 / 2250 Loss: 0.7820\n",
      "Epoch: 1 / 3, Step: 494 / 2250 Loss: 0.7619\n",
      "Epoch: 1 / 3, Step: 495 / 2250 Loss: 0.8054\n",
      "Epoch: 1 / 3, Step: 496 / 2250 Loss: 0.8264\n",
      "Epoch: 1 / 3, Step: 497 / 2250 Loss: 0.5750\n",
      "Epoch: 1 / 3, Step: 498 / 2250 Loss: 0.6264\n",
      "Epoch: 1 / 3, Step: 499 / 2250 Loss: 0.7334\n",
      "Epoch: 1 / 3, Step: 500 / 2250 Loss: 0.7689\n",
      "Epoch: 1 / 3, Step: 501 / 2250 Loss: 0.7933\n",
      "Epoch: 1 / 3, Step: 502 / 2250 Loss: 0.6730\n",
      "Epoch: 1 / 3, Step: 503 / 2250 Loss: 0.5432\n",
      "Epoch: 1 / 3, Step: 504 / 2250 Loss: 0.5232\n",
      "Epoch: 1 / 3, Step: 505 / 2250 Loss: 0.9002\n",
      "Epoch: 1 / 3, Step: 506 / 2250 Loss: 0.9091\n",
      "Epoch: 1 / 3, Step: 507 / 2250 Loss: 0.6253\n",
      "Epoch: 1 / 3, Step: 508 / 2250 Loss: 0.5665\n",
      "Epoch: 1 / 3, Step: 509 / 2250 Loss: 0.7651\n",
      "Epoch: 1 / 3, Step: 510 / 2250 Loss: 0.6030\n",
      "Epoch: 1 / 3, Step: 511 / 2250 Loss: 0.8452\n",
      "Epoch: 1 / 3, Step: 512 / 2250 Loss: 0.5297\n",
      "Epoch: 1 / 3, Step: 513 / 2250 Loss: 0.6397\n",
      "Epoch: 1 / 3, Step: 514 / 2250 Loss: 0.6110\n",
      "Epoch: 1 / 3, Step: 515 / 2250 Loss: 0.6287\n",
      "Epoch: 1 / 3, Step: 516 / 2250 Loss: 0.7470\n",
      "Epoch: 1 / 3, Step: 517 / 2250 Loss: 0.5476\n",
      "Epoch: 1 / 3, Step: 518 / 2250 Loss: 0.6446\n",
      "Epoch: 1 / 3, Step: 519 / 2250 Loss: 0.6367\n",
      "Epoch: 1 / 3, Step: 520 / 2250 Loss: 0.7137\n",
      "Epoch: 1 / 3, Step: 521 / 2250 Loss: 0.8924\n",
      "Epoch: 1 / 3, Step: 522 / 2250 Loss: 0.5418\n",
      "Epoch: 1 / 3, Step: 523 / 2250 Loss: 0.4748\n",
      "Epoch: 1 / 3, Step: 524 / 2250 Loss: 0.8681\n",
      "Epoch: 1 / 3, Step: 525 / 2250 Loss: 0.6105\n",
      "Epoch: 1 / 3, Step: 526 / 2250 Loss: 0.7435\n",
      "Epoch: 1 / 3, Step: 527 / 2250 Loss: 0.8125\n",
      "Epoch: 1 / 3, Step: 528 / 2250 Loss: 0.6056\n",
      "Epoch: 1 / 3, Step: 529 / 2250 Loss: 0.5677\n",
      "Epoch: 1 / 3, Step: 530 / 2250 Loss: 0.7708\n",
      "Epoch: 1 / 3, Step: 531 / 2250 Loss: 0.9264\n",
      "Epoch: 1 / 3, Step: 532 / 2250 Loss: 0.5277\n",
      "Epoch: 1 / 3, Step: 533 / 2250 Loss: 0.9879\n",
      "Epoch: 1 / 3, Step: 534 / 2250 Loss: 0.7772\n",
      "Epoch: 1 / 3, Step: 535 / 2250 Loss: 0.8583\n",
      "Epoch: 1 / 3, Step: 536 / 2250 Loss: 0.7231\n",
      "Epoch: 1 / 3, Step: 537 / 2250 Loss: 0.6379\n",
      "Epoch: 1 / 3, Step: 538 / 2250 Loss: 0.6534\n",
      "Epoch: 1 / 3, Step: 539 / 2250 Loss: 0.6026\n",
      "Epoch: 1 / 3, Step: 540 / 2250 Loss: 1.0074\n",
      "Epoch: 1 / 3, Step: 541 / 2250 Loss: 0.4930\n",
      "Epoch: 1 / 3, Step: 542 / 2250 Loss: 0.7705\n",
      "Epoch: 1 / 3, Step: 543 / 2250 Loss: 0.6862\n",
      "Epoch: 1 / 3, Step: 544 / 2250 Loss: 0.7605\n",
      "Epoch: 1 / 3, Step: 545 / 2250 Loss: 0.5927\n",
      "Epoch: 1 / 3, Step: 546 / 2250 Loss: 0.4573\n",
      "Epoch: 1 / 3, Step: 547 / 2250 Loss: 0.4912\n",
      "Epoch: 1 / 3, Step: 548 / 2250 Loss: 0.7606\n",
      "Epoch: 1 / 3, Step: 549 / 2250 Loss: 0.6649\n",
      "Epoch: 1 / 3, Step: 550 / 2250 Loss: 0.8945\n",
      "Epoch: 1 / 3, Step: 551 / 2250 Loss: 0.6829\n",
      "Epoch: 1 / 3, Step: 552 / 2250 Loss: 0.5918\n",
      "Epoch: 1 / 3, Step: 553 / 2250 Loss: 0.4738\n",
      "Epoch: 1 / 3, Step: 554 / 2250 Loss: 0.8286\n",
      "Epoch: 1 / 3, Step: 555 / 2250 Loss: 0.9060\n",
      "Epoch: 1 / 3, Step: 556 / 2250 Loss: 0.7726\n",
      "Epoch: 1 / 3, Step: 557 / 2250 Loss: 0.6404\n",
      "Epoch: 1 / 3, Step: 558 / 2250 Loss: 0.7009\n",
      "Epoch: 1 / 3, Step: 559 / 2250 Loss: 0.5921\n",
      "Epoch: 1 / 3, Step: 560 / 2250 Loss: 0.5241\n",
      "Epoch: 1 / 3, Step: 561 / 2250 Loss: 0.8275\n",
      "Epoch: 1 / 3, Step: 562 / 2250 Loss: 0.6526\n",
      "Epoch: 1 / 3, Step: 563 / 2250 Loss: 0.4712\n",
      "Epoch: 1 / 3, Step: 564 / 2250 Loss: 0.9027\n",
      "Epoch: 1 / 3, Step: 565 / 2250 Loss: 0.8898\n",
      "Epoch: 1 / 3, Step: 566 / 2250 Loss: 0.5809\n",
      "Epoch: 1 / 3, Step: 567 / 2250 Loss: 0.6641\n",
      "Epoch: 1 / 3, Step: 568 / 2250 Loss: 0.9112\n",
      "Epoch: 1 / 3, Step: 569 / 2250 Loss: 0.8454\n",
      "Epoch: 1 / 3, Step: 570 / 2250 Loss: 0.8029\n",
      "Epoch: 1 / 3, Step: 571 / 2250 Loss: 0.6674\n",
      "Epoch: 1 / 3, Step: 572 / 2250 Loss: 0.7557\n",
      "Epoch: 1 / 3, Step: 573 / 2250 Loss: 0.8948\n",
      "Epoch: 1 / 3, Step: 574 / 2250 Loss: 0.8255\n",
      "Epoch: 1 / 3, Step: 575 / 2250 Loss: 0.7111\n",
      "Epoch: 1 / 3, Step: 576 / 2250 Loss: 0.9051\n",
      "Epoch: 1 / 3, Step: 577 / 2250 Loss: 0.7337\n",
      "Epoch: 1 / 3, Step: 578 / 2250 Loss: 0.8310\n",
      "Epoch: 1 / 3, Step: 579 / 2250 Loss: 0.7271\n",
      "Epoch: 1 / 3, Step: 580 / 2250 Loss: 0.6342\n",
      "Epoch: 1 / 3, Step: 581 / 2250 Loss: 0.6395\n",
      "Epoch: 1 / 3, Step: 582 / 2250 Loss: 0.6509\n",
      "Epoch: 1 / 3, Step: 583 / 2250 Loss: 0.6932\n",
      "Epoch: 1 / 3, Step: 584 / 2250 Loss: 0.6662\n",
      "Epoch: 1 / 3, Step: 585 / 2250 Loss: 0.9594\n",
      "Epoch: 1 / 3, Step: 586 / 2250 Loss: 0.8433\n",
      "Epoch: 1 / 3, Step: 587 / 2250 Loss: 0.7556\n",
      "Epoch: 1 / 3, Step: 588 / 2250 Loss: 0.4277\n",
      "Epoch: 1 / 3, Step: 589 / 2250 Loss: 0.5590\n",
      "Epoch: 1 / 3, Step: 590 / 2250 Loss: 0.6933\n",
      "Epoch: 1 / 3, Step: 591 / 2250 Loss: 0.6077\n",
      "Epoch: 1 / 3, Step: 592 / 2250 Loss: 0.7596\n",
      "Epoch: 1 / 3, Step: 593 / 2250 Loss: 0.7513\n",
      "Epoch: 1 / 3, Step: 594 / 2250 Loss: 0.4429\n",
      "Epoch: 1 / 3, Step: 595 / 2250 Loss: 0.7915\n",
      "Epoch: 1 / 3, Step: 596 / 2250 Loss: 0.6516\n",
      "Epoch: 1 / 3, Step: 597 / 2250 Loss: 0.7869\n",
      "Epoch: 1 / 3, Step: 598 / 2250 Loss: 0.6756\n",
      "Epoch: 1 / 3, Step: 599 / 2250 Loss: 0.8561\n",
      "Epoch: 1 / 3, Step: 600 / 2250 Loss: 0.6524\n",
      "Epoch: 1 / 3, Step: 601 / 2250 Loss: 0.5167\n",
      "Epoch: 1 / 3, Step: 602 / 2250 Loss: 0.7272\n",
      "Epoch: 1 / 3, Step: 603 / 2250 Loss: 0.6186\n",
      "Epoch: 1 / 3, Step: 604 / 2250 Loss: 0.5914\n",
      "Epoch: 1 / 3, Step: 605 / 2250 Loss: 0.6378\n",
      "Epoch: 1 / 3, Step: 606 / 2250 Loss: 0.3482\n",
      "Epoch: 1 / 3, Step: 607 / 2250 Loss: 0.4783\n",
      "Epoch: 1 / 3, Step: 608 / 2250 Loss: 0.7251\n",
      "Epoch: 1 / 3, Step: 609 / 2250 Loss: 0.6017\n",
      "Epoch: 1 / 3, Step: 610 / 2250 Loss: 0.9161\n",
      "Epoch: 1 / 3, Step: 611 / 2250 Loss: 0.6377\n",
      "Epoch: 1 / 3, Step: 612 / 2250 Loss: 0.8168\n",
      "Epoch: 1 / 3, Step: 613 / 2250 Loss: 0.6146\n",
      "Epoch: 1 / 3, Step: 614 / 2250 Loss: 0.8375\n",
      "Epoch: 1 / 3, Step: 615 / 2250 Loss: 0.7779\n",
      "Epoch: 1 / 3, Step: 616 / 2250 Loss: 0.8975\n",
      "Epoch: 1 / 3, Step: 617 / 2250 Loss: 0.4028\n",
      "Epoch: 1 / 3, Step: 618 / 2250 Loss: 0.6664\n",
      "Epoch: 1 / 3, Step: 619 / 2250 Loss: 0.6360\n",
      "Epoch: 1 / 3, Step: 620 / 2250 Loss: 0.5247\n",
      "Epoch: 1 / 3, Step: 621 / 2250 Loss: 0.7365\n",
      "Epoch: 1 / 3, Step: 622 / 2250 Loss: 0.7413\n",
      "Epoch: 1 / 3, Step: 623 / 2250 Loss: 0.5931\n",
      "Epoch: 1 / 3, Step: 624 / 2250 Loss: 0.4410\n",
      "Epoch: 1 / 3, Step: 625 / 2250 Loss: 0.5464\n",
      "Epoch: 1 / 3, Step: 626 / 2250 Loss: 0.6232\n",
      "Epoch: 1 / 3, Step: 627 / 2250 Loss: 0.6693\n",
      "Epoch: 1 / 3, Step: 628 / 2250 Loss: 0.8079\n",
      "Epoch: 1 / 3, Step: 629 / 2250 Loss: 0.6207\n",
      "Epoch: 1 / 3, Step: 630 / 2250 Loss: 0.6146\n",
      "Epoch: 1 / 3, Step: 631 / 2250 Loss: 0.4883\n",
      "Epoch: 1 / 3, Step: 632 / 2250 Loss: 0.7491\n",
      "Epoch: 1 / 3, Step: 633 / 2250 Loss: 0.7234\n",
      "Epoch: 1 / 3, Step: 634 / 2250 Loss: 0.7471\n",
      "Epoch: 1 / 3, Step: 635 / 2250 Loss: 0.8811\n",
      "Epoch: 1 / 3, Step: 636 / 2250 Loss: 0.6113\n",
      "Epoch: 1 / 3, Step: 637 / 2250 Loss: 0.5691\n",
      "Epoch: 1 / 3, Step: 638 / 2250 Loss: 0.5054\n",
      "Epoch: 1 / 3, Step: 639 / 2250 Loss: 0.6934\n",
      "Epoch: 1 / 3, Step: 640 / 2250 Loss: 0.8177\n",
      "Epoch: 1 / 3, Step: 641 / 2250 Loss: 0.7133\n",
      "Epoch: 1 / 3, Step: 642 / 2250 Loss: 0.6695\n",
      "Epoch: 1 / 3, Step: 643 / 2250 Loss: 0.5764\n",
      "Epoch: 1 / 3, Step: 644 / 2250 Loss: 0.5948\n",
      "Epoch: 1 / 3, Step: 645 / 2250 Loss: 0.5899\n",
      "Epoch: 1 / 3, Step: 646 / 2250 Loss: 0.5943\n",
      "Epoch: 1 / 3, Step: 647 / 2250 Loss: 0.5519\n",
      "Epoch: 1 / 3, Step: 648 / 2250 Loss: 0.7265\n",
      "Epoch: 1 / 3, Step: 649 / 2250 Loss: 0.9763\n",
      "Epoch: 1 / 3, Step: 650 / 2250 Loss: 0.6816\n",
      "Epoch: 1 / 3, Step: 651 / 2250 Loss: 0.4793\n",
      "Epoch: 1 / 3, Step: 652 / 2250 Loss: 0.5857\n",
      "Epoch: 1 / 3, Step: 653 / 2250 Loss: 0.5185\n",
      "Epoch: 1 / 3, Step: 654 / 2250 Loss: 0.6696\n",
      "Epoch: 1 / 3, Step: 655 / 2250 Loss: 0.6913\n",
      "Epoch: 1 / 3, Step: 656 / 2250 Loss: 0.8481\n",
      "Epoch: 1 / 3, Step: 657 / 2250 Loss: 0.8724\n",
      "Epoch: 1 / 3, Step: 658 / 2250 Loss: 0.5633\n",
      "Epoch: 1 / 3, Step: 659 / 2250 Loss: 0.7067\n",
      "Epoch: 1 / 3, Step: 660 / 2250 Loss: 0.5061\n",
      "Epoch: 1 / 3, Step: 661 / 2250 Loss: 0.5019\n",
      "Epoch: 1 / 3, Step: 662 / 2250 Loss: 0.5754\n",
      "Epoch: 1 / 3, Step: 663 / 2250 Loss: 0.8583\n",
      "Epoch: 1 / 3, Step: 664 / 2250 Loss: 0.6288\n",
      "Epoch: 1 / 3, Step: 665 / 2250 Loss: 0.6667\n",
      "Epoch: 1 / 3, Step: 666 / 2250 Loss: 0.5888\n",
      "Epoch: 1 / 3, Step: 667 / 2250 Loss: 0.7302\n",
      "Epoch: 1 / 3, Step: 668 / 2250 Loss: 0.7095\n",
      "Epoch: 1 / 3, Step: 669 / 2250 Loss: 0.5669\n",
      "Epoch: 1 / 3, Step: 670 / 2250 Loss: 0.6237\n",
      "Epoch: 1 / 3, Step: 671 / 2250 Loss: 0.7554\n",
      "Epoch: 1 / 3, Step: 672 / 2250 Loss: 0.6253\n",
      "Epoch: 1 / 3, Step: 673 / 2250 Loss: 0.6978\n",
      "Epoch: 1 / 3, Step: 674 / 2250 Loss: 0.7245\n",
      "Epoch: 1 / 3, Step: 675 / 2250 Loss: 0.7205\n",
      "Epoch: 1 / 3, Step: 676 / 2250 Loss: 0.5615\n",
      "Epoch: 1 / 3, Step: 677 / 2250 Loss: 0.8177\n",
      "Epoch: 1 / 3, Step: 678 / 2250 Loss: 0.8236\n",
      "Epoch: 1 / 3, Step: 679 / 2250 Loss: 0.6495\n",
      "Epoch: 1 / 3, Step: 680 / 2250 Loss: 0.8082\n",
      "Epoch: 1 / 3, Step: 681 / 2250 Loss: 0.6025\n",
      "Epoch: 1 / 3, Step: 682 / 2250 Loss: 0.6026\n",
      "Epoch: 1 / 3, Step: 683 / 2250 Loss: 0.4313\n",
      "Epoch: 1 / 3, Step: 684 / 2250 Loss: 0.6618\n",
      "Epoch: 1 / 3, Step: 685 / 2250 Loss: 0.6510\n",
      "Epoch: 1 / 3, Step: 686 / 2250 Loss: 0.6387\n",
      "Epoch: 1 / 3, Step: 687 / 2250 Loss: 0.9350\n",
      "Epoch: 1 / 3, Step: 688 / 2250 Loss: 0.7371\n",
      "Epoch: 1 / 3, Step: 689 / 2250 Loss: 0.7288\n",
      "Epoch: 1 / 3, Step: 690 / 2250 Loss: 0.2838\n",
      "Epoch: 1 / 3, Step: 691 / 2250 Loss: 0.5905\n",
      "Epoch: 1 / 3, Step: 692 / 2250 Loss: 0.6576\n",
      "Epoch: 1 / 3, Step: 693 / 2250 Loss: 0.7300\n",
      "Epoch: 1 / 3, Step: 694 / 2250 Loss: 0.5882\n",
      "Epoch: 1 / 3, Step: 695 / 2250 Loss: 0.5184\n",
      "Epoch: 1 / 3, Step: 696 / 2250 Loss: 0.6041\n",
      "Epoch: 1 / 3, Step: 697 / 2250 Loss: 0.4667\n",
      "Epoch: 1 / 3, Step: 698 / 2250 Loss: 0.6661\n",
      "Epoch: 1 / 3, Step: 699 / 2250 Loss: 0.5277\n",
      "Epoch: 1 / 3, Step: 700 / 2250 Loss: 0.5032\n",
      "Epoch: 1 / 3, Step: 701 / 2250 Loss: 0.7294\n",
      "Epoch: 1 / 3, Step: 702 / 2250 Loss: 0.5415\n",
      "Epoch: 1 / 3, Step: 703 / 2250 Loss: 0.4106\n",
      "Epoch: 1 / 3, Step: 704 / 2250 Loss: 0.4581\n",
      "Epoch: 1 / 3, Step: 705 / 2250 Loss: 0.5769\n",
      "Epoch: 1 / 3, Step: 706 / 2250 Loss: 0.4924\n",
      "Epoch: 1 / 3, Step: 707 / 2250 Loss: 0.7538\n",
      "Epoch: 1 / 3, Step: 708 / 2250 Loss: 0.4807\n",
      "Epoch: 1 / 3, Step: 709 / 2250 Loss: 0.7335\n",
      "Epoch: 1 / 3, Step: 710 / 2250 Loss: 0.5346\n",
      "Epoch: 1 / 3, Step: 711 / 2250 Loss: 0.7888\n",
      "Epoch: 1 / 3, Step: 712 / 2250 Loss: 0.5758\n",
      "Epoch: 1 / 3, Step: 713 / 2250 Loss: 0.6683\n",
      "Epoch: 1 / 3, Step: 714 / 2250 Loss: 0.7666\n",
      "Epoch: 1 / 3, Step: 715 / 2250 Loss: 0.5580\n",
      "Epoch: 1 / 3, Step: 716 / 2250 Loss: 0.7855\n",
      "Epoch: 1 / 3, Step: 717 / 2250 Loss: 0.8913\n",
      "Epoch: 1 / 3, Step: 718 / 2250 Loss: 0.7906\n",
      "Epoch: 1 / 3, Step: 719 / 2250 Loss: 0.7257\n",
      "Epoch: 1 / 3, Step: 720 / 2250 Loss: 0.4706\n",
      "Epoch: 1 / 3, Step: 721 / 2250 Loss: 0.7365\n",
      "Epoch: 1 / 3, Step: 722 / 2250 Loss: 0.5590\n",
      "Epoch: 1 / 3, Step: 723 / 2250 Loss: 0.6396\n",
      "Epoch: 1 / 3, Step: 724 / 2250 Loss: 0.6314\n",
      "Epoch: 1 / 3, Step: 725 / 2250 Loss: 0.6110\n",
      "Epoch: 1 / 3, Step: 726 / 2250 Loss: 0.6638\n",
      "Epoch: 1 / 3, Step: 727 / 2250 Loss: 0.5655\n",
      "Epoch: 1 / 3, Step: 728 / 2250 Loss: 0.8748\n",
      "Epoch: 1 / 3, Step: 729 / 2250 Loss: 0.7326\n",
      "Epoch: 1 / 3, Step: 730 / 2250 Loss: 0.5432\n",
      "Epoch: 1 / 3, Step: 731 / 2250 Loss: 0.5326\n",
      "Epoch: 1 / 3, Step: 732 / 2250 Loss: 0.5766\n",
      "Epoch: 1 / 3, Step: 733 / 2250 Loss: 0.6402\n",
      "Epoch: 1 / 3, Step: 734 / 2250 Loss: 0.5077\n",
      "Epoch: 1 / 3, Step: 735 / 2250 Loss: 0.7948\n",
      "Epoch: 1 / 3, Step: 736 / 2250 Loss: 0.6429\n",
      "Epoch: 1 / 3, Step: 737 / 2250 Loss: 0.6819\n",
      "Epoch: 1 / 3, Step: 738 / 2250 Loss: 0.7142\n",
      "Epoch: 1 / 3, Step: 739 / 2250 Loss: 0.8141\n",
      "Epoch: 1 / 3, Step: 740 / 2250 Loss: 0.6524\n",
      "Epoch: 1 / 3, Step: 741 / 2250 Loss: 0.6741\n",
      "Epoch: 1 / 3, Step: 742 / 2250 Loss: 0.9374\n",
      "Epoch: 1 / 3, Step: 743 / 2250 Loss: 0.8623\n",
      "Epoch: 1 / 3, Step: 744 / 2250 Loss: 0.6125\n",
      "Epoch: 1 / 3, Step: 745 / 2250 Loss: 0.6634\n",
      "Epoch: 1 / 3, Step: 746 / 2250 Loss: 0.7215\n",
      "Epoch: 1 / 3, Step: 747 / 2250 Loss: 0.4563\n",
      "Epoch: 1 / 3, Step: 748 / 2250 Loss: 0.5479\n",
      "Epoch: 1 / 3, Step: 749 / 2250 Loss: 0.9371\n",
      "Epoch: 1 / 3, Step: 750 / 2250 Loss: 0.5075\n",
      "Epoch: 1 / 3, Step: 751 / 2250 Loss: 0.7171\n",
      "Epoch: 1 / 3, Step: 752 / 2250 Loss: 0.6020\n",
      "Epoch: 1 / 3, Step: 753 / 2250 Loss: 0.6364\n",
      "Epoch: 1 / 3, Step: 754 / 2250 Loss: 0.6811\n",
      "Epoch: 1 / 3, Step: 755 / 2250 Loss: 0.7046\n",
      "Epoch: 1 / 3, Step: 756 / 2250 Loss: 0.5456\n",
      "Epoch: 1 / 3, Step: 757 / 2250 Loss: 0.8086\n",
      "Epoch: 1 / 3, Step: 758 / 2250 Loss: 0.6403\n",
      "Epoch: 1 / 3, Step: 759 / 2250 Loss: 0.6251\n",
      "Epoch: 1 / 3, Step: 760 / 2250 Loss: 0.6226\n",
      "Epoch: 1 / 3, Step: 761 / 2250 Loss: 0.8585\n",
      "Epoch: 1 / 3, Step: 762 / 2250 Loss: 0.6251\n",
      "Epoch: 1 / 3, Step: 763 / 2250 Loss: 0.7425\n",
      "Epoch: 1 / 3, Step: 764 / 2250 Loss: 0.6334\n",
      "Epoch: 1 / 3, Step: 765 / 2250 Loss: 0.5233\n",
      "Epoch: 1 / 3, Step: 766 / 2250 Loss: 0.7075\n",
      "Epoch: 1 / 3, Step: 767 / 2250 Loss: 0.6905\n",
      "Epoch: 1 / 3, Step: 768 / 2250 Loss: 0.7060\n",
      "Epoch: 1 / 3, Step: 769 / 2250 Loss: 0.8105\n",
      "Epoch: 1 / 3, Step: 770 / 2250 Loss: 0.5752\n",
      "Epoch: 1 / 3, Step: 771 / 2250 Loss: 0.8014\n",
      "Epoch: 1 / 3, Step: 772 / 2250 Loss: 0.5552\n",
      "Epoch: 1 / 3, Step: 773 / 2250 Loss: 0.5248\n",
      "Epoch: 1 / 3, Step: 774 / 2250 Loss: 0.6339\n",
      "Epoch: 1 / 3, Step: 775 / 2250 Loss: 0.3097\n",
      "Epoch: 1 / 3, Step: 776 / 2250 Loss: 0.4814\n",
      "Epoch: 1 / 3, Step: 777 / 2250 Loss: 1.0230\n",
      "Epoch: 1 / 3, Step: 778 / 2250 Loss: 0.7208\n",
      "Epoch: 1 / 3, Step: 779 / 2250 Loss: 0.4149\n",
      "Epoch: 1 / 3, Step: 780 / 2250 Loss: 0.3148\n",
      "Epoch: 1 / 3, Step: 781 / 2250 Loss: 0.9618\n",
      "Epoch: 1 / 3, Step: 782 / 2250 Loss: 0.3895\n",
      "Epoch: 1 / 3, Step: 783 / 2250 Loss: 0.6344\n",
      "Epoch: 1 / 3, Step: 784 / 2250 Loss: 0.8771\n",
      "Epoch: 1 / 3, Step: 785 / 2250 Loss: 0.5033\n",
      "Epoch: 1 / 3, Step: 786 / 2250 Loss: 0.6222\n",
      "Epoch: 1 / 3, Step: 787 / 2250 Loss: 0.5918\n",
      "Epoch: 1 / 3, Step: 788 / 2250 Loss: 0.5630\n",
      "Epoch: 1 / 3, Step: 789 / 2250 Loss: 0.7227\n",
      "Epoch: 1 / 3, Step: 790 / 2250 Loss: 1.0834\n",
      "Epoch: 1 / 3, Step: 791 / 2250 Loss: 0.7836\n",
      "Epoch: 1 / 3, Step: 792 / 2250 Loss: 0.4427\n",
      "Epoch: 1 / 3, Step: 793 / 2250 Loss: 0.7643\n",
      "Epoch: 1 / 3, Step: 794 / 2250 Loss: 0.6325\n",
      "Epoch: 1 / 3, Step: 795 / 2250 Loss: 0.5805\n",
      "Epoch: 1 / 3, Step: 796 / 2250 Loss: 0.4719\n",
      "Epoch: 1 / 3, Step: 797 / 2250 Loss: 0.7728\n",
      "Epoch: 1 / 3, Step: 798 / 2250 Loss: 0.6878\n",
      "Epoch: 1 / 3, Step: 799 / 2250 Loss: 0.6097\n",
      "Epoch: 1 / 3, Step: 800 / 2250 Loss: 0.6431\n",
      "Epoch: 1 / 3, Step: 801 / 2250 Loss: 0.5933\n",
      "Epoch: 1 / 3, Step: 802 / 2250 Loss: 0.6313\n",
      "Epoch: 1 / 3, Step: 803 / 2250 Loss: 0.6343\n",
      "Epoch: 1 / 3, Step: 804 / 2250 Loss: 0.7800\n",
      "Epoch: 1 / 3, Step: 805 / 2250 Loss: 0.5119\n",
      "Epoch: 1 / 3, Step: 806 / 2250 Loss: 0.8138\n",
      "Epoch: 1 / 3, Step: 807 / 2250 Loss: 0.7907\n",
      "Epoch: 1 / 3, Step: 808 / 2250 Loss: 0.7177\n",
      "Epoch: 1 / 3, Step: 809 / 2250 Loss: 0.8222\n",
      "Epoch: 1 / 3, Step: 810 / 2250 Loss: 0.6338\n",
      "Epoch: 1 / 3, Step: 811 / 2250 Loss: 0.4984\n",
      "Epoch: 1 / 3, Step: 812 / 2250 Loss: 0.3486\n",
      "Epoch: 1 / 3, Step: 813 / 2250 Loss: 0.7995\n",
      "Epoch: 1 / 3, Step: 814 / 2250 Loss: 0.6887\n",
      "Epoch: 1 / 3, Step: 815 / 2250 Loss: 1.0030\n",
      "Epoch: 1 / 3, Step: 816 / 2250 Loss: 0.5761\n",
      "Epoch: 1 / 3, Step: 817 / 2250 Loss: 0.5544\n",
      "Epoch: 1 / 3, Step: 818 / 2250 Loss: 0.6565\n",
      "Epoch: 1 / 3, Step: 819 / 2250 Loss: 0.7325\n",
      "Epoch: 1 / 3, Step: 820 / 2250 Loss: 0.6310\n",
      "Epoch: 1 / 3, Step: 821 / 2250 Loss: 0.5451\n",
      "Epoch: 1 / 3, Step: 822 / 2250 Loss: 0.6753\n",
      "Epoch: 1 / 3, Step: 823 / 2250 Loss: 0.7568\n",
      "Epoch: 1 / 3, Step: 824 / 2250 Loss: 0.5851\n",
      "Epoch: 1 / 3, Step: 825 / 2250 Loss: 0.7801\n",
      "Epoch: 1 / 3, Step: 826 / 2250 Loss: 0.5909\n",
      "Epoch: 1 / 3, Step: 827 / 2250 Loss: 0.6651\n",
      "Epoch: 1 / 3, Step: 828 / 2250 Loss: 0.6779\n",
      "Epoch: 1 / 3, Step: 829 / 2250 Loss: 0.6637\n",
      "Epoch: 1 / 3, Step: 830 / 2250 Loss: 0.6231\n",
      "Epoch: 1 / 3, Step: 831 / 2250 Loss: 0.6852\n",
      "Epoch: 1 / 3, Step: 832 / 2250 Loss: 0.6741\n",
      "Epoch: 1 / 3, Step: 833 / 2250 Loss: 0.5399\n",
      "Epoch: 1 / 3, Step: 834 / 2250 Loss: 0.6214\n",
      "Epoch: 1 / 3, Step: 835 / 2250 Loss: 0.4332\n",
      "Epoch: 1 / 3, Step: 836 / 2250 Loss: 0.7044\n",
      "Epoch: 1 / 3, Step: 837 / 2250 Loss: 0.6624\n",
      "Epoch: 1 / 3, Step: 838 / 2250 Loss: 0.6764\n",
      "Epoch: 1 / 3, Step: 839 / 2250 Loss: 0.5167\n",
      "Epoch: 1 / 3, Step: 840 / 2250 Loss: 0.5174\n",
      "Epoch: 1 / 3, Step: 841 / 2250 Loss: 0.5559\n",
      "Epoch: 1 / 3, Step: 842 / 2250 Loss: 0.6565\n",
      "Epoch: 1 / 3, Step: 843 / 2250 Loss: 0.9003\n",
      "Epoch: 1 / 3, Step: 844 / 2250 Loss: 0.6665\n",
      "Epoch: 1 / 3, Step: 845 / 2250 Loss: 0.6404\n",
      "Epoch: 1 / 3, Step: 846 / 2250 Loss: 0.8500\n",
      "Epoch: 1 / 3, Step: 847 / 2250 Loss: 0.6340\n",
      "Epoch: 1 / 3, Step: 848 / 2250 Loss: 0.7499\n",
      "Epoch: 1 / 3, Step: 849 / 2250 Loss: 0.7517\n",
      "Epoch: 1 / 3, Step: 850 / 2250 Loss: 0.5803\n",
      "Epoch: 1 / 3, Step: 851 / 2250 Loss: 0.5040\n",
      "Epoch: 1 / 3, Step: 852 / 2250 Loss: 0.7403\n",
      "Epoch: 1 / 3, Step: 853 / 2250 Loss: 0.7486\n",
      "Epoch: 1 / 3, Step: 854 / 2250 Loss: 0.5524\n",
      "Epoch: 1 / 3, Step: 855 / 2250 Loss: 0.4622\n",
      "Epoch: 1 / 3, Step: 856 / 2250 Loss: 0.5798\n",
      "Epoch: 1 / 3, Step: 857 / 2250 Loss: 0.8295\n",
      "Epoch: 1 / 3, Step: 858 / 2250 Loss: 0.8899\n",
      "Epoch: 1 / 3, Step: 859 / 2250 Loss: 0.6634\n",
      "Epoch: 1 / 3, Step: 860 / 2250 Loss: 0.4283\n",
      "Epoch: 1 / 3, Step: 861 / 2250 Loss: 0.8460\n",
      "Epoch: 1 / 3, Step: 862 / 2250 Loss: 0.6840\n",
      "Epoch: 1 / 3, Step: 863 / 2250 Loss: 0.6733\n",
      "Epoch: 1 / 3, Step: 864 / 2250 Loss: 0.5242\n",
      "Epoch: 1 / 3, Step: 865 / 2250 Loss: 0.6959\n",
      "Epoch: 1 / 3, Step: 866 / 2250 Loss: 0.4994\n",
      "Epoch: 1 / 3, Step: 867 / 2250 Loss: 0.7864\n",
      "Epoch: 1 / 3, Step: 868 / 2250 Loss: 0.5817\n",
      "Epoch: 1 / 3, Step: 869 / 2250 Loss: 1.0138\n",
      "Epoch: 1 / 3, Step: 870 / 2250 Loss: 0.5268\n",
      "Epoch: 1 / 3, Step: 871 / 2250 Loss: 0.6706\n",
      "Epoch: 1 / 3, Step: 872 / 2250 Loss: 0.5974\n",
      "Epoch: 1 / 3, Step: 873 / 2250 Loss: 0.7481\n",
      "Epoch: 1 / 3, Step: 874 / 2250 Loss: 0.6494\n",
      "Epoch: 1 / 3, Step: 875 / 2250 Loss: 0.5537\n",
      "Epoch: 1 / 3, Step: 876 / 2250 Loss: 0.7137\n",
      "Epoch: 1 / 3, Step: 877 / 2250 Loss: 0.6438\n",
      "Epoch: 1 / 3, Step: 878 / 2250 Loss: 0.8461\n",
      "Epoch: 1 / 3, Step: 879 / 2250 Loss: 0.6416\n",
      "Epoch: 1 / 3, Step: 880 / 2250 Loss: 0.4353\n",
      "Epoch: 1 / 3, Step: 881 / 2250 Loss: 0.4876\n",
      "Epoch: 1 / 3, Step: 882 / 2250 Loss: 0.6481\n",
      "Epoch: 1 / 3, Step: 883 / 2250 Loss: 0.6038\n",
      "Epoch: 1 / 3, Step: 884 / 2250 Loss: 0.3616\n",
      "Epoch: 1 / 3, Step: 885 / 2250 Loss: 0.6830\n",
      "Epoch: 1 / 3, Step: 886 / 2250 Loss: 0.6821\n",
      "Epoch: 1 / 3, Step: 887 / 2250 Loss: 0.5130\n",
      "Epoch: 1 / 3, Step: 888 / 2250 Loss: 0.5413\n",
      "Epoch: 1 / 3, Step: 889 / 2250 Loss: 0.5734\n",
      "Epoch: 1 / 3, Step: 890 / 2250 Loss: 0.5629\n",
      "Epoch: 1 / 3, Step: 891 / 2250 Loss: 0.5935\n",
      "Epoch: 1 / 3, Step: 892 / 2250 Loss: 0.6860\n",
      "Epoch: 1 / 3, Step: 893 / 2250 Loss: 0.5873\n",
      "Epoch: 1 / 3, Step: 894 / 2250 Loss: 0.5534\n",
      "Epoch: 1 / 3, Step: 895 / 2250 Loss: 0.6359\n",
      "Epoch: 1 / 3, Step: 896 / 2250 Loss: 0.5323\n",
      "Epoch: 1 / 3, Step: 897 / 2250 Loss: 0.7220\n",
      "Epoch: 1 / 3, Step: 898 / 2250 Loss: 0.7208\n",
      "Epoch: 1 / 3, Step: 899 / 2250 Loss: 0.8313\n",
      "Epoch: 1 / 3, Step: 900 / 2250 Loss: 0.8030\n",
      "Epoch: 1 / 3, Step: 901 / 2250 Loss: 0.5070\n",
      "Epoch: 1 / 3, Step: 902 / 2250 Loss: 0.6123\n",
      "Epoch: 1 / 3, Step: 903 / 2250 Loss: 0.5393\n",
      "Epoch: 1 / 3, Step: 904 / 2250 Loss: 0.6463\n",
      "Epoch: 1 / 3, Step: 905 / 2250 Loss: 0.8463\n",
      "Epoch: 1 / 3, Step: 906 / 2250 Loss: 0.8555\n",
      "Epoch: 1 / 3, Step: 907 / 2250 Loss: 0.7764\n",
      "Epoch: 1 / 3, Step: 908 / 2250 Loss: 0.4983\n",
      "Epoch: 1 / 3, Step: 909 / 2250 Loss: 0.7013\n",
      "Epoch: 1 / 3, Step: 910 / 2250 Loss: 0.6807\n",
      "Epoch: 1 / 3, Step: 911 / 2250 Loss: 0.5494\n",
      "Epoch: 1 / 3, Step: 912 / 2250 Loss: 0.4619\n",
      "Epoch: 1 / 3, Step: 913 / 2250 Loss: 0.4950\n",
      "Epoch: 1 / 3, Step: 914 / 2250 Loss: 0.5765\n",
      "Epoch: 1 / 3, Step: 915 / 2250 Loss: 0.6751\n",
      "Epoch: 1 / 3, Step: 916 / 2250 Loss: 0.5141\n",
      "Epoch: 1 / 3, Step: 917 / 2250 Loss: 0.3964\n",
      "Epoch: 1 / 3, Step: 918 / 2250 Loss: 0.7650\n",
      "Epoch: 1 / 3, Step: 919 / 2250 Loss: 0.6701\n",
      "Epoch: 1 / 3, Step: 920 / 2250 Loss: 0.5418\n",
      "Epoch: 1 / 3, Step: 921 / 2250 Loss: 0.8325\n",
      "Epoch: 1 / 3, Step: 922 / 2250 Loss: 0.6274\n",
      "Epoch: 1 / 3, Step: 923 / 2250 Loss: 0.5946\n",
      "Epoch: 1 / 3, Step: 924 / 2250 Loss: 0.7363\n",
      "Epoch: 1 / 3, Step: 925 / 2250 Loss: 0.8623\n",
      "Epoch: 1 / 3, Step: 926 / 2250 Loss: 0.4598\n",
      "Epoch: 1 / 3, Step: 927 / 2250 Loss: 0.6043\n",
      "Epoch: 1 / 3, Step: 928 / 2250 Loss: 0.9134\n",
      "Epoch: 1 / 3, Step: 929 / 2250 Loss: 0.8460\n",
      "Epoch: 1 / 3, Step: 930 / 2250 Loss: 0.5411\n",
      "Epoch: 1 / 3, Step: 931 / 2250 Loss: 0.6926\n",
      "Epoch: 1 / 3, Step: 932 / 2250 Loss: 0.5017\n",
      "Epoch: 1 / 3, Step: 933 / 2250 Loss: 0.6265\n",
      "Epoch: 1 / 3, Step: 934 / 2250 Loss: 0.4784\n",
      "Epoch: 1 / 3, Step: 935 / 2250 Loss: 0.8016\n",
      "Epoch: 1 / 3, Step: 936 / 2250 Loss: 0.5493\n",
      "Epoch: 1 / 3, Step: 937 / 2250 Loss: 0.6901\n",
      "Epoch: 1 / 3, Step: 938 / 2250 Loss: 0.7084\n",
      "Epoch: 1 / 3, Step: 939 / 2250 Loss: 0.4491\n",
      "Epoch: 1 / 3, Step: 940 / 2250 Loss: 0.5771\n",
      "Epoch: 1 / 3, Step: 941 / 2250 Loss: 0.7981\n",
      "Epoch: 1 / 3, Step: 942 / 2250 Loss: 0.6260\n",
      "Epoch: 1 / 3, Step: 943 / 2250 Loss: 0.4943\n",
      "Epoch: 1 / 3, Step: 944 / 2250 Loss: 0.8609\n",
      "Epoch: 1 / 3, Step: 945 / 2250 Loss: 0.7320\n",
      "Epoch: 1 / 3, Step: 946 / 2250 Loss: 0.4628\n",
      "Epoch: 1 / 3, Step: 947 / 2250 Loss: 0.6479\n",
      "Epoch: 1 / 3, Step: 948 / 2250 Loss: 0.7891\n",
      "Epoch: 1 / 3, Step: 949 / 2250 Loss: 0.6177\n",
      "Epoch: 1 / 3, Step: 950 / 2250 Loss: 0.5983\n",
      "Epoch: 1 / 3, Step: 951 / 2250 Loss: 0.6099\n",
      "Epoch: 1 / 3, Step: 952 / 2250 Loss: 0.4843\n",
      "Epoch: 1 / 3, Step: 953 / 2250 Loss: 0.6050\n",
      "Epoch: 1 / 3, Step: 954 / 2250 Loss: 0.4397\n",
      "Epoch: 1 / 3, Step: 955 / 2250 Loss: 0.7993\n",
      "Epoch: 1 / 3, Step: 956 / 2250 Loss: 0.5094\n",
      "Epoch: 1 / 3, Step: 957 / 2250 Loss: 0.5185\n",
      "Epoch: 1 / 3, Step: 958 / 2250 Loss: 0.5219\n",
      "Epoch: 1 / 3, Step: 959 / 2250 Loss: 0.8493\n",
      "Epoch: 1 / 3, Step: 960 / 2250 Loss: 0.7087\n",
      "Epoch: 1 / 3, Step: 961 / 2250 Loss: 0.7671\n",
      "Epoch: 1 / 3, Step: 962 / 2250 Loss: 0.6074\n",
      "Epoch: 1 / 3, Step: 963 / 2250 Loss: 0.5243\n",
      "Epoch: 1 / 3, Step: 964 / 2250 Loss: 0.5356\n",
      "Epoch: 1 / 3, Step: 965 / 2250 Loss: 0.4524\n",
      "Epoch: 1 / 3, Step: 966 / 2250 Loss: 0.8021\n",
      "Epoch: 1 / 3, Step: 967 / 2250 Loss: 0.5922\n",
      "Epoch: 1 / 3, Step: 968 / 2250 Loss: 0.7144\n",
      "Epoch: 1 / 3, Step: 969 / 2250 Loss: 0.6271\n",
      "Epoch: 1 / 3, Step: 970 / 2250 Loss: 0.6467\n",
      "Epoch: 1 / 3, Step: 971 / 2250 Loss: 0.4721\n",
      "Epoch: 1 / 3, Step: 972 / 2250 Loss: 0.7310\n",
      "Epoch: 1 / 3, Step: 973 / 2250 Loss: 0.4648\n",
      "Epoch: 1 / 3, Step: 974 / 2250 Loss: 0.4564\n",
      "Epoch: 1 / 3, Step: 975 / 2250 Loss: 0.5100\n",
      "Epoch: 1 / 3, Step: 976 / 2250 Loss: 0.7554\n",
      "Epoch: 1 / 3, Step: 977 / 2250 Loss: 0.7471\n",
      "Epoch: 1 / 3, Step: 978 / 2250 Loss: 0.6502\n",
      "Epoch: 1 / 3, Step: 979 / 2250 Loss: 0.5635\n",
      "Epoch: 1 / 3, Step: 980 / 2250 Loss: 0.7635\n",
      "Epoch: 1 / 3, Step: 981 / 2250 Loss: 0.3593\n",
      "Epoch: 1 / 3, Step: 982 / 2250 Loss: 0.3743\n",
      "Epoch: 1 / 3, Step: 983 / 2250 Loss: 0.5508\n",
      "Epoch: 1 / 3, Step: 984 / 2250 Loss: 0.4189\n",
      "Epoch: 1 / 3, Step: 985 / 2250 Loss: 0.8415\n",
      "Epoch: 1 / 3, Step: 986 / 2250 Loss: 0.6240\n",
      "Epoch: 1 / 3, Step: 987 / 2250 Loss: 0.3580\n",
      "Epoch: 1 / 3, Step: 988 / 2250 Loss: 0.4967\n",
      "Epoch: 1 / 3, Step: 989 / 2250 Loss: 0.5750\n",
      "Epoch: 1 / 3, Step: 990 / 2250 Loss: 0.7915\n",
      "Epoch: 1 / 3, Step: 991 / 2250 Loss: 0.4427\n",
      "Epoch: 1 / 3, Step: 992 / 2250 Loss: 0.4264\n",
      "Epoch: 1 / 3, Step: 993 / 2250 Loss: 0.5769\n",
      "Epoch: 1 / 3, Step: 994 / 2250 Loss: 0.4372\n",
      "Epoch: 1 / 3, Step: 995 / 2250 Loss: 0.4156\n",
      "Epoch: 1 / 3, Step: 996 / 2250 Loss: 0.4919\n",
      "Epoch: 1 / 3, Step: 997 / 2250 Loss: 0.4058\n",
      "Epoch: 1 / 3, Step: 998 / 2250 Loss: 0.3641\n",
      "Epoch: 1 / 3, Step: 999 / 2250 Loss: 0.5619\n",
      "Epoch: 1 / 3, Step: 1000 / 2250 Loss: 0.4524\n",
      "Epoch: 1 / 3, Step: 1001 / 2250 Loss: 0.5463\n",
      "Epoch: 1 / 3, Step: 1002 / 2250 Loss: 0.6092\n",
      "Epoch: 1 / 3, Step: 1003 / 2250 Loss: 0.7230\n",
      "Epoch: 1 / 3, Step: 1004 / 2250 Loss: 0.4585\n",
      "Epoch: 1 / 3, Step: 1005 / 2250 Loss: 0.6369\n",
      "Epoch: 1 / 3, Step: 1006 / 2250 Loss: 0.4062\n",
      "Epoch: 1 / 3, Step: 1007 / 2250 Loss: 0.5771\n",
      "Epoch: 1 / 3, Step: 1008 / 2250 Loss: 0.6304\n",
      "Epoch: 1 / 3, Step: 1009 / 2250 Loss: 0.4546\n",
      "Epoch: 1 / 3, Step: 1010 / 2250 Loss: 0.3073\n",
      "Epoch: 1 / 3, Step: 1011 / 2250 Loss: 0.6420\n",
      "Epoch: 1 / 3, Step: 1012 / 2250 Loss: 0.4797\n",
      "Epoch: 1 / 3, Step: 1013 / 2250 Loss: 0.3904\n",
      "Epoch: 1 / 3, Step: 1014 / 2250 Loss: 0.5729\n",
      "Epoch: 1 / 3, Step: 1015 / 2250 Loss: 0.3856\n",
      "Epoch: 1 / 3, Step: 1016 / 2250 Loss: 0.5551\n",
      "Epoch: 1 / 3, Step: 1017 / 2250 Loss: 0.3990\n",
      "Epoch: 1 / 3, Step: 1018 / 2250 Loss: 0.5611\n",
      "Epoch: 1 / 3, Step: 1019 / 2250 Loss: 0.7613\n",
      "Epoch: 1 / 3, Step: 1020 / 2250 Loss: 0.5860\n",
      "Epoch: 1 / 3, Step: 1021 / 2250 Loss: 0.5342\n",
      "Epoch: 1 / 3, Step: 1022 / 2250 Loss: 0.4785\n",
      "Epoch: 1 / 3, Step: 1023 / 2250 Loss: 0.3962\n",
      "Epoch: 1 / 3, Step: 1024 / 2250 Loss: 0.7540\n",
      "Epoch: 1 / 3, Step: 1025 / 2250 Loss: 0.5522\n",
      "Epoch: 1 / 3, Step: 1026 / 2250 Loss: 0.5582\n",
      "Epoch: 1 / 3, Step: 1027 / 2250 Loss: 0.6844\n",
      "Epoch: 1 / 3, Step: 1028 / 2250 Loss: 0.3650\n",
      "Epoch: 1 / 3, Step: 1029 / 2250 Loss: 0.5798\n",
      "Epoch: 1 / 3, Step: 1030 / 2250 Loss: 0.7358\n",
      "Epoch: 1 / 3, Step: 1031 / 2250 Loss: 0.6129\n",
      "Epoch: 1 / 3, Step: 1032 / 2250 Loss: 0.6036\n",
      "Epoch: 1 / 3, Step: 1033 / 2250 Loss: 0.6888\n",
      "Epoch: 1 / 3, Step: 1034 / 2250 Loss: 0.3811\n",
      "Epoch: 1 / 3, Step: 1035 / 2250 Loss: 0.4490\n",
      "Epoch: 1 / 3, Step: 1036 / 2250 Loss: 0.6525\n",
      "Epoch: 1 / 3, Step: 1037 / 2250 Loss: 0.5662\n",
      "Epoch: 1 / 3, Step: 1038 / 2250 Loss: 0.5578\n",
      "Epoch: 1 / 3, Step: 1039 / 2250 Loss: 0.7524\n",
      "Epoch: 1 / 3, Step: 1040 / 2250 Loss: 0.6764\n",
      "Epoch: 1 / 3, Step: 1041 / 2250 Loss: 0.3751\n",
      "Epoch: 1 / 3, Step: 1042 / 2250 Loss: 0.5328\n",
      "Epoch: 1 / 3, Step: 1043 / 2250 Loss: 0.4451\n",
      "Epoch: 1 / 3, Step: 1044 / 2250 Loss: 0.5672\n",
      "Epoch: 1 / 3, Step: 1045 / 2250 Loss: 0.6462\n",
      "Epoch: 1 / 3, Step: 1046 / 2250 Loss: 0.4824\n",
      "Epoch: 1 / 3, Step: 1047 / 2250 Loss: 0.5398\n",
      "Epoch: 1 / 3, Step: 1048 / 2250 Loss: 0.3668\n",
      "Epoch: 1 / 3, Step: 1049 / 2250 Loss: 0.4872\n",
      "Epoch: 1 / 3, Step: 1050 / 2250 Loss: 0.3542\n",
      "Epoch: 1 / 3, Step: 1051 / 2250 Loss: 0.5671\n",
      "Epoch: 1 / 3, Step: 1052 / 2250 Loss: 0.4696\n",
      "Epoch: 1 / 3, Step: 1053 / 2250 Loss: 0.6543\n",
      "Epoch: 1 / 3, Step: 1054 / 2250 Loss: 0.2742\n",
      "Epoch: 1 / 3, Step: 1055 / 2250 Loss: 0.5100\n",
      "Epoch: 1 / 3, Step: 1056 / 2250 Loss: 0.3623\n",
      "Epoch: 1 / 3, Step: 1057 / 2250 Loss: 0.4125\n",
      "Epoch: 1 / 3, Step: 1058 / 2250 Loss: 0.6294\n",
      "Epoch: 1 / 3, Step: 1059 / 2250 Loss: 0.2618\n",
      "Epoch: 1 / 3, Step: 1060 / 2250 Loss: 0.7020\n",
      "Epoch: 1 / 3, Step: 1061 / 2250 Loss: 0.5216\n",
      "Epoch: 1 / 3, Step: 1062 / 2250 Loss: 0.7753\n",
      "Epoch: 1 / 3, Step: 1063 / 2250 Loss: 0.4879\n",
      "Epoch: 1 / 3, Step: 1064 / 2250 Loss: 0.4068\n",
      "Epoch: 1 / 3, Step: 1065 / 2250 Loss: 0.3270\n",
      "Epoch: 1 / 3, Step: 1066 / 2250 Loss: 0.4994\n",
      "Epoch: 1 / 3, Step: 1067 / 2250 Loss: 0.4746\n",
      "Epoch: 1 / 3, Step: 1068 / 2250 Loss: 0.7198\n",
      "Epoch: 1 / 3, Step: 1069 / 2250 Loss: 0.9136\n",
      "Epoch: 1 / 3, Step: 1070 / 2250 Loss: 0.6431\n",
      "Epoch: 1 / 3, Step: 1071 / 2250 Loss: 0.3574\n",
      "Epoch: 1 / 3, Step: 1072 / 2250 Loss: 0.2872\n",
      "Epoch: 1 / 3, Step: 1073 / 2250 Loss: 0.3982\n",
      "Epoch: 1 / 3, Step: 1074 / 2250 Loss: 0.4799\n",
      "Epoch: 1 / 3, Step: 1075 / 2250 Loss: 0.3244\n",
      "Epoch: 1 / 3, Step: 1076 / 2250 Loss: 0.4259\n",
      "Epoch: 1 / 3, Step: 1077 / 2250 Loss: 0.5101\n",
      "Epoch: 1 / 3, Step: 1078 / 2250 Loss: 0.5300\n",
      "Epoch: 1 / 3, Step: 1079 / 2250 Loss: 0.7522\n",
      "Epoch: 1 / 3, Step: 1080 / 2250 Loss: 0.4463\n",
      "Epoch: 1 / 3, Step: 1081 / 2250 Loss: 0.5073\n",
      "Epoch: 1 / 3, Step: 1082 / 2250 Loss: 0.4758\n",
      "Epoch: 1 / 3, Step: 1083 / 2250 Loss: 0.5576\n",
      "Epoch: 1 / 3, Step: 1084 / 2250 Loss: 0.3423\n",
      "Epoch: 1 / 3, Step: 1085 / 2250 Loss: 0.3607\n",
      "Epoch: 1 / 3, Step: 1086 / 2250 Loss: 0.6525\n",
      "Epoch: 1 / 3, Step: 1087 / 2250 Loss: 0.5456\n",
      "Epoch: 1 / 3, Step: 1088 / 2250 Loss: 0.6392\n",
      "Epoch: 1 / 3, Step: 1089 / 2250 Loss: 0.4648\n",
      "Epoch: 1 / 3, Step: 1090 / 2250 Loss: 0.2277\n",
      "Epoch: 1 / 3, Step: 1091 / 2250 Loss: 0.6113\n",
      "Epoch: 1 / 3, Step: 1092 / 2250 Loss: 0.6687\n",
      "Epoch: 1 / 3, Step: 1093 / 2250 Loss: 0.4484\n",
      "Epoch: 1 / 3, Step: 1094 / 2250 Loss: 0.6574\n",
      "Epoch: 1 / 3, Step: 1095 / 2250 Loss: 0.6275\n",
      "Epoch: 1 / 3, Step: 1096 / 2250 Loss: 0.3156\n",
      "Epoch: 1 / 3, Step: 1097 / 2250 Loss: 0.5691\n",
      "Epoch: 1 / 3, Step: 1098 / 2250 Loss: 0.4369\n",
      "Epoch: 1 / 3, Step: 1099 / 2250 Loss: 0.3220\n",
      "Epoch: 1 / 3, Step: 1100 / 2250 Loss: 0.5686\n",
      "Epoch: 1 / 3, Step: 1101 / 2250 Loss: 0.3504\n",
      "Epoch: 1 / 3, Step: 1102 / 2250 Loss: 0.3814\n",
      "Epoch: 1 / 3, Step: 1103 / 2250 Loss: 0.3486\n",
      "Epoch: 1 / 3, Step: 1104 / 2250 Loss: 0.6950\n",
      "Epoch: 1 / 3, Step: 1105 / 2250 Loss: 0.6035\n",
      "Epoch: 1 / 3, Step: 1106 / 2250 Loss: 0.5196\n",
      "Epoch: 1 / 3, Step: 1107 / 2250 Loss: 0.1919\n",
      "Epoch: 1 / 3, Step: 1108 / 2250 Loss: 0.4119\n",
      "Epoch: 1 / 3, Step: 1109 / 2250 Loss: 0.2521\n",
      "Epoch: 1 / 3, Step: 1110 / 2250 Loss: 0.4259\n",
      "Epoch: 1 / 3, Step: 1111 / 2250 Loss: 0.4796\n",
      "Epoch: 1 / 3, Step: 1112 / 2250 Loss: 0.3500\n",
      "Epoch: 1 / 3, Step: 1113 / 2250 Loss: 0.3968\n",
      "Epoch: 1 / 3, Step: 1114 / 2250 Loss: 0.4686\n",
      "Epoch: 1 / 3, Step: 1115 / 2250 Loss: 0.2760\n",
      "Epoch: 1 / 3, Step: 1116 / 2250 Loss: 0.3538\n",
      "Epoch: 1 / 3, Step: 1117 / 2250 Loss: 0.3594\n",
      "Epoch: 1 / 3, Step: 1118 / 2250 Loss: 0.3404\n",
      "Epoch: 1 / 3, Step: 1119 / 2250 Loss: 0.4167\n",
      "Epoch: 1 / 3, Step: 1120 / 2250 Loss: 0.3990\n",
      "Epoch: 1 / 3, Step: 1121 / 2250 Loss: 0.2735\n",
      "Epoch: 1 / 3, Step: 1122 / 2250 Loss: 0.4229\n",
      "Epoch: 1 / 3, Step: 1123 / 2250 Loss: 0.4037\n",
      "Epoch: 1 / 3, Step: 1124 / 2250 Loss: 0.5956\n",
      "Epoch: 1 / 3, Step: 1125 / 2250 Loss: 0.3152\n",
      "Epoch: 1 / 3, Step: 1126 / 2250 Loss: 0.5351\n",
      "Epoch: 1 / 3, Step: 1127 / 2250 Loss: 0.5293\n",
      "Epoch: 1 / 3, Step: 1128 / 2250 Loss: 0.3114\n",
      "Epoch: 1 / 3, Step: 1129 / 2250 Loss: 0.4795\n",
      "Epoch: 1 / 3, Step: 1130 / 2250 Loss: 0.6455\n",
      "Epoch: 1 / 3, Step: 1131 / 2250 Loss: 0.6958\n",
      "Epoch: 1 / 3, Step: 1132 / 2250 Loss: 0.4597\n",
      "Epoch: 1 / 3, Step: 1133 / 2250 Loss: 0.5244\n",
      "Epoch: 1 / 3, Step: 1134 / 2250 Loss: 0.5053\n",
      "Epoch: 1 / 3, Step: 1135 / 2250 Loss: 0.3471\n",
      "Epoch: 1 / 3, Step: 1136 / 2250 Loss: 0.4628\n",
      "Epoch: 1 / 3, Step: 1137 / 2250 Loss: 0.4326\n",
      "Epoch: 1 / 3, Step: 1138 / 2250 Loss: 0.4706\n",
      "Epoch: 1 / 3, Step: 1139 / 2250 Loss: 0.7398\n",
      "Epoch: 1 / 3, Step: 1140 / 2250 Loss: 0.3479\n",
      "Epoch: 1 / 3, Step: 1141 / 2250 Loss: 0.8602\n",
      "Epoch: 1 / 3, Step: 1142 / 2250 Loss: 0.4772\n",
      "Epoch: 1 / 3, Step: 1143 / 2250 Loss: 0.4534\n",
      "Epoch: 1 / 3, Step: 1144 / 2250 Loss: 0.4216\n",
      "Epoch: 1 / 3, Step: 1145 / 2250 Loss: 0.6677\n",
      "Epoch: 1 / 3, Step: 1146 / 2250 Loss: 0.2723\n",
      "Epoch: 1 / 3, Step: 1147 / 2250 Loss: 0.3287\n",
      "Epoch: 1 / 3, Step: 1148 / 2250 Loss: 0.3313\n",
      "Epoch: 1 / 3, Step: 1149 / 2250 Loss: 0.4824\n",
      "Epoch: 1 / 3, Step: 1150 / 2250 Loss: 0.3776\n",
      "Epoch: 1 / 3, Step: 1151 / 2250 Loss: 0.2178\n",
      "Epoch: 1 / 3, Step: 1152 / 2250 Loss: 0.5043\n",
      "Epoch: 1 / 3, Step: 1153 / 2250 Loss: 0.7183\n",
      "Epoch: 1 / 3, Step: 1154 / 2250 Loss: 0.3980\n",
      "Epoch: 1 / 3, Step: 1155 / 2250 Loss: 0.4698\n",
      "Epoch: 1 / 3, Step: 1156 / 2250 Loss: 0.3323\n",
      "Epoch: 1 / 3, Step: 1157 / 2250 Loss: 0.1688\n",
      "Epoch: 1 / 3, Step: 1158 / 2250 Loss: 0.3741\n",
      "Epoch: 1 / 3, Step: 1159 / 2250 Loss: 0.6916\n",
      "Epoch: 1 / 3, Step: 1160 / 2250 Loss: 0.5372\n",
      "Epoch: 1 / 3, Step: 1161 / 2250 Loss: 0.8277\n",
      "Epoch: 1 / 3, Step: 1162 / 2250 Loss: 0.4379\n",
      "Epoch: 1 / 3, Step: 1163 / 2250 Loss: 0.3761\n",
      "Epoch: 1 / 3, Step: 1164 / 2250 Loss: 0.6336\n",
      "Epoch: 1 / 3, Step: 1165 / 2250 Loss: 0.3774\n",
      "Epoch: 1 / 3, Step: 1166 / 2250 Loss: 0.4086\n",
      "Epoch: 1 / 3, Step: 1167 / 2250 Loss: 0.3752\n",
      "Epoch: 1 / 3, Step: 1168 / 2250 Loss: 0.3154\n",
      "Epoch: 1 / 3, Step: 1169 / 2250 Loss: 0.3856\n",
      "Epoch: 1 / 3, Step: 1170 / 2250 Loss: 0.5051\n",
      "Epoch: 1 / 3, Step: 1171 / 2250 Loss: 0.3681\n",
      "Epoch: 1 / 3, Step: 1172 / 2250 Loss: 0.5114\n",
      "Epoch: 1 / 3, Step: 1173 / 2250 Loss: 0.3959\n",
      "Epoch: 1 / 3, Step: 1174 / 2250 Loss: 0.4714\n",
      "Epoch: 1 / 3, Step: 1175 / 2250 Loss: 0.3994\n",
      "Epoch: 1 / 3, Step: 1176 / 2250 Loss: 0.4736\n",
      "Epoch: 1 / 3, Step: 1177 / 2250 Loss: 0.3289\n",
      "Epoch: 1 / 3, Step: 1178 / 2250 Loss: 0.5932\n",
      "Epoch: 1 / 3, Step: 1179 / 2250 Loss: 0.7371\n",
      "Epoch: 1 / 3, Step: 1180 / 2250 Loss: 0.2531\n",
      "Epoch: 1 / 3, Step: 1181 / 2250 Loss: 0.3685\n",
      "Epoch: 1 / 3, Step: 1182 / 2250 Loss: 0.3416\n",
      "Epoch: 1 / 3, Step: 1183 / 2250 Loss: 0.3699\n",
      "Epoch: 1 / 3, Step: 1184 / 2250 Loss: 0.3431\n",
      "Epoch: 1 / 3, Step: 1185 / 2250 Loss: 0.4619\n",
      "Epoch: 1 / 3, Step: 1186 / 2250 Loss: 0.2187\n",
      "Epoch: 1 / 3, Step: 1187 / 2250 Loss: 0.2817\n",
      "Epoch: 1 / 3, Step: 1188 / 2250 Loss: 0.6575\n",
      "Epoch: 1 / 3, Step: 1189 / 2250 Loss: 0.3749\n",
      "Epoch: 1 / 3, Step: 1190 / 2250 Loss: 0.3963\n",
      "Epoch: 1 / 3, Step: 1191 / 2250 Loss: 0.3337\n",
      "Epoch: 1 / 3, Step: 1192 / 2250 Loss: 0.1713\n",
      "Epoch: 1 / 3, Step: 1193 / 2250 Loss: 0.3738\n",
      "Epoch: 1 / 3, Step: 1194 / 2250 Loss: 0.2428\n",
      "Epoch: 1 / 3, Step: 1195 / 2250 Loss: 0.3557\n",
      "Epoch: 1 / 3, Step: 1196 / 2250 Loss: 0.4635\n",
      "Epoch: 1 / 3, Step: 1197 / 2250 Loss: 0.4561\n",
      "Epoch: 1 / 3, Step: 1198 / 2250 Loss: 0.2857\n",
      "Epoch: 1 / 3, Step: 1199 / 2250 Loss: 0.5642\n",
      "Epoch: 1 / 3, Step: 1200 / 2250 Loss: 0.3567\n",
      "Epoch: 1 / 3, Step: 1201 / 2250 Loss: 0.7860\n",
      "Epoch: 1 / 3, Step: 1202 / 2250 Loss: 0.2148\n",
      "Epoch: 1 / 3, Step: 1203 / 2250 Loss: 0.2803\n",
      "Epoch: 1 / 3, Step: 1204 / 2250 Loss: 0.4890\n",
      "Epoch: 1 / 3, Step: 1205 / 2250 Loss: 0.6362\n",
      "Epoch: 1 / 3, Step: 1206 / 2250 Loss: 0.3258\n",
      "Epoch: 1 / 3, Step: 1207 / 2250 Loss: 0.3883\n",
      "Epoch: 1 / 3, Step: 1208 / 2250 Loss: 0.4551\n",
      "Epoch: 1 / 3, Step: 1209 / 2250 Loss: 0.2707\n",
      "Epoch: 1 / 3, Step: 1210 / 2250 Loss: 0.6101\n",
      "Epoch: 1 / 3, Step: 1211 / 2250 Loss: 0.5480\n",
      "Epoch: 1 / 3, Step: 1212 / 2250 Loss: 0.3452\n",
      "Epoch: 1 / 3, Step: 1213 / 2250 Loss: 0.4126\n",
      "Epoch: 1 / 3, Step: 1214 / 2250 Loss: 0.7289\n",
      "Epoch: 1 / 3, Step: 1215 / 2250 Loss: 0.3273\n",
      "Epoch: 1 / 3, Step: 1216 / 2250 Loss: 0.2433\n",
      "Epoch: 1 / 3, Step: 1217 / 2250 Loss: 0.4170\n",
      "Epoch: 1 / 3, Step: 1218 / 2250 Loss: 0.3352\n",
      "Epoch: 1 / 3, Step: 1219 / 2250 Loss: 0.4000\n",
      "Epoch: 1 / 3, Step: 1220 / 2250 Loss: 0.3146\n",
      "Epoch: 1 / 3, Step: 1221 / 2250 Loss: 0.3708\n",
      "Epoch: 1 / 3, Step: 1222 / 2250 Loss: 0.8210\n",
      "Epoch: 1 / 3, Step: 1223 / 2250 Loss: 0.2010\n",
      "Epoch: 1 / 3, Step: 1224 / 2250 Loss: 0.3439\n",
      "Epoch: 1 / 3, Step: 1225 / 2250 Loss: 0.5628\n",
      "Epoch: 1 / 3, Step: 1226 / 2250 Loss: 0.5951\n",
      "Epoch: 1 / 3, Step: 1227 / 2250 Loss: 0.7286\n",
      "Epoch: 1 / 3, Step: 1228 / 2250 Loss: 0.1953\n",
      "Epoch: 1 / 3, Step: 1229 / 2250 Loss: 0.5251\n",
      "Epoch: 1 / 3, Step: 1230 / 2250 Loss: 0.4518\n",
      "Epoch: 1 / 3, Step: 1231 / 2250 Loss: 0.5205\n",
      "Epoch: 1 / 3, Step: 1232 / 2250 Loss: 0.3349\n",
      "Epoch: 1 / 3, Step: 1233 / 2250 Loss: 0.3067\n",
      "Epoch: 1 / 3, Step: 1234 / 2250 Loss: 0.3921\n",
      "Epoch: 1 / 3, Step: 1235 / 2250 Loss: 0.4577\n",
      "Epoch: 1 / 3, Step: 1236 / 2250 Loss: 0.4571\n",
      "Epoch: 1 / 3, Step: 1237 / 2250 Loss: 0.3368\n",
      "Epoch: 1 / 3, Step: 1238 / 2250 Loss: 0.5503\n",
      "Epoch: 1 / 3, Step: 1239 / 2250 Loss: 0.4841\n",
      "Epoch: 1 / 3, Step: 1240 / 2250 Loss: 0.5631\n",
      "Epoch: 1 / 3, Step: 1241 / 2250 Loss: 0.4239\n",
      "Epoch: 1 / 3, Step: 1242 / 2250 Loss: 0.5120\n",
      "Epoch: 1 / 3, Step: 1243 / 2250 Loss: 0.4623\n",
      "Epoch: 1 / 3, Step: 1244 / 2250 Loss: 0.3563\n",
      "Epoch: 1 / 3, Step: 1245 / 2250 Loss: 0.3754\n",
      "Epoch: 1 / 3, Step: 1246 / 2250 Loss: 0.3327\n",
      "Epoch: 1 / 3, Step: 1247 / 2250 Loss: 0.5114\n",
      "Epoch: 1 / 3, Step: 1248 / 2250 Loss: 0.6158\n",
      "Epoch: 1 / 3, Step: 1249 / 2250 Loss: 0.7736\n",
      "Epoch: 1 / 3, Step: 1250 / 2250 Loss: 0.4626\n",
      "Epoch: 1 / 3, Step: 1251 / 2250 Loss: 0.5169\n",
      "Epoch: 1 / 3, Step: 1252 / 2250 Loss: 0.2484\n",
      "Epoch: 1 / 3, Step: 1253 / 2250 Loss: 0.4030\n",
      "Epoch: 1 / 3, Step: 1254 / 2250 Loss: 0.3902\n",
      "Epoch: 1 / 3, Step: 1255 / 2250 Loss: 0.3016\n",
      "Epoch: 1 / 3, Step: 1256 / 2250 Loss: 0.4172\n",
      "Epoch: 1 / 3, Step: 1257 / 2250 Loss: 0.4622\n",
      "Epoch: 1 / 3, Step: 1258 / 2250 Loss: 0.3604\n",
      "Epoch: 1 / 3, Step: 1259 / 2250 Loss: 0.6466\n",
      "Epoch: 1 / 3, Step: 1260 / 2250 Loss: 0.4231\n",
      "Epoch: 1 / 3, Step: 1261 / 2250 Loss: 0.4060\n",
      "Epoch: 1 / 3, Step: 1262 / 2250 Loss: 0.5552\n",
      "Epoch: 1 / 3, Step: 1263 / 2250 Loss: 0.4291\n",
      "Epoch: 1 / 3, Step: 1264 / 2250 Loss: 0.6475\n",
      "Epoch: 1 / 3, Step: 1265 / 2250 Loss: 0.5308\n",
      "Epoch: 1 / 3, Step: 1266 / 2250 Loss: 0.4101\n",
      "Epoch: 1 / 3, Step: 1267 / 2250 Loss: 0.5323\n",
      "Epoch: 1 / 3, Step: 1268 / 2250 Loss: 0.5729\n",
      "Epoch: 1 / 3, Step: 1269 / 2250 Loss: 0.4436\n",
      "Epoch: 1 / 3, Step: 1270 / 2250 Loss: 0.4130\n",
      "Epoch: 1 / 3, Step: 1271 / 2250 Loss: 0.3005\n",
      "Epoch: 1 / 3, Step: 1272 / 2250 Loss: 0.5122\n",
      "Epoch: 1 / 3, Step: 1273 / 2250 Loss: 0.4841\n",
      "Epoch: 1 / 3, Step: 1274 / 2250 Loss: 0.5227\n",
      "Epoch: 1 / 3, Step: 1275 / 2250 Loss: 0.4431\n",
      "Epoch: 1 / 3, Step: 1276 / 2250 Loss: 0.5733\n",
      "Epoch: 1 / 3, Step: 1277 / 2250 Loss: 0.5484\n",
      "Epoch: 1 / 3, Step: 1278 / 2250 Loss: 0.4054\n",
      "Epoch: 1 / 3, Step: 1279 / 2250 Loss: 0.4230\n",
      "Epoch: 1 / 3, Step: 1280 / 2250 Loss: 0.3244\n",
      "Epoch: 1 / 3, Step: 1281 / 2250 Loss: 0.6053\n",
      "Epoch: 1 / 3, Step: 1282 / 2250 Loss: 0.2447\n",
      "Epoch: 1 / 3, Step: 1283 / 2250 Loss: 0.2819\n",
      "Epoch: 1 / 3, Step: 1284 / 2250 Loss: 0.5382\n",
      "Epoch: 1 / 3, Step: 1285 / 2250 Loss: 0.2361\n",
      "Epoch: 1 / 3, Step: 1286 / 2250 Loss: 0.4557\n",
      "Epoch: 1 / 3, Step: 1287 / 2250 Loss: 0.4200\n",
      "Epoch: 1 / 3, Step: 1288 / 2250 Loss: 0.4971\n",
      "Epoch: 1 / 3, Step: 1289 / 2250 Loss: 0.5905\n",
      "Epoch: 1 / 3, Step: 1290 / 2250 Loss: 0.5552\n",
      "Epoch: 1 / 3, Step: 1291 / 2250 Loss: 0.5389\n",
      "Epoch: 1 / 3, Step: 1292 / 2250 Loss: 0.7779\n",
      "Epoch: 1 / 3, Step: 1293 / 2250 Loss: 0.5179\n",
      "Epoch: 1 / 3, Step: 1294 / 2250 Loss: 0.3127\n",
      "Epoch: 1 / 3, Step: 1295 / 2250 Loss: 0.4624\n",
      "Epoch: 1 / 3, Step: 1296 / 2250 Loss: 0.3908\n",
      "Epoch: 1 / 3, Step: 1297 / 2250 Loss: 0.5976\n",
      "Epoch: 1 / 3, Step: 1298 / 2250 Loss: 0.5351\n",
      "Epoch: 1 / 3, Step: 1299 / 2250 Loss: 0.4553\n",
      "Epoch: 1 / 3, Step: 1300 / 2250 Loss: 0.5901\n",
      "Epoch: 1 / 3, Step: 1301 / 2250 Loss: 0.2925\n",
      "Epoch: 1 / 3, Step: 1302 / 2250 Loss: 0.5759\n",
      "Epoch: 1 / 3, Step: 1303 / 2250 Loss: 0.5314\n",
      "Epoch: 1 / 3, Step: 1304 / 2250 Loss: 0.4164\n",
      "Epoch: 1 / 3, Step: 1305 / 2250 Loss: 0.3097\n",
      "Epoch: 1 / 3, Step: 1306 / 2250 Loss: 0.5895\n",
      "Epoch: 1 / 3, Step: 1307 / 2250 Loss: 0.4709\n",
      "Epoch: 1 / 3, Step: 1308 / 2250 Loss: 0.4435\n",
      "Epoch: 1 / 3, Step: 1309 / 2250 Loss: 0.6146\n",
      "Epoch: 1 / 3, Step: 1310 / 2250 Loss: 0.2982\n",
      "Epoch: 1 / 3, Step: 1311 / 2250 Loss: 0.2826\n",
      "Epoch: 1 / 3, Step: 1312 / 2250 Loss: 0.5919\n",
      "Epoch: 1 / 3, Step: 1313 / 2250 Loss: 0.4464\n",
      "Epoch: 1 / 3, Step: 1314 / 2250 Loss: 0.3817\n",
      "Epoch: 1 / 3, Step: 1315 / 2250 Loss: 0.5598\n",
      "Epoch: 1 / 3, Step: 1316 / 2250 Loss: 0.4890\n",
      "Epoch: 1 / 3, Step: 1317 / 2250 Loss: 0.6650\n",
      "Epoch: 1 / 3, Step: 1318 / 2250 Loss: 0.7245\n",
      "Epoch: 1 / 3, Step: 1319 / 2250 Loss: 0.5016\n",
      "Epoch: 1 / 3, Step: 1320 / 2250 Loss: 0.3809\n",
      "Epoch: 1 / 3, Step: 1321 / 2250 Loss: 0.4678\n",
      "Epoch: 1 / 3, Step: 1322 / 2250 Loss: 0.3809\n",
      "Epoch: 1 / 3, Step: 1323 / 2250 Loss: 0.3279\n",
      "Epoch: 1 / 3, Step: 1324 / 2250 Loss: 0.4203\n",
      "Epoch: 1 / 3, Step: 1325 / 2250 Loss: 0.4237\n",
      "Epoch: 1 / 3, Step: 1326 / 2250 Loss: 0.3496\n",
      "Epoch: 1 / 3, Step: 1327 / 2250 Loss: 0.1993\n",
      "Epoch: 1 / 3, Step: 1328 / 2250 Loss: 0.4607\n",
      "Epoch: 1 / 3, Step: 1329 / 2250 Loss: 0.5068\n",
      "Epoch: 1 / 3, Step: 1330 / 2250 Loss: 0.5622\n",
      "Epoch: 1 / 3, Step: 1331 / 2250 Loss: 0.4088\n",
      "Epoch: 1 / 3, Step: 1332 / 2250 Loss: 0.2378\n",
      "Epoch: 1 / 3, Step: 1333 / 2250 Loss: 0.3133\n",
      "Epoch: 1 / 3, Step: 1334 / 2250 Loss: 0.6609\n",
      "Epoch: 1 / 3, Step: 1335 / 2250 Loss: 0.4089\n",
      "Epoch: 1 / 3, Step: 1336 / 2250 Loss: 0.3167\n",
      "Epoch: 1 / 3, Step: 1337 / 2250 Loss: 0.3450\n",
      "Epoch: 1 / 3, Step: 1338 / 2250 Loss: 0.7024\n",
      "Epoch: 1 / 3, Step: 1339 / 2250 Loss: 0.4335\n",
      "Epoch: 1 / 3, Step: 1340 / 2250 Loss: 0.3790\n",
      "Epoch: 1 / 3, Step: 1341 / 2250 Loss: 0.4931\n",
      "Epoch: 1 / 3, Step: 1342 / 2250 Loss: 0.4317\n",
      "Epoch: 1 / 3, Step: 1343 / 2250 Loss: 0.7576\n",
      "Epoch: 1 / 3, Step: 1344 / 2250 Loss: 0.5079\n",
      "Epoch: 1 / 3, Step: 1345 / 2250 Loss: 0.4404\n",
      "Epoch: 1 / 3, Step: 1346 / 2250 Loss: 0.5815\n",
      "Epoch: 1 / 3, Step: 1347 / 2250 Loss: 0.2593\n",
      "Epoch: 1 / 3, Step: 1348 / 2250 Loss: 0.2565\n",
      "Epoch: 1 / 3, Step: 1349 / 2250 Loss: 0.3293\n",
      "Epoch: 1 / 3, Step: 1350 / 2250 Loss: 0.4529\n",
      "Epoch: 1 / 3, Step: 1351 / 2250 Loss: 0.2745\n",
      "Epoch: 1 / 3, Step: 1352 / 2250 Loss: 0.6497\n",
      "Epoch: 1 / 3, Step: 1353 / 2250 Loss: 0.7750\n",
      "Epoch: 1 / 3, Step: 1354 / 2250 Loss: 0.2708\n",
      "Epoch: 1 / 3, Step: 1355 / 2250 Loss: 0.3691\n",
      "Epoch: 1 / 3, Step: 1356 / 2250 Loss: 0.4486\n",
      "Epoch: 1 / 3, Step: 1357 / 2250 Loss: 0.5101\n",
      "Epoch: 1 / 3, Step: 1358 / 2250 Loss: 0.3779\n",
      "Epoch: 1 / 3, Step: 1359 / 2250 Loss: 0.3029\n",
      "Epoch: 1 / 3, Step: 1360 / 2250 Loss: 0.5545\n",
      "Epoch: 1 / 3, Step: 1361 / 2250 Loss: 0.2864\n",
      "Epoch: 1 / 3, Step: 1362 / 2250 Loss: 0.2217\n",
      "Epoch: 1 / 3, Step: 1363 / 2250 Loss: 0.5926\n",
      "Epoch: 1 / 3, Step: 1364 / 2250 Loss: 0.3492\n",
      "Epoch: 1 / 3, Step: 1365 / 2250 Loss: 0.5673\n",
      "Epoch: 1 / 3, Step: 1366 / 2250 Loss: 0.3886\n",
      "Epoch: 1 / 3, Step: 1367 / 2250 Loss: 0.6266\n",
      "Epoch: 1 / 3, Step: 1368 / 2250 Loss: 0.6607\n",
      "Epoch: 1 / 3, Step: 1369 / 2250 Loss: 0.5099\n",
      "Epoch: 1 / 3, Step: 1370 / 2250 Loss: 0.5303\n",
      "Epoch: 1 / 3, Step: 1371 / 2250 Loss: 0.3291\n",
      "Epoch: 1 / 3, Step: 1372 / 2250 Loss: 0.4666\n",
      "Epoch: 1 / 3, Step: 1373 / 2250 Loss: 0.2391\n",
      "Epoch: 1 / 3, Step: 1374 / 2250 Loss: 0.6752\n",
      "Epoch: 1 / 3, Step: 1375 / 2250 Loss: 0.2925\n",
      "Epoch: 1 / 3, Step: 1376 / 2250 Loss: 0.3543\n",
      "Epoch: 1 / 3, Step: 1377 / 2250 Loss: 0.3900\n",
      "Epoch: 1 / 3, Step: 1378 / 2250 Loss: 0.4463\n",
      "Epoch: 1 / 3, Step: 1379 / 2250 Loss: 0.3748\n",
      "Epoch: 1 / 3, Step: 1380 / 2250 Loss: 0.4926\n",
      "Epoch: 1 / 3, Step: 1381 / 2250 Loss: 0.3054\n",
      "Epoch: 1 / 3, Step: 1382 / 2250 Loss: 0.4882\n",
      "Epoch: 1 / 3, Step: 1383 / 2250 Loss: 0.3894\n",
      "Epoch: 1 / 3, Step: 1384 / 2250 Loss: 0.3008\n",
      "Epoch: 1 / 3, Step: 1385 / 2250 Loss: 0.4342\n",
      "Epoch: 1 / 3, Step: 1386 / 2250 Loss: 0.4173\n",
      "Epoch: 1 / 3, Step: 1387 / 2250 Loss: 0.3973\n",
      "Epoch: 1 / 3, Step: 1388 / 2250 Loss: 0.3140\n",
      "Epoch: 1 / 3, Step: 1389 / 2250 Loss: 0.4968\n",
      "Epoch: 1 / 3, Step: 1390 / 2250 Loss: 0.3448\n",
      "Epoch: 1 / 3, Step: 1391 / 2250 Loss: 0.2766\n",
      "Epoch: 1 / 3, Step: 1392 / 2250 Loss: 0.4083\n",
      "Epoch: 1 / 3, Step: 1393 / 2250 Loss: 0.2835\n",
      "Epoch: 1 / 3, Step: 1394 / 2250 Loss: 0.3053\n",
      "Epoch: 1 / 3, Step: 1395 / 2250 Loss: 0.3982\n",
      "Epoch: 1 / 3, Step: 1396 / 2250 Loss: 0.6114\n",
      "Epoch: 1 / 3, Step: 1397 / 2250 Loss: 0.2443\n",
      "Epoch: 1 / 3, Step: 1398 / 2250 Loss: 0.4438\n",
      "Epoch: 1 / 3, Step: 1399 / 2250 Loss: 0.2390\n",
      "Epoch: 1 / 3, Step: 1400 / 2250 Loss: 0.1748\n",
      "Epoch: 1 / 3, Step: 1401 / 2250 Loss: 0.4501\n",
      "Epoch: 1 / 3, Step: 1402 / 2250 Loss: 0.4682\n",
      "Epoch: 1 / 3, Step: 1403 / 2250 Loss: 0.6298\n",
      "Epoch: 1 / 3, Step: 1404 / 2250 Loss: 0.2771\n",
      "Epoch: 1 / 3, Step: 1405 / 2250 Loss: 0.4519\n",
      "Epoch: 1 / 3, Step: 1406 / 2250 Loss: 0.3997\n",
      "Epoch: 1 / 3, Step: 1407 / 2250 Loss: 0.2495\n",
      "Epoch: 1 / 3, Step: 1408 / 2250 Loss: 0.5414\n",
      "Epoch: 1 / 3, Step: 1409 / 2250 Loss: 0.5280\n",
      "Epoch: 1 / 3, Step: 1410 / 2250 Loss: 0.3780\n",
      "Epoch: 1 / 3, Step: 1411 / 2250 Loss: 0.5237\n",
      "Epoch: 1 / 3, Step: 1412 / 2250 Loss: 0.2788\n",
      "Epoch: 1 / 3, Step: 1413 / 2250 Loss: 0.3467\n",
      "Epoch: 1 / 3, Step: 1414 / 2250 Loss: 0.5098\n",
      "Epoch: 1 / 3, Step: 1415 / 2250 Loss: 0.5952\n",
      "Epoch: 1 / 3, Step: 1416 / 2250 Loss: 0.4115\n",
      "Epoch: 1 / 3, Step: 1417 / 2250 Loss: 0.2760\n",
      "Epoch: 1 / 3, Step: 1418 / 2250 Loss: 0.4341\n",
      "Epoch: 1 / 3, Step: 1419 / 2250 Loss: 0.6404\n",
      "Epoch: 1 / 3, Step: 1420 / 2250 Loss: 0.4380\n",
      "Epoch: 1 / 3, Step: 1421 / 2250 Loss: 0.1303\n",
      "Epoch: 1 / 3, Step: 1422 / 2250 Loss: 0.2089\n",
      "Epoch: 1 / 3, Step: 1423 / 2250 Loss: 0.3779\n",
      "Epoch: 1 / 3, Step: 1424 / 2250 Loss: 0.8296\n",
      "Epoch: 1 / 3, Step: 1425 / 2250 Loss: 0.3159\n",
      "Epoch: 1 / 3, Step: 1426 / 2250 Loss: 0.5806\n",
      "Epoch: 1 / 3, Step: 1427 / 2250 Loss: 0.1937\n",
      "Epoch: 1 / 3, Step: 1428 / 2250 Loss: 0.4495\n",
      "Epoch: 1 / 3, Step: 1429 / 2250 Loss: 0.6311\n",
      "Epoch: 1 / 3, Step: 1430 / 2250 Loss: 0.5775\n",
      "Epoch: 1 / 3, Step: 1431 / 2250 Loss: 0.6460\n",
      "Epoch: 1 / 3, Step: 1432 / 2250 Loss: 0.3272\n",
      "Epoch: 1 / 3, Step: 1433 / 2250 Loss: 0.2823\n",
      "Epoch: 1 / 3, Step: 1434 / 2250 Loss: 0.5006\n",
      "Epoch: 1 / 3, Step: 1435 / 2250 Loss: 0.2985\n",
      "Epoch: 1 / 3, Step: 1436 / 2250 Loss: 0.2587\n",
      "Epoch: 1 / 3, Step: 1437 / 2250 Loss: 0.3720\n",
      "Epoch: 1 / 3, Step: 1438 / 2250 Loss: 0.4732\n",
      "Epoch: 1 / 3, Step: 1439 / 2250 Loss: 0.3035\n",
      "Epoch: 1 / 3, Step: 1440 / 2250 Loss: 0.2907\n",
      "Epoch: 1 / 3, Step: 1441 / 2250 Loss: 0.3692\n",
      "Epoch: 1 / 3, Step: 1442 / 2250 Loss: 0.2710\n",
      "Epoch: 1 / 3, Step: 1443 / 2250 Loss: 0.2319\n",
      "Epoch: 1 / 3, Step: 1444 / 2250 Loss: 0.4945\n",
      "Epoch: 1 / 3, Step: 1445 / 2250 Loss: 0.1894\n",
      "Epoch: 1 / 3, Step: 1446 / 2250 Loss: 0.1764\n",
      "Epoch: 1 / 3, Step: 1447 / 2250 Loss: 0.3043\n",
      "Epoch: 1 / 3, Step: 1448 / 2250 Loss: 0.3522\n",
      "Epoch: 1 / 3, Step: 1449 / 2250 Loss: 0.6217\n",
      "Epoch: 1 / 3, Step: 1450 / 2250 Loss: 0.5846\n",
      "Epoch: 1 / 3, Step: 1451 / 2250 Loss: 0.3955\n",
      "Epoch: 1 / 3, Step: 1452 / 2250 Loss: 0.3959\n",
      "Epoch: 1 / 3, Step: 1453 / 2250 Loss: 0.2979\n",
      "Epoch: 1 / 3, Step: 1454 / 2250 Loss: 0.6391\n",
      "Epoch: 1 / 3, Step: 1455 / 2250 Loss: 0.4608\n",
      "Epoch: 1 / 3, Step: 1456 / 2250 Loss: 0.6786\n",
      "Epoch: 1 / 3, Step: 1457 / 2250 Loss: 0.4347\n",
      "Epoch: 1 / 3, Step: 1458 / 2250 Loss: 0.4326\n",
      "Epoch: 1 / 3, Step: 1459 / 2250 Loss: 0.4879\n",
      "Epoch: 1 / 3, Step: 1460 / 2250 Loss: 0.3244\n",
      "Epoch: 1 / 3, Step: 1461 / 2250 Loss: 0.4831\n",
      "Epoch: 1 / 3, Step: 1462 / 2250 Loss: 0.5144\n",
      "Epoch: 1 / 3, Step: 1463 / 2250 Loss: 0.2046\n",
      "Epoch: 1 / 3, Step: 1464 / 2250 Loss: 0.1389\n",
      "Epoch: 1 / 3, Step: 1465 / 2250 Loss: 0.5294\n",
      "Epoch: 1 / 3, Step: 1466 / 2250 Loss: 0.5346\n",
      "Epoch: 1 / 3, Step: 1467 / 2250 Loss: 0.4042\n",
      "Epoch: 1 / 3, Step: 1468 / 2250 Loss: 0.3942\n",
      "Epoch: 1 / 3, Step: 1469 / 2250 Loss: 0.2748\n",
      "Epoch: 1 / 3, Step: 1470 / 2250 Loss: 0.5662\n",
      "Epoch: 1 / 3, Step: 1471 / 2250 Loss: 0.3227\n",
      "Epoch: 1 / 3, Step: 1472 / 2250 Loss: 0.4598\n",
      "Epoch: 1 / 3, Step: 1473 / 2250 Loss: 0.3330\n",
      "Epoch: 1 / 3, Step: 1474 / 2250 Loss: 0.2435\n",
      "Epoch: 1 / 3, Step: 1475 / 2250 Loss: 0.5913\n",
      "Epoch: 1 / 3, Step: 1476 / 2250 Loss: 0.3805\n",
      "Epoch: 1 / 3, Step: 1477 / 2250 Loss: 0.4254\n",
      "Epoch: 1 / 3, Step: 1478 / 2250 Loss: 0.4082\n",
      "Epoch: 1 / 3, Step: 1479 / 2250 Loss: 0.2590\n",
      "Epoch: 1 / 3, Step: 1480 / 2250 Loss: 0.7090\n",
      "Epoch: 1 / 3, Step: 1481 / 2250 Loss: 0.2651\n",
      "Epoch: 1 / 3, Step: 1482 / 2250 Loss: 0.2691\n",
      "Epoch: 1 / 3, Step: 1483 / 2250 Loss: 0.5349\n",
      "Epoch: 1 / 3, Step: 1484 / 2250 Loss: 0.2490\n",
      "Epoch: 1 / 3, Step: 1485 / 2250 Loss: 0.3319\n",
      "Epoch: 1 / 3, Step: 1486 / 2250 Loss: 0.3042\n",
      "Epoch: 1 / 3, Step: 1487 / 2250 Loss: 0.2598\n",
      "Epoch: 1 / 3, Step: 1488 / 2250 Loss: 0.4484\n",
      "Epoch: 1 / 3, Step: 1489 / 2250 Loss: 0.4028\n",
      "Epoch: 1 / 3, Step: 1490 / 2250 Loss: 0.2241\n",
      "Epoch: 1 / 3, Step: 1491 / 2250 Loss: 0.4309\n",
      "Epoch: 1 / 3, Step: 1492 / 2250 Loss: 0.2087\n",
      "Epoch: 1 / 3, Step: 1493 / 2250 Loss: 0.2758\n",
      "Epoch: 1 / 3, Step: 1494 / 2250 Loss: 0.2758\n",
      "Epoch: 1 / 3, Step: 1495 / 2250 Loss: 0.5532\n",
      "Epoch: 1 / 3, Step: 1496 / 2250 Loss: 0.4352\n",
      "Epoch: 1 / 3, Step: 1497 / 2250 Loss: 0.9086\n",
      "Epoch: 1 / 3, Step: 1498 / 2250 Loss: 0.2212\n",
      "Epoch: 1 / 3, Step: 1506 / 2250 Loss: 0.4820\n",
      "Epoch: 1 / 3, Step: 1507 / 2250 Loss: 0.3251\n",
      "Epoch: 1 / 3, Step: 1508 / 2250 Loss: 0.3767\n",
      "Epoch: 1 / 3, Step: 1509 / 2250 Loss: 0.3685\n",
      "Epoch: 1 / 3, Step: 1510 / 2250 Loss: 0.4366\n",
      "Epoch: 1 / 3, Step: 1511 / 2250 Loss: 0.4643\n",
      "Epoch: 1 / 3, Step: 1512 / 2250 Loss: 0.2082\n",
      "Epoch: 1 / 3, Step: 1513 / 2250 Loss: 0.5079\n",
      "Epoch: 1 / 3, Step: 1514 / 2250 Loss: 0.2292\n",
      "Epoch: 1 / 3, Step: 1515 / 2250 Loss: 0.6358\n",
      "Epoch: 1 / 3, Step: 1516 / 2250 Loss: 0.3213\n",
      "Epoch: 1 / 3, Step: 1517 / 2250 Loss: 0.3778\n",
      "Epoch: 1 / 3, Step: 1518 / 2250 Loss: 0.2443\n",
      "Epoch: 1 / 3, Step: 1519 / 2250 Loss: 0.1894\n",
      "Epoch: 1 / 3, Step: 1520 / 2250 Loss: 0.4624\n",
      "Epoch: 1 / 3, Step: 1521 / 2250 Loss: 0.2357\n",
      "Epoch: 1 / 3, Step: 1522 / 2250 Loss: 0.2752\n",
      "Epoch: 1 / 3, Step: 1523 / 2250 Loss: 0.2530\n",
      "Epoch: 1 / 3, Step: 1524 / 2250 Loss: 0.4274\n",
      "Epoch: 1 / 3, Step: 1525 / 2250 Loss: 0.2885\n",
      "Epoch: 1 / 3, Step: 1526 / 2250 Loss: 0.2722\n",
      "Epoch: 1 / 3, Step: 1527 / 2250 Loss: 0.3370\n",
      "Epoch: 1 / 3, Step: 1528 / 2250 Loss: 0.2449\n",
      "Epoch: 1 / 3, Step: 1529 / 2250 Loss: 0.5882\n",
      "Epoch: 1 / 3, Step: 1530 / 2250 Loss: 0.3834\n",
      "Epoch: 1 / 3, Step: 1531 / 2250 Loss: 0.6847\n",
      "Epoch: 1 / 3, Step: 1532 / 2250 Loss: 0.3059\n",
      "Epoch: 1 / 3, Step: 1533 / 2250 Loss: 0.3274\n",
      "Epoch: 1 / 3, Step: 1534 / 2250 Loss: 0.4524\n",
      "Epoch: 1 / 3, Step: 1535 / 2250 Loss: 0.2870\n",
      "Epoch: 1 / 3, Step: 1536 / 2250 Loss: 0.3725\n",
      "Epoch: 1 / 3, Step: 1537 / 2250 Loss: 0.2078\n",
      "Epoch: 1 / 3, Step: 1538 / 2250 Loss: 0.6942\n",
      "Epoch: 1 / 3, Step: 1539 / 2250 Loss: 0.2524\n",
      "Epoch: 1 / 3, Step: 1540 / 2250 Loss: 0.2792\n",
      "Epoch: 1 / 3, Step: 1541 / 2250 Loss: 0.5043\n",
      "Epoch: 1 / 3, Step: 1542 / 2250 Loss: 0.1337\n",
      "Epoch: 1 / 3, Step: 1543 / 2250 Loss: 0.5711\n",
      "Epoch: 1 / 3, Step: 1544 / 2250 Loss: 0.4670\n",
      "Epoch: 1 / 3, Step: 1545 / 2250 Loss: 0.1757\n",
      "Epoch: 1 / 3, Step: 1546 / 2250 Loss: 0.4333\n",
      "Epoch: 1 / 3, Step: 1547 / 2250 Loss: 0.5050\n",
      "Epoch: 1 / 3, Step: 1548 / 2250 Loss: 0.2356\n",
      "Epoch: 1 / 3, Step: 1549 / 2250 Loss: 0.4227\n",
      "Epoch: 1 / 3, Step: 1550 / 2250 Loss: 0.3564\n",
      "Epoch: 1 / 3, Step: 1551 / 2250 Loss: 0.4383\n",
      "Epoch: 1 / 3, Step: 1552 / 2250 Loss: 0.2637\n",
      "Epoch: 1 / 3, Step: 1553 / 2250 Loss: 0.2984\n",
      "Epoch: 1 / 3, Step: 1554 / 2250 Loss: 0.2498\n",
      "Epoch: 1 / 3, Step: 1555 / 2250 Loss: 0.4337\n",
      "Epoch: 1 / 3, Step: 1556 / 2250 Loss: 0.4092\n",
      "Epoch: 1 / 3, Step: 1557 / 2250 Loss: 0.4196\n",
      "Epoch: 1 / 3, Step: 1558 / 2250 Loss: 0.4292\n",
      "Epoch: 1 / 3, Step: 1559 / 2250 Loss: 0.3024\n",
      "Epoch: 1 / 3, Step: 1560 / 2250 Loss: 0.2550\n",
      "Epoch: 1 / 3, Step: 1561 / 2250 Loss: 0.8351\n",
      "Epoch: 1 / 3, Step: 1562 / 2250 Loss: 0.2384\n",
      "Epoch: 1 / 3, Step: 1563 / 2250 Loss: 0.3494\n",
      "Epoch: 1 / 3, Step: 1564 / 2250 Loss: 0.2728\n",
      "Epoch: 1 / 3, Step: 1565 / 2250 Loss: 0.3602\n",
      "Epoch: 1 / 3, Step: 1566 / 2250 Loss: 0.3886\n",
      "Epoch: 1 / 3, Step: 1567 / 2250 Loss: 0.4136\n",
      "Epoch: 1 / 3, Step: 1568 / 2250 Loss: 0.5368\n",
      "Epoch: 1 / 3, Step: 1569 / 2250 Loss: 0.3709\n",
      "Epoch: 1 / 3, Step: 1570 / 2250 Loss: 0.3474\n",
      "Epoch: 1 / 3, Step: 1571 / 2250 Loss: 0.4630\n",
      "Epoch: 1 / 3, Step: 1572 / 2250 Loss: 0.5095\n",
      "Epoch: 1 / 3, Step: 1573 / 2250 Loss: 0.3448\n",
      "Epoch: 1 / 3, Step: 1574 / 2250 Loss: 0.1470\n",
      "Epoch: 1 / 3, Step: 1575 / 2250 Loss: 0.1907\n",
      "Epoch: 1 / 3, Step: 1576 / 2250 Loss: 0.3409\n",
      "Epoch: 1 / 3, Step: 1577 / 2250 Loss: 0.6712\n",
      "Epoch: 1 / 3, Step: 1578 / 2250 Loss: 0.5759\n",
      "Epoch: 1 / 3, Step: 1579 / 2250 Loss: 0.1844\n",
      "Epoch: 1 / 3, Step: 1580 / 2250 Loss: 0.3830\n",
      "Epoch: 1 / 3, Step: 1581 / 2250 Loss: 0.3805\n",
      "Epoch: 1 / 3, Step: 1582 / 2250 Loss: 0.4110\n",
      "Epoch: 1 / 3, Step: 1583 / 2250 Loss: 0.2336\n",
      "Epoch: 1 / 3, Step: 1584 / 2250 Loss: 0.5139\n",
      "Epoch: 1 / 3, Step: 1585 / 2250 Loss: 0.3127\n",
      "Epoch: 1 / 3, Step: 1586 / 2250 Loss: 0.4561\n",
      "Epoch: 1 / 3, Step: 1587 / 2250 Loss: 0.2877\n",
      "Epoch: 1 / 3, Step: 1588 / 2250 Loss: 0.3428\n",
      "Epoch: 1 / 3, Step: 1589 / 2250 Loss: 0.2882\n",
      "Epoch: 1 / 3, Step: 1590 / 2250 Loss: 0.4777\n",
      "Epoch: 1 / 3, Step: 1591 / 2250 Loss: 0.7162\n",
      "Epoch: 1 / 3, Step: 1592 / 2250 Loss: 0.4670\n",
      "Epoch: 1 / 3, Step: 1593 / 2250 Loss: 0.4772\n",
      "Epoch: 1 / 3, Step: 1594 / 2250 Loss: 0.3763\n",
      "Epoch: 1 / 3, Step: 1595 / 2250 Loss: 0.3876\n",
      "Epoch: 1 / 3, Step: 1596 / 2250 Loss: 0.3366\n",
      "Epoch: 1 / 3, Step: 1597 / 2250 Loss: 0.3962\n",
      "Epoch: 1 / 3, Step: 1598 / 2250 Loss: 0.1900\n",
      "Epoch: 1 / 3, Step: 1599 / 2250 Loss: 0.5761\n",
      "Epoch: 1 / 3, Step: 1600 / 2250 Loss: 0.3703\n",
      "Epoch: 1 / 3, Step: 1601 / 2250 Loss: 0.5489\n",
      "Epoch: 1 / 3, Step: 1602 / 2250 Loss: 0.3512\n",
      "Epoch: 1 / 3, Step: 1603 / 2250 Loss: 0.4725\n",
      "Epoch: 1 / 3, Step: 1604 / 2250 Loss: 0.2166\n",
      "Epoch: 1 / 3, Step: 1605 / 2250 Loss: 0.4256\n",
      "Epoch: 1 / 3, Step: 1606 / 2250 Loss: 0.7129\n",
      "Epoch: 1 / 3, Step: 1607 / 2250 Loss: 0.6927\n",
      "Epoch: 1 / 3, Step: 1608 / 2250 Loss: 0.3775\n",
      "Epoch: 1 / 3, Step: 1609 / 2250 Loss: 0.3970\n",
      "Epoch: 1 / 3, Step: 1610 / 2250 Loss: 0.3533\n",
      "Epoch: 1 / 3, Step: 1611 / 2250 Loss: 0.4561\n",
      "Epoch: 1 / 3, Step: 1612 / 2250 Loss: 0.5206\n",
      "Epoch: 1 / 3, Step: 1613 / 2250 Loss: 0.3366\n",
      "Epoch: 1 / 3, Step: 1614 / 2250 Loss: 0.4141\n",
      "Epoch: 1 / 3, Step: 1615 / 2250 Loss: 0.3204\n",
      "Epoch: 1 / 3, Step: 1616 / 2250 Loss: 0.4859\n",
      "Epoch: 1 / 3, Step: 1617 / 2250 Loss: 0.3378\n",
      "Epoch: 1 / 3, Step: 1618 / 2250 Loss: 0.3440\n",
      "Epoch: 1 / 3, Step: 1619 / 2250 Loss: 0.4166\n",
      "Epoch: 1 / 3, Step: 1620 / 2250 Loss: 0.4692\n",
      "Epoch: 1 / 3, Step: 1621 / 2250 Loss: 0.4149\n",
      "Epoch: 1 / 3, Step: 1622 / 2250 Loss: 0.2588\n",
      "Epoch: 1 / 3, Step: 1623 / 2250 Loss: 0.5532\n",
      "Epoch: 1 / 3, Step: 1624 / 2250 Loss: 0.4896\n",
      "Epoch: 1 / 3, Step: 1625 / 2250 Loss: 0.2880\n",
      "Epoch: 1 / 3, Step: 1626 / 2250 Loss: 0.2177\n",
      "Epoch: 1 / 3, Step: 1627 / 2250 Loss: 0.3907\n",
      "Epoch: 1 / 3, Step: 1628 / 2250 Loss: 0.3524\n",
      "Epoch: 1 / 3, Step: 1629 / 2250 Loss: 0.4270\n",
      "Epoch: 1 / 3, Step: 1630 / 2250 Loss: 0.4693\n",
      "Epoch: 1 / 3, Step: 1631 / 2250 Loss: 0.4341\n",
      "Epoch: 1 / 3, Step: 1632 / 2250 Loss: 0.2741\n",
      "Epoch: 1 / 3, Step: 1633 / 2250 Loss: 0.3617\n",
      "Epoch: 1 / 3, Step: 1634 / 2250 Loss: 0.5427\n",
      "Epoch: 1 / 3, Step: 1635 / 2250 Loss: 0.6971\n",
      "Epoch: 1 / 3, Step: 1636 / 2250 Loss: 0.2002\n",
      "Epoch: 1 / 3, Step: 1637 / 2250 Loss: 0.4885\n",
      "Epoch: 1 / 3, Step: 1638 / 2250 Loss: 0.6651\n",
      "Epoch: 1 / 3, Step: 1639 / 2250 Loss: 0.6196\n",
      "Epoch: 1 / 3, Step: 1640 / 2250 Loss: 0.2673\n",
      "Epoch: 1 / 3, Step: 1641 / 2250 Loss: 0.5505\n",
      "Epoch: 1 / 3, Step: 1642 / 2250 Loss: 0.2910\n",
      "Epoch: 1 / 3, Step: 1643 / 2250 Loss: 0.2974\n",
      "Epoch: 1 / 3, Step: 1644 / 2250 Loss: 0.3626\n",
      "Epoch: 1 / 3, Step: 1645 / 2250 Loss: 0.6741\n",
      "Epoch: 1 / 3, Step: 1646 / 2250 Loss: 0.1899\n",
      "Epoch: 1 / 3, Step: 1647 / 2250 Loss: 0.5460\n",
      "Epoch: 1 / 3, Step: 1648 / 2250 Loss: 0.3466\n",
      "Epoch: 1 / 3, Step: 1649 / 2250 Loss: 0.3063\n",
      "Epoch: 1 / 3, Step: 1650 / 2250 Loss: 0.2204\n",
      "Epoch: 1 / 3, Step: 1651 / 2250 Loss: 0.4438\n",
      "Epoch: 1 / 3, Step: 1652 / 2250 Loss: 0.4401\n",
      "Epoch: 1 / 3, Step: 1653 / 2250 Loss: 0.2434\n",
      "Epoch: 1 / 3, Step: 1654 / 2250 Loss: 0.5151\n",
      "Epoch: 1 / 3, Step: 1655 / 2250 Loss: 0.8257\n",
      "Epoch: 1 / 3, Step: 1656 / 2250 Loss: 0.3457\n",
      "Epoch: 1 / 3, Step: 1657 / 2250 Loss: 0.6287\n",
      "Epoch: 1 / 3, Step: 1658 / 2250 Loss: 0.2820\n",
      "Epoch: 1 / 3, Step: 1659 / 2250 Loss: 0.3019\n",
      "Epoch: 1 / 3, Step: 1660 / 2250 Loss: 0.4278\n",
      "Epoch: 1 / 3, Step: 1661 / 2250 Loss: 0.4846\n",
      "Epoch: 1 / 3, Step: 1662 / 2250 Loss: 0.2684\n",
      "Epoch: 1 / 3, Step: 1663 / 2250 Loss: 0.3611\n",
      "Epoch: 1 / 3, Step: 1664 / 2250 Loss: 0.3316\n",
      "Epoch: 1 / 3, Step: 1665 / 2250 Loss: 0.6404\n",
      "Epoch: 1 / 3, Step: 1666 / 2250 Loss: 0.3196\n",
      "Epoch: 1 / 3, Step: 1667 / 2250 Loss: 0.3006\n",
      "Epoch: 1 / 3, Step: 1668 / 2250 Loss: 0.4984\n",
      "Epoch: 1 / 3, Step: 1669 / 2250 Loss: 0.5640\n",
      "Epoch: 1 / 3, Step: 1670 / 2250 Loss: 0.3043\n",
      "Epoch: 1 / 3, Step: 1671 / 2250 Loss: 0.2682\n",
      "Epoch: 1 / 3, Step: 1672 / 2250 Loss: 0.3670\n",
      "Epoch: 1 / 3, Step: 1673 / 2250 Loss: 0.5576\n",
      "Epoch: 1 / 3, Step: 1674 / 2250 Loss: 0.4140\n",
      "Epoch: 1 / 3, Step: 1675 / 2250 Loss: 0.3787\n",
      "Epoch: 1 / 3, Step: 1676 / 2250 Loss: 0.4326\n",
      "Epoch: 1 / 3, Step: 1677 / 2250 Loss: 0.4203\n",
      "Epoch: 1 / 3, Step: 1678 / 2250 Loss: 0.4157\n",
      "Epoch: 1 / 3, Step: 1679 / 2250 Loss: 0.5638\n",
      "Epoch: 1 / 3, Step: 1680 / 2250 Loss: 0.3024\n",
      "Epoch: 1 / 3, Step: 1681 / 2250 Loss: 0.3985\n",
      "Epoch: 1 / 3, Step: 1682 / 2250 Loss: 0.1685\n",
      "Epoch: 1 / 3, Step: 1683 / 2250 Loss: 0.4809\n",
      "Epoch: 1 / 3, Step: 1684 / 2250 Loss: 0.3450\n",
      "Epoch: 1 / 3, Step: 1685 / 2250 Loss: 0.3308\n",
      "Epoch: 1 / 3, Step: 1686 / 2250 Loss: 0.3220\n",
      "Epoch: 1 / 3, Step: 1687 / 2250 Loss: 0.3110\n",
      "Epoch: 1 / 3, Step: 1688 / 2250 Loss: 0.3082\n",
      "Epoch: 1 / 3, Step: 1689 / 2250 Loss: 0.2164\n",
      "Epoch: 1 / 3, Step: 1690 / 2250 Loss: 0.3259\n",
      "Epoch: 1 / 3, Step: 1691 / 2250 Loss: 0.3065\n",
      "Epoch: 1 / 3, Step: 1692 / 2250 Loss: 0.4420\n",
      "Epoch: 1 / 3, Step: 1693 / 2250 Loss: 0.4496\n",
      "Epoch: 1 / 3, Step: 1694 / 2250 Loss: 0.2128\n",
      "Epoch: 1 / 3, Step: 1695 / 2250 Loss: 0.6151\n",
      "Epoch: 1 / 3, Step: 1696 / 2250 Loss: 0.3821\n",
      "Epoch: 1 / 3, Step: 1697 / 2250 Loss: 0.4784\n",
      "Epoch: 1 / 3, Step: 1698 / 2250 Loss: 0.4608\n",
      "Epoch: 1 / 3, Step: 1699 / 2250 Loss: 0.7273\n",
      "Epoch: 1 / 3, Step: 1700 / 2250 Loss: 0.4092\n",
      "Epoch: 1 / 3, Step: 1701 / 2250 Loss: 0.4498\n",
      "Epoch: 1 / 3, Step: 1702 / 2250 Loss: 0.4322\n",
      "Epoch: 1 / 3, Step: 1703 / 2250 Loss: 0.1608\n",
      "Epoch: 1 / 3, Step: 1704 / 2250 Loss: 0.2787\n",
      "Epoch: 1 / 3, Step: 1705 / 2250 Loss: 0.6849\n",
      "Epoch: 1 / 3, Step: 1706 / 2250 Loss: 0.1697\n",
      "Epoch: 1 / 3, Step: 1707 / 2250 Loss: 0.3198\n",
      "Epoch: 1 / 3, Step: 1708 / 2250 Loss: 0.5569\n",
      "Epoch: 1 / 3, Step: 1709 / 2250 Loss: 0.5711\n",
      "Epoch: 1 / 3, Step: 1710 / 2250 Loss: 0.4537\n",
      "Epoch: 1 / 3, Step: 1711 / 2250 Loss: 0.4068\n",
      "Epoch: 1 / 3, Step: 1712 / 2250 Loss: 0.3092\n",
      "Epoch: 1 / 3, Step: 1713 / 2250 Loss: 0.3895\n",
      "Epoch: 1 / 3, Step: 1714 / 2250 Loss: 0.3840\n",
      "Epoch: 1 / 3, Step: 1715 / 2250 Loss: 0.3661\n",
      "Epoch: 1 / 3, Step: 1716 / 2250 Loss: 0.6138\n",
      "Epoch: 1 / 3, Step: 1717 / 2250 Loss: 0.5945\n",
      "Epoch: 1 / 3, Step: 1718 / 2250 Loss: 0.3446\n",
      "Epoch: 1 / 3, Step: 1719 / 2250 Loss: 0.5722\n",
      "Epoch: 1 / 3, Step: 1720 / 2250 Loss: 0.4804\n",
      "Epoch: 1 / 3, Step: 1721 / 2250 Loss: 0.4497\n",
      "Epoch: 1 / 3, Step: 1722 / 2250 Loss: 0.4387\n",
      "Epoch: 1 / 3, Step: 1723 / 2250 Loss: 0.4336\n",
      "Epoch: 1 / 3, Step: 1724 / 2250 Loss: 0.7823\n",
      "Epoch: 1 / 3, Step: 1725 / 2250 Loss: 0.3085\n",
      "Epoch: 1 / 3, Step: 1726 / 2250 Loss: 0.1992\n",
      "Epoch: 1 / 3, Step: 1727 / 2250 Loss: 0.3691\n",
      "Epoch: 1 / 3, Step: 1728 / 2250 Loss: 0.4337\n",
      "Epoch: 1 / 3, Step: 1729 / 2250 Loss: 0.4443\n",
      "Epoch: 1 / 3, Step: 1730 / 2250 Loss: 0.2938\n",
      "Epoch: 1 / 3, Step: 1731 / 2250 Loss: 0.2786\n",
      "Epoch: 1 / 3, Step: 1732 / 2250 Loss: 0.3476\n",
      "Epoch: 1 / 3, Step: 1733 / 2250 Loss: 0.4074\n",
      "Epoch: 1 / 3, Step: 1734 / 2250 Loss: 0.1868\n",
      "Epoch: 1 / 3, Step: 1735 / 2250 Loss: 0.2382\n",
      "Epoch: 1 / 3, Step: 1736 / 2250 Loss: 0.4358\n",
      "Epoch: 1 / 3, Step: 1737 / 2250 Loss: 0.4803\n",
      "Epoch: 1 / 3, Step: 1738 / 2250 Loss: 0.3191\n",
      "Epoch: 1 / 3, Step: 1739 / 2250 Loss: 0.1203\n",
      "Epoch: 1 / 3, Step: 1740 / 2250 Loss: 0.7552\n",
      "Epoch: 1 / 3, Step: 1741 / 2250 Loss: 0.5997\n",
      "Epoch: 1 / 3, Step: 1742 / 2250 Loss: 0.5608\n",
      "Epoch: 1 / 3, Step: 1743 / 2250 Loss: 0.6249\n",
      "Epoch: 1 / 3, Step: 1744 / 2250 Loss: 0.4209\n",
      "Epoch: 1 / 3, Step: 1745 / 2250 Loss: 0.3715\n",
      "Epoch: 1 / 3, Step: 1746 / 2250 Loss: 0.3574\n",
      "Epoch: 1 / 3, Step: 1747 / 2250 Loss: 0.2286\n",
      "Epoch: 1 / 3, Step: 1748 / 2250 Loss: 0.3540\n",
      "Epoch: 1 / 3, Step: 1749 / 2250 Loss: 0.3678\n",
      "Epoch: 1 / 3, Step: 1750 / 2250 Loss: 0.5295\n",
      "Epoch: 1 / 3, Step: 1751 / 2250 Loss: 0.5122\n",
      "Epoch: 1 / 3, Step: 1752 / 2250 Loss: 0.1887\n",
      "Epoch: 1 / 3, Step: 1753 / 2250 Loss: 0.4831\n",
      "Epoch: 1 / 3, Step: 1754 / 2250 Loss: 0.3558\n",
      "Epoch: 1 / 3, Step: 1755 / 2250 Loss: 0.2742\n",
      "Epoch: 1 / 3, Step: 1756 / 2250 Loss: 0.4689\n",
      "Epoch: 1 / 3, Step: 1757 / 2250 Loss: 0.4550\n",
      "Epoch: 1 / 3, Step: 1758 / 2250 Loss: 0.3783\n",
      "Epoch: 1 / 3, Step: 1759 / 2250 Loss: 0.2274\n",
      "Epoch: 1 / 3, Step: 1760 / 2250 Loss: 0.4142\n",
      "Epoch: 1 / 3, Step: 1761 / 2250 Loss: 0.3693\n",
      "Epoch: 1 / 3, Step: 1762 / 2250 Loss: 0.4319\n",
      "Epoch: 1 / 3, Step: 1763 / 2250 Loss: 0.2672\n",
      "Epoch: 1 / 3, Step: 1764 / 2250 Loss: 0.7844\n",
      "Epoch: 1 / 3, Step: 1765 / 2250 Loss: 0.3500\n",
      "Epoch: 1 / 3, Step: 1766 / 2250 Loss: 0.3477\n",
      "Epoch: 1 / 3, Step: 1767 / 2250 Loss: 0.3259\n",
      "Epoch: 1 / 3, Step: 1768 / 2250 Loss: 0.6365\n",
      "Epoch: 1 / 3, Step: 1769 / 2250 Loss: 0.4190\n",
      "Epoch: 1 / 3, Step: 1770 / 2250 Loss: 0.2221\n",
      "Epoch: 1 / 3, Step: 1771 / 2250 Loss: 0.3086\n",
      "Epoch: 1 / 3, Step: 1772 / 2250 Loss: 0.3553\n",
      "Epoch: 1 / 3, Step: 1773 / 2250 Loss: 0.3185\n",
      "Epoch: 1 / 3, Step: 1774 / 2250 Loss: 0.4636\n",
      "Epoch: 1 / 3, Step: 1775 / 2250 Loss: 0.2565\n",
      "Epoch: 1 / 3, Step: 1776 / 2250 Loss: 0.2500\n",
      "Epoch: 1 / 3, Step: 1777 / 2250 Loss: 0.2982\n",
      "Epoch: 1 / 3, Step: 1778 / 2250 Loss: 0.3819\n",
      "Epoch: 1 / 3, Step: 1779 / 2250 Loss: 0.3513\n",
      "Epoch: 1 / 3, Step: 1780 / 2250 Loss: 0.2034\n",
      "Epoch: 1 / 3, Step: 1781 / 2250 Loss: 0.5909\n",
      "Epoch: 1 / 3, Step: 1782 / 2250 Loss: 0.4291\n",
      "Epoch: 1 / 3, Step: 1783 / 2250 Loss: 0.3253\n",
      "Epoch: 1 / 3, Step: 1784 / 2250 Loss: 0.4176\n",
      "Epoch: 1 / 3, Step: 1785 / 2250 Loss: 0.3391\n",
      "Epoch: 1 / 3, Step: 1786 / 2250 Loss: 0.3264\n",
      "Epoch: 1 / 3, Step: 1787 / 2250 Loss: 0.3519\n",
      "Epoch: 1 / 3, Step: 1788 / 2250 Loss: 0.2933\n",
      "Epoch: 1 / 3, Step: 1789 / 2250 Loss: 0.4735\n",
      "Epoch: 1 / 3, Step: 1790 / 2250 Loss: 0.6291\n",
      "Epoch: 1 / 3, Step: 1791 / 2250 Loss: 0.4048\n",
      "Epoch: 1 / 3, Step: 1792 / 2250 Loss: 0.2507\n",
      "Epoch: 1 / 3, Step: 1793 / 2250 Loss: 0.3649\n",
      "Epoch: 1 / 3, Step: 1794 / 2250 Loss: 0.2536\n",
      "Epoch: 1 / 3, Step: 1795 / 2250 Loss: 0.3093\n",
      "Epoch: 1 / 3, Step: 1796 / 2250 Loss: 0.2238\n",
      "Epoch: 1 / 3, Step: 1797 / 2250 Loss: 0.2745\n",
      "Epoch: 1 / 3, Step: 1798 / 2250 Loss: 0.2577\n",
      "Epoch: 1 / 3, Step: 1799 / 2250 Loss: 0.2076\n",
      "Epoch: 1 / 3, Step: 1800 / 2250 Loss: 0.4415\n",
      "Epoch: 1 / 3, Step: 1801 / 2250 Loss: 0.2635\n",
      "Epoch: 1 / 3, Step: 1802 / 2250 Loss: 0.3817\n",
      "Epoch: 1 / 3, Step: 1803 / 2250 Loss: 0.2556\n",
      "Epoch: 1 / 3, Step: 1804 / 2250 Loss: 0.4948\n",
      "Epoch: 1 / 3, Step: 1805 / 2250 Loss: 0.7611\n",
      "Epoch: 1 / 3, Step: 1806 / 2250 Loss: 0.2155\n",
      "Epoch: 1 / 3, Step: 1807 / 2250 Loss: 0.1618\n",
      "Epoch: 1 / 3, Step: 1808 / 2250 Loss: 0.3980\n",
      "Epoch: 1 / 3, Step: 1809 / 2250 Loss: 0.2800\n",
      "Epoch: 1 / 3, Step: 1810 / 2250 Loss: 0.3579\n",
      "Epoch: 1 / 3, Step: 1811 / 2250 Loss: 0.2528\n",
      "Epoch: 1 / 3, Step: 1812 / 2250 Loss: 0.3803\n",
      "Epoch: 1 / 3, Step: 1813 / 2250 Loss: 0.4919\n",
      "Epoch: 1 / 3, Step: 1814 / 2250 Loss: 0.4982\n",
      "Epoch: 1 / 3, Step: 1815 / 2250 Loss: 0.5736\n",
      "Epoch: 1 / 3, Step: 1816 / 2250 Loss: 0.3903\n",
      "Epoch: 1 / 3, Step: 1817 / 2250 Loss: 0.1794\n",
      "Epoch: 1 / 3, Step: 1818 / 2250 Loss: 0.3646\n",
      "Epoch: 1 / 3, Step: 1819 / 2250 Loss: 0.1617\n",
      "Epoch: 1 / 3, Step: 1820 / 2250 Loss: 0.2995\n",
      "Epoch: 1 / 3, Step: 1821 / 2250 Loss: 0.2631\n",
      "Epoch: 1 / 3, Step: 1822 / 2250 Loss: 0.4062\n",
      "Epoch: 1 / 3, Step: 1823 / 2250 Loss: 0.1185\n",
      "Epoch: 1 / 3, Step: 1824 / 2250 Loss: 0.2412\n",
      "Epoch: 1 / 3, Step: 1825 / 2250 Loss: 0.1892\n",
      "Epoch: 1 / 3, Step: 1826 / 2250 Loss: 0.4751\n",
      "Epoch: 1 / 3, Step: 1827 / 2250 Loss: 0.3386\n",
      "Epoch: 1 / 3, Step: 1828 / 2250 Loss: 0.5705\n",
      "Epoch: 1 / 3, Step: 1829 / 2250 Loss: 0.4951\n",
      "Epoch: 1 / 3, Step: 1830 / 2250 Loss: 0.6931\n",
      "Epoch: 1 / 3, Step: 1831 / 2250 Loss: 0.6210\n",
      "Epoch: 1 / 3, Step: 1832 / 2250 Loss: 0.4655\n",
      "Epoch: 1 / 3, Step: 1833 / 2250 Loss: 0.5679\n",
      "Epoch: 1 / 3, Step: 1834 / 2250 Loss: 0.3346\n",
      "Epoch: 1 / 3, Step: 1835 / 2250 Loss: 0.1844\n",
      "Epoch: 1 / 3, Step: 1836 / 2250 Loss: 0.1876\n",
      "Epoch: 1 / 3, Step: 1837 / 2250 Loss: 0.7012\n",
      "Epoch: 1 / 3, Step: 1838 / 2250 Loss: 0.2329\n",
      "Epoch: 1 / 3, Step: 1839 / 2250 Loss: 0.3117\n",
      "Epoch: 1 / 3, Step: 1840 / 2250 Loss: 0.4095\n",
      "Epoch: 1 / 3, Step: 1841 / 2250 Loss: 0.2585\n",
      "Epoch: 1 / 3, Step: 1842 / 2250 Loss: 0.6821\n",
      "Epoch: 1 / 3, Step: 1843 / 2250 Loss: 0.3556\n",
      "Epoch: 1 / 3, Step: 1844 / 2250 Loss: 0.2774\n",
      "Epoch: 1 / 3, Step: 1845 / 2250 Loss: 0.3127\n",
      "Epoch: 1 / 3, Step: 1846 / 2250 Loss: 0.3282\n",
      "Epoch: 1 / 3, Step: 1847 / 2250 Loss: 0.5069\n",
      "Epoch: 1 / 3, Step: 1848 / 2250 Loss: 0.2661\n",
      "Epoch: 1 / 3, Step: 1849 / 2250 Loss: 0.4584\n",
      "Epoch: 1 / 3, Step: 1850 / 2250 Loss: 0.1912\n",
      "Epoch: 1 / 3, Step: 1851 / 2250 Loss: 0.3480\n",
      "Epoch: 1 / 3, Step: 1852 / 2250 Loss: 0.6688\n",
      "Epoch: 1 / 3, Step: 1853 / 2250 Loss: 0.4144\n",
      "Epoch: 1 / 3, Step: 1854 / 2250 Loss: 0.3189\n",
      "Epoch: 1 / 3, Step: 1855 / 2250 Loss: 0.6863\n",
      "Epoch: 1 / 3, Step: 1856 / 2250 Loss: 0.3186\n",
      "Epoch: 1 / 3, Step: 1857 / 2250 Loss: 0.1934\n",
      "Epoch: 1 / 3, Step: 1858 / 2250 Loss: 0.3461\n",
      "Epoch: 1 / 3, Step: 1859 / 2250 Loss: 0.2123\n",
      "Epoch: 1 / 3, Step: 1860 / 2250 Loss: 0.6558\n",
      "Epoch: 1 / 3, Step: 1861 / 2250 Loss: 0.3422\n",
      "Epoch: 1 / 3, Step: 1862 / 2250 Loss: 0.1944\n",
      "Epoch: 1 / 3, Step: 1863 / 2250 Loss: 0.2453\n",
      "Epoch: 1 / 3, Step: 1864 / 2250 Loss: 0.5374\n",
      "Epoch: 1 / 3, Step: 1865 / 2250 Loss: 0.2462\n",
      "Epoch: 1 / 3, Step: 1866 / 2250 Loss: 0.5057\n",
      "Epoch: 1 / 3, Step: 1867 / 2250 Loss: 0.2799\n",
      "Epoch: 1 / 3, Step: 1868 / 2250 Loss: 0.2480\n",
      "Epoch: 1 / 3, Step: 1869 / 2250 Loss: 0.5573\n",
      "Epoch: 1 / 3, Step: 1870 / 2250 Loss: 0.3627\n",
      "Epoch: 1 / 3, Step: 1871 / 2250 Loss: 0.2733\n",
      "Epoch: 1 / 3, Step: 1872 / 2250 Loss: 0.8538\n",
      "Epoch: 1 / 3, Step: 1873 / 2250 Loss: 0.4562\n",
      "Epoch: 1 / 3, Step: 1874 / 2250 Loss: 0.1217\n",
      "Epoch: 1 / 3, Step: 1875 / 2250 Loss: 0.3822\n",
      "Epoch: 1 / 3, Step: 1876 / 2250 Loss: 0.3967\n",
      "Epoch: 1 / 3, Step: 1877 / 2250 Loss: 0.3178\n",
      "Epoch: 1 / 3, Step: 1878 / 2250 Loss: 0.3911\n",
      "Epoch: 1 / 3, Step: 1879 / 2250 Loss: 0.5478\n",
      "Epoch: 1 / 3, Step: 1880 / 2250 Loss: 0.2791\n",
      "Epoch: 1 / 3, Step: 1881 / 2250 Loss: 0.4196\n",
      "Epoch: 1 / 3, Step: 1882 / 2250 Loss: 0.2745\n",
      "Epoch: 1 / 3, Step: 1883 / 2250 Loss: 0.3064\n",
      "Epoch: 1 / 3, Step: 1884 / 2250 Loss: 0.2701\n",
      "Epoch: 1 / 3, Step: 1885 / 2250 Loss: 0.5012\n",
      "Epoch: 1 / 3, Step: 1886 / 2250 Loss: 0.2236\n",
      "Epoch: 1 / 3, Step: 1887 / 2250 Loss: 0.3112\n",
      "Epoch: 1 / 3, Step: 1888 / 2250 Loss: 0.4231\n",
      "Epoch: 1 / 3, Step: 1889 / 2250 Loss: 0.5151\n",
      "Epoch: 1 / 3, Step: 1890 / 2250 Loss: 0.4312\n",
      "Epoch: 1 / 3, Step: 1891 / 2250 Loss: 0.2378\n",
      "Epoch: 1 / 3, Step: 1892 / 2250 Loss: 0.3970\n",
      "Epoch: 1 / 3, Step: 1893 / 2250 Loss: 0.5379\n",
      "Epoch: 1 / 3, Step: 1894 / 2250 Loss: 0.3612\n",
      "Epoch: 1 / 3, Step: 1895 / 2250 Loss: 0.5521\n",
      "Epoch: 1 / 3, Step: 1896 / 2250 Loss: 0.4736\n",
      "Epoch: 1 / 3, Step: 1897 / 2250 Loss: 0.2352\n",
      "Epoch: 1 / 3, Step: 1898 / 2250 Loss: 0.3276\n",
      "Epoch: 1 / 3, Step: 1899 / 2250 Loss: 0.2793\n",
      "Epoch: 1 / 3, Step: 1900 / 2250 Loss: 0.3606\n",
      "Epoch: 1 / 3, Step: 1901 / 2250 Loss: 0.5341\n",
      "Epoch: 1 / 3, Step: 1902 / 2250 Loss: 0.3077\n",
      "Epoch: 1 / 3, Step: 1903 / 2250 Loss: 0.3924\n",
      "Epoch: 1 / 3, Step: 1904 / 2250 Loss: 0.3789\n",
      "Epoch: 1 / 3, Step: 1905 / 2250 Loss: 0.4589\n",
      "Epoch: 1 / 3, Step: 1906 / 2250 Loss: 0.5955\n",
      "Epoch: 1 / 3, Step: 1907 / 2250 Loss: 0.4436\n",
      "Epoch: 1 / 3, Step: 1908 / 2250 Loss: 0.1947\n",
      "Epoch: 1 / 3, Step: 1909 / 2250 Loss: 0.3364\n",
      "Epoch: 1 / 3, Step: 1910 / 2250 Loss: 0.3983\n",
      "Epoch: 1 / 3, Step: 1911 / 2250 Loss: 0.3597\n",
      "Epoch: 1 / 3, Step: 1912 / 2250 Loss: 0.3781\n",
      "Epoch: 1 / 3, Step: 1913 / 2250 Loss: 0.2658\n",
      "Epoch: 1 / 3, Step: 1914 / 2250 Loss: 0.2324\n",
      "Epoch: 1 / 3, Step: 1915 / 2250 Loss: 0.3569\n",
      "Epoch: 1 / 3, Step: 1916 / 2250 Loss: 0.3193\n",
      "Epoch: 1 / 3, Step: 1917 / 2250 Loss: 0.2900\n",
      "Epoch: 1 / 3, Step: 1918 / 2250 Loss: 0.7808\n",
      "Epoch: 1 / 3, Step: 1919 / 2250 Loss: 0.4654\n",
      "Epoch: 1 / 3, Step: 1920 / 2250 Loss: 0.1956\n",
      "Epoch: 1 / 3, Step: 1921 / 2250 Loss: 0.3110\n",
      "Epoch: 1 / 3, Step: 1922 / 2250 Loss: 0.3494\n",
      "Epoch: 1 / 3, Step: 1923 / 2250 Loss: 0.3233\n",
      "Epoch: 1 / 3, Step: 1924 / 2250 Loss: 0.2793\n",
      "Epoch: 1 / 3, Step: 1925 / 2250 Loss: 0.2438\n",
      "Epoch: 1 / 3, Step: 1926 / 2250 Loss: 0.2775\n",
      "Epoch: 1 / 3, Step: 1927 / 2250 Loss: 0.4227\n",
      "Epoch: 1 / 3, Step: 1928 / 2250 Loss: 0.1493\n",
      "Epoch: 1 / 3, Step: 1929 / 2250 Loss: 0.1857\n",
      "Epoch: 1 / 3, Step: 1930 / 2250 Loss: 0.1954\n",
      "Epoch: 1 / 3, Step: 1931 / 2250 Loss: 0.2131\n",
      "Epoch: 1 / 3, Step: 1932 / 2250 Loss: 0.2867\n",
      "Epoch: 1 / 3, Step: 1933 / 2250 Loss: 0.2824\n",
      "Epoch: 1 / 3, Step: 1934 / 2250 Loss: 0.3154\n",
      "Epoch: 1 / 3, Step: 1935 / 2250 Loss: 0.5564\n",
      "Epoch: 1 / 3, Step: 1936 / 2250 Loss: 0.2591\n",
      "Epoch: 1 / 3, Step: 1937 / 2250 Loss: 0.4377\n",
      "Epoch: 1 / 3, Step: 1938 / 2250 Loss: 0.1867\n",
      "Epoch: 1 / 3, Step: 1939 / 2250 Loss: 0.2813\n",
      "Epoch: 1 / 3, Step: 1940 / 2250 Loss: 0.3829\n",
      "Epoch: 1 / 3, Step: 1941 / 2250 Loss: 0.2430\n",
      "Epoch: 1 / 3, Step: 1942 / 2250 Loss: 0.3936\n",
      "Epoch: 1 / 3, Step: 1943 / 2250 Loss: 0.5219\n",
      "Epoch: 1 / 3, Step: 1944 / 2250 Loss: 0.5346\n",
      "Epoch: 1 / 3, Step: 1945 / 2250 Loss: 0.3520\n",
      "Epoch: 1 / 3, Step: 1946 / 2250 Loss: 0.3280\n",
      "Epoch: 1 / 3, Step: 1947 / 2250 Loss: 0.3589\n",
      "Epoch: 1 / 3, Step: 1948 / 2250 Loss: 0.4657\n",
      "Epoch: 1 / 3, Step: 1949 / 2250 Loss: 0.4452\n",
      "Epoch: 1 / 3, Step: 1950 / 2250 Loss: 0.4092\n",
      "Epoch: 1 / 3, Step: 1951 / 2250 Loss: 0.3295\n",
      "Epoch: 1 / 3, Step: 1952 / 2250 Loss: 0.5099\n",
      "Epoch: 1 / 3, Step: 1953 / 2250 Loss: 0.4664\n",
      "Epoch: 1 / 3, Step: 1954 / 2250 Loss: 0.4091\n",
      "Epoch: 1 / 3, Step: 1955 / 2250 Loss: 0.4331\n",
      "Epoch: 1 / 3, Step: 1956 / 2250 Loss: 0.4004\n",
      "Epoch: 1 / 3, Step: 1957 / 2250 Loss: 0.3249\n",
      "Epoch: 1 / 3, Step: 1958 / 2250 Loss: 0.4770\n",
      "Epoch: 1 / 3, Step: 1959 / 2250 Loss: 0.2606\n",
      "Epoch: 1 / 3, Step: 1960 / 2250 Loss: 0.3277\n",
      "Epoch: 1 / 3, Step: 1961 / 2250 Loss: 0.3151\n",
      "Epoch: 1 / 3, Step: 1962 / 2250 Loss: 0.3844\n",
      "Epoch: 1 / 3, Step: 1963 / 2250 Loss: 0.2166\n",
      "Epoch: 1 / 3, Step: 1964 / 2250 Loss: 0.3712\n",
      "Epoch: 1 / 3, Step: 1965 / 2250 Loss: 0.5402\n",
      "Epoch: 1 / 3, Step: 1966 / 2250 Loss: 0.3326\n",
      "Epoch: 1 / 3, Step: 1967 / 2250 Loss: 0.4358\n",
      "Epoch: 1 / 3, Step: 1968 / 2250 Loss: 0.4383\n",
      "Epoch: 1 / 3, Step: 1969 / 2250 Loss: 0.5099\n",
      "Epoch: 1 / 3, Step: 1970 / 2250 Loss: 0.3085\n",
      "Epoch: 1 / 3, Step: 1971 / 2250 Loss: 0.4306\n",
      "Epoch: 1 / 3, Step: 1972 / 2250 Loss: 0.3108\n",
      "Epoch: 1 / 3, Step: 1973 / 2250 Loss: 0.5759\n",
      "Epoch: 1 / 3, Step: 1974 / 2250 Loss: 0.3523\n",
      "Epoch: 1 / 3, Step: 1975 / 2250 Loss: 0.3669\n",
      "Epoch: 1 / 3, Step: 1976 / 2250 Loss: 0.5455\n",
      "Epoch: 1 / 3, Step: 1977 / 2250 Loss: 0.5939\n",
      "Epoch: 1 / 3, Step: 1978 / 2250 Loss: 0.4666\n",
      "Epoch: 1 / 3, Step: 1979 / 2250 Loss: 0.6054\n",
      "Epoch: 1 / 3, Step: 1980 / 2250 Loss: 0.2986\n",
      "Epoch: 1 / 3, Step: 1981 / 2250 Loss: 0.4910\n",
      "Epoch: 1 / 3, Step: 1982 / 2250 Loss: 0.4952\n",
      "Epoch: 1 / 3, Step: 1983 / 2250 Loss: 0.3825\n",
      "Epoch: 1 / 3, Step: 1984 / 2250 Loss: 0.4601\n",
      "Epoch: 1 / 3, Step: 1985 / 2250 Loss: 0.4575\n",
      "Epoch: 1 / 3, Step: 1986 / 2250 Loss: 0.3514\n",
      "Epoch: 1 / 3, Step: 1987 / 2250 Loss: 0.2866\n",
      "Epoch: 1 / 3, Step: 1988 / 2250 Loss: 0.3070\n",
      "Epoch: 1 / 3, Step: 1989 / 2250 Loss: 0.2737\n",
      "Epoch: 1 / 3, Step: 1990 / 2250 Loss: 0.3446\n",
      "Epoch: 1 / 3, Step: 1991 / 2250 Loss: 0.3458\n",
      "Epoch: 1 / 3, Step: 1992 / 2250 Loss: 0.5101\n",
      "Epoch: 1 / 3, Step: 1993 / 2250 Loss: 0.2915\n",
      "Epoch: 1 / 3, Step: 1994 / 2250 Loss: 0.7256\n",
      "Epoch: 1 / 3, Step: 1995 / 2250 Loss: 0.3593\n",
      "Epoch: 1 / 3, Step: 1996 / 2250 Loss: 0.6079\n",
      "Epoch: 1 / 3, Step: 1997 / 2250 Loss: 0.4030\n",
      "Epoch: 1 / 3, Step: 1998 / 2250 Loss: 0.1370\n",
      "Epoch: 1 / 3, Step: 1999 / 2250 Loss: 0.4274\n",
      "Epoch: 1 / 3, Step: 2000 / 2250 Loss: 0.4846\n",
      "Epoch: 1 / 3, Step: 2001 / 2250 Loss: 0.2505\n",
      "Epoch: 1 / 3, Step: 2002 / 2250 Loss: 0.4887\n",
      "Epoch: 1 / 3, Step: 2003 / 2250 Loss: 0.4080\n",
      "Epoch: 1 / 3, Step: 2004 / 2250 Loss: 0.4257\n",
      "Epoch: 1 / 3, Step: 2005 / 2250 Loss: 0.5287\n",
      "Epoch: 1 / 3, Step: 2006 / 2250 Loss: 0.5224\n",
      "Epoch: 1 / 3, Step: 2007 / 2250 Loss: 0.2840\n",
      "Epoch: 1 / 3, Step: 2008 / 2250 Loss: 0.1778\n",
      "Epoch: 1 / 3, Step: 2009 / 2250 Loss: 0.3760\n",
      "Epoch: 1 / 3, Step: 2010 / 2250 Loss: 0.5747\n",
      "Epoch: 1 / 3, Step: 2011 / 2250 Loss: 0.4420\n",
      "Epoch: 1 / 3, Step: 2012 / 2250 Loss: 0.1654\n",
      "Epoch: 1 / 3, Step: 2013 / 2250 Loss: 0.4133\n",
      "Epoch: 1 / 3, Step: 2014 / 2250 Loss: 0.2618\n",
      "Epoch: 1 / 3, Step: 2015 / 2250 Loss: 0.3811\n",
      "Epoch: 1 / 3, Step: 2016 / 2250 Loss: 0.3051\n",
      "Epoch: 1 / 3, Step: 2017 / 2250 Loss: 0.3640\n",
      "Epoch: 1 / 3, Step: 2018 / 2250 Loss: 0.4652\n",
      "Epoch: 1 / 3, Step: 2019 / 2250 Loss: 0.4561\n",
      "Epoch: 1 / 3, Step: 2020 / 2250 Loss: 0.1190\n",
      "Epoch: 1 / 3, Step: 2021 / 2250 Loss: 0.2229\n",
      "Epoch: 1 / 3, Step: 2022 / 2250 Loss: 0.4172\n",
      "Epoch: 1 / 3, Step: 2023 / 2250 Loss: 0.2369\n",
      "Epoch: 1 / 3, Step: 2024 / 2250 Loss: 0.2126\n",
      "Epoch: 1 / 3, Step: 2025 / 2250 Loss: 0.2071\n",
      "Epoch: 1 / 3, Step: 2026 / 2250 Loss: 0.2819\n",
      "Epoch: 1 / 3, Step: 2027 / 2250 Loss: 0.1666\n",
      "Epoch: 1 / 3, Step: 2028 / 2250 Loss: 0.2729\n",
      "Epoch: 1 / 3, Step: 2029 / 2250 Loss: 0.4178\n",
      "Epoch: 1 / 3, Step: 2030 / 2250 Loss: 0.3162\n",
      "Epoch: 1 / 3, Step: 2031 / 2250 Loss: 0.5775\n",
      "Epoch: 1 / 3, Step: 2032 / 2250 Loss: 0.3969\n",
      "Epoch: 1 / 3, Step: 2033 / 2250 Loss: 0.4111\n",
      "Epoch: 1 / 3, Step: 2034 / 2250 Loss: 0.2572\n",
      "Epoch: 1 / 3, Step: 2035 / 2250 Loss: 0.4191\n",
      "Epoch: 1 / 3, Step: 2036 / 2250 Loss: 0.4545\n",
      "Epoch: 1 / 3, Step: 2037 / 2250 Loss: 0.3707\n",
      "Epoch: 1 / 3, Step: 2038 / 2250 Loss: 0.5776\n",
      "Epoch: 1 / 3, Step: 2039 / 2250 Loss: 0.2611\n",
      "Epoch: 1 / 3, Step: 2040 / 2250 Loss: 0.2144\n",
      "Epoch: 1 / 3, Step: 2041 / 2250 Loss: 0.3349\n",
      "Epoch: 1 / 3, Step: 2042 / 2250 Loss: 0.3535\n",
      "Epoch: 1 / 3, Step: 2043 / 2250 Loss: 0.5507\n",
      "Epoch: 1 / 3, Step: 2044 / 2250 Loss: 0.2949\n",
      "Epoch: 1 / 3, Step: 2045 / 2250 Loss: 0.4225\n",
      "Epoch: 1 / 3, Step: 2046 / 2250 Loss: 0.6792\n",
      "Epoch: 1 / 3, Step: 2047 / 2250 Loss: 0.6794\n",
      "Epoch: 1 / 3, Step: 2048 / 2250 Loss: 0.1815\n",
      "Epoch: 1 / 3, Step: 2049 / 2250 Loss: 0.4138\n",
      "Epoch: 1 / 3, Step: 2050 / 2250 Loss: 0.2895\n",
      "Epoch: 1 / 3, Step: 2051 / 2250 Loss: 0.4593\n",
      "Epoch: 1 / 3, Step: 2052 / 2250 Loss: 0.2989\n",
      "Epoch: 1 / 3, Step: 2053 / 2250 Loss: 0.5412\n",
      "Epoch: 1 / 3, Step: 2054 / 2250 Loss: 0.2426\n",
      "Epoch: 1 / 3, Step: 2055 / 2250 Loss: 0.4563\n",
      "Epoch: 1 / 3, Step: 2056 / 2250 Loss: 0.3727\n",
      "Epoch: 1 / 3, Step: 2057 / 2250 Loss: 0.2920\n",
      "Epoch: 1 / 3, Step: 2058 / 2250 Loss: 0.3655\n",
      "Epoch: 1 / 3, Step: 2059 / 2250 Loss: 0.2081\n",
      "Epoch: 1 / 3, Step: 2060 / 2250 Loss: 0.2223\n",
      "Epoch: 1 / 3, Step: 2061 / 2250 Loss: 0.2355\n",
      "Epoch: 1 / 3, Step: 2062 / 2250 Loss: 0.1904\n",
      "Epoch: 1 / 3, Step: 2063 / 2250 Loss: 0.1953\n",
      "Epoch: 1 / 3, Step: 2064 / 2250 Loss: 0.1188\n",
      "Epoch: 1 / 3, Step: 2065 / 2250 Loss: 0.4686\n",
      "Epoch: 1 / 3, Step: 2066 / 2250 Loss: 0.4100\n",
      "Epoch: 1 / 3, Step: 2067 / 2250 Loss: 0.2901\n",
      "Epoch: 1 / 3, Step: 2068 / 2250 Loss: 0.5689\n",
      "Epoch: 1 / 3, Step: 2069 / 2250 Loss: 0.1937\n",
      "Epoch: 1 / 3, Step: 2070 / 2250 Loss: 0.2812\n",
      "Epoch: 1 / 3, Step: 2071 / 2250 Loss: 0.4649\n",
      "Epoch: 1 / 3, Step: 2072 / 2250 Loss: 0.2110\n",
      "Epoch: 1 / 3, Step: 2073 / 2250 Loss: 0.4651\n",
      "Epoch: 1 / 3, Step: 2074 / 2250 Loss: 0.2194\n",
      "Epoch: 1 / 3, Step: 2075 / 2250 Loss: 0.6040\n",
      "Epoch: 1 / 3, Step: 2076 / 2250 Loss: 0.4421\n",
      "Epoch: 1 / 3, Step: 2077 / 2250 Loss: 0.3282\n",
      "Epoch: 1 / 3, Step: 2078 / 2250 Loss: 0.6063\n",
      "Epoch: 1 / 3, Step: 2079 / 2250 Loss: 0.2231\n",
      "Epoch: 1 / 3, Step: 2080 / 2250 Loss: 0.3499\n",
      "Epoch: 1 / 3, Step: 2081 / 2250 Loss: 0.3611\n",
      "Epoch: 1 / 3, Step: 2082 / 2250 Loss: 0.4467\n",
      "Epoch: 1 / 3, Step: 2083 / 2250 Loss: 0.3383\n",
      "Epoch: 1 / 3, Step: 2084 / 2250 Loss: 0.5156\n",
      "Epoch: 1 / 3, Step: 2085 / 2250 Loss: 0.5304\n",
      "Epoch: 1 / 3, Step: 2086 / 2250 Loss: 0.3456\n",
      "Epoch: 1 / 3, Step: 2087 / 2250 Loss: 0.2986\n",
      "Epoch: 1 / 3, Step: 2088 / 2250 Loss: 0.3385\n",
      "Epoch: 1 / 3, Step: 2089 / 2250 Loss: 0.2549\n",
      "Epoch: 1 / 3, Step: 2090 / 2250 Loss: 0.3546\n",
      "Epoch: 1 / 3, Step: 2091 / 2250 Loss: 0.3306\n",
      "Epoch: 1 / 3, Step: 2092 / 2250 Loss: 0.3149\n",
      "Epoch: 1 / 3, Step: 2093 / 2250 Loss: 0.5361\n",
      "Epoch: 1 / 3, Step: 2094 / 2250 Loss: 0.6031\n",
      "Epoch: 1 / 3, Step: 2095 / 2250 Loss: 0.3952\n",
      "Epoch: 1 / 3, Step: 2096 / 2250 Loss: 0.4152\n",
      "Epoch: 1 / 3, Step: 2097 / 2250 Loss: 0.4993\n",
      "Epoch: 1 / 3, Step: 2098 / 2250 Loss: 0.2128\n",
      "Epoch: 1 / 3, Step: 2099 / 2250 Loss: 0.2010\n",
      "Epoch: 1 / 3, Step: 2100 / 2250 Loss: 0.2559\n",
      "Epoch: 1 / 3, Step: 2101 / 2250 Loss: 0.6173\n",
      "Epoch: 1 / 3, Step: 2102 / 2250 Loss: 0.4377\n",
      "Epoch: 1 / 3, Step: 2103 / 2250 Loss: 0.3202\n",
      "Epoch: 1 / 3, Step: 2104 / 2250 Loss: 0.4757\n",
      "Epoch: 1 / 3, Step: 2105 / 2250 Loss: 0.1830\n",
      "Epoch: 1 / 3, Step: 2106 / 2250 Loss: 0.4607\n",
      "Epoch: 1 / 3, Step: 2107 / 2250 Loss: 0.3220\n",
      "Epoch: 1 / 3, Step: 2108 / 2250 Loss: 0.2306\n",
      "Epoch: 1 / 3, Step: 2109 / 2250 Loss: 0.2095\n",
      "Epoch: 1 / 3, Step: 2110 / 2250 Loss: 0.3787\n",
      "Epoch: 1 / 3, Step: 2111 / 2250 Loss: 0.5473\n",
      "Epoch: 1 / 3, Step: 2112 / 2250 Loss: 0.3991\n",
      "Epoch: 1 / 3, Step: 2113 / 2250 Loss: 0.3832\n",
      "Epoch: 1 / 3, Step: 2114 / 2250 Loss: 0.5000\n",
      "Epoch: 1 / 3, Step: 2115 / 2250 Loss: 0.3883\n",
      "Epoch: 1 / 3, Step: 2116 / 2250 Loss: 0.4252\n",
      "Epoch: 1 / 3, Step: 2117 / 2250 Loss: 0.2784\n",
      "Epoch: 1 / 3, Step: 2118 / 2250 Loss: 0.3273\n",
      "Epoch: 1 / 3, Step: 2119 / 2250 Loss: 0.4373\n",
      "Epoch: 1 / 3, Step: 2120 / 2250 Loss: 0.3153\n",
      "Epoch: 1 / 3, Step: 2121 / 2250 Loss: 0.3114\n",
      "Epoch: 1 / 3, Step: 2122 / 2250 Loss: 0.4063\n",
      "Epoch: 1 / 3, Step: 2123 / 2250 Loss: 0.3931\n",
      "Epoch: 1 / 3, Step: 2124 / 2250 Loss: 0.3648\n",
      "Epoch: 1 / 3, Step: 2125 / 2250 Loss: 0.3169\n",
      "Epoch: 1 / 3, Step: 2126 / 2250 Loss: 0.2225\n",
      "Epoch: 1 / 3, Step: 2127 / 2250 Loss: 0.2982\n",
      "Epoch: 1 / 3, Step: 2128 / 2250 Loss: 0.1563\n",
      "Epoch: 1 / 3, Step: 2129 / 2250 Loss: 0.4588\n",
      "Epoch: 1 / 3, Step: 2130 / 2250 Loss: 0.4206\n",
      "Epoch: 1 / 3, Step: 2131 / 2250 Loss: 0.3332\n",
      "Epoch: 1 / 3, Step: 2132 / 2250 Loss: 0.2192\n",
      "Epoch: 1 / 3, Step: 2133 / 2250 Loss: 0.3232\n",
      "Epoch: 1 / 3, Step: 2134 / 2250 Loss: 0.4794\n",
      "Epoch: 1 / 3, Step: 2135 / 2250 Loss: 0.3518\n",
      "Epoch: 1 / 3, Step: 2136 / 2250 Loss: 0.2762\n",
      "Epoch: 1 / 3, Step: 2137 / 2250 Loss: 0.3210\n",
      "Epoch: 1 / 3, Step: 2138 / 2250 Loss: 0.2156\n",
      "Epoch: 1 / 3, Step: 2139 / 2250 Loss: 0.3229\n",
      "Epoch: 1 / 3, Step: 2140 / 2250 Loss: 0.3785\n",
      "Epoch: 1 / 3, Step: 2141 / 2250 Loss: 0.1798\n",
      "Epoch: 1 / 3, Step: 2142 / 2250 Loss: 0.2199\n",
      "Epoch: 1 / 3, Step: 2143 / 2250 Loss: 0.1127\n",
      "Epoch: 1 / 3, Step: 2144 / 2250 Loss: 0.3741\n",
      "Epoch: 1 / 3, Step: 2145 / 2250 Loss: 0.3535\n",
      "Epoch: 1 / 3, Step: 2146 / 2250 Loss: 0.3384\n",
      "Epoch: 1 / 3, Step: 2147 / 2250 Loss: 0.3112\n",
      "Epoch: 1 / 3, Step: 2148 / 2250 Loss: 0.3786\n",
      "Epoch: 1 / 3, Step: 2149 / 2250 Loss: 0.2814\n",
      "Epoch: 1 / 3, Step: 2150 / 2250 Loss: 0.2181\n",
      "Epoch: 1 / 3, Step: 2151 / 2250 Loss: 0.3081\n",
      "Epoch: 1 / 3, Step: 2152 / 2250 Loss: 0.3134\n",
      "Epoch: 1 / 3, Step: 2153 / 2250 Loss: 0.3958\n",
      "Epoch: 1 / 3, Step: 2154 / 2250 Loss: 0.2193\n",
      "Epoch: 1 / 3, Step: 2155 / 2250 Loss: 0.4476\n",
      "Epoch: 1 / 3, Step: 2156 / 2250 Loss: 0.4898\n",
      "Epoch: 1 / 3, Step: 2157 / 2250 Loss: 0.2013\n",
      "Epoch: 1 / 3, Step: 2158 / 2250 Loss: 0.4277\n",
      "Epoch: 1 / 3, Step: 2159 / 2250 Loss: 0.2451\n",
      "Epoch: 1 / 3, Step: 2160 / 2250 Loss: 0.6497\n",
      "Epoch: 1 / 3, Step: 2161 / 2250 Loss: 0.3249\n",
      "Epoch: 1 / 3, Step: 2162 / 2250 Loss: 0.4335\n",
      "Epoch: 1 / 3, Step: 2163 / 2250 Loss: 0.3400\n",
      "Epoch: 1 / 3, Step: 2164 / 2250 Loss: 0.5204\n",
      "Epoch: 1 / 3, Step: 2165 / 2250 Loss: 0.4984\n",
      "Epoch: 1 / 3, Step: 2166 / 2250 Loss: 0.3964\n",
      "Epoch: 1 / 3, Step: 2167 / 2250 Loss: 0.2857\n",
      "Epoch: 1 / 3, Step: 2168 / 2250 Loss: 0.4958\n",
      "Epoch: 1 / 3, Step: 2169 / 2250 Loss: 0.3092\n",
      "Epoch: 1 / 3, Step: 2170 / 2250 Loss: 0.5418\n",
      "Epoch: 1 / 3, Step: 2171 / 2250 Loss: 0.2644\n",
      "Epoch: 1 / 3, Step: 2172 / 2250 Loss: 0.4006\n",
      "Epoch: 1 / 3, Step: 2173 / 2250 Loss: 0.5247\n",
      "Epoch: 1 / 3, Step: 2174 / 2250 Loss: 0.3721\n",
      "Epoch: 1 / 3, Step: 2175 / 2250 Loss: 0.2015\n",
      "Epoch: 1 / 3, Step: 2176 / 2250 Loss: 0.3002\n",
      "Epoch: 1 / 3, Step: 2177 / 2250 Loss: 0.2916\n",
      "Epoch: 1 / 3, Step: 2178 / 2250 Loss: 0.3136\n",
      "Epoch: 1 / 3, Step: 2179 / 2250 Loss: 0.3053\n",
      "Epoch: 1 / 3, Step: 2180 / 2250 Loss: 0.2563\n",
      "Epoch: 1 / 3, Step: 2181 / 2250 Loss: 0.4098\n",
      "Epoch: 1 / 3, Step: 2182 / 2250 Loss: 0.3489\n",
      "Epoch: 1 / 3, Step: 2183 / 2250 Loss: 0.5321\n",
      "Epoch: 1 / 3, Step: 2184 / 2250 Loss: 0.2977\n",
      "Epoch: 1 / 3, Step: 2185 / 2250 Loss: 0.3992\n",
      "Epoch: 1 / 3, Step: 2186 / 2250 Loss: 0.2093\n",
      "Epoch: 1 / 3, Step: 2187 / 2250 Loss: 0.1882\n",
      "Epoch: 1 / 3, Step: 2188 / 2250 Loss: 0.3276\n",
      "Epoch: 1 / 3, Step: 2189 / 2250 Loss: 0.5251\n",
      "Epoch: 1 / 3, Step: 2190 / 2250 Loss: 0.3598\n",
      "Epoch: 1 / 3, Step: 2191 / 2250 Loss: 0.3334\n",
      "Epoch: 1 / 3, Step: 2192 / 2250 Loss: 0.3138\n",
      "Epoch: 1 / 3, Step: 2193 / 2250 Loss: 0.2535\n",
      "Epoch: 1 / 3, Step: 2194 / 2250 Loss: 0.3982\n",
      "Epoch: 1 / 3, Step: 2195 / 2250 Loss: 0.4228\n",
      "Epoch: 1 / 3, Step: 2196 / 2250 Loss: 0.2100\n",
      "Epoch: 1 / 3, Step: 2197 / 2250 Loss: 0.1995\n",
      "Epoch: 1 / 3, Step: 2198 / 2250 Loss: 0.2461\n",
      "Epoch: 1 / 3, Step: 2199 / 2250 Loss: 0.3403\n",
      "Epoch: 1 / 3, Step: 2200 / 2250 Loss: 0.6459\n",
      "Epoch: 1 / 3, Step: 2201 / 2250 Loss: 0.1860\n",
      "Epoch: 1 / 3, Step: 2202 / 2250 Loss: 0.3632\n",
      "Epoch: 1 / 3, Step: 2203 / 2250 Loss: 0.4268\n",
      "Epoch: 1 / 3, Step: 2204 / 2250 Loss: 0.4011\n",
      "Epoch: 1 / 3, Step: 2205 / 2250 Loss: 0.1789\n",
      "Epoch: 1 / 3, Step: 2206 / 2250 Loss: 0.4306\n",
      "Epoch: 1 / 3, Step: 2207 / 2250 Loss: 0.3787\n",
      "Epoch: 1 / 3, Step: 2208 / 2250 Loss: 0.3011\n",
      "Epoch: 1 / 3, Step: 2209 / 2250 Loss: 0.4397\n",
      "Epoch: 1 / 3, Step: 2210 / 2250 Loss: 0.4004\n",
      "Epoch: 1 / 3, Step: 2211 / 2250 Loss: 0.1122\n",
      "Epoch: 1 / 3, Step: 2212 / 2250 Loss: 0.5781\n",
      "Epoch: 1 / 3, Step: 2213 / 2250 Loss: 0.8480\n",
      "Epoch: 1 / 3, Step: 2214 / 2250 Loss: 0.4499\n",
      "Epoch: 1 / 3, Step: 2215 / 2250 Loss: 0.3354\n",
      "Epoch: 1 / 3, Step: 2216 / 2250 Loss: 0.3215\n",
      "Epoch: 1 / 3, Step: 2217 / 2250 Loss: 0.1771\n",
      "Epoch: 1 / 3, Step: 2218 / 2250 Loss: 0.5908\n",
      "Epoch: 1 / 3, Step: 2219 / 2250 Loss: 0.3287\n",
      "Epoch: 1 / 3, Step: 2220 / 2250 Loss: 0.3964\n",
      "Epoch: 1 / 3, Step: 2221 / 2250 Loss: 0.3205\n",
      "Epoch: 1 / 3, Step: 2222 / 2250 Loss: 0.2521\n",
      "Epoch: 1 / 3, Step: 2223 / 2250 Loss: 0.5241\n",
      "Epoch: 1 / 3, Step: 2224 / 2250 Loss: 0.3673\n",
      "Epoch: 1 / 3, Step: 2225 / 2250 Loss: 0.6347\n",
      "Epoch: 1 / 3, Step: 2226 / 2250 Loss: 0.3198\n",
      "Epoch: 1 / 3, Step: 2227 / 2250 Loss: 0.3673\n",
      "Epoch: 1 / 3, Step: 2228 / 2250 Loss: 0.3130\n",
      "Epoch: 1 / 3, Step: 2229 / 2250 Loss: 0.3233\n",
      "Epoch: 1 / 3, Step: 2230 / 2250 Loss: 0.3724\n",
      "Epoch: 1 / 3, Step: 2231 / 2250 Loss: 0.4508\n",
      "Epoch: 1 / 3, Step: 2232 / 2250 Loss: 0.1545\n",
      "Epoch: 1 / 3, Step: 2233 / 2250 Loss: 0.2398\n",
      "Epoch: 1 / 3, Step: 2234 / 2250 Loss: 0.3341\n",
      "Epoch: 1 / 3, Step: 2235 / 2250 Loss: 0.5599\n",
      "Epoch: 1 / 3, Step: 2236 / 2250 Loss: 0.4055\n",
      "Epoch: 1 / 3, Step: 2237 / 2250 Loss: 0.2721\n",
      "Epoch: 1 / 3, Step: 2238 / 2250 Loss: 0.4802\n",
      "Epoch: 1 / 3, Step: 2239 / 2250 Loss: 0.3287\n",
      "Epoch: 1 / 3, Step: 2240 / 2250 Loss: 0.1171\n",
      "Epoch: 1 / 3, Step: 2241 / 2250 Loss: 0.3232\n",
      "Epoch: 1 / 3, Step: 2242 / 2250 Loss: 0.2664\n",
      "Epoch: 1 / 3, Step: 2243 / 2250 Loss: 0.5476\n",
      "Epoch: 1 / 3, Step: 2244 / 2250 Loss: 0.4541\n",
      "Epoch: 1 / 3, Step: 2245 / 2250 Loss: 0.4576\n",
      "Epoch: 1 / 3, Step: 2246 / 2250 Loss: 0.2638\n",
      "Epoch: 1 / 3, Step: 2247 / 2250 Loss: 0.5122\n",
      "Epoch: 1 / 3, Step: 2248 / 2250 Loss: 0.3800\n",
      "Epoch: 1 / 3, Step: 2249 / 2250 Loss: 0.2853\n",
      "Epoch: 2 / 3, Step: 0 / 2250 Loss: 0.3573\n",
      "Epoch: 2 / 3, Step: 1 / 2250 Loss: 0.5884\n",
      "Epoch: 2 / 3, Step: 2 / 2250 Loss: 0.2671\n",
      "Epoch: 2 / 3, Step: 3 / 2250 Loss: 0.4448\n",
      "Epoch: 2 / 3, Step: 4 / 2250 Loss: 0.3169\n",
      "Epoch: 2 / 3, Step: 5 / 2250 Loss: 0.4598\n",
      "Epoch: 2 / 3, Step: 6 / 2250 Loss: 0.2539\n",
      "Epoch: 2 / 3, Step: 7 / 2250 Loss: 0.2531\n",
      "Epoch: 2 / 3, Step: 8 / 2250 Loss: 0.3992\n",
      "Epoch: 2 / 3, Step: 9 / 2250 Loss: 0.4084\n",
      "Epoch: 2 / 3, Step: 10 / 2250 Loss: 0.1868\n",
      "Epoch: 2 / 3, Step: 11 / 2250 Loss: 0.3760\n",
      "Epoch: 2 / 3, Step: 12 / 2250 Loss: 0.3241\n",
      "Epoch: 2 / 3, Step: 13 / 2250 Loss: 0.5198\n",
      "Epoch: 2 / 3, Step: 14 / 2250 Loss: 0.5288\n",
      "Epoch: 2 / 3, Step: 15 / 2250 Loss: 0.2368\n",
      "Epoch: 2 / 3, Step: 16 / 2250 Loss: 0.2086\n",
      "Epoch: 2 / 3, Step: 17 / 2250 Loss: 0.1896\n",
      "Epoch: 2 / 3, Step: 18 / 2250 Loss: 0.2025\n",
      "Epoch: 2 / 3, Step: 19 / 2250 Loss: 0.2940\n",
      "Epoch: 2 / 3, Step: 20 / 2250 Loss: 0.3771\n",
      "Epoch: 2 / 3, Step: 21 / 2250 Loss: 0.2678\n",
      "Epoch: 2 / 3, Step: 22 / 2250 Loss: 0.5267\n",
      "Epoch: 2 / 3, Step: 23 / 2250 Loss: 0.3920\n",
      "Epoch: 2 / 3, Step: 24 / 2250 Loss: 0.2789\n",
      "Epoch: 2 / 3, Step: 25 / 2250 Loss: 0.1275\n",
      "Epoch: 2 / 3, Step: 26 / 2250 Loss: 0.2258\n",
      "Epoch: 2 / 3, Step: 27 / 2250 Loss: 0.5200\n",
      "Epoch: 2 / 3, Step: 28 / 2250 Loss: 0.3075\n",
      "Epoch: 2 / 3, Step: 29 / 2250 Loss: 0.2266\n",
      "Epoch: 2 / 3, Step: 30 / 2250 Loss: 0.4721\n",
      "Epoch: 2 / 3, Step: 31 / 2250 Loss: 0.3325\n",
      "Epoch: 2 / 3, Step: 32 / 2250 Loss: 0.4280\n",
      "Epoch: 2 / 3, Step: 33 / 2250 Loss: 0.2691\n",
      "Epoch: 2 / 3, Step: 34 / 2250 Loss: 0.2232\n",
      "Epoch: 2 / 3, Step: 35 / 2250 Loss: 0.3801\n",
      "Epoch: 2 / 3, Step: 36 / 2250 Loss: 0.3459\n",
      "Epoch: 2 / 3, Step: 37 / 2250 Loss: 0.1351\n",
      "Epoch: 2 / 3, Step: 38 / 2250 Loss: 0.3029\n",
      "Epoch: 2 / 3, Step: 39 / 2250 Loss: 0.4264\n",
      "Epoch: 2 / 3, Step: 40 / 2250 Loss: 0.4014\n",
      "Epoch: 2 / 3, Step: 41 / 2250 Loss: 0.4491\n",
      "Epoch: 2 / 3, Step: 42 / 2250 Loss: 0.3600\n",
      "Epoch: 2 / 3, Step: 43 / 2250 Loss: 0.4490\n",
      "Epoch: 2 / 3, Step: 44 / 2250 Loss: 0.3459\n",
      "Epoch: 2 / 3, Step: 45 / 2250 Loss: 0.3916\n",
      "Epoch: 2 / 3, Step: 46 / 2250 Loss: 0.2183\n",
      "Epoch: 2 / 3, Step: 47 / 2250 Loss: 0.4835\n",
      "Epoch: 2 / 3, Step: 48 / 2250 Loss: 0.4336\n",
      "Epoch: 2 / 3, Step: 49 / 2250 Loss: 0.1191\n",
      "Epoch: 2 / 3, Step: 50 / 2250 Loss: 0.2351\n",
      "Epoch: 2 / 3, Step: 51 / 2250 Loss: 0.4228\n",
      "Epoch: 2 / 3, Step: 52 / 2250 Loss: 0.4915\n",
      "Epoch: 2 / 3, Step: 53 / 2250 Loss: 0.3474\n",
      "Epoch: 2 / 3, Step: 54 / 2250 Loss: 0.2138\n",
      "Epoch: 2 / 3, Step: 55 / 2250 Loss: 0.3944\n",
      "Epoch: 2 / 3, Step: 56 / 2250 Loss: 0.4616\n",
      "Epoch: 2 / 3, Step: 57 / 2250 Loss: 0.2363\n",
      "Epoch: 2 / 3, Step: 58 / 2250 Loss: 0.3451\n",
      "Epoch: 2 / 3, Step: 59 / 2250 Loss: 0.4492\n",
      "Epoch: 2 / 3, Step: 60 / 2250 Loss: 0.4995\n",
      "Epoch: 2 / 3, Step: 61 / 2250 Loss: 0.4440\n",
      "Epoch: 2 / 3, Step: 62 / 2250 Loss: 0.4569\n",
      "Epoch: 2 / 3, Step: 63 / 2250 Loss: 0.3717\n",
      "Epoch: 2 / 3, Step: 64 / 2250 Loss: 0.2970\n",
      "Epoch: 2 / 3, Step: 65 / 2250 Loss: 0.4290\n",
      "Epoch: 2 / 3, Step: 66 / 2250 Loss: 0.2708\n",
      "Epoch: 2 / 3, Step: 67 / 2250 Loss: 0.2986\n",
      "Epoch: 2 / 3, Step: 68 / 2250 Loss: 0.1661\n",
      "Epoch: 2 / 3, Step: 69 / 2250 Loss: 0.4770\n",
      "Epoch: 2 / 3, Step: 70 / 2250 Loss: 0.4230\n",
      "Epoch: 2 / 3, Step: 71 / 2250 Loss: 0.5212\n",
      "Epoch: 2 / 3, Step: 72 / 2250 Loss: 0.3878\n",
      "Epoch: 2 / 3, Step: 73 / 2250 Loss: 0.3316\n",
      "Epoch: 2 / 3, Step: 74 / 2250 Loss: 0.2221\n",
      "Epoch: 2 / 3, Step: 75 / 2250 Loss: 0.4111\n",
      "Epoch: 2 / 3, Step: 76 / 2250 Loss: 0.5239\n",
      "Epoch: 2 / 3, Step: 77 / 2250 Loss: 0.4554\n",
      "Epoch: 2 / 3, Step: 78 / 2250 Loss: 0.2715\n",
      "Epoch: 2 / 3, Step: 79 / 2250 Loss: 0.2064\n",
      "Epoch: 2 / 3, Step: 80 / 2250 Loss: 0.1891\n",
      "Epoch: 2 / 3, Step: 81 / 2250 Loss: 0.1787\n",
      "Epoch: 2 / 3, Step: 82 / 2250 Loss: 0.2926\n",
      "Epoch: 2 / 3, Step: 83 / 2250 Loss: 0.3623\n",
      "Epoch: 2 / 3, Step: 84 / 2250 Loss: 0.3050\n",
      "Epoch: 2 / 3, Step: 85 / 2250 Loss: 0.5913\n",
      "Epoch: 2 / 3, Step: 86 / 2250 Loss: 0.4388\n",
      "Epoch: 2 / 3, Step: 87 / 2250 Loss: 0.4181\n",
      "Epoch: 2 / 3, Step: 88 / 2250 Loss: 0.3666\n",
      "Epoch: 2 / 3, Step: 89 / 2250 Loss: 0.4060\n",
      "Epoch: 2 / 3, Step: 90 / 2250 Loss: 0.2608\n",
      "Epoch: 2 / 3, Step: 91 / 2250 Loss: 0.2895\n",
      "Epoch: 2 / 3, Step: 92 / 2250 Loss: 0.4245\n",
      "Epoch: 2 / 3, Step: 93 / 2250 Loss: 0.3282\n",
      "Epoch: 2 / 3, Step: 94 / 2250 Loss: 0.2672\n",
      "Epoch: 2 / 3, Step: 95 / 2250 Loss: 0.3657\n",
      "Epoch: 2 / 3, Step: 96 / 2250 Loss: 0.3797\n",
      "Epoch: 2 / 3, Step: 97 / 2250 Loss: 0.2981\n",
      "Epoch: 2 / 3, Step: 98 / 2250 Loss: 0.3259\n",
      "Epoch: 2 / 3, Step: 99 / 2250 Loss: 0.5617\n",
      "Epoch: 2 / 3, Step: 100 / 2250 Loss: 0.2152\n",
      "Epoch: 2 / 3, Step: 101 / 2250 Loss: 0.2075\n",
      "Epoch: 2 / 3, Step: 102 / 2250 Loss: 0.4785\n",
      "Epoch: 2 / 3, Step: 103 / 2250 Loss: 0.4176\n",
      "Epoch: 2 / 3, Step: 104 / 2250 Loss: 0.3142\n",
      "Epoch: 2 / 3, Step: 105 / 2250 Loss: 0.5496\n",
      "Epoch: 2 / 3, Step: 106 / 2250 Loss: 0.2916\n",
      "Epoch: 2 / 3, Step: 107 / 2250 Loss: 0.2948\n",
      "Epoch: 2 / 3, Step: 108 / 2250 Loss: 0.5296\n",
      "Epoch: 2 / 3, Step: 109 / 2250 Loss: 0.3660\n",
      "Epoch: 2 / 3, Step: 110 / 2250 Loss: 0.1659\n",
      "Epoch: 2 / 3, Step: 111 / 2250 Loss: 0.4592\n",
      "Epoch: 2 / 3, Step: 112 / 2250 Loss: 0.2515\n",
      "Epoch: 2 / 3, Step: 113 / 2250 Loss: 0.3714\n",
      "Epoch: 2 / 3, Step: 114 / 2250 Loss: 0.2507\n",
      "Epoch: 2 / 3, Step: 115 / 2250 Loss: 0.3730\n",
      "Epoch: 2 / 3, Step: 116 / 2250 Loss: 0.1969\n",
      "Epoch: 2 / 3, Step: 117 / 2250 Loss: 0.2284\n",
      "Epoch: 2 / 3, Step: 118 / 2250 Loss: 0.4379\n",
      "Epoch: 2 / 3, Step: 119 / 2250 Loss: 0.2156\n",
      "Epoch: 2 / 3, Step: 120 / 2250 Loss: 0.3646\n",
      "Epoch: 2 / 3, Step: 121 / 2250 Loss: 0.3361\n",
      "Epoch: 2 / 3, Step: 122 / 2250 Loss: 0.4407\n",
      "Epoch: 2 / 3, Step: 123 / 2250 Loss: 0.2787\n",
      "Epoch: 2 / 3, Step: 124 / 2250 Loss: 0.2774\n",
      "Epoch: 2 / 3, Step: 125 / 2250 Loss: 0.1752\n",
      "Epoch: 2 / 3, Step: 126 / 2250 Loss: 0.2701\n",
      "Epoch: 2 / 3, Step: 127 / 2250 Loss: 0.2399\n",
      "Epoch: 2 / 3, Step: 128 / 2250 Loss: 0.3900\n",
      "Epoch: 2 / 3, Step: 129 / 2250 Loss: 0.5541\n",
      "Epoch: 2 / 3, Step: 130 / 2250 Loss: 0.4407\n",
      "Epoch: 2 / 3, Step: 131 / 2250 Loss: 0.3411\n",
      "Epoch: 2 / 3, Step: 132 / 2250 Loss: 0.2540\n",
      "Epoch: 2 / 3, Step: 133 / 2250 Loss: 0.4051\n",
      "Epoch: 2 / 3, Step: 134 / 2250 Loss: 0.1484\n",
      "Epoch: 2 / 3, Step: 135 / 2250 Loss: 0.2973\n",
      "Epoch: 2 / 3, Step: 136 / 2250 Loss: 0.2027\n",
      "Epoch: 2 / 3, Step: 137 / 2250 Loss: 0.3234\n",
      "Epoch: 2 / 3, Step: 138 / 2250 Loss: 0.2032\n",
      "Epoch: 2 / 3, Step: 139 / 2250 Loss: 0.3385\n",
      "Epoch: 2 / 3, Step: 140 / 2250 Loss: 0.3894\n",
      "Epoch: 2 / 3, Step: 141 / 2250 Loss: 0.2703\n",
      "Epoch: 2 / 3, Step: 142 / 2250 Loss: 0.2747\n",
      "Epoch: 2 / 3, Step: 143 / 2250 Loss: 0.2227\n",
      "Epoch: 2 / 3, Step: 144 / 2250 Loss: 0.4096\n",
      "Epoch: 2 / 3, Step: 145 / 2250 Loss: 0.1474\n",
      "Epoch: 2 / 3, Step: 146 / 2250 Loss: 0.2897\n",
      "Epoch: 2 / 3, Step: 147 / 2250 Loss: 0.3806\n",
      "Epoch: 2 / 3, Step: 148 / 2250 Loss: 0.1609\n",
      "Epoch: 2 / 3, Step: 149 / 2250 Loss: 0.3708\n",
      "Epoch: 2 / 3, Step: 150 / 2250 Loss: 0.5406\n",
      "Epoch: 2 / 3, Step: 151 / 2250 Loss: 0.4184\n",
      "Epoch: 2 / 3, Step: 152 / 2250 Loss: 0.3643\n",
      "Epoch: 2 / 3, Step: 153 / 2250 Loss: 0.3305\n",
      "Epoch: 2 / 3, Step: 154 / 2250 Loss: 0.3655\n",
      "Epoch: 2 / 3, Step: 155 / 2250 Loss: 0.5666\n",
      "Epoch: 2 / 3, Step: 156 / 2250 Loss: 0.1444\n",
      "Epoch: 2 / 3, Step: 157 / 2250 Loss: 0.2028\n",
      "Epoch: 2 / 3, Step: 158 / 2250 Loss: 0.3068\n",
      "Epoch: 2 / 3, Step: 159 / 2250 Loss: 0.1743\n",
      "Epoch: 2 / 3, Step: 160 / 2250 Loss: 0.2429\n",
      "Epoch: 2 / 3, Step: 161 / 2250 Loss: 0.3430\n",
      "Epoch: 2 / 3, Step: 162 / 2250 Loss: 0.4284\n",
      "Epoch: 2 / 3, Step: 163 / 2250 Loss: 0.2680\n",
      "Epoch: 2 / 3, Step: 164 / 2250 Loss: 0.3367\n",
      "Epoch: 2 / 3, Step: 165 / 2250 Loss: 0.3216\n",
      "Epoch: 2 / 3, Step: 166 / 2250 Loss: 0.1865\n",
      "Epoch: 2 / 3, Step: 167 / 2250 Loss: 0.3235\n",
      "Epoch: 2 / 3, Step: 168 / 2250 Loss: 0.3409\n",
      "Epoch: 2 / 3, Step: 169 / 2250 Loss: 0.3566\n",
      "Epoch: 2 / 3, Step: 170 / 2250 Loss: 0.2774\n",
      "Epoch: 2 / 3, Step: 171 / 2250 Loss: 0.5063\n",
      "Epoch: 2 / 3, Step: 172 / 2250 Loss: 0.2898\n",
      "Epoch: 2 / 3, Step: 173 / 2250 Loss: 0.2268\n",
      "Epoch: 2 / 3, Step: 174 / 2250 Loss: 0.2137\n",
      "Epoch: 2 / 3, Step: 175 / 2250 Loss: 0.3087\n",
      "Epoch: 2 / 3, Step: 176 / 2250 Loss: 0.2113\n",
      "Epoch: 2 / 3, Step: 177 / 2250 Loss: 0.2740\n",
      "Epoch: 2 / 3, Step: 178 / 2250 Loss: 0.5303\n",
      "Epoch: 2 / 3, Step: 179 / 2250 Loss: 0.3752\n",
      "Epoch: 2 / 3, Step: 180 / 2250 Loss: 0.4627\n",
      "Epoch: 2 / 3, Step: 181 / 2250 Loss: 0.3042\n",
      "Epoch: 2 / 3, Step: 182 / 2250 Loss: 0.4161\n",
      "Epoch: 2 / 3, Step: 183 / 2250 Loss: 0.2488\n",
      "Epoch: 2 / 3, Step: 184 / 2250 Loss: 0.3087\n",
      "Epoch: 2 / 3, Step: 185 / 2250 Loss: 0.3338\n",
      "Epoch: 2 / 3, Step: 186 / 2250 Loss: 0.2303\n",
      "Epoch: 2 / 3, Step: 187 / 2250 Loss: 0.5001\n",
      "Epoch: 2 / 3, Step: 188 / 2250 Loss: 0.3382\n",
      "Epoch: 2 / 3, Step: 189 / 2250 Loss: 0.2452\n",
      "Epoch: 2 / 3, Step: 190 / 2250 Loss: 0.3541\n",
      "Epoch: 2 / 3, Step: 191 / 2250 Loss: 0.3459\n",
      "Epoch: 2 / 3, Step: 192 / 2250 Loss: 0.4939\n",
      "Epoch: 2 / 3, Step: 193 / 2250 Loss: 0.4342\n",
      "Epoch: 2 / 3, Step: 194 / 2250 Loss: 0.3502\n",
      "Epoch: 2 / 3, Step: 195 / 2250 Loss: 0.2777\n",
      "Epoch: 2 / 3, Step: 196 / 2250 Loss: 0.4652\n",
      "Epoch: 2 / 3, Step: 197 / 2250 Loss: 0.3033\n",
      "Epoch: 2 / 3, Step: 198 / 2250 Loss: 0.3411\n",
      "Epoch: 2 / 3, Step: 199 / 2250 Loss: 0.2843\n",
      "Epoch: 2 / 3, Step: 200 / 2250 Loss: 0.2403\n",
      "Epoch: 2 / 3, Step: 201 / 2250 Loss: 0.5954\n",
      "Epoch: 2 / 3, Step: 202 / 2250 Loss: 0.2660\n",
      "Epoch: 2 / 3, Step: 203 / 2250 Loss: 0.1843\n",
      "Epoch: 2 / 3, Step: 204 / 2250 Loss: 0.6874\n",
      "Epoch: 2 / 3, Step: 205 / 2250 Loss: 0.1938\n",
      "Epoch: 2 / 3, Step: 206 / 2250 Loss: 0.3099\n",
      "Epoch: 2 / 3, Step: 207 / 2250 Loss: 0.2681\n",
      "Epoch: 2 / 3, Step: 208 / 2250 Loss: 0.3298\n",
      "Epoch: 2 / 3, Step: 209 / 2250 Loss: 0.2535\n",
      "Epoch: 2 / 3, Step: 210 / 2250 Loss: 0.2596\n",
      "Epoch: 2 / 3, Step: 211 / 2250 Loss: 0.3971\n",
      "Epoch: 2 / 3, Step: 212 / 2250 Loss: 0.4470\n",
      "Epoch: 2 / 3, Step: 213 / 2250 Loss: 0.5383\n",
      "Epoch: 2 / 3, Step: 214 / 2250 Loss: 0.3244\n",
      "Epoch: 2 / 3, Step: 215 / 2250 Loss: 0.2189\n",
      "Epoch: 2 / 3, Step: 216 / 2250 Loss: 0.4721\n",
      "Epoch: 2 / 3, Step: 217 / 2250 Loss: 0.4241\n",
      "Epoch: 2 / 3, Step: 218 / 2250 Loss: 0.3483\n",
      "Epoch: 2 / 3, Step: 219 / 2250 Loss: 0.2264\n",
      "Epoch: 2 / 3, Step: 220 / 2250 Loss: 0.1774\n",
      "Epoch: 2 / 3, Step: 221 / 2250 Loss: 0.4172\n",
      "Epoch: 2 / 3, Step: 222 / 2250 Loss: 0.3116\n",
      "Epoch: 2 / 3, Step: 223 / 2250 Loss: 0.1789\n",
      "Epoch: 2 / 3, Step: 224 / 2250 Loss: 0.4057\n",
      "Epoch: 2 / 3, Step: 225 / 2250 Loss: 0.2760\n",
      "Epoch: 2 / 3, Step: 226 / 2250 Loss: 0.2803\n",
      "Epoch: 2 / 3, Step: 227 / 2250 Loss: 0.2503\n",
      "Epoch: 2 / 3, Step: 228 / 2250 Loss: 0.5052\n",
      "Epoch: 2 / 3, Step: 229 / 2250 Loss: 0.3987\n",
      "Epoch: 2 / 3, Step: 230 / 2250 Loss: 0.2925\n",
      "Epoch: 2 / 3, Step: 231 / 2250 Loss: 0.4523\n",
      "Epoch: 2 / 3, Step: 232 / 2250 Loss: 0.4458\n",
      "Epoch: 2 / 3, Step: 233 / 2250 Loss: 0.5079\n",
      "Epoch: 2 / 3, Step: 234 / 2250 Loss: 0.3296\n",
      "Epoch: 2 / 3, Step: 235 / 2250 Loss: 0.3539\n",
      "Epoch: 2 / 3, Step: 236 / 2250 Loss: 0.2469\n",
      "Epoch: 2 / 3, Step: 237 / 2250 Loss: 0.2902\n",
      "Epoch: 2 / 3, Step: 238 / 2250 Loss: 0.3683\n",
      "Epoch: 2 / 3, Step: 239 / 2250 Loss: 0.4677\n",
      "Epoch: 2 / 3, Step: 240 / 2250 Loss: 0.5089\n",
      "Epoch: 2 / 3, Step: 241 / 2250 Loss: 0.4064\n",
      "Epoch: 2 / 3, Step: 242 / 2250 Loss: 0.3381\n",
      "Epoch: 2 / 3, Step: 243 / 2250 Loss: 0.2598\n",
      "Epoch: 2 / 3, Step: 244 / 2250 Loss: 0.2985\n",
      "Epoch: 2 / 3, Step: 245 / 2250 Loss: 0.4498\n",
      "Epoch: 2 / 3, Step: 246 / 2250 Loss: 0.1341\n",
      "Epoch: 2 / 3, Step: 247 / 2250 Loss: 0.1670\n",
      "Epoch: 2 / 3, Step: 248 / 2250 Loss: 0.3775\n",
      "Epoch: 2 / 3, Step: 249 / 2250 Loss: 0.2423\n",
      "Epoch: 2 / 3, Step: 250 / 2250 Loss: 0.3506\n",
      "Epoch: 2 / 3, Step: 251 / 2250 Loss: 0.3284\n",
      "Epoch: 2 / 3, Step: 252 / 2250 Loss: 0.5117\n",
      "Epoch: 2 / 3, Step: 253 / 2250 Loss: 0.2560\n",
      "Epoch: 2 / 3, Step: 254 / 2250 Loss: 0.3422\n",
      "Epoch: 2 / 3, Step: 255 / 2250 Loss: 0.2419\n",
      "Epoch: 2 / 3, Step: 256 / 2250 Loss: 0.2930\n",
      "Epoch: 2 / 3, Step: 257 / 2250 Loss: 0.0942\n",
      "Epoch: 2 / 3, Step: 258 / 2250 Loss: 0.2812\n",
      "Epoch: 2 / 3, Step: 259 / 2250 Loss: 0.2501\n",
      "Epoch: 2 / 3, Step: 260 / 2250 Loss: 0.3956\n",
      "Epoch: 2 / 3, Step: 261 / 2250 Loss: 0.5033\n",
      "Epoch: 2 / 3, Step: 262 / 2250 Loss: 0.2362\n",
      "Epoch: 2 / 3, Step: 263 / 2250 Loss: 0.2383\n",
      "Epoch: 2 / 3, Step: 264 / 2250 Loss: 0.2608\n",
      "Epoch: 2 / 3, Step: 265 / 2250 Loss: 0.3474\n",
      "Epoch: 2 / 3, Step: 266 / 2250 Loss: 0.1781\n",
      "Epoch: 2 / 3, Step: 267 / 2250 Loss: 0.1605\n",
      "Epoch: 2 / 3, Step: 268 / 2250 Loss: 0.5034\n",
      "Epoch: 2 / 3, Step: 269 / 2250 Loss: 0.3661\n",
      "Epoch: 2 / 3, Step: 270 / 2250 Loss: 0.3295\n",
      "Epoch: 2 / 3, Step: 271 / 2250 Loss: 0.2509\n",
      "Epoch: 2 / 3, Step: 272 / 2250 Loss: 0.1744\n",
      "Epoch: 2 / 3, Step: 273 / 2250 Loss: 0.2764\n",
      "Epoch: 2 / 3, Step: 274 / 2250 Loss: 0.2852\n",
      "Epoch: 2 / 3, Step: 275 / 2250 Loss: 0.2222\n",
      "Epoch: 2 / 3, Step: 276 / 2250 Loss: 0.1899\n",
      "Epoch: 2 / 3, Step: 277 / 2250 Loss: 0.5023\n",
      "Epoch: 2 / 3, Step: 278 / 2250 Loss: 0.1532\n",
      "Epoch: 2 / 3, Step: 279 / 2250 Loss: 0.1722\n",
      "Epoch: 2 / 3, Step: 280 / 2250 Loss: 0.5658\n",
      "Epoch: 2 / 3, Step: 281 / 2250 Loss: 0.2713\n",
      "Epoch: 2 / 3, Step: 282 / 2250 Loss: 0.2698\n",
      "Epoch: 2 / 3, Step: 283 / 2250 Loss: 0.3568\n",
      "Epoch: 2 / 3, Step: 284 / 2250 Loss: 0.1820\n",
      "Epoch: 2 / 3, Step: 285 / 2250 Loss: 0.3268\n",
      "Epoch: 2 / 3, Step: 286 / 2250 Loss: 0.2973\n",
      "Epoch: 2 / 3, Step: 287 / 2250 Loss: 0.5416\n",
      "Epoch: 2 / 3, Step: 288 / 2250 Loss: 0.1310\n",
      "Epoch: 2 / 3, Step: 289 / 2250 Loss: 0.3137\n",
      "Epoch: 2 / 3, Step: 290 / 2250 Loss: 0.3998\n",
      "Epoch: 2 / 3, Step: 291 / 2250 Loss: 0.4489\n",
      "Epoch: 2 / 3, Step: 292 / 2250 Loss: 0.4207\n",
      "Epoch: 2 / 3, Step: 293 / 2250 Loss: 0.5579\n",
      "Epoch: 2 / 3, Step: 294 / 2250 Loss: 0.1455\n",
      "Epoch: 2 / 3, Step: 295 / 2250 Loss: 0.1388\n",
      "Epoch: 2 / 3, Step: 296 / 2250 Loss: 0.3983\n",
      "Epoch: 2 / 3, Step: 297 / 2250 Loss: 0.2680\n",
      "Epoch: 2 / 3, Step: 298 / 2250 Loss: 0.2573\n",
      "Epoch: 2 / 3, Step: 299 / 2250 Loss: 0.1814\n",
      "Epoch: 2 / 3, Step: 300 / 2250 Loss: 0.3703\n",
      "Epoch: 2 / 3, Step: 301 / 2250 Loss: 0.4467\n",
      "Epoch: 2 / 3, Step: 302 / 2250 Loss: 0.2322\n",
      "Epoch: 2 / 3, Step: 303 / 2250 Loss: 0.2498\n",
      "Epoch: 2 / 3, Step: 304 / 2250 Loss: 0.1863\n",
      "Epoch: 2 / 3, Step: 305 / 2250 Loss: 0.5029\n",
      "Epoch: 2 / 3, Step: 306 / 2250 Loss: 0.3084\n",
      "Epoch: 2 / 3, Step: 307 / 2250 Loss: 0.4716\n",
      "Epoch: 2 / 3, Step: 308 / 2250 Loss: 0.3656\n",
      "Epoch: 2 / 3, Step: 309 / 2250 Loss: 0.1824\n",
      "Epoch: 2 / 3, Step: 310 / 2250 Loss: 0.2699\n",
      "Epoch: 2 / 3, Step: 311 / 2250 Loss: 0.3392\n",
      "Epoch: 2 / 3, Step: 312 / 2250 Loss: 0.3763\n",
      "Epoch: 2 / 3, Step: 313 / 2250 Loss: 0.2450\n",
      "Epoch: 2 / 3, Step: 314 / 2250 Loss: 0.2239\n",
      "Epoch: 2 / 3, Step: 315 / 2250 Loss: 0.2954\n",
      "Epoch: 2 / 3, Step: 316 / 2250 Loss: 0.6164\n",
      "Epoch: 2 / 3, Step: 317 / 2250 Loss: 0.5243\n",
      "Epoch: 2 / 3, Step: 318 / 2250 Loss: 0.1419\n",
      "Epoch: 2 / 3, Step: 319 / 2250 Loss: 0.2134\n",
      "Epoch: 2 / 3, Step: 320 / 2250 Loss: 0.1848\n",
      "Epoch: 2 / 3, Step: 321 / 2250 Loss: 0.2557\n",
      "Epoch: 2 / 3, Step: 322 / 2250 Loss: 0.4025\n",
      "Epoch: 2 / 3, Step: 323 / 2250 Loss: 0.4848\n",
      "Epoch: 2 / 3, Step: 324 / 2250 Loss: 0.2420\n",
      "Epoch: 2 / 3, Step: 325 / 2250 Loss: 0.2901\n",
      "Epoch: 2 / 3, Step: 326 / 2250 Loss: 0.2517\n",
      "Epoch: 2 / 3, Step: 327 / 2250 Loss: 0.3355\n",
      "Epoch: 2 / 3, Step: 328 / 2250 Loss: 0.3013\n",
      "Epoch: 2 / 3, Step: 329 / 2250 Loss: 0.3277\n",
      "Epoch: 2 / 3, Step: 330 / 2250 Loss: 0.3245\n",
      "Epoch: 2 / 3, Step: 331 / 2250 Loss: 0.2760\n",
      "Epoch: 2 / 3, Step: 332 / 2250 Loss: 0.1144\n",
      "Epoch: 2 / 3, Step: 333 / 2250 Loss: 0.0725\n",
      "Epoch: 2 / 3, Step: 334 / 2250 Loss: 0.3432\n",
      "Epoch: 2 / 3, Step: 335 / 2250 Loss: 0.1546\n",
      "Epoch: 2 / 3, Step: 336 / 2250 Loss: 0.3940\n",
      "Epoch: 2 / 3, Step: 337 / 2250 Loss: 0.4318\n",
      "Epoch: 2 / 3, Step: 338 / 2250 Loss: 0.1851\n",
      "Epoch: 2 / 3, Step: 339 / 2250 Loss: 0.1370\n",
      "Epoch: 2 / 3, Step: 340 / 2250 Loss: 0.3818\n",
      "Epoch: 2 / 3, Step: 341 / 2250 Loss: 0.2780\n",
      "Epoch: 2 / 3, Step: 342 / 2250 Loss: 0.1601\n",
      "Epoch: 2 / 3, Step: 343 / 2250 Loss: 0.2613\n",
      "Epoch: 2 / 3, Step: 344 / 2250 Loss: 0.1738\n",
      "Epoch: 2 / 3, Step: 345 / 2250 Loss: 0.1854\n",
      "Epoch: 2 / 3, Step: 346 / 2250 Loss: 0.3411\n",
      "Epoch: 2 / 3, Step: 347 / 2250 Loss: 0.1490\n",
      "Epoch: 2 / 3, Step: 348 / 2250 Loss: 0.3342\n",
      "Epoch: 2 / 3, Step: 349 / 2250 Loss: 0.1887\n",
      "Epoch: 2 / 3, Step: 350 / 2250 Loss: 0.0913\n",
      "Epoch: 2 / 3, Step: 351 / 2250 Loss: 0.4582\n",
      "Epoch: 2 / 3, Step: 352 / 2250 Loss: 0.1496\n",
      "Epoch: 2 / 3, Step: 353 / 2250 Loss: 0.5267\n",
      "Epoch: 2 / 3, Step: 354 / 2250 Loss: 0.5812\n",
      "Epoch: 2 / 3, Step: 355 / 2250 Loss: 0.3403\n",
      "Epoch: 2 / 3, Step: 356 / 2250 Loss: 0.3848\n",
      "Epoch: 2 / 3, Step: 357 / 2250 Loss: 0.6580\n",
      "Epoch: 2 / 3, Step: 358 / 2250 Loss: 0.3301\n",
      "Epoch: 2 / 3, Step: 359 / 2250 Loss: 0.3058\n",
      "Epoch: 2 / 3, Step: 360 / 2250 Loss: 0.3893\n",
      "Epoch: 2 / 3, Step: 361 / 2250 Loss: 0.2228\n",
      "Epoch: 2 / 3, Step: 362 / 2250 Loss: 0.3615\n",
      "Epoch: 2 / 3, Step: 363 / 2250 Loss: 0.2662\n",
      "Epoch: 2 / 3, Step: 364 / 2250 Loss: 0.4823\n",
      "Epoch: 2 / 3, Step: 365 / 2250 Loss: 0.3931\n",
      "Epoch: 2 / 3, Step: 366 / 2250 Loss: 0.3837\n",
      "Epoch: 2 / 3, Step: 367 / 2250 Loss: 0.2696\n",
      "Epoch: 2 / 3, Step: 368 / 2250 Loss: 0.2386\n",
      "Epoch: 2 / 3, Step: 369 / 2250 Loss: 0.4038\n",
      "Epoch: 2 / 3, Step: 370 / 2250 Loss: 0.5286\n",
      "Epoch: 2 / 3, Step: 371 / 2250 Loss: 0.2341\n",
      "Epoch: 2 / 3, Step: 372 / 2250 Loss: 0.2085\n",
      "Epoch: 2 / 3, Step: 373 / 2250 Loss: 0.2281\n",
      "Epoch: 2 / 3, Step: 374 / 2250 Loss: 0.4323\n",
      "Epoch: 2 / 3, Step: 375 / 2250 Loss: 0.1539\n",
      "Epoch: 2 / 3, Step: 376 / 2250 Loss: 0.1865\n",
      "Epoch: 2 / 3, Step: 377 / 2250 Loss: 0.3237\n",
      "Epoch: 2 / 3, Step: 378 / 2250 Loss: 0.2420\n",
      "Epoch: 2 / 3, Step: 379 / 2250 Loss: 0.1812\n",
      "Epoch: 2 / 3, Step: 380 / 2250 Loss: 0.3229\n",
      "Epoch: 2 / 3, Step: 381 / 2250 Loss: 0.2912\n",
      "Epoch: 2 / 3, Step: 382 / 2250 Loss: 0.4414\n",
      "Epoch: 2 / 3, Step: 383 / 2250 Loss: 0.4737\n",
      "Epoch: 2 / 3, Step: 384 / 2250 Loss: 0.2629\n",
      "Epoch: 2 / 3, Step: 385 / 2250 Loss: 0.3371\n",
      "Epoch: 2 / 3, Step: 386 / 2250 Loss: 0.1830\n",
      "Epoch: 2 / 3, Step: 387 / 2250 Loss: 0.2003\n",
      "Epoch: 2 / 3, Step: 388 / 2250 Loss: 0.3308\n",
      "Epoch: 2 / 3, Step: 389 / 2250 Loss: 0.6719\n",
      "Epoch: 2 / 3, Step: 390 / 2250 Loss: 0.3987\n",
      "Epoch: 2 / 3, Step: 391 / 2250 Loss: 0.2277\n",
      "Epoch: 2 / 3, Step: 392 / 2250 Loss: 0.5241\n",
      "Epoch: 2 / 3, Step: 393 / 2250 Loss: 0.2691\n",
      "Epoch: 2 / 3, Step: 394 / 2250 Loss: 0.2278\n",
      "Epoch: 2 / 3, Step: 395 / 2250 Loss: 0.3747\n",
      "Epoch: 2 / 3, Step: 396 / 2250 Loss: 0.4118\n",
      "Epoch: 2 / 3, Step: 397 / 2250 Loss: 0.1527\n",
      "Epoch: 2 / 3, Step: 398 / 2250 Loss: 0.2654\n",
      "Epoch: 2 / 3, Step: 399 / 2250 Loss: 0.4578\n",
      "Epoch: 2 / 3, Step: 400 / 2250 Loss: 0.3704\n",
      "Epoch: 2 / 3, Step: 401 / 2250 Loss: 0.1119\n",
      "Epoch: 2 / 3, Step: 402 / 2250 Loss: 0.2458\n",
      "Epoch: 2 / 3, Step: 403 / 2250 Loss: 0.2638\n",
      "Epoch: 2 / 3, Step: 404 / 2250 Loss: 0.5229\n",
      "Epoch: 2 / 3, Step: 405 / 2250 Loss: 0.3333\n",
      "Epoch: 2 / 3, Step: 406 / 2250 Loss: 0.2740\n",
      "Epoch: 2 / 3, Step: 407 / 2250 Loss: 0.1708\n",
      "Epoch: 2 / 3, Step: 408 / 2250 Loss: 0.2243\n",
      "Epoch: 2 / 3, Step: 409 / 2250 Loss: 0.2884\n",
      "Epoch: 2 / 3, Step: 410 / 2250 Loss: 0.3677\n",
      "Epoch: 2 / 3, Step: 411 / 2250 Loss: 0.3878\n",
      "Epoch: 2 / 3, Step: 412 / 2250 Loss: 0.1680\n",
      "Epoch: 2 / 3, Step: 413 / 2250 Loss: 0.3074\n",
      "Epoch: 2 / 3, Step: 414 / 2250 Loss: 0.3059\n",
      "Epoch: 2 / 3, Step: 415 / 2250 Loss: 0.3177\n",
      "Epoch: 2 / 3, Step: 416 / 2250 Loss: 0.2266\n",
      "Epoch: 2 / 3, Step: 417 / 2250 Loss: 0.6571\n",
      "Epoch: 2 / 3, Step: 418 / 2250 Loss: 0.3247\n",
      "Epoch: 2 / 3, Step: 419 / 2250 Loss: 0.1724\n",
      "Epoch: 2 / 3, Step: 420 / 2250 Loss: 0.2652\n",
      "Epoch: 2 / 3, Step: 421 / 2250 Loss: 0.3433\n",
      "Epoch: 2 / 3, Step: 422 / 2250 Loss: 0.1193\n",
      "Epoch: 2 / 3, Step: 423 / 2250 Loss: 0.3143\n",
      "Epoch: 2 / 3, Step: 424 / 2250 Loss: 0.3110\n",
      "Epoch: 2 / 3, Step: 425 / 2250 Loss: 0.4578\n",
      "Epoch: 2 / 3, Step: 426 / 2250 Loss: 0.5456\n",
      "Epoch: 2 / 3, Step: 427 / 2250 Loss: 0.3382\n",
      "Epoch: 2 / 3, Step: 428 / 2250 Loss: 0.3948\n",
      "Epoch: 2 / 3, Step: 429 / 2250 Loss: 0.4543\n",
      "Epoch: 2 / 3, Step: 430 / 2250 Loss: 0.2866\n",
      "Epoch: 2 / 3, Step: 431 / 2250 Loss: 0.3685\n",
      "Epoch: 2 / 3, Step: 432 / 2250 Loss: 0.2284\n",
      "Epoch: 2 / 3, Step: 433 / 2250 Loss: 0.2273\n",
      "Epoch: 2 / 3, Step: 434 / 2250 Loss: 0.2544\n",
      "Epoch: 2 / 3, Step: 435 / 2250 Loss: 0.2980\n",
      "Epoch: 2 / 3, Step: 436 / 2250 Loss: 0.3971\n",
      "Epoch: 2 / 3, Step: 437 / 2250 Loss: 0.2544\n",
      "Epoch: 2 / 3, Step: 438 / 2250 Loss: 0.3897\n",
      "Epoch: 2 / 3, Step: 439 / 2250 Loss: 0.3805\n",
      "Epoch: 2 / 3, Step: 440 / 2250 Loss: 0.3209\n",
      "Epoch: 2 / 3, Step: 441 / 2250 Loss: 0.2464\n",
      "Epoch: 2 / 3, Step: 442 / 2250 Loss: 0.2172\n",
      "Epoch: 2 / 3, Step: 443 / 2250 Loss: 0.2132\n",
      "Epoch: 2 / 3, Step: 444 / 2250 Loss: 0.3846\n",
      "Epoch: 2 / 3, Step: 445 / 2250 Loss: 0.2454\n",
      "Epoch: 2 / 3, Step: 446 / 2250 Loss: 0.2887\n",
      "Epoch: 2 / 3, Step: 447 / 2250 Loss: 0.3091\n",
      "Epoch: 2 / 3, Step: 448 / 2250 Loss: 0.4770\n",
      "Epoch: 2 / 3, Step: 449 / 2250 Loss: 0.2260\n",
      "Epoch: 2 / 3, Step: 450 / 2250 Loss: 0.2576\n",
      "Epoch: 2 / 3, Step: 451 / 2250 Loss: 0.3268\n",
      "Epoch: 2 / 3, Step: 452 / 2250 Loss: 0.3503\n",
      "Epoch: 2 / 3, Step: 453 / 2250 Loss: 0.3360\n",
      "Epoch: 2 / 3, Step: 454 / 2250 Loss: 0.3440\n",
      "Epoch: 2 / 3, Step: 455 / 2250 Loss: 0.2916\n",
      "Epoch: 2 / 3, Step: 456 / 2250 Loss: 0.4424\n",
      "Epoch: 2 / 3, Step: 457 / 2250 Loss: 0.2765\n",
      "Epoch: 2 / 3, Step: 458 / 2250 Loss: 0.2500\n",
      "Epoch: 2 / 3, Step: 459 / 2250 Loss: 0.3144\n",
      "Epoch: 2 / 3, Step: 460 / 2250 Loss: 0.5155\n",
      "Epoch: 2 / 3, Step: 461 / 2250 Loss: 0.1935\n",
      "Epoch: 2 / 3, Step: 462 / 2250 Loss: 0.4160\n",
      "Epoch: 2 / 3, Step: 463 / 2250 Loss: 0.3070\n",
      "Epoch: 2 / 3, Step: 464 / 2250 Loss: 0.4794\n",
      "Epoch: 2 / 3, Step: 465 / 2250 Loss: 0.1575\n",
      "Epoch: 2 / 3, Step: 466 / 2250 Loss: 0.2605\n",
      "Epoch: 2 / 3, Step: 467 / 2250 Loss: 0.3730\n",
      "Epoch: 2 / 3, Step: 468 / 2250 Loss: 0.5423\n",
      "Epoch: 2 / 3, Step: 469 / 2250 Loss: 0.4638\n",
      "Epoch: 2 / 3, Step: 470 / 2250 Loss: 0.4347\n",
      "Epoch: 2 / 3, Step: 471 / 2250 Loss: 0.2877\n",
      "Epoch: 2 / 3, Step: 472 / 2250 Loss: 0.3115\n",
      "Epoch: 2 / 3, Step: 473 / 2250 Loss: 0.2426\n",
      "Epoch: 2 / 3, Step: 474 / 2250 Loss: 0.3443\n",
      "Epoch: 2 / 3, Step: 475 / 2250 Loss: 0.3737\n",
      "Epoch: 2 / 3, Step: 476 / 2250 Loss: 0.2893\n",
      "Epoch: 2 / 3, Step: 477 / 2250 Loss: 0.4418\n",
      "Epoch: 2 / 3, Step: 478 / 2250 Loss: 0.2041\n",
      "Epoch: 2 / 3, Step: 479 / 2250 Loss: 0.2851\n",
      "Epoch: 2 / 3, Step: 480 / 2250 Loss: 0.1807\n",
      "Epoch: 2 / 3, Step: 481 / 2250 Loss: 0.2140\n",
      "Epoch: 2 / 3, Step: 482 / 2250 Loss: 0.4441\n",
      "Epoch: 2 / 3, Step: 483 / 2250 Loss: 0.6378\n",
      "Epoch: 2 / 3, Step: 484 / 2250 Loss: 0.1295\n",
      "Epoch: 2 / 3, Step: 485 / 2250 Loss: 0.3239\n",
      "Epoch: 2 / 3, Step: 486 / 2250 Loss: 0.1604\n",
      "Epoch: 2 / 3, Step: 487 / 2250 Loss: 0.2481\n",
      "Epoch: 2 / 3, Step: 488 / 2250 Loss: 0.1752\n",
      "Epoch: 2 / 3, Step: 489 / 2250 Loss: 0.2436\n",
      "Epoch: 2 / 3, Step: 490 / 2250 Loss: 0.3476\n",
      "Epoch: 2 / 3, Step: 491 / 2250 Loss: 0.2818\n",
      "Epoch: 2 / 3, Step: 492 / 2250 Loss: 0.3223\n",
      "Epoch: 2 / 3, Step: 493 / 2250 Loss: 0.4317\n",
      "Epoch: 2 / 3, Step: 494 / 2250 Loss: 0.2684\n",
      "Epoch: 2 / 3, Step: 495 / 2250 Loss: 0.3133\n",
      "Epoch: 2 / 3, Step: 496 / 2250 Loss: 0.4612\n",
      "Epoch: 2 / 3, Step: 497 / 2250 Loss: 0.3145\n",
      "Epoch: 2 / 3, Step: 498 / 2250 Loss: 0.2752\n",
      "Epoch: 2 / 3, Step: 499 / 2250 Loss: 0.2815\n",
      "Epoch: 2 / 3, Step: 500 / 2250 Loss: 0.2698\n",
      "Epoch: 2 / 3, Step: 501 / 2250 Loss: 0.0818\n",
      "Epoch: 2 / 3, Step: 502 / 2250 Loss: 0.4666\n",
      "Epoch: 2 / 3, Step: 503 / 2250 Loss: 0.1582\n",
      "Epoch: 2 / 3, Step: 504 / 2250 Loss: 0.1913\n",
      "Epoch: 2 / 3, Step: 505 / 2250 Loss: 0.1410\n",
      "Epoch: 2 / 3, Step: 506 / 2250 Loss: 0.2246\n",
      "Epoch: 2 / 3, Step: 507 / 2250 Loss: 0.1226\n",
      "Epoch: 2 / 3, Step: 508 / 2250 Loss: 0.3067\n",
      "Epoch: 2 / 3, Step: 509 / 2250 Loss: 0.3512\n",
      "Epoch: 2 / 3, Step: 510 / 2250 Loss: 0.3008\n",
      "Epoch: 2 / 3, Step: 511 / 2250 Loss: 0.2709\n",
      "Epoch: 2 / 3, Step: 512 / 2250 Loss: 0.2675\n",
      "Epoch: 2 / 3, Step: 513 / 2250 Loss: 0.3860\n",
      "Epoch: 2 / 3, Step: 514 / 2250 Loss: 0.3205\n",
      "Epoch: 2 / 3, Step: 515 / 2250 Loss: 0.2142\n",
      "Epoch: 2 / 3, Step: 516 / 2250 Loss: 0.1833\n",
      "Epoch: 2 / 3, Step: 517 / 2250 Loss: 0.1829\n",
      "Epoch: 2 / 3, Step: 518 / 2250 Loss: 0.1317\n",
      "Epoch: 2 / 3, Step: 519 / 2250 Loss: 0.4011\n",
      "Epoch: 2 / 3, Step: 520 / 2250 Loss: 0.5002\n",
      "Epoch: 2 / 3, Step: 521 / 2250 Loss: 0.3925\n",
      "Epoch: 2 / 3, Step: 522 / 2250 Loss: 0.5096\n",
      "Epoch: 2 / 3, Step: 523 / 2250 Loss: 0.5062\n",
      "Epoch: 2 / 3, Step: 524 / 2250 Loss: 0.2399\n",
      "Epoch: 2 / 3, Step: 525 / 2250 Loss: 0.1532\n",
      "Epoch: 2 / 3, Step: 526 / 2250 Loss: 0.2197\n",
      "Epoch: 2 / 3, Step: 527 / 2250 Loss: 0.1915\n",
      "Epoch: 2 / 3, Step: 528 / 2250 Loss: 0.4481\n",
      "Epoch: 2 / 3, Step: 529 / 2250 Loss: 0.2217\n",
      "Epoch: 2 / 3, Step: 530 / 2250 Loss: 0.3708\n",
      "Epoch: 2 / 3, Step: 531 / 2250 Loss: 0.2724\n",
      "Epoch: 2 / 3, Step: 532 / 2250 Loss: 0.2228\n",
      "Epoch: 2 / 3, Step: 533 / 2250 Loss: 0.3453\n",
      "Epoch: 2 / 3, Step: 534 / 2250 Loss: 0.4024\n",
      "Epoch: 2 / 3, Step: 535 / 2250 Loss: 0.2030\n",
      "Epoch: 2 / 3, Step: 536 / 2250 Loss: 0.5308\n",
      "Epoch: 2 / 3, Step: 537 / 2250 Loss: 0.3318\n",
      "Epoch: 2 / 3, Step: 538 / 2250 Loss: 0.3954\n",
      "Epoch: 2 / 3, Step: 539 / 2250 Loss: 0.4507\n",
      "Epoch: 2 / 3, Step: 540 / 2250 Loss: 0.3561\n",
      "Epoch: 2 / 3, Step: 541 / 2250 Loss: 0.2573\n",
      "Epoch: 2 / 3, Step: 542 / 2250 Loss: 0.1683\n",
      "Epoch: 2 / 3, Step: 543 / 2250 Loss: 0.5066\n",
      "Epoch: 2 / 3, Step: 544 / 2250 Loss: 0.3223\n",
      "Epoch: 2 / 3, Step: 545 / 2250 Loss: 0.4004\n",
      "Epoch: 2 / 3, Step: 546 / 2250 Loss: 0.3864\n",
      "Epoch: 2 / 3, Step: 547 / 2250 Loss: 0.4133\n",
      "Epoch: 2 / 3, Step: 548 / 2250 Loss: 0.2265\n",
      "Epoch: 2 / 3, Step: 549 / 2250 Loss: 0.2591\n",
      "Epoch: 2 / 3, Step: 550 / 2250 Loss: 0.2524\n",
      "Epoch: 2 / 3, Step: 551 / 2250 Loss: 0.2886\n",
      "Epoch: 2 / 3, Step: 552 / 2250 Loss: 0.2367\n",
      "Epoch: 2 / 3, Step: 553 / 2250 Loss: 0.1709\n",
      "Epoch: 2 / 3, Step: 554 / 2250 Loss: 0.1998\n",
      "Epoch: 2 / 3, Step: 555 / 2250 Loss: 0.2456\n",
      "Epoch: 2 / 3, Step: 556 / 2250 Loss: 0.1115\n",
      "Epoch: 2 / 3, Step: 557 / 2250 Loss: 0.1633\n",
      "Epoch: 2 / 3, Step: 558 / 2250 Loss: 0.1794\n",
      "Epoch: 2 / 3, Step: 559 / 2250 Loss: 0.2924\n",
      "Epoch: 2 / 3, Step: 560 / 2250 Loss: 0.4939\n",
      "Epoch: 2 / 3, Step: 561 / 2250 Loss: 0.4907\n",
      "Epoch: 2 / 3, Step: 562 / 2250 Loss: 0.2291\n",
      "Epoch: 2 / 3, Step: 563 / 2250 Loss: 0.2473\n",
      "Epoch: 2 / 3, Step: 564 / 2250 Loss: 0.3033\n",
      "Epoch: 2 / 3, Step: 565 / 2250 Loss: 0.3071\n",
      "Epoch: 2 / 3, Step: 566 / 2250 Loss: 0.4599\n",
      "Epoch: 2 / 3, Step: 567 / 2250 Loss: 0.2689\n",
      "Epoch: 2 / 3, Step: 568 / 2250 Loss: 0.3112\n",
      "Epoch: 2 / 3, Step: 569 / 2250 Loss: 0.3435\n",
      "Epoch: 2 / 3, Step: 570 / 2250 Loss: 0.2541\n",
      "Epoch: 2 / 3, Step: 571 / 2250 Loss: 0.2527\n",
      "Epoch: 2 / 3, Step: 572 / 2250 Loss: 0.4829\n",
      "Epoch: 2 / 3, Step: 573 / 2250 Loss: 0.2361\n",
      "Epoch: 2 / 3, Step: 574 / 2250 Loss: 0.4465\n",
      "Epoch: 2 / 3, Step: 575 / 2250 Loss: 0.3957\n",
      "Epoch: 2 / 3, Step: 576 / 2250 Loss: 0.1595\n",
      "Epoch: 2 / 3, Step: 577 / 2250 Loss: 0.3505\n",
      "Epoch: 2 / 3, Step: 578 / 2250 Loss: 0.3955\n",
      "Epoch: 2 / 3, Step: 579 / 2250 Loss: 0.3334\n",
      "Epoch: 2 / 3, Step: 580 / 2250 Loss: 0.5120\n",
      "Epoch: 2 / 3, Step: 581 / 2250 Loss: 0.1628\n",
      "Epoch: 2 / 3, Step: 582 / 2250 Loss: 0.1116\n",
      "Epoch: 2 / 3, Step: 583 / 2250 Loss: 0.2856\n",
      "Epoch: 2 / 3, Step: 584 / 2250 Loss: 0.4160\n",
      "Epoch: 2 / 3, Step: 585 / 2250 Loss: 0.3450\n",
      "Epoch: 2 / 3, Step: 586 / 2250 Loss: 0.4510\n",
      "Epoch: 2 / 3, Step: 587 / 2250 Loss: 0.6064\n",
      "Epoch: 2 / 3, Step: 588 / 2250 Loss: 0.2529\n",
      "Epoch: 2 / 3, Step: 589 / 2250 Loss: 0.4124\n",
      "Epoch: 2 / 3, Step: 590 / 2250 Loss: 0.2669\n",
      "Epoch: 2 / 3, Step: 591 / 2250 Loss: 0.2313\n",
      "Epoch: 2 / 3, Step: 592 / 2250 Loss: 0.3685\n",
      "Epoch: 2 / 3, Step: 593 / 2250 Loss: 0.2680\n",
      "Epoch: 2 / 3, Step: 594 / 2250 Loss: 0.3837\n",
      "Epoch: 2 / 3, Step: 595 / 2250 Loss: 0.2756\n",
      "Epoch: 2 / 3, Step: 596 / 2250 Loss: 0.2148\n",
      "Epoch: 2 / 3, Step: 597 / 2250 Loss: 0.3237\n",
      "Epoch: 2 / 3, Step: 598 / 2250 Loss: 0.1866\n",
      "Epoch: 2 / 3, Step: 599 / 2250 Loss: 0.4383\n",
      "Epoch: 2 / 3, Step: 600 / 2250 Loss: 0.1353\n",
      "Epoch: 2 / 3, Step: 601 / 2250 Loss: 0.2748\n",
      "Epoch: 2 / 3, Step: 602 / 2250 Loss: 0.5240\n",
      "Epoch: 2 / 3, Step: 603 / 2250 Loss: 0.4455\n",
      "Epoch: 2 / 3, Step: 604 / 2250 Loss: 0.2416\n",
      "Epoch: 2 / 3, Step: 605 / 2250 Loss: 0.5258\n",
      "Epoch: 2 / 3, Step: 606 / 2250 Loss: 0.2061\n",
      "Epoch: 2 / 3, Step: 607 / 2250 Loss: 0.6361\n",
      "Epoch: 2 / 3, Step: 608 / 2250 Loss: 0.1363\n",
      "Epoch: 2 / 3, Step: 609 / 2250 Loss: 0.2324\n",
      "Epoch: 2 / 3, Step: 610 / 2250 Loss: 0.2023\n",
      "Epoch: 2 / 3, Step: 611 / 2250 Loss: 0.4897\n",
      "Epoch: 2 / 3, Step: 612 / 2250 Loss: 0.2855\n",
      "Epoch: 2 / 3, Step: 613 / 2250 Loss: 0.4309\n",
      "Epoch: 2 / 3, Step: 614 / 2250 Loss: 0.1575\n",
      "Epoch: 2 / 3, Step: 615 / 2250 Loss: 0.2479\n",
      "Epoch: 2 / 3, Step: 616 / 2250 Loss: 0.1137\n",
      "Epoch: 2 / 3, Step: 617 / 2250 Loss: 0.5118\n",
      "Epoch: 2 / 3, Step: 618 / 2250 Loss: 0.3989\n",
      "Epoch: 2 / 3, Step: 619 / 2250 Loss: 0.2324\n",
      "Epoch: 2 / 3, Step: 620 / 2250 Loss: 0.5386\n",
      "Epoch: 2 / 3, Step: 621 / 2250 Loss: 0.1830\n",
      "Epoch: 2 / 3, Step: 622 / 2250 Loss: 0.6314\n",
      "Epoch: 2 / 3, Step: 623 / 2250 Loss: 0.0530\n",
      "Epoch: 2 / 3, Step: 624 / 2250 Loss: 0.4886\n",
      "Epoch: 2 / 3, Step: 625 / 2250 Loss: 0.2353\n",
      "Epoch: 2 / 3, Step: 626 / 2250 Loss: 0.4170\n",
      "Epoch: 2 / 3, Step: 627 / 2250 Loss: 0.3361\n",
      "Epoch: 2 / 3, Step: 628 / 2250 Loss: 0.2759\n",
      "Epoch: 2 / 3, Step: 629 / 2250 Loss: 0.2508\n",
      "Epoch: 2 / 3, Step: 630 / 2250 Loss: 0.4477\n",
      "Epoch: 2 / 3, Step: 631 / 2250 Loss: 0.3328\n",
      "Epoch: 2 / 3, Step: 632 / 2250 Loss: 0.3400\n",
      "Epoch: 2 / 3, Step: 633 / 2250 Loss: 0.4922\n",
      "Epoch: 2 / 3, Step: 634 / 2250 Loss: 0.2067\n",
      "Epoch: 2 / 3, Step: 635 / 2250 Loss: 0.3189\n",
      "Epoch: 2 / 3, Step: 636 / 2250 Loss: 0.2928\n",
      "Epoch: 2 / 3, Step: 637 / 2250 Loss: 0.4427\n",
      "Epoch: 2 / 3, Step: 638 / 2250 Loss: 0.1484\n",
      "Epoch: 2 / 3, Step: 639 / 2250 Loss: 0.2419\n",
      "Epoch: 2 / 3, Step: 640 / 2250 Loss: 0.4220\n",
      "Epoch: 2 / 3, Step: 641 / 2250 Loss: 0.3267\n",
      "Epoch: 2 / 3, Step: 642 / 2250 Loss: 0.3679\n",
      "Epoch: 2 / 3, Step: 643 / 2250 Loss: 0.3309\n",
      "Epoch: 2 / 3, Step: 644 / 2250 Loss: 0.3085\n",
      "Epoch: 2 / 3, Step: 645 / 2250 Loss: 0.1377\n",
      "Epoch: 2 / 3, Step: 646 / 2250 Loss: 0.2771\n",
      "Epoch: 2 / 3, Step: 647 / 2250 Loss: 0.5201\n",
      "Epoch: 2 / 3, Step: 648 / 2250 Loss: 0.1656\n",
      "Epoch: 2 / 3, Step: 649 / 2250 Loss: 0.2268\n",
      "Epoch: 2 / 3, Step: 650 / 2250 Loss: 0.5894\n",
      "Epoch: 2 / 3, Step: 651 / 2250 Loss: 0.2849\n",
      "Epoch: 2 / 3, Step: 652 / 2250 Loss: 0.3211\n",
      "Epoch: 2 / 3, Step: 653 / 2250 Loss: 0.4046\n",
      "Epoch: 2 / 3, Step: 654 / 2250 Loss: 0.3208\n",
      "Epoch: 2 / 3, Step: 655 / 2250 Loss: 0.3479\n",
      "Epoch: 2 / 3, Step: 656 / 2250 Loss: 0.2562\n",
      "Epoch: 2 / 3, Step: 657 / 2250 Loss: 0.3934\n",
      "Epoch: 2 / 3, Step: 658 / 2250 Loss: 0.4650\n",
      "Epoch: 2 / 3, Step: 659 / 2250 Loss: 0.3780\n",
      "Epoch: 2 / 3, Step: 660 / 2250 Loss: 0.4077\n",
      "Epoch: 2 / 3, Step: 661 / 2250 Loss: 0.2748\n",
      "Epoch: 2 / 3, Step: 662 / 2250 Loss: 0.1485\n",
      "Epoch: 2 / 3, Step: 663 / 2250 Loss: 0.2100\n",
      "Epoch: 2 / 3, Step: 664 / 2250 Loss: 0.5056\n",
      "Epoch: 2 / 3, Step: 665 / 2250 Loss: 0.4242\n",
      "Epoch: 2 / 3, Step: 666 / 2250 Loss: 0.4677\n",
      "Epoch: 2 / 3, Step: 667 / 2250 Loss: 0.4068\n",
      "Epoch: 2 / 3, Step: 668 / 2250 Loss: 0.2150\n",
      "Epoch: 2 / 3, Step: 669 / 2250 Loss: 0.5372\n",
      "Epoch: 2 / 3, Step: 670 / 2250 Loss: 0.1992\n",
      "Epoch: 2 / 3, Step: 671 / 2250 Loss: 0.3976\n",
      "Epoch: 2 / 3, Step: 672 / 2250 Loss: 0.4523\n",
      "Epoch: 2 / 3, Step: 673 / 2250 Loss: 0.4593\n",
      "Epoch: 2 / 3, Step: 674 / 2250 Loss: 0.2874\n",
      "Epoch: 2 / 3, Step: 675 / 2250 Loss: 0.3433\n",
      "Epoch: 2 / 3, Step: 676 / 2250 Loss: 0.4682\n",
      "Epoch: 2 / 3, Step: 677 / 2250 Loss: 0.3501\n",
      "Epoch: 2 / 3, Step: 678 / 2250 Loss: 0.4156\n",
      "Epoch: 2 / 3, Step: 679 / 2250 Loss: 0.3398\n",
      "Epoch: 2 / 3, Step: 680 / 2250 Loss: 0.2222\n",
      "Epoch: 2 / 3, Step: 681 / 2250 Loss: 0.3502\n",
      "Epoch: 2 / 3, Step: 682 / 2250 Loss: 0.2592\n",
      "Epoch: 2 / 3, Step: 683 / 2250 Loss: 0.2822\n",
      "Epoch: 2 / 3, Step: 684 / 2250 Loss: 0.2102\n",
      "Epoch: 2 / 3, Step: 685 / 2250 Loss: 0.3966\n",
      "Epoch: 2 / 3, Step: 686 / 2250 Loss: 0.3028\n",
      "Epoch: 2 / 3, Step: 687 / 2250 Loss: 0.1612\n",
      "Epoch: 2 / 3, Step: 688 / 2250 Loss: 0.1309\n",
      "Epoch: 2 / 3, Step: 689 / 2250 Loss: 0.2928\n",
      "Epoch: 2 / 3, Step: 690 / 2250 Loss: 0.2854\n",
      "Epoch: 2 / 3, Step: 691 / 2250 Loss: 0.2760\n",
      "Epoch: 2 / 3, Step: 692 / 2250 Loss: 0.1015\n",
      "Epoch: 2 / 3, Step: 693 / 2250 Loss: 0.1434\n",
      "Epoch: 2 / 3, Step: 694 / 2250 Loss: 0.2659\n",
      "Epoch: 2 / 3, Step: 695 / 2250 Loss: 0.1903\n",
      "Epoch: 2 / 3, Step: 696 / 2250 Loss: 0.2373\n",
      "Epoch: 2 / 3, Step: 697 / 2250 Loss: 0.2388\n",
      "Epoch: 2 / 3, Step: 698 / 2250 Loss: 0.3924\n",
      "Epoch: 2 / 3, Step: 699 / 2250 Loss: 0.1286\n",
      "Epoch: 2 / 3, Step: 700 / 2250 Loss: 0.2289\n",
      "Epoch: 2 / 3, Step: 701 / 2250 Loss: 0.2270\n",
      "Epoch: 2 / 3, Step: 702 / 2250 Loss: 0.2096\n",
      "Epoch: 2 / 3, Step: 703 / 2250 Loss: 0.1752\n",
      "Epoch: 2 / 3, Step: 704 / 2250 Loss: 0.3096\n",
      "Epoch: 2 / 3, Step: 705 / 2250 Loss: 0.3754\n",
      "Epoch: 2 / 3, Step: 706 / 2250 Loss: 0.3126\n",
      "Epoch: 2 / 3, Step: 707 / 2250 Loss: 0.4266\n",
      "Epoch: 2 / 3, Step: 708 / 2250 Loss: 0.2900\n",
      "Epoch: 2 / 3, Step: 709 / 2250 Loss: 0.3101\n",
      "Epoch: 2 / 3, Step: 710 / 2250 Loss: 0.1892\n",
      "Epoch: 2 / 3, Step: 711 / 2250 Loss: 0.1990\n",
      "Epoch: 2 / 3, Step: 712 / 2250 Loss: 0.2986\n",
      "Epoch: 2 / 3, Step: 713 / 2250 Loss: 0.1611\n",
      "Epoch: 2 / 3, Step: 714 / 2250 Loss: 0.3483\n",
      "Epoch: 2 / 3, Step: 715 / 2250 Loss: 0.1755\n",
      "Epoch: 2 / 3, Step: 716 / 2250 Loss: 0.2720\n",
      "Epoch: 2 / 3, Step: 717 / 2250 Loss: 0.3149\n",
      "Epoch: 2 / 3, Step: 718 / 2250 Loss: 0.2138\n",
      "Epoch: 2 / 3, Step: 719 / 2250 Loss: 0.2677\n",
      "Epoch: 2 / 3, Step: 720 / 2250 Loss: 0.3509\n",
      "Epoch: 2 / 3, Step: 721 / 2250 Loss: 0.3795\n",
      "Epoch: 2 / 3, Step: 722 / 2250 Loss: 0.1183\n",
      "Epoch: 2 / 3, Step: 723 / 2250 Loss: 0.3015\n",
      "Epoch: 2 / 3, Step: 724 / 2250 Loss: 0.2102\n",
      "Epoch: 2 / 3, Step: 725 / 2250 Loss: 0.7446\n",
      "Epoch: 2 / 3, Step: 726 / 2250 Loss: 0.2970\n",
      "Epoch: 2 / 3, Step: 727 / 2250 Loss: 0.5201\n",
      "Epoch: 2 / 3, Step: 728 / 2250 Loss: 0.3429\n",
      "Epoch: 2 / 3, Step: 729 / 2250 Loss: 0.1780\n",
      "Epoch: 2 / 3, Step: 730 / 2250 Loss: 0.4451\n",
      "Epoch: 2 / 3, Step: 731 / 2250 Loss: 0.2994\n",
      "Epoch: 2 / 3, Step: 732 / 2250 Loss: 0.4684\n",
      "Epoch: 2 / 3, Step: 733 / 2250 Loss: 0.1357\n",
      "Epoch: 2 / 3, Step: 734 / 2250 Loss: 0.2842\n",
      "Epoch: 2 / 3, Step: 735 / 2250 Loss: 0.3496\n",
      "Epoch: 2 / 3, Step: 736 / 2250 Loss: 0.3685\n",
      "Epoch: 2 / 3, Step: 737 / 2250 Loss: 0.2370\n",
      "Epoch: 2 / 3, Step: 738 / 2250 Loss: 0.2706\n",
      "Epoch: 2 / 3, Step: 739 / 2250 Loss: 0.4578\n",
      "Epoch: 2 / 3, Step: 740 / 2250 Loss: 0.2304\n",
      "Epoch: 2 / 3, Step: 741 / 2250 Loss: 0.2027\n",
      "Epoch: 2 / 3, Step: 742 / 2250 Loss: 0.1769\n",
      "Epoch: 2 / 3, Step: 743 / 2250 Loss: 0.3356\n",
      "Epoch: 2 / 3, Step: 744 / 2250 Loss: 0.3474\n",
      "Epoch: 2 / 3, Step: 745 / 2250 Loss: 0.2341\n",
      "Epoch: 2 / 3, Step: 746 / 2250 Loss: 0.3517\n",
      "Epoch: 2 / 3, Step: 747 / 2250 Loss: 0.3836\n",
      "Epoch: 2 / 3, Step: 748 / 2250 Loss: 0.3854\n",
      "Epoch: 2 / 3, Step: 749 / 2250 Loss: 0.3987\n",
      "Epoch: 2 / 3, Step: 750 / 2250 Loss: 0.3996\n",
      "Epoch: 2 / 3, Step: 751 / 2250 Loss: 0.4198\n",
      "Epoch: 2 / 3, Step: 752 / 2250 Loss: 0.1673\n",
      "Epoch: 2 / 3, Step: 753 / 2250 Loss: 0.1493\n",
      "Epoch: 2 / 3, Step: 754 / 2250 Loss: 0.2816\n",
      "Epoch: 2 / 3, Step: 755 / 2250 Loss: 0.1813\n",
      "Epoch: 2 / 3, Step: 756 / 2250 Loss: 0.5380\n",
      "Epoch: 2 / 3, Step: 757 / 2250 Loss: 0.2584\n",
      "Epoch: 2 / 3, Step: 758 / 2250 Loss: 0.2038\n",
      "Epoch: 2 / 3, Step: 759 / 2250 Loss: 0.4929\n",
      "Epoch: 2 / 3, Step: 760 / 2250 Loss: 0.2051\n",
      "Epoch: 2 / 3, Step: 761 / 2250 Loss: 0.3377\n",
      "Epoch: 2 / 3, Step: 762 / 2250 Loss: 0.3364\n",
      "Epoch: 2 / 3, Step: 763 / 2250 Loss: 0.4271\n",
      "Epoch: 2 / 3, Step: 764 / 2250 Loss: 0.2873\n",
      "Epoch: 2 / 3, Step: 765 / 2250 Loss: 0.3046\n",
      "Epoch: 2 / 3, Step: 766 / 2250 Loss: 0.2420\n",
      "Epoch: 2 / 3, Step: 767 / 2250 Loss: 0.0989\n",
      "Epoch: 2 / 3, Step: 768 / 2250 Loss: 0.3021\n",
      "Epoch: 2 / 3, Step: 769 / 2250 Loss: 0.2270\n",
      "Epoch: 2 / 3, Step: 770 / 2250 Loss: 0.2562\n",
      "Epoch: 2 / 3, Step: 771 / 2250 Loss: 0.3372\n",
      "Epoch: 2 / 3, Step: 772 / 2250 Loss: 0.2039\n",
      "Epoch: 2 / 3, Step: 773 / 2250 Loss: 0.1014\n",
      "Epoch: 2 / 3, Step: 774 / 2250 Loss: 0.2865\n",
      "Epoch: 2 / 3, Step: 775 / 2250 Loss: 0.4870\n",
      "Epoch: 2 / 3, Step: 776 / 2250 Loss: 0.3296\n",
      "Epoch: 2 / 3, Step: 777 / 2250 Loss: 0.3300\n",
      "Epoch: 2 / 3, Step: 778 / 2250 Loss: 0.1997\n",
      "Epoch: 2 / 3, Step: 779 / 2250 Loss: 0.4260\n",
      "Epoch: 2 / 3, Step: 780 / 2250 Loss: 0.1826\n",
      "Epoch: 2 / 3, Step: 781 / 2250 Loss: 0.4023\n",
      "Epoch: 2 / 3, Step: 782 / 2250 Loss: 0.1187\n",
      "Epoch: 2 / 3, Step: 783 / 2250 Loss: 0.2646\n",
      "Epoch: 2 / 3, Step: 784 / 2250 Loss: 0.2884\n",
      "Epoch: 2 / 3, Step: 785 / 2250 Loss: 0.2655\n",
      "Epoch: 2 / 3, Step: 786 / 2250 Loss: 0.1364\n",
      "Epoch: 2 / 3, Step: 787 / 2250 Loss: 0.4599\n",
      "Epoch: 2 / 3, Step: 788 / 2250 Loss: 0.2334\n",
      "Epoch: 2 / 3, Step: 789 / 2250 Loss: 0.2755\n",
      "Epoch: 2 / 3, Step: 790 / 2250 Loss: 0.2981\n",
      "Epoch: 2 / 3, Step: 791 / 2250 Loss: 0.3296\n",
      "Epoch: 2 / 3, Step: 792 / 2250 Loss: 0.4779\n",
      "Epoch: 2 / 3, Step: 793 / 2250 Loss: 0.2524\n",
      "Epoch: 2 / 3, Step: 794 / 2250 Loss: 0.7638\n",
      "Epoch: 2 / 3, Step: 795 / 2250 Loss: 0.5607\n",
      "Epoch: 2 / 3, Step: 796 / 2250 Loss: 0.4776\n",
      "Epoch: 2 / 3, Step: 797 / 2250 Loss: 0.2225\n",
      "Epoch: 2 / 3, Step: 798 / 2250 Loss: 0.4392\n",
      "Epoch: 2 / 3, Step: 799 / 2250 Loss: 0.3728\n",
      "Epoch: 2 / 3, Step: 800 / 2250 Loss: 0.2490\n",
      "Epoch: 2 / 3, Step: 801 / 2250 Loss: 0.2052\n",
      "Epoch: 2 / 3, Step: 802 / 2250 Loss: 0.3841\n",
      "Epoch: 2 / 3, Step: 803 / 2250 Loss: 0.2200\n",
      "Epoch: 2 / 3, Step: 804 / 2250 Loss: 0.1902\n",
      "Epoch: 2 / 3, Step: 805 / 2250 Loss: 0.1845\n",
      "Epoch: 2 / 3, Step: 806 / 2250 Loss: 0.4294\n",
      "Epoch: 2 / 3, Step: 807 / 2250 Loss: 0.5921\n",
      "Epoch: 2 / 3, Step: 808 / 2250 Loss: 0.3503\n",
      "Epoch: 2 / 3, Step: 809 / 2250 Loss: 0.1657\n",
      "Epoch: 2 / 3, Step: 810 / 2250 Loss: 0.1240\n",
      "Epoch: 2 / 3, Step: 811 / 2250 Loss: 0.3131\n",
      "Epoch: 2 / 3, Step: 812 / 2250 Loss: 0.3608\n",
      "Epoch: 2 / 3, Step: 813 / 2250 Loss: 0.1472\n",
      "Epoch: 2 / 3, Step: 814 / 2250 Loss: 0.3835\n",
      "Epoch: 2 / 3, Step: 815 / 2250 Loss: 0.2105\n",
      "Epoch: 2 / 3, Step: 816 / 2250 Loss: 0.6755\n",
      "Epoch: 2 / 3, Step: 817 / 2250 Loss: 0.3354\n",
      "Epoch: 2 / 3, Step: 818 / 2250 Loss: 0.2483\n",
      "Epoch: 2 / 3, Step: 819 / 2250 Loss: 0.1814\n",
      "Epoch: 2 / 3, Step: 820 / 2250 Loss: 0.2264\n",
      "Epoch: 2 / 3, Step: 821 / 2250 Loss: 0.2560\n",
      "Epoch: 2 / 3, Step: 822 / 2250 Loss: 0.2530\n",
      "Epoch: 2 / 3, Step: 823 / 2250 Loss: 0.5063\n",
      "Epoch: 2 / 3, Step: 824 / 2250 Loss: 0.3636\n",
      "Epoch: 2 / 3, Step: 825 / 2250 Loss: 0.3348\n",
      "Epoch: 2 / 3, Step: 826 / 2250 Loss: 0.2239\n",
      "Epoch: 2 / 3, Step: 827 / 2250 Loss: 0.1817\n",
      "Epoch: 2 / 3, Step: 828 / 2250 Loss: 0.1903\n",
      "Epoch: 2 / 3, Step: 829 / 2250 Loss: 0.5499\n",
      "Epoch: 2 / 3, Step: 830 / 2250 Loss: 0.2822\n",
      "Epoch: 2 / 3, Step: 831 / 2250 Loss: 0.1726\n",
      "Epoch: 2 / 3, Step: 832 / 2250 Loss: 0.2970\n",
      "Epoch: 2 / 3, Step: 833 / 2250 Loss: 0.1482\n",
      "Epoch: 2 / 3, Step: 834 / 2250 Loss: 0.2329\n",
      "Epoch: 2 / 3, Step: 835 / 2250 Loss: 0.3207\n",
      "Epoch: 2 / 3, Step: 836 / 2250 Loss: 0.2892\n",
      "Epoch: 2 / 3, Step: 837 / 2250 Loss: 0.5068\n",
      "Epoch: 2 / 3, Step: 838 / 2250 Loss: 0.3020\n",
      "Epoch: 2 / 3, Step: 839 / 2250 Loss: 0.3720\n",
      "Epoch: 2 / 3, Step: 840 / 2250 Loss: 0.3574\n",
      "Epoch: 2 / 3, Step: 841 / 2250 Loss: 0.2277\n",
      "Epoch: 2 / 3, Step: 842 / 2250 Loss: 0.2677\n",
      "Epoch: 2 / 3, Step: 843 / 2250 Loss: 0.1621\n",
      "Epoch: 2 / 3, Step: 844 / 2250 Loss: 0.1354\n",
      "Epoch: 2 / 3, Step: 845 / 2250 Loss: 0.4444\n",
      "Epoch: 2 / 3, Step: 846 / 2250 Loss: 0.4621\n",
      "Epoch: 2 / 3, Step: 847 / 2250 Loss: 0.2012\n",
      "Epoch: 2 / 3, Step: 848 / 2250 Loss: 0.2400\n",
      "Epoch: 2 / 3, Step: 849 / 2250 Loss: 0.3628\n",
      "Epoch: 2 / 3, Step: 850 / 2250 Loss: 0.2391\n",
      "Epoch: 2 / 3, Step: 851 / 2250 Loss: 0.1593\n",
      "Epoch: 2 / 3, Step: 852 / 2250 Loss: 0.5249\n",
      "Epoch: 2 / 3, Step: 853 / 2250 Loss: 0.2939\n",
      "Epoch: 2 / 3, Step: 854 / 2250 Loss: 0.1280\n",
      "Epoch: 2 / 3, Step: 855 / 2250 Loss: 0.2083\n",
      "Epoch: 2 / 3, Step: 856 / 2250 Loss: 0.6214\n",
      "Epoch: 2 / 3, Step: 857 / 2250 Loss: 0.2733\n",
      "Epoch: 2 / 3, Step: 858 / 2250 Loss: 0.2998\n",
      "Epoch: 2 / 3, Step: 859 / 2250 Loss: 0.3770\n",
      "Epoch: 2 / 3, Step: 860 / 2250 Loss: 0.3528\n",
      "Epoch: 2 / 3, Step: 861 / 2250 Loss: 0.1418\n",
      "Epoch: 2 / 3, Step: 862 / 2250 Loss: 0.2012\n",
      "Epoch: 2 / 3, Step: 863 / 2250 Loss: 0.0814\n",
      "Epoch: 2 / 3, Step: 864 / 2250 Loss: 0.3980\n",
      "Epoch: 2 / 3, Step: 865 / 2250 Loss: 0.1966\n",
      "Epoch: 2 / 3, Step: 866 / 2250 Loss: 0.5534\n",
      "Epoch: 2 / 3, Step: 867 / 2250 Loss: 0.2634\n",
      "Epoch: 2 / 3, Step: 868 / 2250 Loss: 0.2497\n",
      "Epoch: 2 / 3, Step: 869 / 2250 Loss: 0.1306\n",
      "Epoch: 2 / 3, Step: 870 / 2250 Loss: 0.2965\n",
      "Epoch: 2 / 3, Step: 871 / 2250 Loss: 0.4071\n",
      "Epoch: 2 / 3, Step: 872 / 2250 Loss: 0.2050\n",
      "Epoch: 2 / 3, Step: 873 / 2250 Loss: 0.2399\n",
      "Epoch: 2 / 3, Step: 874 / 2250 Loss: 0.3280\n",
      "Epoch: 2 / 3, Step: 875 / 2250 Loss: 0.4684\n",
      "Epoch: 2 / 3, Step: 876 / 2250 Loss: 0.4343\n",
      "Epoch: 2 / 3, Step: 877 / 2250 Loss: 0.4361\n",
      "Epoch: 2 / 3, Step: 878 / 2250 Loss: 0.5194\n",
      "Epoch: 2 / 3, Step: 879 / 2250 Loss: 0.1627\n",
      "Epoch: 2 / 3, Step: 880 / 2250 Loss: 0.2552\n",
      "Epoch: 2 / 3, Step: 881 / 2250 Loss: 0.1675\n",
      "Epoch: 2 / 3, Step: 882 / 2250 Loss: 0.2334\n",
      "Epoch: 2 / 3, Step: 883 / 2250 Loss: 0.2860\n",
      "Epoch: 2 / 3, Step: 884 / 2250 Loss: 0.4093\n",
      "Epoch: 2 / 3, Step: 885 / 2250 Loss: 0.2045\n",
      "Epoch: 2 / 3, Step: 886 / 2250 Loss: 0.1522\n",
      "Epoch: 2 / 3, Step: 887 / 2250 Loss: 0.1842\n",
      "Epoch: 2 / 3, Step: 888 / 2250 Loss: 0.1578\n",
      "Epoch: 2 / 3, Step: 889 / 2250 Loss: 0.2018\n",
      "Epoch: 2 / 3, Step: 890 / 2250 Loss: 0.3966\n",
      "Epoch: 2 / 3, Step: 891 / 2250 Loss: 0.4139\n",
      "Epoch: 2 / 3, Step: 892 / 2250 Loss: 0.6233\n",
      "Epoch: 2 / 3, Step: 893 / 2250 Loss: 0.4840\n",
      "Epoch: 2 / 3, Step: 894 / 2250 Loss: 0.4096\n",
      "Epoch: 2 / 3, Step: 895 / 2250 Loss: 0.3416\n",
      "Epoch: 2 / 3, Step: 896 / 2250 Loss: 0.3783\n",
      "Epoch: 2 / 3, Step: 897 / 2250 Loss: 0.3298\n",
      "Epoch: 2 / 3, Step: 898 / 2250 Loss: 0.1866\n",
      "Epoch: 2 / 3, Step: 899 / 2250 Loss: 0.2469\n",
      "Epoch: 2 / 3, Step: 900 / 2250 Loss: 0.3423\n",
      "Epoch: 2 / 3, Step: 901 / 2250 Loss: 0.1424\n",
      "Epoch: 2 / 3, Step: 902 / 2250 Loss: 0.2872\n",
      "Epoch: 2 / 3, Step: 903 / 2250 Loss: 0.1882\n",
      "Epoch: 2 / 3, Step: 904 / 2250 Loss: 0.1648\n",
      "Epoch: 2 / 3, Step: 905 / 2250 Loss: 0.3289\n",
      "Epoch: 2 / 3, Step: 906 / 2250 Loss: 0.3443\n",
      "Epoch: 2 / 3, Step: 907 / 2250 Loss: 0.2894\n",
      "Epoch: 2 / 3, Step: 908 / 2250 Loss: 0.2556\n",
      "Epoch: 2 / 3, Step: 909 / 2250 Loss: 0.5566\n",
      "Epoch: 2 / 3, Step: 910 / 2250 Loss: 0.3028\n",
      "Epoch: 2 / 3, Step: 911 / 2250 Loss: 0.4707\n",
      "Epoch: 2 / 3, Step: 912 / 2250 Loss: 0.2493\n",
      "Epoch: 2 / 3, Step: 913 / 2250 Loss: 0.1854\n",
      "Epoch: 2 / 3, Step: 914 / 2250 Loss: 0.2705\n",
      "Epoch: 2 / 3, Step: 915 / 2250 Loss: 0.5071\n",
      "Epoch: 2 / 3, Step: 916 / 2250 Loss: 0.4839\n",
      "Epoch: 2 / 3, Step: 917 / 2250 Loss: 0.5598\n",
      "Epoch: 2 / 3, Step: 918 / 2250 Loss: 0.3052\n",
      "Epoch: 2 / 3, Step: 919 / 2250 Loss: 0.6854\n",
      "Epoch: 2 / 3, Step: 920 / 2250 Loss: 0.2913\n",
      "Epoch: 2 / 3, Step: 921 / 2250 Loss: 0.4822\n",
      "Epoch: 2 / 3, Step: 922 / 2250 Loss: 0.3426\n",
      "Epoch: 2 / 3, Step: 923 / 2250 Loss: 0.2438\n",
      "Epoch: 2 / 3, Step: 924 / 2250 Loss: 0.2635\n",
      "Epoch: 2 / 3, Step: 925 / 2250 Loss: 0.4342\n",
      "Epoch: 2 / 3, Step: 926 / 2250 Loss: 0.3024\n",
      "Epoch: 2 / 3, Step: 927 / 2250 Loss: 0.2529\n",
      "Epoch: 2 / 3, Step: 928 / 2250 Loss: 0.1731\n",
      "Epoch: 2 / 3, Step: 929 / 2250 Loss: 0.5223\n",
      "Epoch: 2 / 3, Step: 930 / 2250 Loss: 0.4012\n",
      "Epoch: 2 / 3, Step: 931 / 2250 Loss: 0.6343\n",
      "Epoch: 2 / 3, Step: 932 / 2250 Loss: 0.3381\n",
      "Epoch: 2 / 3, Step: 933 / 2250 Loss: 0.3281\n",
      "Epoch: 2 / 3, Step: 934 / 2250 Loss: 0.3409\n",
      "Epoch: 2 / 3, Step: 935 / 2250 Loss: 0.4192\n",
      "Epoch: 2 / 3, Step: 936 / 2250 Loss: 0.2670\n",
      "Epoch: 2 / 3, Step: 937 / 2250 Loss: 0.3122\n",
      "Epoch: 2 / 3, Step: 938 / 2250 Loss: 0.2061\n",
      "Epoch: 2 / 3, Step: 939 / 2250 Loss: 0.2757\n",
      "Epoch: 2 / 3, Step: 940 / 2250 Loss: 0.3620\n",
      "Epoch: 2 / 3, Step: 941 / 2250 Loss: 0.1748\n",
      "Epoch: 2 / 3, Step: 942 / 2250 Loss: 0.4314\n",
      "Epoch: 2 / 3, Step: 943 / 2250 Loss: 0.2682\n",
      "Epoch: 2 / 3, Step: 944 / 2250 Loss: 0.4995\n",
      "Epoch: 2 / 3, Step: 945 / 2250 Loss: 0.2621\n",
      "Epoch: 2 / 3, Step: 946 / 2250 Loss: 0.5594\n",
      "Epoch: 2 / 3, Step: 947 / 2250 Loss: 0.3585\n",
      "Epoch: 2 / 3, Step: 948 / 2250 Loss: 0.5546\n",
      "Epoch: 2 / 3, Step: 949 / 2250 Loss: 0.2838\n",
      "Epoch: 2 / 3, Step: 950 / 2250 Loss: 0.3352\n",
      "Epoch: 2 / 3, Step: 951 / 2250 Loss: 0.2548\n",
      "Epoch: 2 / 3, Step: 952 / 2250 Loss: 0.3284\n",
      "Epoch: 2 / 3, Step: 953 / 2250 Loss: 0.1998\n",
      "Epoch: 2 / 3, Step: 954 / 2250 Loss: 0.2705\n",
      "Epoch: 2 / 3, Step: 955 / 2250 Loss: 0.2514\n",
      "Epoch: 2 / 3, Step: 956 / 2250 Loss: 0.2639\n",
      "Epoch: 2 / 3, Step: 957 / 2250 Loss: 0.3267\n",
      "Epoch: 2 / 3, Step: 958 / 2250 Loss: 0.2373\n",
      "Epoch: 2 / 3, Step: 959 / 2250 Loss: 0.1489\n",
      "Epoch: 2 / 3, Step: 960 / 2250 Loss: 0.3295\n",
      "Epoch: 2 / 3, Step: 961 / 2250 Loss: 0.6583\n",
      "Epoch: 2 / 3, Step: 962 / 2250 Loss: 0.1889\n",
      "Epoch: 2 / 3, Step: 963 / 2250 Loss: 0.1380\n",
      "Epoch: 2 / 3, Step: 964 / 2250 Loss: 0.2639\n",
      "Epoch: 2 / 3, Step: 965 / 2250 Loss: 0.3308\n",
      "Epoch: 2 / 3, Step: 966 / 2250 Loss: 0.3619\n",
      "Epoch: 2 / 3, Step: 967 / 2250 Loss: 0.1709\n",
      "Epoch: 2 / 3, Step: 968 / 2250 Loss: 0.3757\n",
      "Epoch: 2 / 3, Step: 969 / 2250 Loss: 0.2022\n",
      "Epoch: 2 / 3, Step: 970 / 2250 Loss: 0.3356\n",
      "Epoch: 2 / 3, Step: 971 / 2250 Loss: 0.2632\n",
      "Epoch: 2 / 3, Step: 972 / 2250 Loss: 0.2207\n",
      "Epoch: 2 / 3, Step: 973 / 2250 Loss: 0.2533\n",
      "Epoch: 2 / 3, Step: 974 / 2250 Loss: 0.7002\n",
      "Epoch: 2 / 3, Step: 975 / 2250 Loss: 0.1500\n",
      "Epoch: 2 / 3, Step: 976 / 2250 Loss: 0.4314\n",
      "Epoch: 2 / 3, Step: 977 / 2250 Loss: 0.2560\n",
      "Epoch: 2 / 3, Step: 978 / 2250 Loss: 0.2316\n",
      "Epoch: 2 / 3, Step: 979 / 2250 Loss: 0.4883\n",
      "Epoch: 2 / 3, Step: 980 / 2250 Loss: 0.5882\n",
      "Epoch: 2 / 3, Step: 981 / 2250 Loss: 0.1179\n",
      "Epoch: 2 / 3, Step: 982 / 2250 Loss: 0.2447\n",
      "Epoch: 2 / 3, Step: 983 / 2250 Loss: 0.5865\n",
      "Epoch: 2 / 3, Step: 984 / 2250 Loss: 0.5161\n",
      "Epoch: 2 / 3, Step: 985 / 2250 Loss: 0.2747\n",
      "Epoch: 2 / 3, Step: 986 / 2250 Loss: 0.2030\n",
      "Epoch: 2 / 3, Step: 987 / 2250 Loss: 0.3837\n",
      "Epoch: 2 / 3, Step: 988 / 2250 Loss: 0.2333\n",
      "Epoch: 2 / 3, Step: 989 / 2250 Loss: 0.3958\n",
      "Epoch: 2 / 3, Step: 990 / 2250 Loss: 0.2443\n",
      "Epoch: 2 / 3, Step: 991 / 2250 Loss: 0.1914\n",
      "Epoch: 2 / 3, Step: 992 / 2250 Loss: 0.4787\n",
      "Epoch: 2 / 3, Step: 993 / 2250 Loss: 0.1589\n",
      "Epoch: 2 / 3, Step: 994 / 2250 Loss: 0.1744\n",
      "Epoch: 2 / 3, Step: 995 / 2250 Loss: 0.3879\n",
      "Epoch: 2 / 3, Step: 996 / 2250 Loss: 0.3623\n",
      "Epoch: 2 / 3, Step: 997 / 2250 Loss: 0.4508\n",
      "Epoch: 2 / 3, Step: 998 / 2250 Loss: 0.4645\n",
      "Epoch: 2 / 3, Step: 999 / 2250 Loss: 0.1384\n",
      "Epoch: 2 / 3, Step: 1000 / 2250 Loss: 0.2770\n",
      "Epoch: 2 / 3, Step: 1001 / 2250 Loss: 0.3198\n",
      "Epoch: 2 / 3, Step: 1002 / 2250 Loss: 0.2644\n",
      "Epoch: 2 / 3, Step: 1003 / 2250 Loss: 0.4170\n",
      "Epoch: 2 / 3, Step: 1004 / 2250 Loss: 0.2199\n",
      "Epoch: 2 / 3, Step: 1005 / 2250 Loss: 0.2750\n",
      "Epoch: 2 / 3, Step: 1006 / 2250 Loss: 0.4634\n",
      "Epoch: 2 / 3, Step: 1007 / 2250 Loss: 0.2333\n",
      "Epoch: 2 / 3, Step: 1008 / 2250 Loss: 0.1757\n",
      "Epoch: 2 / 3, Step: 1009 / 2250 Loss: 0.3142\n",
      "Epoch: 2 / 3, Step: 1010 / 2250 Loss: 0.2144\n",
      "Epoch: 2 / 3, Step: 1011 / 2250 Loss: 0.0990\n",
      "Epoch: 2 / 3, Step: 1012 / 2250 Loss: 0.3864\n",
      "Epoch: 2 / 3, Step: 1013 / 2250 Loss: 0.5701\n",
      "Epoch: 2 / 3, Step: 1014 / 2250 Loss: 0.5392\n",
      "Epoch: 2 / 3, Step: 1015 / 2250 Loss: 0.2397\n",
      "Epoch: 2 / 3, Step: 1016 / 2250 Loss: 0.3047\n",
      "Epoch: 2 / 3, Step: 1017 / 2250 Loss: 0.3426\n",
      "Epoch: 2 / 3, Step: 1018 / 2250 Loss: 0.2267\n",
      "Epoch: 2 / 3, Step: 1019 / 2250 Loss: 0.1766\n",
      "Epoch: 2 / 3, Step: 1020 / 2250 Loss: 0.2303\n",
      "Epoch: 2 / 3, Step: 1021 / 2250 Loss: 0.1412\n",
      "Epoch: 2 / 3, Step: 1022 / 2250 Loss: 0.3892\n",
      "Epoch: 2 / 3, Step: 1023 / 2250 Loss: 0.1669\n",
      "Epoch: 2 / 3, Step: 1024 / 2250 Loss: 0.4386\n",
      "Epoch: 2 / 3, Step: 1025 / 2250 Loss: 0.3988\n",
      "Epoch: 2 / 3, Step: 1026 / 2250 Loss: 0.2280\n",
      "Epoch: 2 / 3, Step: 1027 / 2250 Loss: 0.2261\n",
      "Epoch: 2 / 3, Step: 1028 / 2250 Loss: 0.4974\n",
      "Epoch: 2 / 3, Step: 1029 / 2250 Loss: 0.3035\n",
      "Epoch: 2 / 3, Step: 1030 / 2250 Loss: 0.6574\n",
      "Epoch: 2 / 3, Step: 1031 / 2250 Loss: 0.3951\n",
      "Epoch: 2 / 3, Step: 1032 / 2250 Loss: 0.1323\n",
      "Epoch: 2 / 3, Step: 1033 / 2250 Loss: 0.2700\n",
      "Epoch: 2 / 3, Step: 1034 / 2250 Loss: 0.2537\n",
      "Epoch: 2 / 3, Step: 1035 / 2250 Loss: 0.2745\n",
      "Epoch: 2 / 3, Step: 1036 / 2250 Loss: 0.2388\n",
      "Epoch: 2 / 3, Step: 1037 / 2250 Loss: 0.3194\n",
      "Epoch: 2 / 3, Step: 1038 / 2250 Loss: 0.6221\n",
      "Epoch: 2 / 3, Step: 1039 / 2250 Loss: 0.2067\n",
      "Epoch: 2 / 3, Step: 1040 / 2250 Loss: 0.1093\n",
      "Epoch: 2 / 3, Step: 1041 / 2250 Loss: 0.2091\n",
      "Epoch: 2 / 3, Step: 1042 / 2250 Loss: 0.4607\n",
      "Epoch: 2 / 3, Step: 1043 / 2250 Loss: 0.2572\n",
      "Epoch: 2 / 3, Step: 1044 / 2250 Loss: 0.4044\n",
      "Epoch: 2 / 3, Step: 1045 / 2250 Loss: 0.3286\n",
      "Epoch: 2 / 3, Step: 1046 / 2250 Loss: 0.4105\n",
      "Epoch: 2 / 3, Step: 1047 / 2250 Loss: 0.2357\n",
      "Epoch: 2 / 3, Step: 1048 / 2250 Loss: 0.2662\n",
      "Epoch: 2 / 3, Step: 1049 / 2250 Loss: 0.3593\n",
      "Epoch: 2 / 3, Step: 1050 / 2250 Loss: 0.1805\n",
      "Epoch: 2 / 3, Step: 1051 / 2250 Loss: 0.1063\n",
      "Epoch: 2 / 3, Step: 1052 / 2250 Loss: 0.2318\n",
      "Epoch: 2 / 3, Step: 1053 / 2250 Loss: 0.2986\n",
      "Epoch: 2 / 3, Step: 1054 / 2250 Loss: 0.3158\n",
      "Epoch: 2 / 3, Step: 1055 / 2250 Loss: 0.2250\n",
      "Epoch: 2 / 3, Step: 1056 / 2250 Loss: 0.2815\n",
      "Epoch: 2 / 3, Step: 1057 / 2250 Loss: 0.3565\n",
      "Epoch: 2 / 3, Step: 1058 / 2250 Loss: 0.1489\n",
      "Epoch: 2 / 3, Step: 1059 / 2250 Loss: 0.1953\n",
      "Epoch: 2 / 3, Step: 1060 / 2250 Loss: 0.2528\n",
      "Epoch: 2 / 3, Step: 1061 / 2250 Loss: 0.2085\n",
      "Epoch: 2 / 3, Step: 1062 / 2250 Loss: 0.4653\n",
      "Epoch: 2 / 3, Step: 1063 / 2250 Loss: 0.1534\n",
      "Epoch: 2 / 3, Step: 1064 / 2250 Loss: 0.1986\n",
      "Epoch: 2 / 3, Step: 1065 / 2250 Loss: 0.2887\n",
      "Epoch: 2 / 3, Step: 1066 / 2250 Loss: 0.3837\n",
      "Epoch: 2 / 3, Step: 1067 / 2250 Loss: 0.4462\n",
      "Epoch: 2 / 3, Step: 1068 / 2250 Loss: 0.2966\n",
      "Epoch: 2 / 3, Step: 1069 / 2250 Loss: 0.4552\n",
      "Epoch: 2 / 3, Step: 1070 / 2250 Loss: 0.4380\n",
      "Epoch: 2 / 3, Step: 1071 / 2250 Loss: 0.2934\n",
      "Epoch: 2 / 3, Step: 1072 / 2250 Loss: 0.3594\n",
      "Epoch: 2 / 3, Step: 1073 / 2250 Loss: 0.1805\n",
      "Epoch: 2 / 3, Step: 1074 / 2250 Loss: 0.3570\n",
      "Epoch: 2 / 3, Step: 1075 / 2250 Loss: 0.3630\n",
      "Epoch: 2 / 3, Step: 1076 / 2250 Loss: 0.1204\n",
      "Epoch: 2 / 3, Step: 1077 / 2250 Loss: 0.2534\n",
      "Epoch: 2 / 3, Step: 1078 / 2250 Loss: 0.1134\n",
      "Epoch: 2 / 3, Step: 1079 / 2250 Loss: 0.1758\n",
      "Epoch: 2 / 3, Step: 1080 / 2250 Loss: 0.3561\n",
      "Epoch: 2 / 3, Step: 1081 / 2250 Loss: 0.0998\n",
      "Epoch: 2 / 3, Step: 1082 / 2250 Loss: 0.2228\n",
      "Epoch: 2 / 3, Step: 1083 / 2250 Loss: 0.3745\n",
      "Epoch: 2 / 3, Step: 1084 / 2250 Loss: 0.2934\n",
      "Epoch: 2 / 3, Step: 1085 / 2250 Loss: 0.2820\n",
      "Epoch: 2 / 3, Step: 1086 / 2250 Loss: 0.1240\n",
      "Epoch: 2 / 3, Step: 1087 / 2250 Loss: 0.2237\n",
      "Epoch: 2 / 3, Step: 1088 / 2250 Loss: 0.4595\n",
      "Epoch: 2 / 3, Step: 1089 / 2250 Loss: 0.1354\n",
      "Epoch: 2 / 3, Step: 1090 / 2250 Loss: 0.4896\n",
      "Epoch: 2 / 3, Step: 1091 / 2250 Loss: 0.5442\n",
      "Epoch: 2 / 3, Step: 1092 / 2250 Loss: 0.1646\n",
      "Epoch: 2 / 3, Step: 1093 / 2250 Loss: 0.4483\n",
      "Epoch: 2 / 3, Step: 1094 / 2250 Loss: 0.1987\n",
      "Epoch: 2 / 3, Step: 1095 / 2250 Loss: 0.4672\n",
      "Epoch: 2 / 3, Step: 1096 / 2250 Loss: 0.1641\n",
      "Epoch: 2 / 3, Step: 1097 / 2250 Loss: 0.2628\n",
      "Epoch: 2 / 3, Step: 1098 / 2250 Loss: 0.2395\n",
      "Epoch: 2 / 3, Step: 1099 / 2250 Loss: 0.1721\n",
      "Epoch: 2 / 3, Step: 1100 / 2250 Loss: 0.4050\n",
      "Epoch: 2 / 3, Step: 1101 / 2250 Loss: 0.4057\n",
      "Epoch: 2 / 3, Step: 1102 / 2250 Loss: 0.2609\n",
      "Epoch: 2 / 3, Step: 1103 / 2250 Loss: 0.3453\n",
      "Epoch: 2 / 3, Step: 1104 / 2250 Loss: 0.3682\n",
      "Epoch: 2 / 3, Step: 1105 / 2250 Loss: 0.3585\n",
      "Epoch: 2 / 3, Step: 1106 / 2250 Loss: 0.1939\n",
      "Epoch: 2 / 3, Step: 1107 / 2250 Loss: 0.2955\n",
      "Epoch: 2 / 3, Step: 1108 / 2250 Loss: 0.1798\n",
      "Epoch: 2 / 3, Step: 1109 / 2250 Loss: 0.2080\n",
      "Epoch: 2 / 3, Step: 1110 / 2250 Loss: 0.3755\n",
      "Epoch: 2 / 3, Step: 1111 / 2250 Loss: 0.5101\n",
      "Epoch: 2 / 3, Step: 1112 / 2250 Loss: 0.2139\n",
      "Epoch: 2 / 3, Step: 1113 / 2250 Loss: 0.1402\n",
      "Epoch: 2 / 3, Step: 1114 / 2250 Loss: 0.1665\n",
      "Epoch: 2 / 3, Step: 1115 / 2250 Loss: 0.3616\n",
      "Epoch: 2 / 3, Step: 1116 / 2250 Loss: 0.2631\n",
      "Epoch: 2 / 3, Step: 1117 / 2250 Loss: 0.3426\n",
      "Epoch: 2 / 3, Step: 1118 / 2250 Loss: 0.3278\n",
      "Epoch: 2 / 3, Step: 1119 / 2250 Loss: 0.1979\n",
      "Epoch: 2 / 3, Step: 1120 / 2250 Loss: 0.4087\n",
      "Epoch: 2 / 3, Step: 1121 / 2250 Loss: 0.1168\n",
      "Epoch: 2 / 3, Step: 1122 / 2250 Loss: 0.3322\n",
      "Epoch: 2 / 3, Step: 1123 / 2250 Loss: 0.3053\n",
      "Epoch: 2 / 3, Step: 1124 / 2250 Loss: 0.3318\n",
      "Epoch: 2 / 3, Step: 1125 / 2250 Loss: 0.3551\n",
      "Epoch: 2 / 3, Step: 1126 / 2250 Loss: 0.0659\n",
      "Epoch: 2 / 3, Step: 1127 / 2250 Loss: 0.6154\n",
      "Epoch: 2 / 3, Step: 1128 / 2250 Loss: 0.3089\n",
      "Epoch: 2 / 3, Step: 1129 / 2250 Loss: 0.2241\n",
      "Epoch: 2 / 3, Step: 1130 / 2250 Loss: 0.3179\n",
      "Epoch: 2 / 3, Step: 1131 / 2250 Loss: 0.2902\n",
      "Epoch: 2 / 3, Step: 1132 / 2250 Loss: 0.5286\n",
      "Epoch: 2 / 3, Step: 1133 / 2250 Loss: 0.2780\n",
      "Epoch: 2 / 3, Step: 1134 / 2250 Loss: 0.2495\n",
      "Epoch: 2 / 3, Step: 1135 / 2250 Loss: 0.3121\n",
      "Epoch: 2 / 3, Step: 1136 / 2250 Loss: 0.2974\n",
      "Epoch: 2 / 3, Step: 1137 / 2250 Loss: 0.3056\n",
      "Epoch: 2 / 3, Step: 1138 / 2250 Loss: 0.6367\n",
      "Epoch: 2 / 3, Step: 1139 / 2250 Loss: 0.2353\n",
      "Epoch: 2 / 3, Step: 1140 / 2250 Loss: 0.3149\n",
      "Epoch: 2 / 3, Step: 1141 / 2250 Loss: 0.4601\n",
      "Epoch: 2 / 3, Step: 1142 / 2250 Loss: 0.3167\n",
      "Epoch: 2 / 3, Step: 1143 / 2250 Loss: 0.3720\n",
      "Epoch: 2 / 3, Step: 1144 / 2250 Loss: 0.3718\n",
      "Epoch: 2 / 3, Step: 1145 / 2250 Loss: 0.3110\n",
      "Epoch: 2 / 3, Step: 1146 / 2250 Loss: 0.1891\n",
      "Epoch: 2 / 3, Step: 1147 / 2250 Loss: 0.2984\n",
      "Epoch: 2 / 3, Step: 1148 / 2250 Loss: 0.3952\n",
      "Epoch: 2 / 3, Step: 1149 / 2250 Loss: 0.5453\n",
      "Epoch: 2 / 3, Step: 1150 / 2250 Loss: 0.1661\n",
      "Epoch: 2 / 3, Step: 1151 / 2250 Loss: 0.2060\n",
      "Epoch: 2 / 3, Step: 1152 / 2250 Loss: 0.2439\n",
      "Epoch: 2 / 3, Step: 1153 / 2250 Loss: 0.2553\n",
      "Epoch: 2 / 3, Step: 1154 / 2250 Loss: 0.2725\n",
      "Epoch: 2 / 3, Step: 1155 / 2250 Loss: 0.4036\n",
      "Epoch: 2 / 3, Step: 1156 / 2250 Loss: 0.0817\n",
      "Epoch: 2 / 3, Step: 1157 / 2250 Loss: 0.2442\n",
      "Epoch: 2 / 3, Step: 1158 / 2250 Loss: 0.1638\n",
      "Epoch: 2 / 3, Step: 1159 / 2250 Loss: 0.1252\n",
      "Epoch: 2 / 3, Step: 1160 / 2250 Loss: 0.4185\n",
      "Epoch: 2 / 3, Step: 1161 / 2250 Loss: 0.4355\n",
      "Epoch: 2 / 3, Step: 1162 / 2250 Loss: 0.4204\n",
      "Epoch: 2 / 3, Step: 1163 / 2250 Loss: 0.1868\n",
      "Epoch: 2 / 3, Step: 1164 / 2250 Loss: 0.2180\n",
      "Epoch: 2 / 3, Step: 1165 / 2250 Loss: 0.2373\n",
      "Epoch: 2 / 3, Step: 1166 / 2250 Loss: 0.1439\n",
      "Epoch: 2 / 3, Step: 1167 / 2250 Loss: 0.2999\n",
      "Epoch: 2 / 3, Step: 1168 / 2250 Loss: 0.3630\n",
      "Epoch: 2 / 3, Step: 1169 / 2250 Loss: 0.2021\n",
      "Epoch: 2 / 3, Step: 1170 / 2250 Loss: 0.3689\n",
      "Epoch: 2 / 3, Step: 1171 / 2250 Loss: 0.2334\n",
      "Epoch: 2 / 3, Step: 1172 / 2250 Loss: 0.1421\n",
      "Epoch: 2 / 3, Step: 1173 / 2250 Loss: 0.4613\n",
      "Epoch: 2 / 3, Step: 1174 / 2250 Loss: 0.1034\n",
      "Epoch: 2 / 3, Step: 1175 / 2250 Loss: 0.3093\n",
      "Epoch: 2 / 3, Step: 1176 / 2250 Loss: 0.1974\n",
      "Epoch: 2 / 3, Step: 1177 / 2250 Loss: 0.1941\n",
      "Epoch: 2 / 3, Step: 1178 / 2250 Loss: 0.3478\n",
      "Epoch: 2 / 3, Step: 1179 / 2250 Loss: 0.1938\n",
      "Epoch: 2 / 3, Step: 1180 / 2250 Loss: 0.2627\n",
      "Epoch: 2 / 3, Step: 1181 / 2250 Loss: 0.1499\n",
      "Epoch: 2 / 3, Step: 1182 / 2250 Loss: 0.4172\n",
      "Epoch: 2 / 3, Step: 1183 / 2250 Loss: 0.1787\n",
      "Epoch: 2 / 3, Step: 1184 / 2250 Loss: 0.5290\n",
      "Epoch: 2 / 3, Step: 1185 / 2250 Loss: 0.3117\n",
      "Epoch: 2 / 3, Step: 1186 / 2250 Loss: 0.2711\n",
      "Epoch: 2 / 3, Step: 1187 / 2250 Loss: 0.1592\n",
      "Epoch: 2 / 3, Step: 1188 / 2250 Loss: 0.0951\n",
      "Epoch: 2 / 3, Step: 1189 / 2250 Loss: 0.1770\n",
      "Epoch: 2 / 3, Step: 1190 / 2250 Loss: 0.3159\n",
      "Epoch: 2 / 3, Step: 1191 / 2250 Loss: 0.0943\n",
      "Epoch: 2 / 3, Step: 1192 / 2250 Loss: 0.2847\n",
      "Epoch: 2 / 3, Step: 1193 / 2250 Loss: 0.1513\n",
      "Epoch: 2 / 3, Step: 1194 / 2250 Loss: 0.4621\n",
      "Epoch: 2 / 3, Step: 1195 / 2250 Loss: 0.2561\n",
      "Epoch: 2 / 3, Step: 1196 / 2250 Loss: 0.6472\n",
      "Epoch: 2 / 3, Step: 1197 / 2250 Loss: 0.1658\n",
      "Epoch: 2 / 3, Step: 1198 / 2250 Loss: 0.3853\n",
      "Epoch: 2 / 3, Step: 1199 / 2250 Loss: 0.3913\n",
      "Epoch: 2 / 3, Step: 1200 / 2250 Loss: 0.0764\n",
      "Epoch: 2 / 3, Step: 1201 / 2250 Loss: 0.2232\n",
      "Epoch: 2 / 3, Step: 1202 / 2250 Loss: 0.5438\n",
      "Epoch: 2 / 3, Step: 1203 / 2250 Loss: 0.1550\n",
      "Epoch: 2 / 3, Step: 1204 / 2250 Loss: 0.2379\n",
      "Epoch: 2 / 3, Step: 1205 / 2250 Loss: 0.2334\n",
      "Epoch: 2 / 3, Step: 1206 / 2250 Loss: 0.1918\n",
      "Epoch: 2 / 3, Step: 1207 / 2250 Loss: 0.4054\n",
      "Epoch: 2 / 3, Step: 1208 / 2250 Loss: 0.2511\n",
      "Epoch: 2 / 3, Step: 1209 / 2250 Loss: 0.1927\n",
      "Epoch: 2 / 3, Step: 1210 / 2250 Loss: 0.4272\n",
      "Epoch: 2 / 3, Step: 1211 / 2250 Loss: 0.3230\n",
      "Epoch: 2 / 3, Step: 1212 / 2250 Loss: 0.2604\n",
      "Epoch: 2 / 3, Step: 1213 / 2250 Loss: 0.2307\n",
      "Epoch: 2 / 3, Step: 1214 / 2250 Loss: 0.3168\n",
      "Epoch: 2 / 3, Step: 1215 / 2250 Loss: 0.2296\n",
      "Epoch: 2 / 3, Step: 1216 / 2250 Loss: 0.3408\n",
      "Epoch: 2 / 3, Step: 1217 / 2250 Loss: 0.5362\n",
      "Epoch: 2 / 3, Step: 1218 / 2250 Loss: 0.3944\n",
      "Epoch: 2 / 3, Step: 1219 / 2250 Loss: 0.1266\n",
      "Epoch: 2 / 3, Step: 1220 / 2250 Loss: 0.3537\n",
      "Epoch: 2 / 3, Step: 1221 / 2250 Loss: 0.0749\n",
      "Epoch: 2 / 3, Step: 1222 / 2250 Loss: 0.1495\n",
      "Epoch: 2 / 3, Step: 1223 / 2250 Loss: 0.2003\n",
      "Epoch: 2 / 3, Step: 1224 / 2250 Loss: 0.2523\n",
      "Epoch: 2 / 3, Step: 1225 / 2250 Loss: 0.4539\n",
      "Epoch: 2 / 3, Step: 1226 / 2250 Loss: 0.2094\n",
      "Epoch: 2 / 3, Step: 1227 / 2250 Loss: 0.3409\n",
      "Epoch: 2 / 3, Step: 1228 / 2250 Loss: 0.1575\n",
      "Epoch: 2 / 3, Step: 1229 / 2250 Loss: 0.1992\n",
      "Epoch: 2 / 3, Step: 1230 / 2250 Loss: 0.4863\n",
      "Epoch: 2 / 3, Step: 1231 / 2250 Loss: 0.3600\n",
      "Epoch: 2 / 3, Step: 1232 / 2250 Loss: 0.1364\n",
      "Epoch: 2 / 3, Step: 1233 / 2250 Loss: 0.4757\n",
      "Epoch: 2 / 3, Step: 1234 / 2250 Loss: 0.2418\n",
      "Epoch: 2 / 3, Step: 1235 / 2250 Loss: 0.2904\n",
      "Epoch: 2 / 3, Step: 1236 / 2250 Loss: 0.1307\n",
      "Epoch: 2 / 3, Step: 1237 / 2250 Loss: 0.2195\n",
      "Epoch: 2 / 3, Step: 1238 / 2250 Loss: 0.2106\n",
      "Epoch: 2 / 3, Step: 1239 / 2250 Loss: 0.1610\n",
      "Epoch: 2 / 3, Step: 1240 / 2250 Loss: 0.2202\n",
      "Epoch: 2 / 3, Step: 1241 / 2250 Loss: 0.3762\n",
      "Epoch: 2 / 3, Step: 1242 / 2250 Loss: 0.1411\n",
      "Epoch: 2 / 3, Step: 1243 / 2250 Loss: 0.4464\n",
      "Epoch: 2 / 3, Step: 1244 / 2250 Loss: 0.2630\n",
      "Epoch: 2 / 3, Step: 1245 / 2250 Loss: 0.3338\n",
      "Epoch: 2 / 3, Step: 1246 / 2250 Loss: 0.3621\n",
      "Epoch: 2 / 3, Step: 1247 / 2250 Loss: 0.2649\n",
      "Epoch: 2 / 3, Step: 1248 / 2250 Loss: 0.1811\n",
      "Epoch: 2 / 3, Step: 1249 / 2250 Loss: 0.2352\n",
      "Epoch: 2 / 3, Step: 1250 / 2250 Loss: 0.6727\n",
      "Epoch: 2 / 3, Step: 1251 / 2250 Loss: 0.6500\n",
      "Epoch: 2 / 3, Step: 1252 / 2250 Loss: 0.3575\n",
      "Epoch: 2 / 3, Step: 1253 / 2250 Loss: 0.3197\n",
      "Epoch: 2 / 3, Step: 1254 / 2250 Loss: 0.4475\n",
      "Epoch: 2 / 3, Step: 1255 / 2250 Loss: 0.2720\n",
      "Epoch: 2 / 3, Step: 1256 / 2250 Loss: 0.4663\n",
      "Epoch: 2 / 3, Step: 1257 / 2250 Loss: 0.2789\n",
      "Epoch: 2 / 3, Step: 1258 / 2250 Loss: 0.3726\n",
      "Epoch: 2 / 3, Step: 1259 / 2250 Loss: 0.3885\n",
      "Epoch: 2 / 3, Step: 1260 / 2250 Loss: 0.3250\n",
      "Epoch: 2 / 3, Step: 1261 / 2250 Loss: 0.3280\n",
      "Epoch: 2 / 3, Step: 1262 / 2250 Loss: 0.2348\n",
      "Epoch: 2 / 3, Step: 1263 / 2250 Loss: 0.2913\n",
      "Epoch: 2 / 3, Step: 1264 / 2250 Loss: 0.3534\n",
      "Epoch: 2 / 3, Step: 1265 / 2250 Loss: 0.2840\n",
      "Epoch: 2 / 3, Step: 1266 / 2250 Loss: 0.4087\n",
      "Epoch: 2 / 3, Step: 1267 / 2250 Loss: 0.4344\n",
      "Epoch: 2 / 3, Step: 1268 / 2250 Loss: 0.3857\n",
      "Epoch: 2 / 3, Step: 1269 / 2250 Loss: 0.5919\n",
      "Epoch: 2 / 3, Step: 1270 / 2250 Loss: 0.2540\n",
      "Epoch: 2 / 3, Step: 1271 / 2250 Loss: 0.2273\n",
      "Epoch: 2 / 3, Step: 1272 / 2250 Loss: 0.3659\n",
      "Epoch: 2 / 3, Step: 1273 / 2250 Loss: 0.7306\n",
      "Epoch: 2 / 3, Step: 1274 / 2250 Loss: 0.3097\n",
      "Epoch: 2 / 3, Step: 1275 / 2250 Loss: 0.3054\n",
      "Epoch: 2 / 3, Step: 1276 / 2250 Loss: 0.3381\n",
      "Epoch: 2 / 3, Step: 1277 / 2250 Loss: 0.1789\n",
      "Epoch: 2 / 3, Step: 1278 / 2250 Loss: 0.2764\n",
      "Epoch: 2 / 3, Step: 1279 / 2250 Loss: 0.2168\n",
      "Epoch: 2 / 3, Step: 1280 / 2250 Loss: 0.3302\n",
      "Epoch: 2 / 3, Step: 1281 / 2250 Loss: 0.1681\n",
      "Epoch: 2 / 3, Step: 1282 / 2250 Loss: 0.2924\n",
      "Epoch: 2 / 3, Step: 1283 / 2250 Loss: 0.1167\n",
      "Epoch: 2 / 3, Step: 1284 / 2250 Loss: 0.2665\n",
      "Epoch: 2 / 3, Step: 1285 / 2250 Loss: 0.1799\n",
      "Epoch: 2 / 3, Step: 1286 / 2250 Loss: 0.2317\n",
      "Epoch: 2 / 3, Step: 1287 / 2250 Loss: 0.1521\n",
      "Epoch: 2 / 3, Step: 1288 / 2250 Loss: 0.2595\n",
      "Epoch: 2 / 3, Step: 1289 / 2250 Loss: 0.2353\n",
      "Epoch: 2 / 3, Step: 1290 / 2250 Loss: 0.0805\n",
      "Epoch: 2 / 3, Step: 1291 / 2250 Loss: 0.1322\n",
      "Epoch: 2 / 3, Step: 1292 / 2250 Loss: 0.2849\n",
      "Epoch: 2 / 3, Step: 1293 / 2250 Loss: 0.4363\n",
      "Epoch: 2 / 3, Step: 1294 / 2250 Loss: 0.1160\n",
      "Epoch: 2 / 3, Step: 1295 / 2250 Loss: 0.2556\n",
      "Epoch: 2 / 3, Step: 1296 / 2250 Loss: 0.2865\n",
      "Epoch: 2 / 3, Step: 1297 / 2250 Loss: 0.1234\n",
      "Epoch: 2 / 3, Step: 1298 / 2250 Loss: 0.4282\n",
      "Epoch: 2 / 3, Step: 1299 / 2250 Loss: 0.2733\n",
      "Epoch: 2 / 3, Step: 1300 / 2250 Loss: 0.3610\n",
      "Epoch: 2 / 3, Step: 1301 / 2250 Loss: 0.1471\n",
      "Epoch: 2 / 3, Step: 1302 / 2250 Loss: 0.2758\n",
      "Epoch: 2 / 3, Step: 1303 / 2250 Loss: 0.5738\n",
      "Epoch: 2 / 3, Step: 1304 / 2250 Loss: 0.2106\n",
      "Epoch: 2 / 3, Step: 1305 / 2250 Loss: 0.2445\n",
      "Epoch: 2 / 3, Step: 1306 / 2250 Loss: 0.3848\n",
      "Epoch: 2 / 3, Step: 1307 / 2250 Loss: 0.1130\n",
      "Epoch: 2 / 3, Step: 1308 / 2250 Loss: 0.3870\n",
      "Epoch: 2 / 3, Step: 1309 / 2250 Loss: 0.1517\n",
      "Epoch: 2 / 3, Step: 1310 / 2250 Loss: 0.3742\n",
      "Epoch: 2 / 3, Step: 1311 / 2250 Loss: 0.1235\n",
      "Epoch: 2 / 3, Step: 1312 / 2250 Loss: 0.1443\n",
      "Epoch: 2 / 3, Step: 1313 / 2250 Loss: 0.1615\n",
      "Epoch: 2 / 3, Step: 1314 / 2250 Loss: 0.3820\n",
      "Epoch: 2 / 3, Step: 1315 / 2250 Loss: 0.3490\n",
      "Epoch: 2 / 3, Step: 1316 / 2250 Loss: 0.1836\n",
      "Epoch: 2 / 3, Step: 1317 / 2250 Loss: 0.3573\n",
      "Epoch: 2 / 3, Step: 1318 / 2250 Loss: 0.5031\n",
      "Epoch: 2 / 3, Step: 1319 / 2250 Loss: 0.3676\n",
      "Epoch: 2 / 3, Step: 1320 / 2250 Loss: 0.2421\n",
      "Epoch: 2 / 3, Step: 1321 / 2250 Loss: 0.2254\n",
      "Epoch: 2 / 3, Step: 1322 / 2250 Loss: 0.1066\n",
      "Epoch: 2 / 3, Step: 1323 / 2250 Loss: 0.1583\n",
      "Epoch: 2 / 3, Step: 1324 / 2250 Loss: 0.4205\n",
      "Epoch: 2 / 3, Step: 1325 / 2250 Loss: 0.2274\n",
      "Epoch: 2 / 3, Step: 1326 / 2250 Loss: 0.2093\n",
      "Epoch: 2 / 3, Step: 1327 / 2250 Loss: 0.2854\n",
      "Epoch: 2 / 3, Step: 1328 / 2250 Loss: 0.1426\n",
      "Epoch: 2 / 3, Step: 1329 / 2250 Loss: 0.1537\n",
      "Epoch: 2 / 3, Step: 1330 / 2250 Loss: 0.2814\n",
      "Epoch: 2 / 3, Step: 1331 / 2250 Loss: 0.3615\n",
      "Epoch: 2 / 3, Step: 1332 / 2250 Loss: 0.1860\n",
      "Epoch: 2 / 3, Step: 1333 / 2250 Loss: 0.3106\n",
      "Epoch: 2 / 3, Step: 1334 / 2250 Loss: 0.1909\n",
      "Epoch: 2 / 3, Step: 1335 / 2250 Loss: 0.1612\n",
      "Epoch: 2 / 3, Step: 1336 / 2250 Loss: 0.4693\n",
      "Epoch: 2 / 3, Step: 1337 / 2250 Loss: 0.5544\n",
      "Epoch: 2 / 3, Step: 1338 / 2250 Loss: 0.4128\n",
      "Epoch: 2 / 3, Step: 1339 / 2250 Loss: 0.2092\n",
      "Epoch: 2 / 3, Step: 1340 / 2250 Loss: 0.1384\n",
      "Epoch: 2 / 3, Step: 1341 / 2250 Loss: 0.3637\n",
      "Epoch: 2 / 3, Step: 1342 / 2250 Loss: 0.2172\n",
      "Epoch: 2 / 3, Step: 1343 / 2250 Loss: 0.3127\n",
      "Epoch: 2 / 3, Step: 1344 / 2250 Loss: 0.5435\n",
      "Epoch: 2 / 3, Step: 1345 / 2250 Loss: 0.5895\n",
      "Epoch: 2 / 3, Step: 1346 / 2250 Loss: 0.3108\n",
      "Epoch: 2 / 3, Step: 1347 / 2250 Loss: 0.3535\n",
      "Epoch: 2 / 3, Step: 1348 / 2250 Loss: 0.2194\n",
      "Epoch: 2 / 3, Step: 1349 / 2250 Loss: 0.2607\n",
      "Epoch: 2 / 3, Step: 1350 / 2250 Loss: 0.3356\n",
      "Epoch: 2 / 3, Step: 1351 / 2250 Loss: 0.3535\n",
      "Epoch: 2 / 3, Step: 1352 / 2250 Loss: 0.0677\n",
      "Epoch: 2 / 3, Step: 1353 / 2250 Loss: 0.1268\n",
      "Epoch: 2 / 3, Step: 1354 / 2250 Loss: 0.4716\n",
      "Epoch: 2 / 3, Step: 1355 / 2250 Loss: 0.1526\n",
      "Epoch: 2 / 3, Step: 1356 / 2250 Loss: 0.4306\n",
      "Epoch: 2 / 3, Step: 1357 / 2250 Loss: 0.2728\n",
      "Epoch: 2 / 3, Step: 1358 / 2250 Loss: 0.3153\n",
      "Epoch: 2 / 3, Step: 1359 / 2250 Loss: 0.2242\n",
      "Epoch: 2 / 3, Step: 1360 / 2250 Loss: 0.3282\n",
      "Epoch: 2 / 3, Step: 1361 / 2250 Loss: 0.2844\n",
      "Epoch: 2 / 3, Step: 1362 / 2250 Loss: 0.4182\n",
      "Epoch: 2 / 3, Step: 1363 / 2250 Loss: 0.0898\n",
      "Epoch: 2 / 3, Step: 1364 / 2250 Loss: 0.2426\n",
      "Epoch: 2 / 3, Step: 1365 / 2250 Loss: 0.2830\n",
      "Epoch: 2 / 3, Step: 1366 / 2250 Loss: 0.0970\n",
      "Epoch: 2 / 3, Step: 1367 / 2250 Loss: 0.2362\n",
      "Epoch: 2 / 3, Step: 1368 / 2250 Loss: 0.4155\n",
      "Epoch: 2 / 3, Step: 1369 / 2250 Loss: 0.1875\n",
      "Epoch: 2 / 3, Step: 1370 / 2250 Loss: 0.3906\n",
      "Epoch: 2 / 3, Step: 1371 / 2250 Loss: 0.1971\n",
      "Epoch: 2 / 3, Step: 1372 / 2250 Loss: 0.2579\n",
      "Epoch: 2 / 3, Step: 1373 / 2250 Loss: 0.3256\n",
      "Epoch: 2 / 3, Step: 1374 / 2250 Loss: 0.2517\n",
      "Epoch: 2 / 3, Step: 1375 / 2250 Loss: 0.1781\n",
      "Epoch: 2 / 3, Step: 1376 / 2250 Loss: 0.1304\n",
      "Epoch: 2 / 3, Step: 1377 / 2250 Loss: 0.1817\n",
      "Epoch: 2 / 3, Step: 1378 / 2250 Loss: 0.0972\n",
      "Epoch: 2 / 3, Step: 1379 / 2250 Loss: 0.3824\n",
      "Epoch: 2 / 3, Step: 1380 / 2250 Loss: 0.3950\n",
      "Epoch: 2 / 3, Step: 1381 / 2250 Loss: 0.3480\n",
      "Epoch: 2 / 3, Step: 1382 / 2250 Loss: 0.7310\n",
      "Epoch: 2 / 3, Step: 1383 / 2250 Loss: 0.5117\n",
      "Epoch: 2 / 3, Step: 1384 / 2250 Loss: 0.1020\n",
      "Epoch: 2 / 3, Step: 1385 / 2250 Loss: 0.2363\n",
      "Epoch: 2 / 3, Step: 1386 / 2250 Loss: 0.1214\n",
      "Epoch: 2 / 3, Step: 1387 / 2250 Loss: 0.3013\n",
      "Epoch: 2 / 3, Step: 1388 / 2250 Loss: 0.3924\n",
      "Epoch: 2 / 3, Step: 1389 / 2250 Loss: 0.3452\n",
      "Epoch: 2 / 3, Step: 1390 / 2250 Loss: 0.1664\n",
      "Epoch: 2 / 3, Step: 1391 / 2250 Loss: 0.1722\n",
      "Epoch: 2 / 3, Step: 1392 / 2250 Loss: 0.4647\n",
      "Epoch: 2 / 3, Step: 1393 / 2250 Loss: 0.4027\n",
      "Epoch: 2 / 3, Step: 1394 / 2250 Loss: 0.2964\n",
      "Epoch: 2 / 3, Step: 1395 / 2250 Loss: 0.3808\n",
      "Epoch: 2 / 3, Step: 1396 / 2250 Loss: 0.1420\n",
      "Epoch: 2 / 3, Step: 1397 / 2250 Loss: 0.6538\n",
      "Epoch: 2 / 3, Step: 1398 / 2250 Loss: 0.1554\n",
      "Epoch: 2 / 3, Step: 1399 / 2250 Loss: 0.2697\n",
      "Epoch: 2 / 3, Step: 1400 / 2250 Loss: 0.4604\n",
      "Epoch: 2 / 3, Step: 1401 / 2250 Loss: 0.1335\n",
      "Epoch: 2 / 3, Step: 1402 / 2250 Loss: 0.5046\n",
      "Epoch: 2 / 3, Step: 1403 / 2250 Loss: 0.3644\n",
      "Epoch: 2 / 3, Step: 1404 / 2250 Loss: 0.3233\n",
      "Epoch: 2 / 3, Step: 1405 / 2250 Loss: 0.1274\n",
      "Epoch: 2 / 3, Step: 1406 / 2250 Loss: 0.5528\n",
      "Epoch: 2 / 3, Step: 1407 / 2250 Loss: 0.1037\n",
      "Epoch: 2 / 3, Step: 1408 / 2250 Loss: 0.3158\n",
      "Epoch: 2 / 3, Step: 1409 / 2250 Loss: 0.3468\n",
      "Epoch: 2 / 3, Step: 1410 / 2250 Loss: 0.1830\n",
      "Epoch: 2 / 3, Step: 1411 / 2250 Loss: 0.2443\n",
      "Epoch: 2 / 3, Step: 1412 / 2250 Loss: 0.4925\n",
      "Epoch: 2 / 3, Step: 1413 / 2250 Loss: 0.1904\n",
      "Epoch: 2 / 3, Step: 1414 / 2250 Loss: 0.4206\n",
      "Epoch: 2 / 3, Step: 1415 / 2250 Loss: 0.2406\n",
      "Epoch: 2 / 3, Step: 1416 / 2250 Loss: 0.2485\n",
      "Epoch: 2 / 3, Step: 1417 / 2250 Loss: 0.4298\n",
      "Epoch: 2 / 3, Step: 1418 / 2250 Loss: 0.2246\n",
      "Epoch: 2 / 3, Step: 1419 / 2250 Loss: 0.2518\n",
      "Epoch: 2 / 3, Step: 1420 / 2250 Loss: 0.3041\n",
      "Epoch: 2 / 3, Step: 1421 / 2250 Loss: 0.2857\n",
      "Epoch: 2 / 3, Step: 1422 / 2250 Loss: 0.2198\n",
      "Epoch: 2 / 3, Step: 1423 / 2250 Loss: 0.2179\n",
      "Epoch: 2 / 3, Step: 1424 / 2250 Loss: 0.1725\n",
      "Epoch: 2 / 3, Step: 1425 / 2250 Loss: 0.1615\n",
      "Epoch: 2 / 3, Step: 1426 / 2250 Loss: 0.1537\n",
      "Epoch: 2 / 3, Step: 1427 / 2250 Loss: 0.2581\n",
      "Epoch: 2 / 3, Step: 1428 / 2250 Loss: 0.1956\n",
      "Epoch: 2 / 3, Step: 1429 / 2250 Loss: 0.2952\n",
      "Epoch: 2 / 3, Step: 1430 / 2250 Loss: 0.1291\n",
      "Epoch: 2 / 3, Step: 1431 / 2250 Loss: 0.1716\n",
      "Epoch: 2 / 3, Step: 1432 / 2250 Loss: 0.5532\n",
      "Epoch: 2 / 3, Step: 1433 / 2250 Loss: 0.1325\n",
      "Epoch: 2 / 3, Step: 1434 / 2250 Loss: 0.1229\n",
      "Epoch: 2 / 3, Step: 1435 / 2250 Loss: 0.2815\n",
      "Epoch: 2 / 3, Step: 1436 / 2250 Loss: 0.2305\n",
      "Epoch: 2 / 3, Step: 1437 / 2250 Loss: 0.5088\n",
      "Epoch: 2 / 3, Step: 1438 / 2250 Loss: 0.3421\n",
      "Epoch: 2 / 3, Step: 1439 / 2250 Loss: 0.2451\n",
      "Epoch: 2 / 3, Step: 1440 / 2250 Loss: 0.2156\n",
      "Epoch: 2 / 3, Step: 1441 / 2250 Loss: 0.1499\n",
      "Epoch: 2 / 3, Step: 1442 / 2250 Loss: 0.1203\n",
      "Epoch: 2 / 3, Step: 1443 / 2250 Loss: 0.2334\n",
      "Epoch: 2 / 3, Step: 1444 / 2250 Loss: 0.3291\n",
      "Epoch: 2 / 3, Step: 1445 / 2250 Loss: 0.1841\n",
      "Epoch: 2 / 3, Step: 1446 / 2250 Loss: 0.2901\n",
      "Epoch: 2 / 3, Step: 1447 / 2250 Loss: 0.2462\n",
      "Epoch: 2 / 3, Step: 1448 / 2250 Loss: 0.3646\n",
      "Epoch: 2 / 3, Step: 1449 / 2250 Loss: 0.2655\n",
      "Epoch: 2 / 3, Step: 1450 / 2250 Loss: 0.5686\n",
      "Epoch: 2 / 3, Step: 1451 / 2250 Loss: 0.5164\n",
      "Epoch: 2 / 3, Step: 1452 / 2250 Loss: 0.3584\n",
      "Epoch: 2 / 3, Step: 1453 / 2250 Loss: 0.3608\n",
      "Epoch: 2 / 3, Step: 1454 / 2250 Loss: 0.3234\n",
      "Epoch: 2 / 3, Step: 1455 / 2250 Loss: 0.1206\n",
      "Epoch: 2 / 3, Step: 1456 / 2250 Loss: 0.2123\n",
      "Epoch: 2 / 3, Step: 1457 / 2250 Loss: 0.2060\n",
      "Epoch: 2 / 3, Step: 1458 / 2250 Loss: 0.3333\n",
      "Epoch: 2 / 3, Step: 1459 / 2250 Loss: 0.4859\n",
      "Epoch: 2 / 3, Step: 1460 / 2250 Loss: 0.3541\n",
      "Epoch: 2 / 3, Step: 1461 / 2250 Loss: 0.2487\n",
      "Epoch: 2 / 3, Step: 1462 / 2250 Loss: 0.2981\n",
      "Epoch: 2 / 3, Step: 1463 / 2250 Loss: 0.1100\n",
      "Epoch: 2 / 3, Step: 1464 / 2250 Loss: 0.4474\n",
      "Epoch: 2 / 3, Step: 1465 / 2250 Loss: 0.2163\n",
      "Epoch: 2 / 3, Step: 1466 / 2250 Loss: 0.3432\n",
      "Epoch: 2 / 3, Step: 1467 / 2250 Loss: 0.1342\n",
      "Epoch: 2 / 3, Step: 1468 / 2250 Loss: 0.1904\n",
      "Epoch: 2 / 3, Step: 1469 / 2250 Loss: 0.5825\n",
      "Epoch: 2 / 3, Step: 1470 / 2250 Loss: 0.2080\n",
      "Epoch: 2 / 3, Step: 1471 / 2250 Loss: 0.4022\n",
      "Epoch: 2 / 3, Step: 1472 / 2250 Loss: 0.2675\n",
      "Epoch: 2 / 3, Step: 1473 / 2250 Loss: 0.2750\n",
      "Epoch: 2 / 3, Step: 1474 / 2250 Loss: 0.4067\n",
      "Epoch: 2 / 3, Step: 1475 / 2250 Loss: 0.4419\n",
      "Epoch: 2 / 3, Step: 1476 / 2250 Loss: 0.2736\n",
      "Epoch: 2 / 3, Step: 1477 / 2250 Loss: 0.1179\n",
      "Epoch: 2 / 3, Step: 1478 / 2250 Loss: 0.1611\n",
      "Epoch: 2 / 3, Step: 1479 / 2250 Loss: 0.3859\n",
      "Epoch: 2 / 3, Step: 1480 / 2250 Loss: 0.4340\n",
      "Epoch: 2 / 3, Step: 1481 / 2250 Loss: 0.3270\n",
      "Epoch: 2 / 3, Step: 1482 / 2250 Loss: 0.1258\n",
      "Epoch: 2 / 3, Step: 1483 / 2250 Loss: 0.2893\n",
      "Epoch: 2 / 3, Step: 1484 / 2250 Loss: 0.4022\n",
      "Epoch: 2 / 3, Step: 1485 / 2250 Loss: 0.1242\n",
      "Epoch: 2 / 3, Step: 1486 / 2250 Loss: 0.3923\n",
      "Epoch: 2 / 3, Step: 1487 / 2250 Loss: 0.0707\n",
      "Epoch: 2 / 3, Step: 1488 / 2250 Loss: 0.2714\n",
      "Epoch: 2 / 3, Step: 1489 / 2250 Loss: 0.2110\n",
      "Epoch: 2 / 3, Step: 1490 / 2250 Loss: 0.1961\n",
      "Epoch: 2 / 3, Step: 1491 / 2250 Loss: 0.3557\n",
      "Epoch: 2 / 3, Step: 1492 / 2250 Loss: 0.3194\n",
      "Epoch: 2 / 3, Step: 1493 / 2250 Loss: 0.3765\n",
      "Epoch: 2 / 3, Step: 1494 / 2250 Loss: 0.2047\n",
      "Epoch: 2 / 3, Step: 1495 / 2250 Loss: 0.3108\n",
      "Epoch: 2 / 3, Step: 1496 / 2250 Loss: 0.1998\n",
      "Epoch: 2 / 3, Step: 1497 / 2250 Loss: 0.5434\n",
      "Epoch: 2 / 3, Step: 1498 / 2250 Loss: 0.2111\n",
      "Epoch: 2 / 3, Step: 1499 / 2250 Loss: 0.2011\n",
      "Epoch: 2 / 3, Step: 1500 / 2250 Loss: 0.2352\n",
      "Epoch: 2 / 3, Step: 1501 / 2250 Loss: 0.3080\n",
      "Epoch: 2 / 3, Step: 1502 / 2250 Loss: 0.3490\n",
      "Epoch: 2 / 3, Step: 1503 / 2250 Loss: 0.1464\n",
      "Epoch: 2 / 3, Step: 1504 / 2250 Loss: 0.2144\n",
      "Epoch: 2 / 3, Step: 1505 / 2250 Loss: 0.4429\n",
      "Epoch: 2 / 3, Step: 1506 / 2250 Loss: 0.1315\n",
      "Epoch: 2 / 3, Step: 1507 / 2250 Loss: 0.5361\n",
      "Epoch: 2 / 3, Step: 1508 / 2250 Loss: 0.1999\n",
      "Epoch: 2 / 3, Step: 1509 / 2250 Loss: 0.1917\n",
      "Epoch: 2 / 3, Step: 1510 / 2250 Loss: 0.3602\n",
      "Epoch: 2 / 3, Step: 1511 / 2250 Loss: 0.2500\n",
      "Epoch: 2 / 3, Step: 1512 / 2250 Loss: 0.4622\n",
      "Epoch: 2 / 3, Step: 1513 / 2250 Loss: 0.3105\n",
      "Epoch: 2 / 3, Step: 1514 / 2250 Loss: 0.1957\n",
      "Epoch: 2 / 3, Step: 1515 / 2250 Loss: 0.2569\n",
      "Epoch: 2 / 3, Step: 1516 / 2250 Loss: 0.2244\n",
      "Epoch: 2 / 3, Step: 1517 / 2250 Loss: 0.2178\n",
      "Epoch: 2 / 3, Step: 1518 / 2250 Loss: 0.2771\n",
      "Epoch: 2 / 3, Step: 1519 / 2250 Loss: 0.2555\n",
      "Epoch: 2 / 3, Step: 1520 / 2250 Loss: 0.1780\n",
      "Epoch: 2 / 3, Step: 1521 / 2250 Loss: 0.4733\n",
      "Epoch: 2 / 3, Step: 1522 / 2250 Loss: 0.0707\n",
      "Epoch: 2 / 3, Step: 1523 / 2250 Loss: 0.3290\n",
      "Epoch: 2 / 3, Step: 1524 / 2250 Loss: 0.1432\n",
      "Epoch: 2 / 3, Step: 1525 / 2250 Loss: 0.4014\n",
      "Epoch: 2 / 3, Step: 1526 / 2250 Loss: 0.4373\n",
      "Epoch: 2 / 3, Step: 1527 / 2250 Loss: 0.4021\n",
      "Epoch: 2 / 3, Step: 1528 / 2250 Loss: 0.2771\n",
      "Epoch: 2 / 3, Step: 1529 / 2250 Loss: 0.1531\n",
      "Epoch: 2 / 3, Step: 1530 / 2250 Loss: 0.4043\n",
      "Epoch: 2 / 3, Step: 1531 / 2250 Loss: 0.3123\n",
      "Epoch: 2 / 3, Step: 1532 / 2250 Loss: 0.1288\n",
      "Epoch: 2 / 3, Step: 1533 / 2250 Loss: 0.2044\n",
      "Epoch: 2 / 3, Step: 1534 / 2250 Loss: 0.3004\n",
      "Epoch: 2 / 3, Step: 1535 / 2250 Loss: 0.1788\n",
      "Epoch: 2 / 3, Step: 1536 / 2250 Loss: 0.1250\n",
      "Epoch: 2 / 3, Step: 1537 / 2250 Loss: 0.3469\n",
      "Epoch: 2 / 3, Step: 1538 / 2250 Loss: 0.3719\n",
      "Epoch: 2 / 3, Step: 1539 / 2250 Loss: 0.4748\n",
      "Epoch: 2 / 3, Step: 1540 / 2250 Loss: 0.3087\n",
      "Epoch: 2 / 3, Step: 1541 / 2250 Loss: 0.3751\n",
      "Epoch: 2 / 3, Step: 1542 / 2250 Loss: 0.2513\n",
      "Epoch: 2 / 3, Step: 1543 / 2250 Loss: 0.2313\n",
      "Epoch: 2 / 3, Step: 1544 / 2250 Loss: 0.4255\n",
      "Epoch: 2 / 3, Step: 1545 / 2250 Loss: 0.2009\n",
      "Epoch: 2 / 3, Step: 1546 / 2250 Loss: 0.2683\n",
      "Epoch: 2 / 3, Step: 1547 / 2250 Loss: 0.4525\n",
      "Epoch: 2 / 3, Step: 1548 / 2250 Loss: 0.1362\n",
      "Epoch: 2 / 3, Step: 1549 / 2250 Loss: 0.1077\n",
      "Epoch: 2 / 3, Step: 1550 / 2250 Loss: 0.3240\n",
      "Epoch: 2 / 3, Step: 1551 / 2250 Loss: 0.3478\n",
      "Epoch: 2 / 3, Step: 1552 / 2250 Loss: 0.1926\n",
      "Epoch: 2 / 3, Step: 1553 / 2250 Loss: 0.1771\n",
      "Epoch: 2 / 3, Step: 1554 / 2250 Loss: 0.4156\n",
      "Epoch: 2 / 3, Step: 1555 / 2250 Loss: 0.2781\n",
      "Epoch: 2 / 3, Step: 1556 / 2250 Loss: 0.2297\n",
      "Epoch: 2 / 3, Step: 1557 / 2250 Loss: 0.1275\n",
      "Epoch: 2 / 3, Step: 1558 / 2250 Loss: 0.4600\n",
      "Epoch: 2 / 3, Step: 1559 / 2250 Loss: 0.3443\n",
      "Epoch: 2 / 3, Step: 1560 / 2250 Loss: 0.2033\n",
      "Epoch: 2 / 3, Step: 1561 / 2250 Loss: 0.1695\n",
      "Epoch: 2 / 3, Step: 1562 / 2250 Loss: 0.2656\n",
      "Epoch: 2 / 3, Step: 1563 / 2250 Loss: 0.2611\n",
      "Epoch: 2 / 3, Step: 1564 / 2250 Loss: 0.3720\n",
      "Epoch: 2 / 3, Step: 1565 / 2250 Loss: 0.1802\n",
      "Epoch: 2 / 3, Step: 1566 / 2250 Loss: 0.4303\n",
      "Epoch: 2 / 3, Step: 1567 / 2250 Loss: 0.1609\n",
      "Epoch: 2 / 3, Step: 1568 / 2250 Loss: 0.4955\n",
      "Epoch: 2 / 3, Step: 1569 / 2250 Loss: 0.1289\n",
      "Epoch: 2 / 3, Step: 1570 / 2250 Loss: 0.1415\n",
      "Epoch: 2 / 3, Step: 1571 / 2250 Loss: 0.2414\n",
      "Epoch: 2 / 3, Step: 1572 / 2250 Loss: 0.3153\n",
      "Epoch: 2 / 3, Step: 1573 / 2250 Loss: 0.2331\n",
      "Epoch: 2 / 3, Step: 1574 / 2250 Loss: 0.3437\n",
      "Epoch: 2 / 3, Step: 1575 / 2250 Loss: 0.3662\n",
      "Epoch: 2 / 3, Step: 1576 / 2250 Loss: 0.1504\n",
      "Epoch: 2 / 3, Step: 1577 / 2250 Loss: 0.2764\n",
      "Epoch: 2 / 3, Step: 1578 / 2250 Loss: 0.2052\n",
      "Epoch: 2 / 3, Step: 1579 / 2250 Loss: 0.2746\n",
      "Epoch: 2 / 3, Step: 1580 / 2250 Loss: 0.3692\n",
      "Epoch: 2 / 3, Step: 1581 / 2250 Loss: 0.4840\n",
      "Epoch: 2 / 3, Step: 1582 / 2250 Loss: 0.2256\n",
      "Epoch: 2 / 3, Step: 1583 / 2250 Loss: 0.3236\n",
      "Epoch: 2 / 3, Step: 1584 / 2250 Loss: 0.3151\n",
      "Epoch: 2 / 3, Step: 1585 / 2250 Loss: 0.3623\n",
      "Epoch: 2 / 3, Step: 1586 / 2250 Loss: 0.3420\n",
      "Epoch: 2 / 3, Step: 1587 / 2250 Loss: 0.1988\n",
      "Epoch: 2 / 3, Step: 1588 / 2250 Loss: 0.1391\n",
      "Epoch: 2 / 3, Step: 1589 / 2250 Loss: 0.4963\n",
      "Epoch: 2 / 3, Step: 1590 / 2250 Loss: 0.3011\n",
      "Epoch: 2 / 3, Step: 1591 / 2250 Loss: 0.6187\n",
      "Epoch: 2 / 3, Step: 1592 / 2250 Loss: 0.3207\n",
      "Epoch: 2 / 3, Step: 1593 / 2250 Loss: 0.1392\n",
      "Epoch: 2 / 3, Step: 1594 / 2250 Loss: 0.2263\n",
      "Epoch: 2 / 3, Step: 1595 / 2250 Loss: 0.2312\n",
      "Epoch: 2 / 3, Step: 1596 / 2250 Loss: 0.2054\n",
      "Epoch: 2 / 3, Step: 1597 / 2250 Loss: 0.1349\n",
      "Epoch: 2 / 3, Step: 1598 / 2250 Loss: 0.1077\n",
      "Epoch: 2 / 3, Step: 1599 / 2250 Loss: 0.4180\n",
      "Epoch: 2 / 3, Step: 1600 / 2250 Loss: 0.1559\n",
      "Epoch: 2 / 3, Step: 1601 / 2250 Loss: 0.1419\n",
      "Epoch: 2 / 3, Step: 1602 / 2250 Loss: 0.1337\n",
      "Epoch: 2 / 3, Step: 1603 / 2250 Loss: 0.2239\n",
      "Epoch: 2 / 3, Step: 1604 / 2250 Loss: 0.3975\n",
      "Epoch: 2 / 3, Step: 1605 / 2250 Loss: 0.1285\n",
      "Epoch: 2 / 3, Step: 1606 / 2250 Loss: 0.2737\n",
      "Epoch: 2 / 3, Step: 1607 / 2250 Loss: 0.1388\n",
      "Epoch: 2 / 3, Step: 1608 / 2250 Loss: 0.4828\n",
      "Epoch: 2 / 3, Step: 1609 / 2250 Loss: 0.1859\n",
      "Epoch: 2 / 3, Step: 1610 / 2250 Loss: 0.2876\n",
      "Epoch: 2 / 3, Step: 1611 / 2250 Loss: 0.2095\n",
      "Epoch: 2 / 3, Step: 1612 / 2250 Loss: 0.1944\n",
      "Epoch: 2 / 3, Step: 1613 / 2250 Loss: 0.2719\n",
      "Epoch: 2 / 3, Step: 1614 / 2250 Loss: 0.4265\n",
      "Epoch: 2 / 3, Step: 1615 / 2250 Loss: 0.1611\n",
      "Epoch: 2 / 3, Step: 1616 / 2250 Loss: 0.2976\n",
      "Epoch: 2 / 3, Step: 1617 / 2250 Loss: 0.5285\n",
      "Epoch: 2 / 3, Step: 1618 / 2250 Loss: 0.1306\n",
      "Epoch: 2 / 3, Step: 1619 / 2250 Loss: 0.2003\n",
      "Epoch: 2 / 3, Step: 1620 / 2250 Loss: 0.2847\n",
      "Epoch: 2 / 3, Step: 1621 / 2250 Loss: 0.3116\n",
      "Epoch: 2 / 3, Step: 1622 / 2250 Loss: 0.2403\n",
      "Epoch: 2 / 3, Step: 1623 / 2250 Loss: 0.2789\n",
      "Epoch: 2 / 3, Step: 1624 / 2250 Loss: 0.5588\n",
      "Epoch: 2 / 3, Step: 1625 / 2250 Loss: 0.4064\n",
      "Epoch: 2 / 3, Step: 1626 / 2250 Loss: 0.1370\n",
      "Epoch: 2 / 3, Step: 1627 / 2250 Loss: 0.4406\n",
      "Epoch: 2 / 3, Step: 1628 / 2250 Loss: 0.1176\n",
      "Epoch: 2 / 3, Step: 1629 / 2250 Loss: 0.3209\n",
      "Epoch: 2 / 3, Step: 1630 / 2250 Loss: 0.3942\n",
      "Epoch: 2 / 3, Step: 1631 / 2250 Loss: 0.2176\n",
      "Epoch: 2 / 3, Step: 1632 / 2250 Loss: 0.2047\n",
      "Epoch: 2 / 3, Step: 1633 / 2250 Loss: 0.1130\n",
      "Epoch: 2 / 3, Step: 1634 / 2250 Loss: 0.2497\n",
      "Epoch: 2 / 3, Step: 1635 / 2250 Loss: 0.5131\n",
      "Epoch: 2 / 3, Step: 1636 / 2250 Loss: 0.3497\n",
      "Epoch: 2 / 3, Step: 1637 / 2250 Loss: 0.2860\n",
      "Epoch: 2 / 3, Step: 1638 / 2250 Loss: 0.1826\n",
      "Epoch: 2 / 3, Step: 1639 / 2250 Loss: 0.1760\n",
      "Epoch: 2 / 3, Step: 1640 / 2250 Loss: 0.1761\n",
      "Epoch: 2 / 3, Step: 1641 / 2250 Loss: 0.3298\n",
      "Epoch: 2 / 3, Step: 1642 / 2250 Loss: 0.3733\n",
      "Epoch: 2 / 3, Step: 1643 / 2250 Loss: 0.2595\n",
      "Epoch: 2 / 3, Step: 1644 / 2250 Loss: 0.1828\n",
      "Epoch: 2 / 3, Step: 1645 / 2250 Loss: 0.4126\n",
      "Epoch: 2 / 3, Step: 1646 / 2250 Loss: 0.3056\n",
      "Epoch: 2 / 3, Step: 1647 / 2250 Loss: 0.1730\n",
      "Epoch: 2 / 3, Step: 1648 / 2250 Loss: 0.3211\n",
      "Epoch: 2 / 3, Step: 1649 / 2250 Loss: 0.1763\n",
      "Epoch: 2 / 3, Step: 1650 / 2250 Loss: 0.7062\n",
      "Epoch: 2 / 3, Step: 1651 / 2250 Loss: 0.1878\n",
      "Epoch: 2 / 3, Step: 1652 / 2250 Loss: 0.3761\n",
      "Epoch: 2 / 3, Step: 1653 / 2250 Loss: 0.4192\n",
      "Epoch: 2 / 3, Step: 1654 / 2250 Loss: 0.3214\n",
      "Epoch: 2 / 3, Step: 1655 / 2250 Loss: 0.2603\n",
      "Epoch: 2 / 3, Step: 1656 / 2250 Loss: 0.4508\n",
      "Epoch: 2 / 3, Step: 1657 / 2250 Loss: 0.4548\n",
      "Epoch: 2 / 3, Step: 1658 / 2250 Loss: 0.3111\n",
      "Epoch: 2 / 3, Step: 1659 / 2250 Loss: 0.4764\n",
      "Epoch: 2 / 3, Step: 1660 / 2250 Loss: 0.5964\n",
      "Epoch: 2 / 3, Step: 1661 / 2250 Loss: 0.3151\n",
      "Epoch: 2 / 3, Step: 1662 / 2250 Loss: 0.3393\n",
      "Epoch: 2 / 3, Step: 1663 / 2250 Loss: 0.2070\n",
      "Epoch: 2 / 3, Step: 1664 / 2250 Loss: 0.2185\n",
      "Epoch: 2 / 3, Step: 1665 / 2250 Loss: 0.3278\n",
      "Epoch: 2 / 3, Step: 1666 / 2250 Loss: 0.1865\n",
      "Epoch: 2 / 3, Step: 1667 / 2250 Loss: 0.1560\n",
      "Epoch: 2 / 3, Step: 1668 / 2250 Loss: 0.5776\n",
      "Epoch: 2 / 3, Step: 1669 / 2250 Loss: 0.3459\n",
      "Epoch: 2 / 3, Step: 1670 / 2250 Loss: 0.2096\n",
      "Epoch: 2 / 3, Step: 1671 / 2250 Loss: 0.4493\n",
      "Epoch: 2 / 3, Step: 1672 / 2250 Loss: 0.2168\n",
      "Epoch: 2 / 3, Step: 1673 / 2250 Loss: 0.1245\n",
      "Epoch: 2 / 3, Step: 1674 / 2250 Loss: 0.3444\n",
      "Epoch: 2 / 3, Step: 1675 / 2250 Loss: 0.2975\n",
      "Epoch: 2 / 3, Step: 1676 / 2250 Loss: 0.2935\n",
      "Epoch: 2 / 3, Step: 1677 / 2250 Loss: 0.4488\n",
      "Epoch: 2 / 3, Step: 1678 / 2250 Loss: 0.2618\n",
      "Epoch: 2 / 3, Step: 1679 / 2250 Loss: 0.6289\n",
      "Epoch: 2 / 3, Step: 1680 / 2250 Loss: 0.3123\n",
      "Epoch: 2 / 3, Step: 1681 / 2250 Loss: 0.3434\n",
      "Epoch: 2 / 3, Step: 1682 / 2250 Loss: 0.5000\n",
      "Epoch: 2 / 3, Step: 1683 / 2250 Loss: 0.1070\n",
      "Epoch: 2 / 3, Step: 1684 / 2250 Loss: 0.6198\n",
      "Epoch: 2 / 3, Step: 1685 / 2250 Loss: 0.2590\n",
      "Epoch: 2 / 3, Step: 1686 / 2250 Loss: 0.1944\n",
      "Epoch: 2 / 3, Step: 1687 / 2250 Loss: 0.1672\n",
      "Epoch: 2 / 3, Step: 1688 / 2250 Loss: 0.4226\n",
      "Epoch: 2 / 3, Step: 1689 / 2250 Loss: 0.2557\n",
      "Epoch: 2 / 3, Step: 1690 / 2250 Loss: 0.3624\n",
      "Epoch: 2 / 3, Step: 1691 / 2250 Loss: 0.3054\n",
      "Epoch: 2 / 3, Step: 1692 / 2250 Loss: 0.2609\n",
      "Epoch: 2 / 3, Step: 1693 / 2250 Loss: 0.2675\n",
      "Epoch: 2 / 3, Step: 1694 / 2250 Loss: 0.1293\n",
      "Epoch: 2 / 3, Step: 1695 / 2250 Loss: 0.3413\n",
      "Epoch: 2 / 3, Step: 1696 / 2250 Loss: 0.2294\n",
      "Epoch: 2 / 3, Step: 1697 / 2250 Loss: 0.2316\n",
      "Epoch: 2 / 3, Step: 1698 / 2250 Loss: 0.2764\n",
      "Epoch: 2 / 3, Step: 1699 / 2250 Loss: 0.2145\n",
      "Epoch: 2 / 3, Step: 1700 / 2250 Loss: 0.2813\n",
      "Epoch: 2 / 3, Step: 1701 / 2250 Loss: 0.2805\n",
      "Epoch: 2 / 3, Step: 1702 / 2250 Loss: 0.2007\n",
      "Epoch: 2 / 3, Step: 1703 / 2250 Loss: 0.1648\n",
      "Epoch: 2 / 3, Step: 1704 / 2250 Loss: 0.1820\n",
      "Epoch: 2 / 3, Step: 1705 / 2250 Loss: 0.4119\n",
      "Epoch: 2 / 3, Step: 1706 / 2250 Loss: 0.3233\n",
      "Epoch: 2 / 3, Step: 1707 / 2250 Loss: 0.3322\n",
      "Epoch: 2 / 3, Step: 1708 / 2250 Loss: 0.2223\n",
      "Epoch: 2 / 3, Step: 1709 / 2250 Loss: 0.1574\n",
      "Epoch: 2 / 3, Step: 1710 / 2250 Loss: 0.1906\n",
      "Epoch: 2 / 3, Step: 1711 / 2250 Loss: 0.5355\n",
      "Epoch: 2 / 3, Step: 1712 / 2250 Loss: 0.2283\n",
      "Epoch: 2 / 3, Step: 1713 / 2250 Loss: 0.2199\n",
      "Epoch: 2 / 3, Step: 1714 / 2250 Loss: 0.4242\n",
      "Epoch: 2 / 3, Step: 1715 / 2250 Loss: 0.1378\n",
      "Epoch: 2 / 3, Step: 1716 / 2250 Loss: 0.2835\n",
      "Epoch: 2 / 3, Step: 1717 / 2250 Loss: 0.3048\n",
      "Epoch: 2 / 3, Step: 1718 / 2250 Loss: 0.2489\n",
      "Epoch: 2 / 3, Step: 1719 / 2250 Loss: 0.1727\n",
      "Epoch: 2 / 3, Step: 1720 / 2250 Loss: 0.1274\n",
      "Epoch: 2 / 3, Step: 1721 / 2250 Loss: 0.2145\n",
      "Epoch: 2 / 3, Step: 1722 / 2250 Loss: 0.2735\n",
      "Epoch: 2 / 3, Step: 1723 / 2250 Loss: 0.4593\n",
      "Epoch: 2 / 3, Step: 1724 / 2250 Loss: 0.4515\n",
      "Epoch: 2 / 3, Step: 1725 / 2250 Loss: 0.2225\n",
      "Epoch: 2 / 3, Step: 1726 / 2250 Loss: 0.1985\n",
      "Epoch: 2 / 3, Step: 1727 / 2250 Loss: 0.1687\n",
      "Epoch: 2 / 3, Step: 1728 / 2250 Loss: 0.2743\n",
      "Epoch: 2 / 3, Step: 1729 / 2250 Loss: 0.3824\n",
      "Epoch: 2 / 3, Step: 1730 / 2250 Loss: 0.2959\n",
      "Epoch: 2 / 3, Step: 1731 / 2250 Loss: 0.2631\n",
      "Epoch: 2 / 3, Step: 1732 / 2250 Loss: 0.1987\n",
      "Epoch: 2 / 3, Step: 1733 / 2250 Loss: 0.1555\n",
      "Epoch: 2 / 3, Step: 1734 / 2250 Loss: 0.3042\n",
      "Epoch: 2 / 3, Step: 1735 / 2250 Loss: 0.3145\n",
      "Epoch: 2 / 3, Step: 1736 / 2250 Loss: 0.1993\n",
      "Epoch: 2 / 3, Step: 1737 / 2250 Loss: 0.2665\n",
      "Epoch: 2 / 3, Step: 1738 / 2250 Loss: 0.0904\n",
      "Epoch: 2 / 3, Step: 1739 / 2250 Loss: 0.3117\n",
      "Epoch: 2 / 3, Step: 1740 / 2250 Loss: 0.3600\n",
      "Epoch: 2 / 3, Step: 1741 / 2250 Loss: 0.3929\n",
      "Epoch: 2 / 3, Step: 1742 / 2250 Loss: 0.3643\n",
      "Epoch: 2 / 3, Step: 1743 / 2250 Loss: 0.3711\n",
      "Epoch: 2 / 3, Step: 1744 / 2250 Loss: 0.3525\n",
      "Epoch: 2 / 3, Step: 1745 / 2250 Loss: 0.3536\n",
      "Epoch: 2 / 3, Step: 1746 / 2250 Loss: 0.4704\n",
      "Epoch: 2 / 3, Step: 1747 / 2250 Loss: 0.1545\n",
      "Epoch: 2 / 3, Step: 1748 / 2250 Loss: 0.1860\n",
      "Epoch: 2 / 3, Step: 1749 / 2250 Loss: 0.3056\n",
      "Epoch: 2 / 3, Step: 1750 / 2250 Loss: 0.2591\n",
      "Epoch: 2 / 3, Step: 1751 / 2250 Loss: 0.2695\n",
      "Epoch: 2 / 3, Step: 1752 / 2250 Loss: 0.3913\n",
      "Epoch: 2 / 3, Step: 1753 / 2250 Loss: 0.4605\n",
      "Epoch: 2 / 3, Step: 1754 / 2250 Loss: 0.2552\n",
      "Epoch: 2 / 3, Step: 1755 / 2250 Loss: 0.5387\n",
      "Epoch: 2 / 3, Step: 1756 / 2250 Loss: 0.3554\n",
      "Epoch: 2 / 3, Step: 1757 / 2250 Loss: 0.1100\n",
      "Epoch: 2 / 3, Step: 1758 / 2250 Loss: 0.3248\n",
      "Epoch: 2 / 3, Step: 1759 / 2250 Loss: 0.3735\n",
      "Epoch: 2 / 3, Step: 1760 / 2250 Loss: 0.1914\n",
      "Epoch: 2 / 3, Step: 1761 / 2250 Loss: 0.3308\n",
      "Epoch: 2 / 3, Step: 1762 / 2250 Loss: 0.2103\n",
      "Epoch: 2 / 3, Step: 1763 / 2250 Loss: 0.3350\n",
      "Epoch: 2 / 3, Step: 1764 / 2250 Loss: 0.1640\n",
      "Epoch: 2 / 3, Step: 1765 / 2250 Loss: 0.2421\n",
      "Epoch: 2 / 3, Step: 1766 / 2250 Loss: 0.0942\n",
      "Epoch: 2 / 3, Step: 1767 / 2250 Loss: 0.1563\n",
      "Epoch: 2 / 3, Step: 1768 / 2250 Loss: 0.1846\n",
      "Epoch: 2 / 3, Step: 1769 / 2250 Loss: 0.1471\n",
      "Epoch: 2 / 3, Step: 1770 / 2250 Loss: 0.4948\n",
      "Epoch: 2 / 3, Step: 1771 / 2250 Loss: 0.1396\n",
      "Epoch: 2 / 3, Step: 1772 / 2250 Loss: 0.2867\n",
      "Epoch: 2 / 3, Step: 1773 / 2250 Loss: 0.2237\n",
      "Epoch: 2 / 3, Step: 1774 / 2250 Loss: 0.3426\n",
      "Epoch: 2 / 3, Step: 1775 / 2250 Loss: 0.2593\n",
      "Epoch: 2 / 3, Step: 1776 / 2250 Loss: 0.2730\n",
      "Epoch: 2 / 3, Step: 1777 / 2250 Loss: 0.3347\n",
      "Epoch: 2 / 3, Step: 1778 / 2250 Loss: 0.2050\n",
      "Epoch: 2 / 3, Step: 1779 / 2250 Loss: 0.2119\n",
      "Epoch: 2 / 3, Step: 1780 / 2250 Loss: 0.1771\n",
      "Epoch: 2 / 3, Step: 1781 / 2250 Loss: 0.4415\n",
      "Epoch: 2 / 3, Step: 1782 / 2250 Loss: 0.1746\n",
      "Epoch: 2 / 3, Step: 1783 / 2250 Loss: 0.2592\n",
      "Epoch: 2 / 3, Step: 1784 / 2250 Loss: 0.1332\n",
      "Epoch: 2 / 3, Step: 1785 / 2250 Loss: 0.3045\n",
      "Epoch: 2 / 3, Step: 1786 / 2250 Loss: 0.2155\n",
      "Epoch: 2 / 3, Step: 1787 / 2250 Loss: 0.1654\n",
      "Epoch: 2 / 3, Step: 1788 / 2250 Loss: 0.5968\n",
      "Epoch: 2 / 3, Step: 1789 / 2250 Loss: 0.1154\n",
      "Epoch: 2 / 3, Step: 1790 / 2250 Loss: 0.4040\n",
      "Epoch: 2 / 3, Step: 1791 / 2250 Loss: 0.3308\n",
      "Epoch: 2 / 3, Step: 1792 / 2250 Loss: 0.3923\n",
      "Epoch: 2 / 3, Step: 1793 / 2250 Loss: 0.2085\n",
      "Epoch: 2 / 3, Step: 1794 / 2250 Loss: 0.3261\n",
      "Epoch: 2 / 3, Step: 1795 / 2250 Loss: 0.3826\n",
      "Epoch: 2 / 3, Step: 1796 / 2250 Loss: 0.2830\n",
      "Epoch: 2 / 3, Step: 1797 / 2250 Loss: 0.2305\n",
      "Epoch: 2 / 3, Step: 1798 / 2250 Loss: 0.1902\n",
      "Epoch: 2 / 3, Step: 1799 / 2250 Loss: 0.1227\n",
      "Epoch: 2 / 3, Step: 1800 / 2250 Loss: 0.1184\n",
      "Epoch: 2 / 3, Step: 1801 / 2250 Loss: 0.2134\n",
      "Epoch: 2 / 3, Step: 1802 / 2250 Loss: 0.4588\n",
      "Epoch: 2 / 3, Step: 1803 / 2250 Loss: 0.7848\n",
      "Epoch: 2 / 3, Step: 1804 / 2250 Loss: 0.1836\n",
      "Epoch: 2 / 3, Step: 1805 / 2250 Loss: 0.1376\n",
      "Epoch: 2 / 3, Step: 1806 / 2250 Loss: 0.1770\n",
      "Epoch: 2 / 3, Step: 1807 / 2250 Loss: 0.2364\n",
      "Epoch: 2 / 3, Step: 1808 / 2250 Loss: 0.4078\n",
      "Epoch: 2 / 3, Step: 1809 / 2250 Loss: 0.1738\n",
      "Epoch: 2 / 3, Step: 1810 / 2250 Loss: 0.3364\n",
      "Epoch: 2 / 3, Step: 1811 / 2250 Loss: 0.3441\n",
      "Epoch: 2 / 3, Step: 1812 / 2250 Loss: 0.2653\n",
      "Epoch: 2 / 3, Step: 1813 / 2250 Loss: 0.1371\n",
      "Epoch: 2 / 3, Step: 1814 / 2250 Loss: 0.2867\n",
      "Epoch: 2 / 3, Step: 1815 / 2250 Loss: 0.3262\n",
      "Epoch: 2 / 3, Step: 1816 / 2250 Loss: 0.4184\n",
      "Epoch: 2 / 3, Step: 1817 / 2250 Loss: 0.5943\n",
      "Epoch: 2 / 3, Step: 1818 / 2250 Loss: 0.3100\n",
      "Epoch: 2 / 3, Step: 1819 / 2250 Loss: 0.2835\n",
      "Epoch: 2 / 3, Step: 1820 / 2250 Loss: 0.2531\n",
      "Epoch: 2 / 3, Step: 1821 / 2250 Loss: 0.3425\n",
      "Epoch: 2 / 3, Step: 1822 / 2250 Loss: 0.3514\n",
      "Epoch: 2 / 3, Step: 1823 / 2250 Loss: 0.3573\n",
      "Epoch: 2 / 3, Step: 1824 / 2250 Loss: 0.1825\n",
      "Epoch: 2 / 3, Step: 1825 / 2250 Loss: 0.2427\n",
      "Epoch: 2 / 3, Step: 1826 / 2250 Loss: 0.2253\n",
      "Epoch: 2 / 3, Step: 1827 / 2250 Loss: 0.2003\n",
      "Epoch: 2 / 3, Step: 1828 / 2250 Loss: 0.2856\n",
      "Epoch: 2 / 3, Step: 1829 / 2250 Loss: 0.3974\n",
      "Epoch: 2 / 3, Step: 1830 / 2250 Loss: 0.2501\n",
      "Epoch: 2 / 3, Step: 1831 / 2250 Loss: 0.1384\n",
      "Epoch: 2 / 3, Step: 1832 / 2250 Loss: 0.2796\n",
      "Epoch: 2 / 3, Step: 1833 / 2250 Loss: 0.4087\n",
      "Epoch: 2 / 3, Step: 1834 / 2250 Loss: 0.2230\n",
      "Epoch: 2 / 3, Step: 1835 / 2250 Loss: 0.3741\n",
      "Epoch: 2 / 3, Step: 1836 / 2250 Loss: 0.2192\n",
      "Epoch: 2 / 3, Step: 1837 / 2250 Loss: 0.2362\n",
      "Epoch: 2 / 3, Step: 1838 / 2250 Loss: 0.1697\n",
      "Epoch: 2 / 3, Step: 1839 / 2250 Loss: 0.1698\n",
      "Epoch: 2 / 3, Step: 1840 / 2250 Loss: 0.3228\n",
      "Epoch: 2 / 3, Step: 1841 / 2250 Loss: 0.2330\n",
      "Epoch: 2 / 3, Step: 1842 / 2250 Loss: 0.3198\n",
      "Epoch: 2 / 3, Step: 1843 / 2250 Loss: 0.3545\n",
      "Epoch: 2 / 3, Step: 1844 / 2250 Loss: 0.3690\n",
      "Epoch: 2 / 3, Step: 1845 / 2250 Loss: 0.1627\n",
      "Epoch: 2 / 3, Step: 1846 / 2250 Loss: 0.3534\n",
      "Epoch: 2 / 3, Step: 1847 / 2250 Loss: 0.3040\n",
      "Epoch: 2 / 3, Step: 1848 / 2250 Loss: 0.1496\n",
      "Epoch: 2 / 3, Step: 1849 / 2250 Loss: 0.1123\n",
      "Epoch: 2 / 3, Step: 1850 / 2250 Loss: 0.2383\n",
      "Epoch: 2 / 3, Step: 1851 / 2250 Loss: 0.2003\n",
      "Epoch: 2 / 3, Step: 1852 / 2250 Loss: 0.3145\n",
      "Epoch: 2 / 3, Step: 1853 / 2250 Loss: 0.2217\n",
      "Epoch: 2 / 3, Step: 1854 / 2250 Loss: 0.1832\n",
      "Epoch: 2 / 3, Step: 1855 / 2250 Loss: 0.1929\n",
      "Epoch: 2 / 3, Step: 1856 / 2250 Loss: 0.3138\n",
      "Epoch: 2 / 3, Step: 1857 / 2250 Loss: 0.1511\n",
      "Epoch: 2 / 3, Step: 1858 / 2250 Loss: 0.1482\n",
      "Epoch: 2 / 3, Step: 1859 / 2250 Loss: 0.4571\n",
      "Epoch: 2 / 3, Step: 1860 / 2250 Loss: 0.3037\n",
      "Epoch: 2 / 3, Step: 1861 / 2250 Loss: 0.1784\n",
      "Epoch: 2 / 3, Step: 1862 / 2250 Loss: 0.3042\n",
      "Epoch: 2 / 3, Step: 1863 / 2250 Loss: 0.1979\n",
      "Epoch: 2 / 3, Step: 1864 / 2250 Loss: 0.3348\n",
      "Epoch: 2 / 3, Step: 1865 / 2250 Loss: 0.4084\n",
      "Epoch: 2 / 3, Step: 1866 / 2250 Loss: 0.2781\n",
      "Epoch: 2 / 3, Step: 1867 / 2250 Loss: 0.2746\n",
      "Epoch: 2 / 3, Step: 1868 / 2250 Loss: 0.3903\n",
      "Epoch: 2 / 3, Step: 1869 / 2250 Loss: 0.1636\n",
      "Epoch: 2 / 3, Step: 1870 / 2250 Loss: 0.3664\n",
      "Epoch: 2 / 3, Step: 1871 / 2250 Loss: 0.2327\n",
      "Epoch: 2 / 3, Step: 1872 / 2250 Loss: 0.5151\n",
      "Epoch: 2 / 3, Step: 1873 / 2250 Loss: 0.1391\n",
      "Epoch: 2 / 3, Step: 1874 / 2250 Loss: 0.2600\n",
      "Epoch: 2 / 3, Step: 1875 / 2250 Loss: 0.2062\n",
      "Epoch: 2 / 3, Step: 1876 / 2250 Loss: 0.1471\n",
      "Epoch: 2 / 3, Step: 1877 / 2250 Loss: 0.3346\n",
      "Epoch: 2 / 3, Step: 1878 / 2250 Loss: 0.4844\n",
      "Epoch: 2 / 3, Step: 1879 / 2250 Loss: 0.2272\n",
      "Epoch: 2 / 3, Step: 1880 / 2250 Loss: 0.4530\n",
      "Epoch: 2 / 3, Step: 1881 / 2250 Loss: 0.4006\n",
      "Epoch: 2 / 3, Step: 1882 / 2250 Loss: 0.4916\n",
      "Epoch: 2 / 3, Step: 1883 / 2250 Loss: 0.1749\n",
      "Epoch: 2 / 3, Step: 1884 / 2250 Loss: 0.1204\n",
      "Epoch: 2 / 3, Step: 1885 / 2250 Loss: 0.1008\n",
      "Epoch: 2 / 3, Step: 1886 / 2250 Loss: 0.3786\n",
      "Epoch: 2 / 3, Step: 1887 / 2250 Loss: 0.2311\n",
      "Epoch: 2 / 3, Step: 1888 / 2250 Loss: 0.3261\n",
      "Epoch: 2 / 3, Step: 1889 / 2250 Loss: 0.5172\n",
      "Epoch: 2 / 3, Step: 1890 / 2250 Loss: 0.1290\n",
      "Epoch: 2 / 3, Step: 1891 / 2250 Loss: 0.3077\n",
      "Epoch: 2 / 3, Step: 1892 / 2250 Loss: 0.3232\n",
      "Epoch: 2 / 3, Step: 1893 / 2250 Loss: 0.4563\n",
      "Epoch: 2 / 3, Step: 1894 / 2250 Loss: 0.2305\n",
      "Epoch: 2 / 3, Step: 1895 / 2250 Loss: 0.0813\n",
      "Epoch: 2 / 3, Step: 1896 / 2250 Loss: 0.0961\n",
      "Epoch: 2 / 3, Step: 1897 / 2250 Loss: 0.1854\n",
      "Epoch: 2 / 3, Step: 1898 / 2250 Loss: 0.2144\n",
      "Epoch: 2 / 3, Step: 1899 / 2250 Loss: 0.4104\n",
      "Epoch: 2 / 3, Step: 1900 / 2250 Loss: 0.4701\n",
      "Epoch: 2 / 3, Step: 1901 / 2250 Loss: 0.1134\n",
      "Epoch: 2 / 3, Step: 1902 / 2250 Loss: 0.2269\n",
      "Epoch: 2 / 3, Step: 1903 / 2250 Loss: 0.2599\n",
      "Epoch: 2 / 3, Step: 1904 / 2250 Loss: 0.1415\n",
      "Epoch: 2 / 3, Step: 1905 / 2250 Loss: 0.3563\n",
      "Epoch: 2 / 3, Step: 1906 / 2250 Loss: 0.1616\n",
      "Epoch: 2 / 3, Step: 1907 / 2250 Loss: 0.1072\n",
      "Epoch: 2 / 3, Step: 1908 / 2250 Loss: 0.2196\n",
      "Epoch: 2 / 3, Step: 1909 / 2250 Loss: 0.3050\n",
      "Epoch: 2 / 3, Step: 1910 / 2250 Loss: 0.5078\n",
      "Epoch: 2 / 3, Step: 1911 / 2250 Loss: 0.2164\n",
      "Epoch: 2 / 3, Step: 1912 / 2250 Loss: 0.2781\n",
      "Epoch: 2 / 3, Step: 1913 / 2250 Loss: 0.4688\n",
      "Epoch: 2 / 3, Step: 1914 / 2250 Loss: 0.3330\n",
      "Epoch: 2 / 3, Step: 1915 / 2250 Loss: 0.2359\n",
      "Epoch: 2 / 3, Step: 1916 / 2250 Loss: 0.2871\n",
      "Epoch: 2 / 3, Step: 1917 / 2250 Loss: 0.3492\n",
      "Epoch: 2 / 3, Step: 1918 / 2250 Loss: 0.2153\n",
      "Epoch: 2 / 3, Step: 1919 / 2250 Loss: 0.5600\n",
      "Epoch: 2 / 3, Step: 1920 / 2250 Loss: 0.2530\n",
      "Epoch: 2 / 3, Step: 1921 / 2250 Loss: 0.2070\n",
      "Epoch: 2 / 3, Step: 1922 / 2250 Loss: 0.3215\n",
      "Epoch: 2 / 3, Step: 1923 / 2250 Loss: 0.1760\n",
      "Epoch: 2 / 3, Step: 1924 / 2250 Loss: 0.1389\n",
      "Epoch: 2 / 3, Step: 1925 / 2250 Loss: 0.2123\n",
      "Epoch: 2 / 3, Step: 1926 / 2250 Loss: 0.2580\n",
      "Epoch: 2 / 3, Step: 1927 / 2250 Loss: 0.3799\n",
      "Epoch: 2 / 3, Step: 1928 / 2250 Loss: 0.3199\n",
      "Epoch: 2 / 3, Step: 1929 / 2250 Loss: 0.3411\n",
      "Epoch: 2 / 3, Step: 1930 / 2250 Loss: 0.4347\n",
      "Epoch: 2 / 3, Step: 1931 / 2250 Loss: 0.2467\n",
      "Epoch: 2 / 3, Step: 1932 / 2250 Loss: 0.1245\n",
      "Epoch: 2 / 3, Step: 1933 / 2250 Loss: 0.3368\n",
      "Epoch: 2 / 3, Step: 1934 / 2250 Loss: 0.2931\n",
      "Epoch: 2 / 3, Step: 1935 / 2250 Loss: 0.3144\n",
      "Epoch: 2 / 3, Step: 1936 / 2250 Loss: 0.3423\n",
      "Epoch: 2 / 3, Step: 1937 / 2250 Loss: 0.1535\n",
      "Epoch: 2 / 3, Step: 1938 / 2250 Loss: 0.2476\n",
      "Epoch: 2 / 3, Step: 1939 / 2250 Loss: 0.5033\n",
      "Epoch: 2 / 3, Step: 1940 / 2250 Loss: 0.3257\n",
      "Epoch: 2 / 3, Step: 1941 / 2250 Loss: 0.1895\n",
      "Epoch: 2 / 3, Step: 1942 / 2250 Loss: 0.3944\n",
      "Epoch: 2 / 3, Step: 1943 / 2250 Loss: 0.1548\n",
      "Epoch: 2 / 3, Step: 1944 / 2250 Loss: 0.4400\n",
      "Epoch: 2 / 3, Step: 1945 / 2250 Loss: 0.1436\n",
      "Epoch: 2 / 3, Step: 1946 / 2250 Loss: 0.2488\n",
      "Epoch: 2 / 3, Step: 1947 / 2250 Loss: 0.1814\n",
      "Epoch: 2 / 3, Step: 1948 / 2250 Loss: 0.0794\n",
      "Epoch: 2 / 3, Step: 1949 / 2250 Loss: 0.1766\n",
      "Epoch: 2 / 3, Step: 1950 / 2250 Loss: 0.2166\n",
      "Epoch: 2 / 3, Step: 1951 / 2250 Loss: 0.4803\n",
      "Epoch: 2 / 3, Step: 1952 / 2250 Loss: 0.1354\n",
      "Epoch: 2 / 3, Step: 1953 / 2250 Loss: 0.2401\n",
      "Epoch: 2 / 3, Step: 1954 / 2250 Loss: 0.3462\n",
      "Epoch: 2 / 3, Step: 1955 / 2250 Loss: 0.0676\n",
      "Epoch: 2 / 3, Step: 1956 / 2250 Loss: 0.1430\n",
      "Epoch: 2 / 3, Step: 1957 / 2250 Loss: 0.5264\n",
      "Epoch: 2 / 3, Step: 1958 / 2250 Loss: 0.4049\n",
      "Epoch: 2 / 3, Step: 1959 / 2250 Loss: 0.3600\n",
      "Epoch: 2 / 3, Step: 1960 / 2250 Loss: 0.3250\n",
      "Epoch: 2 / 3, Step: 1961 / 2250 Loss: 0.3790\n",
      "Epoch: 2 / 3, Step: 1962 / 2250 Loss: 0.1913\n",
      "Epoch: 2 / 3, Step: 1963 / 2250 Loss: 0.2303\n",
      "Epoch: 2 / 3, Step: 1964 / 2250 Loss: 0.2102\n",
      "Epoch: 2 / 3, Step: 1965 / 2250 Loss: 0.3740\n",
      "Epoch: 2 / 3, Step: 1966 / 2250 Loss: 0.2609\n",
      "Epoch: 2 / 3, Step: 1967 / 2250 Loss: 0.3177\n",
      "Epoch: 2 / 3, Step: 1968 / 2250 Loss: 0.3248\n",
      "Epoch: 2 / 3, Step: 1969 / 2250 Loss: 0.0855\n",
      "Epoch: 2 / 3, Step: 1970 / 2250 Loss: 0.3635\n",
      "Epoch: 2 / 3, Step: 1971 / 2250 Loss: 0.1913\n",
      "Epoch: 2 / 3, Step: 1972 / 2250 Loss: 0.1987\n",
      "Epoch: 2 / 3, Step: 1973 / 2250 Loss: 0.1289\n",
      "Epoch: 2 / 3, Step: 1974 / 2250 Loss: 0.1233\n",
      "Epoch: 2 / 3, Step: 1975 / 2250 Loss: 0.0734\n",
      "Epoch: 2 / 3, Step: 1976 / 2250 Loss: 0.0908\n",
      "Epoch: 2 / 3, Step: 1977 / 2250 Loss: 0.1680\n",
      "Epoch: 2 / 3, Step: 1978 / 2250 Loss: 0.1657\n",
      "Epoch: 2 / 3, Step: 1979 / 2250 Loss: 0.4934\n",
      "Epoch: 2 / 3, Step: 1980 / 2250 Loss: 0.2056\n",
      "Epoch: 2 / 3, Step: 1981 / 2250 Loss: 0.2413\n",
      "Epoch: 2 / 3, Step: 1982 / 2250 Loss: 0.2928\n",
      "Epoch: 2 / 3, Step: 1983 / 2250 Loss: 0.0886\n",
      "Epoch: 2 / 3, Step: 1984 / 2250 Loss: 0.5329\n",
      "Epoch: 2 / 3, Step: 1985 / 2250 Loss: 0.2471\n",
      "Epoch: 2 / 3, Step: 1986 / 2250 Loss: 0.1896\n",
      "Epoch: 2 / 3, Step: 1987 / 2250 Loss: 0.4389\n",
      "Epoch: 2 / 3, Step: 1988 / 2250 Loss: 0.3756\n",
      "Epoch: 2 / 3, Step: 1989 / 2250 Loss: 0.3225\n",
      "Epoch: 2 / 3, Step: 1990 / 2250 Loss: 0.7178\n",
      "Epoch: 2 / 3, Step: 1991 / 2250 Loss: 0.3094\n",
      "Epoch: 2 / 3, Step: 1992 / 2250 Loss: 0.1940\n",
      "Epoch: 2 / 3, Step: 1993 / 2250 Loss: 0.2532\n",
      "Epoch: 2 / 3, Step: 1994 / 2250 Loss: 0.1374\n",
      "Epoch: 2 / 3, Step: 1995 / 2250 Loss: 0.3396\n",
      "Epoch: 2 / 3, Step: 1996 / 2250 Loss: 0.1327\n",
      "Epoch: 2 / 3, Step: 1997 / 2250 Loss: 0.2294\n",
      "Epoch: 2 / 3, Step: 1998 / 2250 Loss: 0.1997\n",
      "Epoch: 2 / 3, Step: 1999 / 2250 Loss: 0.2689\n",
      "Epoch: 2 / 3, Step: 2000 / 2250 Loss: 0.4223\n",
      "Epoch: 2 / 3, Step: 2001 / 2250 Loss: 0.2313\n",
      "Epoch: 2 / 3, Step: 2002 / 2250 Loss: 0.4914\n",
      "Epoch: 2 / 3, Step: 2003 / 2250 Loss: 0.1244\n",
      "Epoch: 2 / 3, Step: 2004 / 2250 Loss: 0.3247\n",
      "Epoch: 2 / 3, Step: 2005 / 2250 Loss: 0.3869\n",
      "Epoch: 2 / 3, Step: 2006 / 2250 Loss: 0.4811\n",
      "Epoch: 2 / 3, Step: 2007 / 2250 Loss: 0.1534\n",
      "Epoch: 2 / 3, Step: 2008 / 2250 Loss: 0.1826\n",
      "Epoch: 2 / 3, Step: 2009 / 2250 Loss: 0.2734\n",
      "Epoch: 2 / 3, Step: 2010 / 2250 Loss: 0.4423\n",
      "Epoch: 2 / 3, Step: 2011 / 2250 Loss: 0.2201\n",
      "Epoch: 2 / 3, Step: 2012 / 2250 Loss: 0.3205\n",
      "Epoch: 2 / 3, Step: 2013 / 2250 Loss: 0.3244\n",
      "Epoch: 2 / 3, Step: 2014 / 2250 Loss: 0.1896\n",
      "Epoch: 2 / 3, Step: 2015 / 2250 Loss: 0.3470\n",
      "Epoch: 2 / 3, Step: 2016 / 2250 Loss: 0.1139\n",
      "Epoch: 2 / 3, Step: 2017 / 2250 Loss: 0.3806\n",
      "Epoch: 2 / 3, Step: 2018 / 2250 Loss: 0.2050\n",
      "Epoch: 2 / 3, Step: 2019 / 2250 Loss: 0.3912\n",
      "Epoch: 2 / 3, Step: 2020 / 2250 Loss: 0.2324\n",
      "Epoch: 2 / 3, Step: 2021 / 2250 Loss: 0.4880\n",
      "Epoch: 2 / 3, Step: 2022 / 2250 Loss: 0.2412\n",
      "Epoch: 2 / 3, Step: 2023 / 2250 Loss: 0.1657\n",
      "Epoch: 2 / 3, Step: 2024 / 2250 Loss: 0.3051\n",
      "Epoch: 2 / 3, Step: 2025 / 2250 Loss: 0.1949\n",
      "Epoch: 2 / 3, Step: 2026 / 2250 Loss: 0.2316\n",
      "Epoch: 2 / 3, Step: 2027 / 2250 Loss: 0.2312\n",
      "Epoch: 2 / 3, Step: 2028 / 2250 Loss: 0.1987\n",
      "Epoch: 2 / 3, Step: 2029 / 2250 Loss: 0.2694\n",
      "Epoch: 2 / 3, Step: 2030 / 2250 Loss: 0.1389\n",
      "Epoch: 2 / 3, Step: 2031 / 2250 Loss: 0.3917\n",
      "Epoch: 2 / 3, Step: 2032 / 2250 Loss: 0.1767\n",
      "Epoch: 2 / 3, Step: 2033 / 2250 Loss: 0.1793\n",
      "Epoch: 2 / 3, Step: 2034 / 2250 Loss: 0.3428\n",
      "Epoch: 2 / 3, Step: 2035 / 2250 Loss: 0.4635\n",
      "Epoch: 2 / 3, Step: 2036 / 2250 Loss: 0.3715\n",
      "Epoch: 2 / 3, Step: 2037 / 2250 Loss: 0.2900\n",
      "Epoch: 2 / 3, Step: 2038 / 2250 Loss: 0.1186\n",
      "Epoch: 2 / 3, Step: 2039 / 2250 Loss: 0.1209\n",
      "Epoch: 2 / 3, Step: 2040 / 2250 Loss: 0.1784\n",
      "Epoch: 2 / 3, Step: 2041 / 2250 Loss: 0.4312\n",
      "Epoch: 2 / 3, Step: 2042 / 2250 Loss: 0.2748\n",
      "Epoch: 2 / 3, Step: 2043 / 2250 Loss: 0.2495\n",
      "Epoch: 2 / 3, Step: 2044 / 2250 Loss: 0.1920\n",
      "Epoch: 2 / 3, Step: 2045 / 2250 Loss: 0.0910\n",
      "Epoch: 2 / 3, Step: 2046 / 2250 Loss: 0.2262\n",
      "Epoch: 2 / 3, Step: 2047 / 2250 Loss: 0.1483\n",
      "Epoch: 2 / 3, Step: 2048 / 2250 Loss: 0.2200\n",
      "Epoch: 2 / 3, Step: 2049 / 2250 Loss: 0.4082\n",
      "Epoch: 2 / 3, Step: 2050 / 2250 Loss: 0.1896\n",
      "Epoch: 2 / 3, Step: 2051 / 2250 Loss: 0.4128\n",
      "Epoch: 2 / 3, Step: 2052 / 2250 Loss: 0.0924\n",
      "Epoch: 2 / 3, Step: 2053 / 2250 Loss: 0.1637\n",
      "Epoch: 2 / 3, Step: 2054 / 2250 Loss: 0.1513\n",
      "Epoch: 2 / 3, Step: 2055 / 2250 Loss: 0.2290\n",
      "Epoch: 2 / 3, Step: 2056 / 2250 Loss: 0.2615\n",
      "Epoch: 2 / 3, Step: 2057 / 2250 Loss: 0.2814\n",
      "Epoch: 2 / 3, Step: 2058 / 2250 Loss: 0.3174\n",
      "Epoch: 2 / 3, Step: 2059 / 2250 Loss: 0.3852\n",
      "Epoch: 2 / 3, Step: 2060 / 2250 Loss: 0.1517\n",
      "Epoch: 2 / 3, Step: 2061 / 2250 Loss: 0.1799\n",
      "Epoch: 2 / 3, Step: 2062 / 2250 Loss: 0.3146\n",
      "Epoch: 2 / 3, Step: 2063 / 2250 Loss: 0.1189\n",
      "Epoch: 2 / 3, Step: 2064 / 2250 Loss: 0.2275\n",
      "Epoch: 2 / 3, Step: 2065 / 2250 Loss: 0.1866\n",
      "Epoch: 2 / 3, Step: 2066 / 2250 Loss: 0.4512\n",
      "Epoch: 2 / 3, Step: 2067 / 2250 Loss: 0.1801\n",
      "Epoch: 2 / 3, Step: 2068 / 2250 Loss: 0.1427\n",
      "Epoch: 2 / 3, Step: 2069 / 2250 Loss: 0.4411\n",
      "Epoch: 2 / 3, Step: 2070 / 2250 Loss: 0.3307\n",
      "Epoch: 2 / 3, Step: 2071 / 2250 Loss: 0.1188\n",
      "Epoch: 2 / 3, Step: 2072 / 2250 Loss: 0.3949\n",
      "Epoch: 2 / 3, Step: 2073 / 2250 Loss: 0.2324\n",
      "Epoch: 2 / 3, Step: 2074 / 2250 Loss: 0.2488\n",
      "Epoch: 2 / 3, Step: 2075 / 2250 Loss: 0.1393\n",
      "Epoch: 2 / 3, Step: 2076 / 2250 Loss: 0.1811\n",
      "Epoch: 2 / 3, Step: 2077 / 2250 Loss: 0.2701\n",
      "Epoch: 2 / 3, Step: 2078 / 2250 Loss: 0.5083\n",
      "Epoch: 2 / 3, Step: 2079 / 2250 Loss: 0.3465\n",
      "Epoch: 2 / 3, Step: 2080 / 2250 Loss: 0.4561\n",
      "Epoch: 2 / 3, Step: 2081 / 2250 Loss: 0.1255\n",
      "Epoch: 2 / 3, Step: 2082 / 2250 Loss: 0.2338\n",
      "Epoch: 2 / 3, Step: 2083 / 2250 Loss: 0.3968\n",
      "Epoch: 2 / 3, Step: 2084 / 2250 Loss: 0.2866\n",
      "Epoch: 2 / 3, Step: 2085 / 2250 Loss: 0.1675\n",
      "Epoch: 2 / 3, Step: 2086 / 2250 Loss: 0.2133\n",
      "Epoch: 2 / 3, Step: 2087 / 2250 Loss: 0.1921\n",
      "Epoch: 2 / 3, Step: 2088 / 2250 Loss: 0.1218\n",
      "Epoch: 2 / 3, Step: 2089 / 2250 Loss: 0.4401\n",
      "Epoch: 2 / 3, Step: 2090 / 2250 Loss: 0.2936\n",
      "Epoch: 2 / 3, Step: 2091 / 2250 Loss: 0.4090\n",
      "Epoch: 2 / 3, Step: 2092 / 2250 Loss: 0.3422\n",
      "Epoch: 2 / 3, Step: 2093 / 2250 Loss: 0.3245\n",
      "Epoch: 2 / 3, Step: 2094 / 2250 Loss: 0.2268\n",
      "Epoch: 2 / 3, Step: 2095 / 2250 Loss: 0.0819\n",
      "Epoch: 2 / 3, Step: 2096 / 2250 Loss: 0.4839\n",
      "Epoch: 2 / 3, Step: 2097 / 2250 Loss: 0.1182\n",
      "Epoch: 2 / 3, Step: 2098 / 2250 Loss: 0.4395\n",
      "Epoch: 2 / 3, Step: 2099 / 2250 Loss: 0.5390\n",
      "Epoch: 2 / 3, Step: 2100 / 2250 Loss: 0.3585\n",
      "Epoch: 2 / 3, Step: 2101 / 2250 Loss: 0.3464\n",
      "Epoch: 2 / 3, Step: 2102 / 2250 Loss: 0.2382\n",
      "Epoch: 2 / 3, Step: 2103 / 2250 Loss: 0.1928\n",
      "Epoch: 2 / 3, Step: 2104 / 2250 Loss: 0.3490\n",
      "Epoch: 2 / 3, Step: 2105 / 2250 Loss: 0.1181\n",
      "Epoch: 2 / 3, Step: 2106 / 2250 Loss: 0.2102\n",
      "Epoch: 2 / 3, Step: 2107 / 2250 Loss: 0.2573\n",
      "Epoch: 2 / 3, Step: 2108 / 2250 Loss: 0.2851\n",
      "Epoch: 2 / 3, Step: 2109 / 2250 Loss: 0.1269\n",
      "Epoch: 2 / 3, Step: 2110 / 2250 Loss: 0.4134\n",
      "Epoch: 2 / 3, Step: 2111 / 2250 Loss: 0.1836\n",
      "Epoch: 2 / 3, Step: 2112 / 2250 Loss: 0.2018\n",
      "Epoch: 2 / 3, Step: 2113 / 2250 Loss: 0.3185\n",
      "Epoch: 2 / 3, Step: 2114 / 2250 Loss: 0.2022\n",
      "Epoch: 2 / 3, Step: 2115 / 2250 Loss: 0.3046\n",
      "Epoch: 2 / 3, Step: 2116 / 2250 Loss: 0.2293\n",
      "Epoch: 2 / 3, Step: 2117 / 2250 Loss: 0.2482\n",
      "Epoch: 2 / 3, Step: 2118 / 2250 Loss: 0.1794\n",
      "Epoch: 2 / 3, Step: 2119 / 2250 Loss: 0.2353\n",
      "Epoch: 2 / 3, Step: 2120 / 2250 Loss: 0.1500\n",
      "Epoch: 2 / 3, Step: 2121 / 2250 Loss: 0.6551\n",
      "Epoch: 2 / 3, Step: 2122 / 2250 Loss: 0.3313\n",
      "Epoch: 2 / 3, Step: 2123 / 2250 Loss: 0.3153\n",
      "Epoch: 2 / 3, Step: 2124 / 2250 Loss: 0.1234\n",
      "Epoch: 2 / 3, Step: 2125 / 2250 Loss: 0.2282\n",
      "Epoch: 2 / 3, Step: 2126 / 2250 Loss: 0.1018\n",
      "Epoch: 2 / 3, Step: 2127 / 2250 Loss: 0.1895\n",
      "Epoch: 2 / 3, Step: 2128 / 2250 Loss: 0.2012\n",
      "Epoch: 2 / 3, Step: 2129 / 2250 Loss: 0.0834\n",
      "Epoch: 2 / 3, Step: 2130 / 2250 Loss: 0.1293\n",
      "Epoch: 2 / 3, Step: 2131 / 2250 Loss: 0.2972\n",
      "Epoch: 2 / 3, Step: 2132 / 2250 Loss: 0.3442\n",
      "Epoch: 2 / 3, Step: 2133 / 2250 Loss: 0.2969\n",
      "Epoch: 2 / 3, Step: 2134 / 2250 Loss: 0.2512\n",
      "Epoch: 2 / 3, Step: 2135 / 2250 Loss: 0.1899\n",
      "Epoch: 2 / 3, Step: 2136 / 2250 Loss: 0.2290\n",
      "Epoch: 2 / 3, Step: 2137 / 2250 Loss: 0.1861\n",
      "Epoch: 2 / 3, Step: 2138 / 2250 Loss: 0.2865\n",
      "Epoch: 2 / 3, Step: 2139 / 2250 Loss: 0.3050\n",
      "Epoch: 2 / 3, Step: 2140 / 2250 Loss: 0.2161\n",
      "Epoch: 2 / 3, Step: 2141 / 2250 Loss: 0.2523\n",
      "Epoch: 2 / 3, Step: 2142 / 2250 Loss: 0.2155\n",
      "Epoch: 2 / 3, Step: 2143 / 2250 Loss: 0.4781\n",
      "Epoch: 2 / 3, Step: 2144 / 2250 Loss: 0.4615\n",
      "Epoch: 2 / 3, Step: 2145 / 2250 Loss: 0.3208\n",
      "Epoch: 2 / 3, Step: 2146 / 2250 Loss: 0.4773\n",
      "Epoch: 2 / 3, Step: 2147 / 2250 Loss: 0.0771\n",
      "Epoch: 2 / 3, Step: 2148 / 2250 Loss: 0.5620\n",
      "Epoch: 2 / 3, Step: 2149 / 2250 Loss: 0.3182\n",
      "Epoch: 2 / 3, Step: 2150 / 2250 Loss: 0.3255\n",
      "Epoch: 2 / 3, Step: 2151 / 2250 Loss: 0.2181\n",
      "Epoch: 2 / 3, Step: 2152 / 2250 Loss: 0.3369\n",
      "Epoch: 2 / 3, Step: 2153 / 2250 Loss: 0.1875\n",
      "Epoch: 2 / 3, Step: 2154 / 2250 Loss: 0.1963\n",
      "Epoch: 2 / 3, Step: 2155 / 2250 Loss: 0.2243\n",
      "Epoch: 2 / 3, Step: 2156 / 2250 Loss: 0.7587\n",
      "Epoch: 2 / 3, Step: 2157 / 2250 Loss: 0.5868\n",
      "Epoch: 2 / 3, Step: 2158 / 2250 Loss: 0.3508\n",
      "Epoch: 2 / 3, Step: 2159 / 2250 Loss: 0.2303\n",
      "Epoch: 2 / 3, Step: 2160 / 2250 Loss: 0.2022\n",
      "Epoch: 2 / 3, Step: 2161 / 2250 Loss: 0.1989\n",
      "Epoch: 2 / 3, Step: 2162 / 2250 Loss: 0.1798\n",
      "Epoch: 2 / 3, Step: 2163 / 2250 Loss: 0.1510\n",
      "Epoch: 2 / 3, Step: 2164 / 2250 Loss: 0.1438\n",
      "Epoch: 2 / 3, Step: 2165 / 2250 Loss: 0.4765\n",
      "Epoch: 2 / 3, Step: 2166 / 2250 Loss: 0.5404\n",
      "Epoch: 2 / 3, Step: 2167 / 2250 Loss: 0.3655\n",
      "Epoch: 2 / 3, Step: 2168 / 2250 Loss: 0.1580\n",
      "Epoch: 2 / 3, Step: 2169 / 2250 Loss: 0.2139\n",
      "Epoch: 2 / 3, Step: 2170 / 2250 Loss: 0.1115\n",
      "Epoch: 2 / 3, Step: 2171 / 2250 Loss: 0.1500\n",
      "Epoch: 2 / 3, Step: 2172 / 2250 Loss: 0.2354\n",
      "Epoch: 2 / 3, Step: 2173 / 2250 Loss: 0.1463\n",
      "Epoch: 2 / 3, Step: 2174 / 2250 Loss: 0.0617\n",
      "Epoch: 2 / 3, Step: 2175 / 2250 Loss: 0.1658\n",
      "Epoch: 2 / 3, Step: 2176 / 2250 Loss: 0.1812\n",
      "Epoch: 2 / 3, Step: 2177 / 2250 Loss: 0.2487\n",
      "Epoch: 2 / 3, Step: 2178 / 2250 Loss: 0.2425\n",
      "Epoch: 2 / 3, Step: 2179 / 2250 Loss: 0.2479\n",
      "Epoch: 2 / 3, Step: 2180 / 2250 Loss: 0.1789\n",
      "Epoch: 2 / 3, Step: 2181 / 2250 Loss: 0.4313\n",
      "Epoch: 2 / 3, Step: 2182 / 2250 Loss: 0.0508\n",
      "Epoch: 2 / 3, Step: 2183 / 2250 Loss: 0.1244\n",
      "Epoch: 2 / 3, Step: 2184 / 2250 Loss: 0.2406\n",
      "Epoch: 2 / 3, Step: 2185 / 2250 Loss: 0.4762\n",
      "Epoch: 2 / 3, Step: 2186 / 2250 Loss: 0.4094\n",
      "Epoch: 2 / 3, Step: 2187 / 2250 Loss: 0.2496\n",
      "Epoch: 2 / 3, Step: 2188 / 2250 Loss: 0.3867\n",
      "Epoch: 2 / 3, Step: 2189 / 2250 Loss: 0.3501\n",
      "Epoch: 2 / 3, Step: 2190 / 2250 Loss: 0.1266\n",
      "Epoch: 2 / 3, Step: 2191 / 2250 Loss: 0.4043\n",
      "Epoch: 2 / 3, Step: 2192 / 2250 Loss: 0.2213\n",
      "Epoch: 2 / 3, Step: 2193 / 2250 Loss: 0.3598\n",
      "Epoch: 2 / 3, Step: 2194 / 2250 Loss: 0.3322\n",
      "Epoch: 2 / 3, Step: 2195 / 2250 Loss: 0.2261\n",
      "Epoch: 2 / 3, Step: 2196 / 2250 Loss: 0.1638\n",
      "Epoch: 2 / 3, Step: 2197 / 2250 Loss: 0.2765\n",
      "Epoch: 2 / 3, Step: 2198 / 2250 Loss: 0.0930\n",
      "Epoch: 2 / 3, Step: 2199 / 2250 Loss: 0.3810\n",
      "Epoch: 2 / 3, Step: 2200 / 2250 Loss: 0.3899\n",
      "Epoch: 2 / 3, Step: 2201 / 2250 Loss: 0.1199\n",
      "Epoch: 2 / 3, Step: 2202 / 2250 Loss: 0.3414\n",
      "Epoch: 2 / 3, Step: 2203 / 2250 Loss: 0.1630\n",
      "Epoch: 2 / 3, Step: 2204 / 2250 Loss: 0.1228\n",
      "Epoch: 2 / 3, Step: 2205 / 2250 Loss: 0.2708\n",
      "Epoch: 2 / 3, Step: 2206 / 2250 Loss: 0.1908\n",
      "Epoch: 2 / 3, Step: 2207 / 2250 Loss: 0.2164\n",
      "Epoch: 2 / 3, Step: 2208 / 2250 Loss: 0.1723\n",
      "Epoch: 2 / 3, Step: 2209 / 2250 Loss: 0.2233\n",
      "Epoch: 2 / 3, Step: 2210 / 2250 Loss: 0.2087\n",
      "Epoch: 2 / 3, Step: 2211 / 2250 Loss: 0.1891\n",
      "Epoch: 2 / 3, Step: 2212 / 2250 Loss: 0.4761\n",
      "Epoch: 2 / 3, Step: 2213 / 2250 Loss: 0.6763\n",
      "Epoch: 2 / 3, Step: 2214 / 2250 Loss: 0.3533\n",
      "Epoch: 2 / 3, Step: 2215 / 2250 Loss: 0.3676\n",
      "Epoch: 2 / 3, Step: 2216 / 2250 Loss: 0.3589\n",
      "Epoch: 2 / 3, Step: 2217 / 2250 Loss: 0.3029\n",
      "Epoch: 2 / 3, Step: 2218 / 2250 Loss: 0.1322\n",
      "Epoch: 2 / 3, Step: 2219 / 2250 Loss: 0.2412\n",
      "Epoch: 2 / 3, Step: 2220 / 2250 Loss: 0.1090\n",
      "Epoch: 2 / 3, Step: 2221 / 2250 Loss: 0.3187\n",
      "Epoch: 2 / 3, Step: 2222 / 2250 Loss: 0.2522\n",
      "Epoch: 2 / 3, Step: 2223 / 2250 Loss: 0.3520\n",
      "Epoch: 2 / 3, Step: 2224 / 2250 Loss: 0.2246\n",
      "Epoch: 2 / 3, Step: 2225 / 2250 Loss: 0.1890\n",
      "Epoch: 2 / 3, Step: 2226 / 2250 Loss: 0.3525\n",
      "Epoch: 2 / 3, Step: 2227 / 2250 Loss: 0.2208\n",
      "Epoch: 2 / 3, Step: 2228 / 2250 Loss: 0.2339\n",
      "Epoch: 2 / 3, Step: 2229 / 2250 Loss: 0.2511\n",
      "Epoch: 2 / 3, Step: 2230 / 2250 Loss: 0.2358\n",
      "Epoch: 2 / 3, Step: 2231 / 2250 Loss: 0.1994\n",
      "Epoch: 2 / 3, Step: 2232 / 2250 Loss: 0.2356\n",
      "Epoch: 2 / 3, Step: 2233 / 2250 Loss: 0.3044\n",
      "Epoch: 2 / 3, Step: 2234 / 2250 Loss: 0.1752\n",
      "Epoch: 2 / 3, Step: 2235 / 2250 Loss: 0.2341\n",
      "Epoch: 2 / 3, Step: 2236 / 2250 Loss: 0.2104\n",
      "Epoch: 2 / 3, Step: 2237 / 2250 Loss: 0.2932\n",
      "Epoch: 2 / 3, Step: 2238 / 2250 Loss: 0.1998\n",
      "Epoch: 2 / 3, Step: 2239 / 2250 Loss: 0.1881\n",
      "Epoch: 2 / 3, Step: 2240 / 2250 Loss: 0.2679\n",
      "Epoch: 2 / 3, Step: 2241 / 2250 Loss: 0.4903\n",
      "Epoch: 2 / 3, Step: 2242 / 2250 Loss: 0.4042\n",
      "Epoch: 2 / 3, Step: 2243 / 2250 Loss: 0.1959\n",
      "Epoch: 2 / 3, Step: 2244 / 2250 Loss: 0.3213\n",
      "Epoch: 2 / 3, Step: 2245 / 2250 Loss: 0.2300\n",
      "Epoch: 2 / 3, Step: 2246 / 2250 Loss: 0.3062\n",
      "Epoch: 2 / 3, Step: 2247 / 2250 Loss: 0.2218\n",
      "Epoch: 2 / 3, Step: 2248 / 2250 Loss: 0.3243\n",
      "Epoch: 2 / 3, Step: 2249 / 2250 Loss: 0.3451\n",
      "Epoch: 3 / 3, Step: 0 / 2250 Loss: 0.3602\n",
      "Epoch: 3 / 3, Step: 1 / 2250 Loss: 0.3980\n",
      "Epoch: 3 / 3, Step: 2 / 2250 Loss: 0.3182\n",
      "Epoch: 3 / 3, Step: 3 / 2250 Loss: 0.3396\n",
      "Epoch: 3 / 3, Step: 4 / 2250 Loss: 0.2869\n",
      "Epoch: 3 / 3, Step: 5 / 2250 Loss: 0.3062\n",
      "Epoch: 3 / 3, Step: 6 / 2250 Loss: 0.2744\n",
      "Epoch: 3 / 3, Step: 7 / 2250 Loss: 0.2930\n",
      "Epoch: 3 / 3, Step: 8 / 2250 Loss: 0.4427\n",
      "Epoch: 3 / 3, Step: 9 / 2250 Loss: 0.3394\n",
      "Epoch: 3 / 3, Step: 10 / 2250 Loss: 0.3057\n",
      "Epoch: 3 / 3, Step: 11 / 2250 Loss: 0.1958\n",
      "Epoch: 3 / 3, Step: 12 / 2250 Loss: 0.3556\n",
      "Epoch: 3 / 3, Step: 13 / 2250 Loss: 0.6330\n",
      "Epoch: 3 / 3, Step: 14 / 2250 Loss: 0.0878\n",
      "Epoch: 3 / 3, Step: 15 / 2250 Loss: 0.1561\n",
      "Epoch: 3 / 3, Step: 16 / 2250 Loss: 0.4634\n",
      "Epoch: 3 / 3, Step: 17 / 2250 Loss: 0.2821\n",
      "Epoch: 3 / 3, Step: 18 / 2250 Loss: 0.1457\n",
      "Epoch: 3 / 3, Step: 19 / 2250 Loss: 0.3178\n",
      "Epoch: 3 / 3, Step: 20 / 2250 Loss: 0.2812\n",
      "Epoch: 3 / 3, Step: 21 / 2250 Loss: 0.3172\n",
      "Epoch: 3 / 3, Step: 22 / 2250 Loss: 0.1560\n",
      "Epoch: 3 / 3, Step: 23 / 2250 Loss: 0.4105\n",
      "Epoch: 3 / 3, Step: 24 / 2250 Loss: 0.2263\n",
      "Epoch: 3 / 3, Step: 25 / 2250 Loss: 0.3788\n",
      "Epoch: 3 / 3, Step: 26 / 2250 Loss: 0.2417\n",
      "Epoch: 3 / 3, Step: 27 / 2250 Loss: 0.2447\n",
      "Epoch: 3 / 3, Step: 28 / 2250 Loss: 0.2460\n",
      "Epoch: 3 / 3, Step: 29 / 2250 Loss: 0.2169\n",
      "Epoch: 3 / 3, Step: 30 / 2250 Loss: 0.1934\n",
      "Epoch: 3 / 3, Step: 31 / 2250 Loss: 0.3380\n",
      "Epoch: 3 / 3, Step: 32 / 2250 Loss: 0.1217\n",
      "Epoch: 3 / 3, Step: 33 / 2250 Loss: 0.3006\n",
      "Epoch: 3 / 3, Step: 34 / 2250 Loss: 0.3315\n",
      "Epoch: 3 / 3, Step: 35 / 2250 Loss: 0.1967\n",
      "Epoch: 3 / 3, Step: 36 / 2250 Loss: 0.2981\n",
      "Epoch: 3 / 3, Step: 37 / 2250 Loss: 0.2121\n",
      "Epoch: 3 / 3, Step: 38 / 2250 Loss: 0.2891\n",
      "Epoch: 3 / 3, Step: 39 / 2250 Loss: 0.3510\n",
      "Epoch: 3 / 3, Step: 40 / 2250 Loss: 0.2676\n",
      "Epoch: 3 / 3, Step: 41 / 2250 Loss: 0.2464\n",
      "Epoch: 3 / 3, Step: 42 / 2250 Loss: 0.2453\n",
      "Epoch: 3 / 3, Step: 43 / 2250 Loss: 0.1120\n",
      "Epoch: 3 / 3, Step: 44 / 2250 Loss: 0.4837\n",
      "Epoch: 3 / 3, Step: 45 / 2250 Loss: 0.2547\n",
      "Epoch: 3 / 3, Step: 46 / 2250 Loss: 0.1471\n",
      "Epoch: 3 / 3, Step: 47 / 2250 Loss: 0.2605\n",
      "Epoch: 3 / 3, Step: 48 / 2250 Loss: 0.4762\n",
      "Epoch: 3 / 3, Step: 49 / 2250 Loss: 0.2436\n",
      "Epoch: 3 / 3, Step: 50 / 2250 Loss: 0.1963\n",
      "Epoch: 3 / 3, Step: 51 / 2250 Loss: 0.4430\n",
      "Epoch: 3 / 3, Step: 52 / 2250 Loss: 0.1804\n",
      "Epoch: 3 / 3, Step: 53 / 2250 Loss: 0.4059\n",
      "Epoch: 3 / 3, Step: 54 / 2250 Loss: 0.3798\n",
      "Epoch: 3 / 3, Step: 55 / 2250 Loss: 0.3144\n",
      "Epoch: 3 / 3, Step: 56 / 2250 Loss: 0.4194\n",
      "Epoch: 3 / 3, Step: 57 / 2250 Loss: 0.2718\n",
      "Epoch: 3 / 3, Step: 58 / 2250 Loss: 0.4314\n",
      "Epoch: 3 / 3, Step: 59 / 2250 Loss: 0.1841\n",
      "Epoch: 3 / 3, Step: 60 / 2250 Loss: 0.2975\n",
      "Epoch: 3 / 3, Step: 61 / 2250 Loss: 0.2889\n",
      "Epoch: 3 / 3, Step: 62 / 2250 Loss: 0.1351\n",
      "Epoch: 3 / 3, Step: 63 / 2250 Loss: 0.0865\n",
      "Epoch: 3 / 3, Step: 64 / 2250 Loss: 0.3671\n",
      "Epoch: 3 / 3, Step: 65 / 2250 Loss: 0.0928\n",
      "Epoch: 3 / 3, Step: 66 / 2250 Loss: 0.3192\n",
      "Epoch: 3 / 3, Step: 67 / 2250 Loss: 0.3231\n",
      "Epoch: 3 / 3, Step: 68 / 2250 Loss: 0.1592\n",
      "Epoch: 3 / 3, Step: 69 / 2250 Loss: 0.1671\n",
      "Epoch: 3 / 3, Step: 70 / 2250 Loss: 0.2626\n",
      "Epoch: 3 / 3, Step: 71 / 2250 Loss: 0.1950\n",
      "Epoch: 3 / 3, Step: 72 / 2250 Loss: 0.1721\n",
      "Epoch: 3 / 3, Step: 73 / 2250 Loss: 0.0884\n",
      "Epoch: 3 / 3, Step: 74 / 2250 Loss: 0.1956\n",
      "Epoch: 3 / 3, Step: 75 / 2250 Loss: 0.1551\n",
      "Epoch: 3 / 3, Step: 76 / 2250 Loss: 0.2491\n",
      "Epoch: 3 / 3, Step: 77 / 2250 Loss: 0.2572\n",
      "Epoch: 3 / 3, Step: 78 / 2250 Loss: 0.4669\n",
      "Epoch: 3 / 3, Step: 79 / 2250 Loss: 0.2940\n",
      "Epoch: 3 / 3, Step: 80 / 2250 Loss: 0.2507\n",
      "Epoch: 3 / 3, Step: 81 / 2250 Loss: 0.1706\n",
      "Epoch: 3 / 3, Step: 82 / 2250 Loss: 0.2315\n",
      "Epoch: 3 / 3, Step: 83 / 2250 Loss: 0.2518\n",
      "Epoch: 3 / 3, Step: 84 / 2250 Loss: 0.1491\n",
      "Epoch: 3 / 3, Step: 85 / 2250 Loss: 0.4288\n",
      "Epoch: 3 / 3, Step: 86 / 2250 Loss: 0.1824\n",
      "Epoch: 3 / 3, Step: 87 / 2250 Loss: 0.1787\n",
      "Epoch: 3 / 3, Step: 88 / 2250 Loss: 0.3146\n",
      "Epoch: 3 / 3, Step: 89 / 2250 Loss: 0.1165\n",
      "Epoch: 3 / 3, Step: 90 / 2250 Loss: 0.0863\n",
      "Epoch: 3 / 3, Step: 91 / 2250 Loss: 0.2804\n",
      "Epoch: 3 / 3, Step: 92 / 2250 Loss: 0.2611\n",
      "Epoch: 3 / 3, Step: 93 / 2250 Loss: 0.1720\n",
      "Epoch: 3 / 3, Step: 94 / 2250 Loss: 0.2999\n",
      "Epoch: 3 / 3, Step: 95 / 2250 Loss: 0.1586\n",
      "Epoch: 3 / 3, Step: 96 / 2250 Loss: 0.1667\n",
      "Epoch: 3 / 3, Step: 97 / 2250 Loss: 0.2676\n",
      "Epoch: 3 / 3, Step: 98 / 2250 Loss: 0.2288\n",
      "Epoch: 3 / 3, Step: 99 / 2250 Loss: 0.1056\n",
      "Epoch: 3 / 3, Step: 100 / 2250 Loss: 0.1561\n",
      "Epoch: 3 / 3, Step: 101 / 2250 Loss: 0.3249\n",
      "Epoch: 3 / 3, Step: 102 / 2250 Loss: 0.2512\n",
      "Epoch: 3 / 3, Step: 103 / 2250 Loss: 0.2429\n",
      "Epoch: 3 / 3, Step: 104 / 2250 Loss: 0.2177\n",
      "Epoch: 3 / 3, Step: 105 / 2250 Loss: 0.1044\n",
      "Epoch: 3 / 3, Step: 106 / 2250 Loss: 0.3662\n",
      "Epoch: 3 / 3, Step: 107 / 2250 Loss: 0.2162\n",
      "Epoch: 3 / 3, Step: 108 / 2250 Loss: 0.1467\n",
      "Epoch: 3 / 3, Step: 109 / 2250 Loss: 0.0813\n",
      "Epoch: 3 / 3, Step: 110 / 2250 Loss: 0.2857\n",
      "Epoch: 3 / 3, Step: 111 / 2250 Loss: 0.1791\n",
      "Epoch: 3 / 3, Step: 112 / 2250 Loss: 0.5200\n",
      "Epoch: 3 / 3, Step: 113 / 2250 Loss: 0.1512\n",
      "Epoch: 3 / 3, Step: 114 / 2250 Loss: 0.1528\n",
      "Epoch: 3 / 3, Step: 115 / 2250 Loss: 0.1663\n",
      "Epoch: 3 / 3, Step: 116 / 2250 Loss: 0.0968\n",
      "Epoch: 3 / 3, Step: 117 / 2250 Loss: 0.1379\n",
      "Epoch: 3 / 3, Step: 118 / 2250 Loss: 0.2383\n",
      "Epoch: 3 / 3, Step: 119 / 2250 Loss: 0.1853\n",
      "Epoch: 3 / 3, Step: 120 / 2250 Loss: 0.3888\n",
      "Epoch: 3 / 3, Step: 121 / 2250 Loss: 0.5907\n",
      "Epoch: 3 / 3, Step: 122 / 2250 Loss: 0.1326\n",
      "Epoch: 3 / 3, Step: 123 / 2250 Loss: 0.1880\n",
      "Epoch: 3 / 3, Step: 124 / 2250 Loss: 0.2136\n",
      "Epoch: 3 / 3, Step: 125 / 2250 Loss: 0.4048\n",
      "Epoch: 3 / 3, Step: 126 / 2250 Loss: 0.4336\n",
      "Epoch: 3 / 3, Step: 127 / 2250 Loss: 0.1566\n",
      "Epoch: 3 / 3, Step: 128 / 2250 Loss: 0.5946\n",
      "Epoch: 3 / 3, Step: 129 / 2250 Loss: 0.4073\n",
      "Epoch: 3 / 3, Step: 130 / 2250 Loss: 0.1516\n",
      "Epoch: 3 / 3, Step: 131 / 2250 Loss: 0.0789\n",
      "Epoch: 3 / 3, Step: 132 / 2250 Loss: 0.1916\n",
      "Epoch: 3 / 3, Step: 133 / 2250 Loss: 0.2015\n",
      "Epoch: 3 / 3, Step: 134 / 2250 Loss: 0.2989\n",
      "Epoch: 3 / 3, Step: 135 / 2250 Loss: 0.4067\n",
      "Epoch: 3 / 3, Step: 136 / 2250 Loss: 0.3062\n",
      "Epoch: 3 / 3, Step: 137 / 2250 Loss: 0.1836\n",
      "Epoch: 3 / 3, Step: 138 / 2250 Loss: 0.1289\n",
      "Epoch: 3 / 3, Step: 139 / 2250 Loss: 0.0674\n",
      "Epoch: 3 / 3, Step: 140 / 2250 Loss: 0.3249\n",
      "Epoch: 3 / 3, Step: 141 / 2250 Loss: 0.1951\n",
      "Epoch: 3 / 3, Step: 142 / 2250 Loss: 0.2179\n",
      "Epoch: 3 / 3, Step: 143 / 2250 Loss: 0.2161\n",
      "Epoch: 3 / 3, Step: 144 / 2250 Loss: 0.4116\n",
      "Epoch: 3 / 3, Step: 145 / 2250 Loss: 0.1672\n",
      "Epoch: 3 / 3, Step: 146 / 2250 Loss: 0.2422\n",
      "Epoch: 3 / 3, Step: 147 / 2250 Loss: 0.1579\n",
      "Epoch: 3 / 3, Step: 148 / 2250 Loss: 0.3325\n",
      "Epoch: 3 / 3, Step: 149 / 2250 Loss: 0.2096\n",
      "Epoch: 3 / 3, Step: 150 / 2250 Loss: 0.1156\n",
      "Epoch: 3 / 3, Step: 151 / 2250 Loss: 0.1483\n",
      "Epoch: 3 / 3, Step: 152 / 2250 Loss: 0.2845\n",
      "Epoch: 3 / 3, Step: 153 / 2250 Loss: 0.4007\n",
      "Epoch: 3 / 3, Step: 154 / 2250 Loss: 0.0884\n",
      "Epoch: 3 / 3, Step: 155 / 2250 Loss: 0.2284\n",
      "Epoch: 3 / 3, Step: 156 / 2250 Loss: 0.2067\n",
      "Epoch: 3 / 3, Step: 157 / 2250 Loss: 0.3663\n",
      "Epoch: 3 / 3, Step: 158 / 2250 Loss: 0.2731\n",
      "Epoch: 3 / 3, Step: 159 / 2250 Loss: 0.3198\n",
      "Epoch: 3 / 3, Step: 160 / 2250 Loss: 0.1331\n",
      "Epoch: 3 / 3, Step: 161 / 2250 Loss: 0.2192\n",
      "Epoch: 3 / 3, Step: 162 / 2250 Loss: 0.2603\n",
      "Epoch: 3 / 3, Step: 163 / 2250 Loss: 0.2246\n",
      "Epoch: 3 / 3, Step: 164 / 2250 Loss: 0.5072\n",
      "Epoch: 3 / 3, Step: 165 / 2250 Loss: 0.2155\n",
      "Epoch: 3 / 3, Step: 166 / 2250 Loss: 0.2538\n",
      "Epoch: 3 / 3, Step: 167 / 2250 Loss: 0.1767\n",
      "Epoch: 3 / 3, Step: 168 / 2250 Loss: 0.1561\n",
      "Epoch: 3 / 3, Step: 169 / 2250 Loss: 0.1721\n",
      "Epoch: 3 / 3, Step: 170 / 2250 Loss: 0.2091\n",
      "Epoch: 3 / 3, Step: 171 / 2250 Loss: 0.1715\n",
      "Epoch: 3 / 3, Step: 172 / 2250 Loss: 0.2885\n",
      "Epoch: 3 / 3, Step: 173 / 2250 Loss: 0.0555\n",
      "Epoch: 3 / 3, Step: 174 / 2250 Loss: 0.2719\n",
      "Epoch: 3 / 3, Step: 175 / 2250 Loss: 0.2097\n",
      "Epoch: 3 / 3, Step: 176 / 2250 Loss: 0.1870\n",
      "Epoch: 3 / 3, Step: 177 / 2250 Loss: 0.2482\n",
      "Epoch: 3 / 3, Step: 178 / 2250 Loss: 0.2717\n",
      "Epoch: 3 / 3, Step: 179 / 2250 Loss: 0.1423\n",
      "Epoch: 3 / 3, Step: 180 / 2250 Loss: 0.3602\n",
      "Epoch: 3 / 3, Step: 181 / 2250 Loss: 0.5022\n",
      "Epoch: 3 / 3, Step: 182 / 2250 Loss: 0.0882\n",
      "Epoch: 3 / 3, Step: 183 / 2250 Loss: 0.0808\n",
      "Epoch: 3 / 3, Step: 184 / 2250 Loss: 0.1083\n",
      "Epoch: 3 / 3, Step: 185 / 2250 Loss: 0.2239\n",
      "Epoch: 3 / 3, Step: 186 / 2250 Loss: 0.0526\n",
      "Epoch: 3 / 3, Step: 187 / 2250 Loss: 0.3292\n",
      "Epoch: 3 / 3, Step: 188 / 2250 Loss: 0.2861\n",
      "Epoch: 3 / 3, Step: 189 / 2250 Loss: 0.4552\n",
      "Epoch: 3 / 3, Step: 190 / 2250 Loss: 0.1718\n",
      "Epoch: 3 / 3, Step: 191 / 2250 Loss: 0.2566\n",
      "Epoch: 3 / 3, Step: 192 / 2250 Loss: 0.1632\n",
      "Epoch: 3 / 3, Step: 193 / 2250 Loss: 0.1796\n",
      "Epoch: 3 / 3, Step: 194 / 2250 Loss: 0.2394\n",
      "Epoch: 3 / 3, Step: 195 / 2250 Loss: 0.2119\n",
      "Epoch: 3 / 3, Step: 196 / 2250 Loss: 0.3480\n",
      "Epoch: 3 / 3, Step: 197 / 2250 Loss: 0.5644\n",
      "Epoch: 3 / 3, Step: 198 / 2250 Loss: 0.2094\n",
      "Epoch: 3 / 3, Step: 199 / 2250 Loss: 0.2210\n",
      "Epoch: 3 / 3, Step: 200 / 2250 Loss: 0.2418\n",
      "Epoch: 3 / 3, Step: 201 / 2250 Loss: 0.3587\n",
      "Epoch: 3 / 3, Step: 202 / 2250 Loss: 0.0973\n",
      "Epoch: 3 / 3, Step: 203 / 2250 Loss: 0.2054\n",
      "Epoch: 3 / 3, Step: 204 / 2250 Loss: 0.2084\n",
      "Epoch: 3 / 3, Step: 205 / 2250 Loss: 0.1021\n",
      "Epoch: 3 / 3, Step: 206 / 2250 Loss: 0.3302\n",
      "Epoch: 3 / 3, Step: 207 / 2250 Loss: 0.1066\n",
      "Epoch: 3 / 3, Step: 208 / 2250 Loss: 0.3648\n",
      "Epoch: 3 / 3, Step: 209 / 2250 Loss: 0.2393\n",
      "Epoch: 3 / 3, Step: 210 / 2250 Loss: 0.3365\n",
      "Epoch: 3 / 3, Step: 211 / 2250 Loss: 0.3808\n",
      "Epoch: 3 / 3, Step: 212 / 2250 Loss: 0.3934\n",
      "Epoch: 3 / 3, Step: 213 / 2250 Loss: 0.1855\n",
      "Epoch: 3 / 3, Step: 214 / 2250 Loss: 0.2196\n",
      "Epoch: 3 / 3, Step: 215 / 2250 Loss: 0.3175\n",
      "Epoch: 3 / 3, Step: 216 / 2250 Loss: 0.0958\n",
      "Epoch: 3 / 3, Step: 217 / 2250 Loss: 0.2544\n",
      "Epoch: 3 / 3, Step: 218 / 2250 Loss: 0.2779\n",
      "Epoch: 3 / 3, Step: 219 / 2250 Loss: 0.3189\n",
      "Epoch: 3 / 3, Step: 220 / 2250 Loss: 0.2652\n",
      "Epoch: 3 / 3, Step: 221 / 2250 Loss: 0.3174\n",
      "Epoch: 3 / 3, Step: 222 / 2250 Loss: 0.2833\n",
      "Epoch: 3 / 3, Step: 223 / 2250 Loss: 0.3660\n",
      "Epoch: 3 / 3, Step: 224 / 2250 Loss: 0.1722\n",
      "Epoch: 3 / 3, Step: 225 / 2250 Loss: 0.1844\n",
      "Epoch: 3 / 3, Step: 226 / 2250 Loss: 0.2228\n",
      "Epoch: 3 / 3, Step: 227 / 2250 Loss: 0.2939\n",
      "Epoch: 3 / 3, Step: 228 / 2250 Loss: 0.1870\n",
      "Epoch: 3 / 3, Step: 229 / 2250 Loss: 0.1788\n",
      "Epoch: 3 / 3, Step: 230 / 2250 Loss: 0.2903\n",
      "Epoch: 3 / 3, Step: 231 / 2250 Loss: 0.3709\n",
      "Epoch: 3 / 3, Step: 232 / 2250 Loss: 0.2332\n",
      "Epoch: 3 / 3, Step: 233 / 2250 Loss: 0.1674\n",
      "Epoch: 3 / 3, Step: 234 / 2250 Loss: 0.4123\n",
      "Epoch: 3 / 3, Step: 235 / 2250 Loss: 0.1225\n",
      "Epoch: 3 / 3, Step: 236 / 2250 Loss: 0.1678\n",
      "Epoch: 3 / 3, Step: 237 / 2250 Loss: 0.0897\n",
      "Epoch: 3 / 3, Step: 238 / 2250 Loss: 0.1644\n",
      "Epoch: 3 / 3, Step: 239 / 2250 Loss: 0.2523\n",
      "Epoch: 3 / 3, Step: 240 / 2250 Loss: 0.1459\n",
      "Epoch: 3 / 3, Step: 241 / 2250 Loss: 0.3022\n",
      "Epoch: 3 / 3, Step: 242 / 2250 Loss: 0.4413\n",
      "Epoch: 3 / 3, Step: 243 / 2250 Loss: 0.0583\n",
      "Epoch: 3 / 3, Step: 244 / 2250 Loss: 0.2557\n",
      "Epoch: 3 / 3, Step: 245 / 2250 Loss: 0.2553\n",
      "Epoch: 3 / 3, Step: 246 / 2250 Loss: 0.2670\n",
      "Epoch: 3 / 3, Step: 247 / 2250 Loss: 0.4276\n",
      "Epoch: 3 / 3, Step: 248 / 2250 Loss: 0.4833\n",
      "Epoch: 3 / 3, Step: 249 / 2250 Loss: 0.1477\n",
      "Epoch: 3 / 3, Step: 250 / 2250 Loss: 0.0929\n",
      "Epoch: 3 / 3, Step: 251 / 2250 Loss: 0.2995\n",
      "Epoch: 3 / 3, Step: 252 / 2250 Loss: 0.1974\n",
      "Epoch: 3 / 3, Step: 253 / 2250 Loss: 0.4291\n",
      "Epoch: 3 / 3, Step: 254 / 2250 Loss: 0.2505\n",
      "Epoch: 3 / 3, Step: 255 / 2250 Loss: 0.2753\n",
      "Epoch: 3 / 3, Step: 256 / 2250 Loss: 0.1644\n",
      "Epoch: 3 / 3, Step: 257 / 2250 Loss: 0.2323\n",
      "Epoch: 3 / 3, Step: 258 / 2250 Loss: 0.0894\n",
      "Epoch: 3 / 3, Step: 259 / 2250 Loss: 0.4373\n",
      "Epoch: 3 / 3, Step: 260 / 2250 Loss: 0.4589\n",
      "Epoch: 3 / 3, Step: 261 / 2250 Loss: 0.2249\n",
      "Epoch: 3 / 3, Step: 262 / 2250 Loss: 0.3460\n",
      "Epoch: 3 / 3, Step: 263 / 2250 Loss: 0.1637\n",
      "Epoch: 3 / 3, Step: 264 / 2250 Loss: 0.1240\n",
      "Epoch: 3 / 3, Step: 265 / 2250 Loss: 0.3336\n",
      "Epoch: 3 / 3, Step: 266 / 2250 Loss: 0.1460\n",
      "Epoch: 3 / 3, Step: 267 / 2250 Loss: 0.2419\n",
      "Epoch: 3 / 3, Step: 268 / 2250 Loss: 0.1444\n",
      "Epoch: 3 / 3, Step: 269 / 2250 Loss: 0.1553\n",
      "Epoch: 3 / 3, Step: 270 / 2250 Loss: 0.2982\n",
      "Epoch: 3 / 3, Step: 271 / 2250 Loss: 0.1738\n",
      "Epoch: 3 / 3, Step: 272 / 2250 Loss: 0.1831\n",
      "Epoch: 3 / 3, Step: 273 / 2250 Loss: 0.2365\n",
      "Epoch: 3 / 3, Step: 274 / 2250 Loss: 0.1429\n",
      "Epoch: 3 / 3, Step: 275 / 2250 Loss: 0.1971\n",
      "Epoch: 3 / 3, Step: 276 / 2250 Loss: 0.0909\n",
      "Epoch: 3 / 3, Step: 277 / 2250 Loss: 0.3100\n",
      "Epoch: 3 / 3, Step: 278 / 2250 Loss: 0.1769\n",
      "Epoch: 3 / 3, Step: 279 / 2250 Loss: 0.3243\n",
      "Epoch: 3 / 3, Step: 280 / 2250 Loss: 0.1386\n",
      "Epoch: 3 / 3, Step: 281 / 2250 Loss: 0.2700\n",
      "Epoch: 3 / 3, Step: 282 / 2250 Loss: 0.2023\n",
      "Epoch: 3 / 3, Step: 283 / 2250 Loss: 0.1534\n",
      "Epoch: 3 / 3, Step: 284 / 2250 Loss: 0.3523\n",
      "Epoch: 3 / 3, Step: 285 / 2250 Loss: 0.1351\n",
      "Epoch: 3 / 3, Step: 286 / 2250 Loss: 0.2920\n",
      "Epoch: 3 / 3, Step: 287 / 2250 Loss: 0.2446\n",
      "Epoch: 3 / 3, Step: 288 / 2250 Loss: 0.1259\n",
      "Epoch: 3 / 3, Step: 289 / 2250 Loss: 0.2734\n",
      "Epoch: 3 / 3, Step: 290 / 2250 Loss: 0.2335\n",
      "Epoch: 3 / 3, Step: 291 / 2250 Loss: 0.1456\n",
      "Epoch: 3 / 3, Step: 292 / 2250 Loss: 0.3578\n",
      "Epoch: 3 / 3, Step: 293 / 2250 Loss: 0.1565\n",
      "Epoch: 3 / 3, Step: 294 / 2250 Loss: 0.3113\n",
      "Epoch: 3 / 3, Step: 295 / 2250 Loss: 0.1611\n",
      "Epoch: 3 / 3, Step: 296 / 2250 Loss: 0.0829\n",
      "Epoch: 3 / 3, Step: 297 / 2250 Loss: 0.2855\n",
      "Epoch: 3 / 3, Step: 298 / 2250 Loss: 0.2614\n",
      "Epoch: 3 / 3, Step: 299 / 2250 Loss: 0.5286\n",
      "Epoch: 3 / 3, Step: 300 / 2250 Loss: 0.1768\n",
      "Epoch: 3 / 3, Step: 301 / 2250 Loss: 0.1034\n",
      "Epoch: 3 / 3, Step: 302 / 2250 Loss: 0.3531\n",
      "Epoch: 3 / 3, Step: 303 / 2250 Loss: 0.3810\n",
      "Epoch: 3 / 3, Step: 304 / 2250 Loss: 0.2500\n",
      "Epoch: 3 / 3, Step: 305 / 2250 Loss: 0.2545\n",
      "Epoch: 3 / 3, Step: 306 / 2250 Loss: 0.1627\n",
      "Epoch: 3 / 3, Step: 307 / 2250 Loss: 0.1701\n",
      "Epoch: 3 / 3, Step: 308 / 2250 Loss: 0.1311\n",
      "Epoch: 3 / 3, Step: 309 / 2250 Loss: 0.1533\n",
      "Epoch: 3 / 3, Step: 310 / 2250 Loss: 0.2395\n",
      "Epoch: 3 / 3, Step: 311 / 2250 Loss: 0.3185\n",
      "Epoch: 3 / 3, Step: 312 / 2250 Loss: 0.1464\n",
      "Epoch: 3 / 3, Step: 313 / 2250 Loss: 0.1979\n",
      "Epoch: 3 / 3, Step: 314 / 2250 Loss: 0.4718\n",
      "Epoch: 3 / 3, Step: 315 / 2250 Loss: 0.3172\n",
      "Epoch: 3 / 3, Step: 316 / 2250 Loss: 0.4140\n",
      "Epoch: 3 / 3, Step: 317 / 2250 Loss: 0.2753\n",
      "Epoch: 3 / 3, Step: 318 / 2250 Loss: 0.2353\n",
      "Epoch: 3 / 3, Step: 319 / 2250 Loss: 0.3570\n",
      "Epoch: 3 / 3, Step: 320 / 2250 Loss: 0.2579\n",
      "Epoch: 3 / 3, Step: 321 / 2250 Loss: 0.1336\n",
      "Epoch: 3 / 3, Step: 322 / 2250 Loss: 0.1162\n",
      "Epoch: 3 / 3, Step: 323 / 2250 Loss: 0.3106\n",
      "Epoch: 3 / 3, Step: 324 / 2250 Loss: 0.4827\n",
      "Epoch: 3 / 3, Step: 325 / 2250 Loss: 0.4740\n",
      "Epoch: 3 / 3, Step: 326 / 2250 Loss: 0.1646\n",
      "Epoch: 3 / 3, Step: 327 / 2250 Loss: 0.1709\n",
      "Epoch: 3 / 3, Step: 328 / 2250 Loss: 0.2195\n",
      "Epoch: 3 / 3, Step: 329 / 2250 Loss: 0.2659\n",
      "Epoch: 3 / 3, Step: 330 / 2250 Loss: 0.3017\n",
      "Epoch: 3 / 3, Step: 331 / 2250 Loss: 0.3364\n",
      "Epoch: 3 / 3, Step: 332 / 2250 Loss: 0.0901\n",
      "Epoch: 3 / 3, Step: 333 / 2250 Loss: 0.2339\n",
      "Epoch: 3 / 3, Step: 334 / 2250 Loss: 0.3495\n",
      "Epoch: 3 / 3, Step: 335 / 2250 Loss: 0.4769\n",
      "Epoch: 3 / 3, Step: 336 / 2250 Loss: 0.2619\n",
      "Epoch: 3 / 3, Step: 337 / 2250 Loss: 0.1267\n",
      "Epoch: 3 / 3, Step: 338 / 2250 Loss: 0.1982\n",
      "Epoch: 3 / 3, Step: 339 / 2250 Loss: 0.1232\n",
      "Epoch: 3 / 3, Step: 340 / 2250 Loss: 0.1276\n",
      "Epoch: 3 / 3, Step: 341 / 2250 Loss: 0.2524\n",
      "Epoch: 3 / 3, Step: 342 / 2250 Loss: 0.1503\n",
      "Epoch: 3 / 3, Step: 343 / 2250 Loss: 0.3110\n",
      "Epoch: 3 / 3, Step: 344 / 2250 Loss: 0.4257\n",
      "Epoch: 3 / 3, Step: 345 / 2250 Loss: 0.3184\n",
      "Epoch: 3 / 3, Step: 346 / 2250 Loss: 0.1813\n",
      "Epoch: 3 / 3, Step: 347 / 2250 Loss: 0.3847\n",
      "Epoch: 3 / 3, Step: 348 / 2250 Loss: 0.2844\n",
      "Epoch: 3 / 3, Step: 349 / 2250 Loss: 0.2850\n",
      "Epoch: 3 / 3, Step: 350 / 2250 Loss: 0.3490\n",
      "Epoch: 3 / 3, Step: 351 / 2250 Loss: 0.1545\n",
      "Epoch: 3 / 3, Step: 352 / 2250 Loss: 0.2681\n",
      "Epoch: 3 / 3, Step: 353 / 2250 Loss: 0.2540\n",
      "Epoch: 3 / 3, Step: 354 / 2250 Loss: 0.1668\n",
      "Epoch: 3 / 3, Step: 355 / 2250 Loss: 0.4401\n",
      "Epoch: 3 / 3, Step: 356 / 2250 Loss: 0.2407\n",
      "Epoch: 3 / 3, Step: 357 / 2250 Loss: 0.2405\n",
      "Epoch: 3 / 3, Step: 358 / 2250 Loss: 0.0432\n",
      "Epoch: 3 / 3, Step: 359 / 2250 Loss: 0.2854\n",
      "Epoch: 3 / 3, Step: 360 / 2250 Loss: 0.0930\n",
      "Epoch: 3 / 3, Step: 361 / 2250 Loss: 0.3381\n",
      "Epoch: 3 / 3, Step: 362 / 2250 Loss: 0.4333\n",
      "Epoch: 3 / 3, Step: 363 / 2250 Loss: 0.4106\n",
      "Epoch: 3 / 3, Step: 364 / 2250 Loss: 0.2847\n",
      "Epoch: 3 / 3, Step: 365 / 2250 Loss: 0.3321\n",
      "Epoch: 3 / 3, Step: 366 / 2250 Loss: 0.2259\n",
      "Epoch: 3 / 3, Step: 367 / 2250 Loss: 0.4724\n",
      "Epoch: 3 / 3, Step: 368 / 2250 Loss: 0.4914\n",
      "Epoch: 3 / 3, Step: 369 / 2250 Loss: 0.1272\n",
      "Epoch: 3 / 3, Step: 370 / 2250 Loss: 0.2587\n",
      "Epoch: 3 / 3, Step: 371 / 2250 Loss: 0.1065\n",
      "Epoch: 3 / 3, Step: 372 / 2250 Loss: 0.3716\n",
      "Epoch: 3 / 3, Step: 373 / 2250 Loss: 0.2287\n",
      "Epoch: 3 / 3, Step: 374 / 2250 Loss: 0.3301\n",
      "Epoch: 3 / 3, Step: 375 / 2250 Loss: 0.1950\n",
      "Epoch: 3 / 3, Step: 376 / 2250 Loss: 0.1362\n",
      "Epoch: 3 / 3, Step: 377 / 2250 Loss: 0.2111\n",
      "Epoch: 3 / 3, Step: 378 / 2250 Loss: 0.3152\n",
      "Epoch: 3 / 3, Step: 379 / 2250 Loss: 0.1570\n",
      "Epoch: 3 / 3, Step: 380 / 2250 Loss: 0.2734\n",
      "Epoch: 3 / 3, Step: 381 / 2250 Loss: 0.1990\n",
      "Epoch: 3 / 3, Step: 382 / 2250 Loss: 0.3254\n",
      "Epoch: 3 / 3, Step: 383 / 2250 Loss: 0.1190\n",
      "Epoch: 3 / 3, Step: 384 / 2250 Loss: 0.2005\n",
      "Epoch: 3 / 3, Step: 385 / 2250 Loss: 0.1539\n",
      "Epoch: 3 / 3, Step: 386 / 2250 Loss: 0.2354\n",
      "Epoch: 3 / 3, Step: 387 / 2250 Loss: 0.3478\n",
      "Epoch: 3 / 3, Step: 388 / 2250 Loss: 0.2873\n",
      "Epoch: 3 / 3, Step: 389 / 2250 Loss: 0.2696\n",
      "Epoch: 3 / 3, Step: 390 / 2250 Loss: 0.0420\n",
      "Epoch: 3 / 3, Step: 391 / 2250 Loss: 0.1398\n",
      "Epoch: 3 / 3, Step: 392 / 2250 Loss: 0.4820\n",
      "Epoch: 3 / 3, Step: 393 / 2250 Loss: 0.1601\n",
      "Epoch: 3 / 3, Step: 394 / 2250 Loss: 0.1886\n",
      "Epoch: 3 / 3, Step: 395 / 2250 Loss: 0.2055\n",
      "Epoch: 3 / 3, Step: 396 / 2250 Loss: 0.3020\n",
      "Epoch: 3 / 3, Step: 397 / 2250 Loss: 0.1967\n",
      "Epoch: 3 / 3, Step: 398 / 2250 Loss: 0.2090\n",
      "Epoch: 3 / 3, Step: 399 / 2250 Loss: 0.1477\n",
      "Epoch: 3 / 3, Step: 400 / 2250 Loss: 0.4922\n",
      "Epoch: 3 / 3, Step: 401 / 2250 Loss: 0.0776\n",
      "Epoch: 3 / 3, Step: 402 / 2250 Loss: 0.2077\n",
      "Epoch: 3 / 3, Step: 403 / 2250 Loss: 0.2070\n",
      "Epoch: 3 / 3, Step: 404 / 2250 Loss: 0.2953\n",
      "Epoch: 3 / 3, Step: 405 / 2250 Loss: 0.2955\n",
      "Epoch: 3 / 3, Step: 406 / 2250 Loss: 0.2824\n",
      "Epoch: 3 / 3, Step: 407 / 2250 Loss: 0.3036\n",
      "Epoch: 3 / 3, Step: 408 / 2250 Loss: 0.1949\n",
      "Epoch: 3 / 3, Step: 409 / 2250 Loss: 0.4511\n",
      "Epoch: 3 / 3, Step: 410 / 2250 Loss: 0.3873\n",
      "Epoch: 3 / 3, Step: 411 / 2250 Loss: 0.2945\n",
      "Epoch: 3 / 3, Step: 412 / 2250 Loss: 0.2656\n",
      "Epoch: 3 / 3, Step: 413 / 2250 Loss: 0.1299\n",
      "Epoch: 3 / 3, Step: 414 / 2250 Loss: 0.6467\n",
      "Epoch: 3 / 3, Step: 415 / 2250 Loss: 0.3474\n",
      "Epoch: 3 / 3, Step: 416 / 2250 Loss: 0.4505\n",
      "Epoch: 3 / 3, Step: 417 / 2250 Loss: 0.1893\n",
      "Epoch: 3 / 3, Step: 418 / 2250 Loss: 0.3472\n",
      "Epoch: 3 / 3, Step: 419 / 2250 Loss: 0.2007\n",
      "Epoch: 3 / 3, Step: 420 / 2250 Loss: 0.2698\n",
      "Epoch: 3 / 3, Step: 421 / 2250 Loss: 0.0929\n",
      "Epoch: 3 / 3, Step: 422 / 2250 Loss: 0.6054\n",
      "Epoch: 3 / 3, Step: 423 / 2250 Loss: 0.2846\n",
      "Epoch: 3 / 3, Step: 424 / 2250 Loss: 0.3061\n",
      "Epoch: 3 / 3, Step: 425 / 2250 Loss: 0.1958\n",
      "Epoch: 3 / 3, Step: 426 / 2250 Loss: 0.2529\n",
      "Epoch: 3 / 3, Step: 427 / 2250 Loss: 0.2389\n",
      "Epoch: 3 / 3, Step: 428 / 2250 Loss: 0.1913\n",
      "Epoch: 3 / 3, Step: 429 / 2250 Loss: 0.3280\n",
      "Epoch: 3 / 3, Step: 430 / 2250 Loss: 0.2141\n",
      "Epoch: 3 / 3, Step: 431 / 2250 Loss: 0.2057\n",
      "Epoch: 3 / 3, Step: 432 / 2250 Loss: 0.2867\n",
      "Epoch: 3 / 3, Step: 433 / 2250 Loss: 0.2048\n",
      "Epoch: 3 / 3, Step: 434 / 2250 Loss: 0.1340\n",
      "Epoch: 3 / 3, Step: 435 / 2250 Loss: 0.1708\n",
      "Epoch: 3 / 3, Step: 436 / 2250 Loss: 0.3181\n",
      "Epoch: 3 / 3, Step: 437 / 2250 Loss: 0.0911\n",
      "Epoch: 3 / 3, Step: 438 / 2250 Loss: 0.3635\n",
      "Epoch: 3 / 3, Step: 439 / 2250 Loss: 0.2650\n",
      "Epoch: 3 / 3, Step: 440 / 2250 Loss: 0.3888\n",
      "Epoch: 3 / 3, Step: 441 / 2250 Loss: 0.2669\n",
      "Epoch: 3 / 3, Step: 442 / 2250 Loss: 0.2413\n",
      "Epoch: 3 / 3, Step: 443 / 2250 Loss: 0.6280\n",
      "Epoch: 3 / 3, Step: 444 / 2250 Loss: 0.1117\n",
      "Epoch: 3 / 3, Step: 445 / 2250 Loss: 0.2318\n",
      "Epoch: 3 / 3, Step: 446 / 2250 Loss: 0.2673\n",
      "Epoch: 3 / 3, Step: 447 / 2250 Loss: 0.1421\n",
      "Epoch: 3 / 3, Step: 448 / 2250 Loss: 0.1897\n",
      "Epoch: 3 / 3, Step: 449 / 2250 Loss: 0.2232\n",
      "Epoch: 3 / 3, Step: 450 / 2250 Loss: 0.1239\n",
      "Epoch: 3 / 3, Step: 451 / 2250 Loss: 0.2951\n",
      "Epoch: 3 / 3, Step: 452 / 2250 Loss: 0.3700\n",
      "Epoch: 3 / 3, Step: 453 / 2250 Loss: 0.2033\n",
      "Epoch: 3 / 3, Step: 454 / 2250 Loss: 0.2855\n",
      "Epoch: 3 / 3, Step: 455 / 2250 Loss: 0.0975\n",
      "Epoch: 3 / 3, Step: 456 / 2250 Loss: 0.2438\n",
      "Epoch: 3 / 3, Step: 457 / 2250 Loss: 0.1751\n",
      "Epoch: 3 / 3, Step: 458 / 2250 Loss: 0.1736\n",
      "Epoch: 3 / 3, Step: 459 / 2250 Loss: 0.3115\n",
      "Epoch: 3 / 3, Step: 460 / 2250 Loss: 0.1029\n",
      "Epoch: 3 / 3, Step: 461 / 2250 Loss: 0.1040\n",
      "Epoch: 3 / 3, Step: 462 / 2250 Loss: 0.0781\n",
      "Epoch: 3 / 3, Step: 463 / 2250 Loss: 0.2486\n",
      "Epoch: 3 / 3, Step: 464 / 2250 Loss: 0.3909\n",
      "Epoch: 3 / 3, Step: 465 / 2250 Loss: 0.3429\n",
      "Epoch: 3 / 3, Step: 466 / 2250 Loss: 0.1567\n",
      "Epoch: 3 / 3, Step: 467 / 2250 Loss: 0.1310\n",
      "Epoch: 3 / 3, Step: 468 / 2250 Loss: 0.3215\n",
      "Epoch: 3 / 3, Step: 469 / 2250 Loss: 0.2707\n",
      "Epoch: 3 / 3, Step: 470 / 2250 Loss: 0.3331\n",
      "Epoch: 3 / 3, Step: 471 / 2250 Loss: 0.2747\n",
      "Epoch: 3 / 3, Step: 472 / 2250 Loss: 0.3437\n",
      "Epoch: 3 / 3, Step: 473 / 2250 Loss: 0.2447\n",
      "Epoch: 3 / 3, Step: 474 / 2250 Loss: 0.1579\n",
      "Epoch: 3 / 3, Step: 475 / 2250 Loss: 0.2302\n",
      "Epoch: 3 / 3, Step: 476 / 2250 Loss: 0.2976\n",
      "Epoch: 3 / 3, Step: 477 / 2250 Loss: 0.1931\n",
      "Epoch: 3 / 3, Step: 478 / 2250 Loss: 0.1815\n",
      "Epoch: 3 / 3, Step: 479 / 2250 Loss: 0.3073\n",
      "Epoch: 3 / 3, Step: 480 / 2250 Loss: 0.0836\n",
      "Epoch: 3 / 3, Step: 481 / 2250 Loss: 0.3703\n",
      "Epoch: 3 / 3, Step: 482 / 2250 Loss: 0.3211\n",
      "Epoch: 3 / 3, Step: 483 / 2250 Loss: 0.2754\n",
      "Epoch: 3 / 3, Step: 484 / 2250 Loss: 0.1641\n",
      "Epoch: 3 / 3, Step: 485 / 2250 Loss: 0.3100\n",
      "Epoch: 3 / 3, Step: 486 / 2250 Loss: 0.1758\n",
      "Epoch: 3 / 3, Step: 487 / 2250 Loss: 0.2088\n",
      "Epoch: 3 / 3, Step: 488 / 2250 Loss: 0.2444\n",
      "Epoch: 3 / 3, Step: 489 / 2250 Loss: 0.2303\n",
      "Epoch: 3 / 3, Step: 490 / 2250 Loss: 0.3331\n",
      "Epoch: 3 / 3, Step: 491 / 2250 Loss: 0.3325\n",
      "Epoch: 3 / 3, Step: 492 / 2250 Loss: 0.1910\n",
      "Epoch: 3 / 3, Step: 493 / 2250 Loss: 0.1946\n",
      "Epoch: 3 / 3, Step: 494 / 2250 Loss: 0.2832\n",
      "Epoch: 3 / 3, Step: 495 / 2250 Loss: 0.2238\n",
      "Epoch: 3 / 3, Step: 496 / 2250 Loss: 0.2222\n",
      "Epoch: 3 / 3, Step: 497 / 2250 Loss: 0.3745\n",
      "Epoch: 3 / 3, Step: 498 / 2250 Loss: 0.3739\n",
      "Epoch: 3 / 3, Step: 499 / 2250 Loss: 0.3697\n",
      "Epoch: 3 / 3, Step: 500 / 2250 Loss: 0.4537\n",
      "Epoch: 3 / 3, Step: 501 / 2250 Loss: 0.4038\n",
      "Epoch: 3 / 3, Step: 502 / 2250 Loss: 0.3292\n",
      "Epoch: 3 / 3, Step: 503 / 2250 Loss: 0.2698\n",
      "Epoch: 3 / 3, Step: 504 / 2250 Loss: 0.1810\n",
      "Epoch: 3 / 3, Step: 505 / 2250 Loss: 0.2634\n",
      "Epoch: 3 / 3, Step: 506 / 2250 Loss: 0.1493\n",
      "Epoch: 3 / 3, Step: 507 / 2250 Loss: 0.0928\n",
      "Epoch: 3 / 3, Step: 508 / 2250 Loss: 0.2662\n",
      "Epoch: 3 / 3, Step: 509 / 2250 Loss: 0.1693\n",
      "Epoch: 3 / 3, Step: 510 / 2250 Loss: 0.1922\n",
      "Epoch: 3 / 3, Step: 511 / 2250 Loss: 0.1894\n",
      "Epoch: 3 / 3, Step: 512 / 2250 Loss: 0.0855\n",
      "Epoch: 3 / 3, Step: 513 / 2250 Loss: 0.1718\n",
      "Epoch: 3 / 3, Step: 514 / 2250 Loss: 0.4720\n",
      "Epoch: 3 / 3, Step: 515 / 2250 Loss: 0.1853\n",
      "Epoch: 3 / 3, Step: 516 / 2250 Loss: 0.2198\n",
      "Epoch: 3 / 3, Step: 517 / 2250 Loss: 0.4763\n",
      "Epoch: 3 / 3, Step: 518 / 2250 Loss: 0.0802\n",
      "Epoch: 3 / 3, Step: 519 / 2250 Loss: 0.4049\n",
      "Epoch: 3 / 3, Step: 520 / 2250 Loss: 0.1713\n",
      "Epoch: 3 / 3, Step: 521 / 2250 Loss: 0.2116\n",
      "Epoch: 3 / 3, Step: 522 / 2250 Loss: 0.3174\n",
      "Epoch: 3 / 3, Step: 523 / 2250 Loss: 0.5437\n",
      "Epoch: 3 / 3, Step: 524 / 2250 Loss: 0.0778\n",
      "Epoch: 3 / 3, Step: 525 / 2250 Loss: 0.2048\n",
      "Epoch: 3 / 3, Step: 526 / 2250 Loss: 0.0982\n",
      "Epoch: 3 / 3, Step: 527 / 2250 Loss: 0.3520\n",
      "Epoch: 3 / 3, Step: 528 / 2250 Loss: 0.1355\n",
      "Epoch: 3 / 3, Step: 529 / 2250 Loss: 0.3173\n",
      "Epoch: 3 / 3, Step: 530 / 2250 Loss: 0.5363\n",
      "Epoch: 3 / 3, Step: 531 / 2250 Loss: 0.2787\n",
      "Epoch: 3 / 3, Step: 532 / 2250 Loss: 0.2461\n",
      "Epoch: 3 / 3, Step: 533 / 2250 Loss: 0.3252\n",
      "Epoch: 3 / 3, Step: 534 / 2250 Loss: 0.1656\n",
      "Epoch: 3 / 3, Step: 535 / 2250 Loss: 0.3221\n",
      "Epoch: 3 / 3, Step: 536 / 2250 Loss: 0.1930\n",
      "Epoch: 3 / 3, Step: 537 / 2250 Loss: 0.1796\n",
      "Epoch: 3 / 3, Step: 538 / 2250 Loss: 0.3593\n",
      "Epoch: 3 / 3, Step: 539 / 2250 Loss: 0.2233\n",
      "Epoch: 3 / 3, Step: 540 / 2250 Loss: 0.1952\n",
      "Epoch: 3 / 3, Step: 541 / 2250 Loss: 0.2803\n",
      "Epoch: 3 / 3, Step: 542 / 2250 Loss: 0.3104\n",
      "Epoch: 3 / 3, Step: 543 / 2250 Loss: 0.3046\n",
      "Epoch: 3 / 3, Step: 544 / 2250 Loss: 0.3237\n",
      "Epoch: 3 / 3, Step: 545 / 2250 Loss: 0.1944\n",
      "Epoch: 3 / 3, Step: 546 / 2250 Loss: 0.3178\n",
      "Epoch: 3 / 3, Step: 547 / 2250 Loss: 0.2044\n",
      "Epoch: 3 / 3, Step: 548 / 2250 Loss: 0.2279\n",
      "Epoch: 3 / 3, Step: 549 / 2250 Loss: 0.3826\n",
      "Epoch: 3 / 3, Step: 550 / 2250 Loss: 0.0658\n",
      "Epoch: 3 / 3, Step: 551 / 2250 Loss: 0.2083\n",
      "Epoch: 3 / 3, Step: 552 / 2250 Loss: 0.0815\n",
      "Epoch: 3 / 3, Step: 553 / 2250 Loss: 0.1365\n",
      "Epoch: 3 / 3, Step: 554 / 2250 Loss: 0.1354\n",
      "Epoch: 3 / 3, Step: 555 / 2250 Loss: 0.3215\n",
      "Epoch: 3 / 3, Step: 556 / 2250 Loss: 0.2756\n",
      "Epoch: 3 / 3, Step: 557 / 2250 Loss: 0.1740\n",
      "Epoch: 3 / 3, Step: 558 / 2250 Loss: 0.4957\n",
      "Epoch: 3 / 3, Step: 559 / 2250 Loss: 0.1402\n",
      "Epoch: 3 / 3, Step: 560 / 2250 Loss: 0.2788\n",
      "Epoch: 3 / 3, Step: 561 / 2250 Loss: 0.3430\n",
      "Epoch: 3 / 3, Step: 562 / 2250 Loss: 0.1831\n",
      "Epoch: 3 / 3, Step: 563 / 2250 Loss: 0.1360\n",
      "Epoch: 3 / 3, Step: 564 / 2250 Loss: 0.1526\n",
      "Epoch: 3 / 3, Step: 565 / 2250 Loss: 0.1311\n",
      "Epoch: 3 / 3, Step: 566 / 2250 Loss: 0.1434\n",
      "Epoch: 3 / 3, Step: 567 / 2250 Loss: 0.1416\n",
      "Epoch: 3 / 3, Step: 568 / 2250 Loss: 0.2178\n",
      "Epoch: 3 / 3, Step: 569 / 2250 Loss: 0.1251\n",
      "Epoch: 3 / 3, Step: 570 / 2250 Loss: 0.3163\n",
      "Epoch: 3 / 3, Step: 571 / 2250 Loss: 0.3546\n",
      "Epoch: 3 / 3, Step: 572 / 2250 Loss: 0.1730\n",
      "Epoch: 3 / 3, Step: 573 / 2250 Loss: 0.3643\n",
      "Epoch: 3 / 3, Step: 574 / 2250 Loss: 0.1610\n",
      "Epoch: 3 / 3, Step: 575 / 2250 Loss: 0.0556\n",
      "Epoch: 3 / 3, Step: 576 / 2250 Loss: 0.1425\n",
      "Epoch: 3 / 3, Step: 577 / 2250 Loss: 0.1702\n",
      "Epoch: 3 / 3, Step: 578 / 2250 Loss: 0.2719\n",
      "Epoch: 3 / 3, Step: 579 / 2250 Loss: 0.5444\n",
      "Epoch: 3 / 3, Step: 580 / 2250 Loss: 0.1303\n",
      "Epoch: 3 / 3, Step: 581 / 2250 Loss: 0.1652\n",
      "Epoch: 3 / 3, Step: 582 / 2250 Loss: 0.2318\n",
      "Epoch: 3 / 3, Step: 583 / 2250 Loss: 0.3224\n",
      "Epoch: 3 / 3, Step: 584 / 2250 Loss: 0.2909\n",
      "Epoch: 3 / 3, Step: 585 / 2250 Loss: 0.3854\n",
      "Epoch: 3 / 3, Step: 586 / 2250 Loss: 0.1363\n",
      "Epoch: 3 / 3, Step: 587 / 2250 Loss: 0.0959\n",
      "Epoch: 3 / 3, Step: 588 / 2250 Loss: 0.1274\n",
      "Epoch: 3 / 3, Step: 589 / 2250 Loss: 0.1849\n",
      "Epoch: 3 / 3, Step: 590 / 2250 Loss: 0.2266\n",
      "Epoch: 3 / 3, Step: 591 / 2250 Loss: 0.3995\n",
      "Epoch: 3 / 3, Step: 592 / 2250 Loss: 0.1407\n",
      "Epoch: 3 / 3, Step: 593 / 2250 Loss: 0.3308\n",
      "Epoch: 3 / 3, Step: 594 / 2250 Loss: 0.2621\n",
      "Epoch: 3 / 3, Step: 595 / 2250 Loss: 0.2776\n",
      "Epoch: 3 / 3, Step: 596 / 2250 Loss: 0.2686\n",
      "Epoch: 3 / 3, Step: 597 / 2250 Loss: 0.3044\n",
      "Epoch: 3 / 3, Step: 598 / 2250 Loss: 0.1925\n",
      "Epoch: 3 / 3, Step: 599 / 2250 Loss: 0.1373\n",
      "Epoch: 3 / 3, Step: 600 / 2250 Loss: 0.2339\n",
      "Epoch: 3 / 3, Step: 601 / 2250 Loss: 0.1736\n",
      "Epoch: 3 / 3, Step: 602 / 2250 Loss: 0.3227\n",
      "Epoch: 3 / 3, Step: 603 / 2250 Loss: 0.2260\n",
      "Epoch: 3 / 3, Step: 604 / 2250 Loss: 0.2604\n",
      "Epoch: 3 / 3, Step: 605 / 2250 Loss: 0.5442\n",
      "Epoch: 3 / 3, Step: 606 / 2250 Loss: 0.3103\n",
      "Epoch: 3 / 3, Step: 607 / 2250 Loss: 0.0878\n",
      "Epoch: 3 / 3, Step: 608 / 2250 Loss: 0.1668\n",
      "Epoch: 3 / 3, Step: 609 / 2250 Loss: 0.2114\n",
      "Epoch: 3 / 3, Step: 610 / 2250 Loss: 0.0570\n",
      "Epoch: 3 / 3, Step: 611 / 2250 Loss: 0.0752\n",
      "Epoch: 3 / 3, Step: 612 / 2250 Loss: 0.2709\n",
      "Epoch: 3 / 3, Step: 613 / 2250 Loss: 0.3339\n",
      "Epoch: 3 / 3, Step: 614 / 2250 Loss: 0.1968\n",
      "Epoch: 3 / 3, Step: 615 / 2250 Loss: 0.1794\n",
      "Epoch: 3 / 3, Step: 616 / 2250 Loss: 0.1020\n",
      "Epoch: 3 / 3, Step: 617 / 2250 Loss: 0.2507\n",
      "Epoch: 3 / 3, Step: 618 / 2250 Loss: 0.4123\n",
      "Epoch: 3 / 3, Step: 619 / 2250 Loss: 0.1310\n",
      "Epoch: 3 / 3, Step: 620 / 2250 Loss: 0.1945\n",
      "Epoch: 3 / 3, Step: 621 / 2250 Loss: 0.0666\n",
      "Epoch: 3 / 3, Step: 622 / 2250 Loss: 0.3614\n",
      "Epoch: 3 / 3, Step: 623 / 2250 Loss: 0.1417\n",
      "Epoch: 3 / 3, Step: 624 / 2250 Loss: 0.1665\n",
      "Epoch: 3 / 3, Step: 625 / 2250 Loss: 0.3661\n",
      "Epoch: 3 / 3, Step: 626 / 2250 Loss: 0.1597\n",
      "Epoch: 3 / 3, Step: 627 / 2250 Loss: 0.0611\n",
      "Epoch: 3 / 3, Step: 628 / 2250 Loss: 0.1518\n",
      "Epoch: 3 / 3, Step: 629 / 2250 Loss: 0.3249\n",
      "Epoch: 3 / 3, Step: 630 / 2250 Loss: 0.1583\n",
      "Epoch: 3 / 3, Step: 631 / 2250 Loss: 0.1522\n",
      "Epoch: 3 / 3, Step: 632 / 2250 Loss: 0.1231\n",
      "Epoch: 3 / 3, Step: 633 / 2250 Loss: 0.3692\n",
      "Epoch: 3 / 3, Step: 634 / 2250 Loss: 0.4038\n",
      "Epoch: 3 / 3, Step: 635 / 2250 Loss: 0.1524\n",
      "Epoch: 3 / 3, Step: 636 / 2250 Loss: 0.2308\n",
      "Epoch: 3 / 3, Step: 637 / 2250 Loss: 0.1661\n",
      "Epoch: 3 / 3, Step: 638 / 2250 Loss: 0.2277\n",
      "Epoch: 3 / 3, Step: 639 / 2250 Loss: 0.0930\n",
      "Epoch: 3 / 3, Step: 640 / 2250 Loss: 0.3353\n",
      "Epoch: 3 / 3, Step: 641 / 2250 Loss: 0.2216\n",
      "Epoch: 3 / 3, Step: 642 / 2250 Loss: 0.0807\n",
      "Epoch: 3 / 3, Step: 643 / 2250 Loss: 0.2161\n",
      "Epoch: 3 / 3, Step: 644 / 2250 Loss: 0.4242\n",
      "Epoch: 3 / 3, Step: 645 / 2250 Loss: 0.4965\n",
      "Epoch: 3 / 3, Step: 646 / 2250 Loss: 0.1242\n",
      "Epoch: 3 / 3, Step: 647 / 2250 Loss: 0.2113\n",
      "Epoch: 3 / 3, Step: 648 / 2250 Loss: 0.2286\n",
      "Epoch: 3 / 3, Step: 649 / 2250 Loss: 0.0493\n",
      "Epoch: 3 / 3, Step: 650 / 2250 Loss: 0.1027\n",
      "Epoch: 3 / 3, Step: 651 / 2250 Loss: 0.2608\n",
      "Epoch: 3 / 3, Step: 652 / 2250 Loss: 0.2086\n",
      "Epoch: 3 / 3, Step: 653 / 2250 Loss: 0.2954\n",
      "Epoch: 3 / 3, Step: 654 / 2250 Loss: 0.3044\n",
      "Epoch: 3 / 3, Step: 655 / 2250 Loss: 0.1221\n",
      "Epoch: 3 / 3, Step: 656 / 2250 Loss: 0.0835\n",
      "Epoch: 3 / 3, Step: 657 / 2250 Loss: 0.1441\n",
      "Epoch: 3 / 3, Step: 658 / 2250 Loss: 0.2082\n",
      "Epoch: 3 / 3, Step: 659 / 2250 Loss: 0.1020\n",
      "Epoch: 3 / 3, Step: 660 / 2250 Loss: 0.3852\n",
      "Epoch: 3 / 3, Step: 661 / 2250 Loss: 0.0791\n",
      "Epoch: 3 / 3, Step: 662 / 2250 Loss: 0.1821\n",
      "Epoch: 3 / 3, Step: 663 / 2250 Loss: 0.1708\n",
      "Epoch: 3 / 3, Step: 664 / 2250 Loss: 0.1026\n",
      "Epoch: 3 / 3, Step: 665 / 2250 Loss: 0.3120\n",
      "Epoch: 3 / 3, Step: 666 / 2250 Loss: 0.1497\n",
      "Epoch: 3 / 3, Step: 667 / 2250 Loss: 0.1638\n",
      "Epoch: 3 / 3, Step: 668 / 2250 Loss: 0.1884\n",
      "Epoch: 3 / 3, Step: 669 / 2250 Loss: 0.4277\n",
      "Epoch: 3 / 3, Step: 670 / 2250 Loss: 0.3063\n",
      "Epoch: 3 / 3, Step: 671 / 2250 Loss: 0.5246\n",
      "Epoch: 3 / 3, Step: 672 / 2250 Loss: 0.2094\n",
      "Epoch: 3 / 3, Step: 673 / 2250 Loss: 0.2252\n",
      "Epoch: 3 / 3, Step: 674 / 2250 Loss: 0.0786\n",
      "Epoch: 3 / 3, Step: 675 / 2250 Loss: 0.1057\n",
      "Epoch: 3 / 3, Step: 676 / 2250 Loss: 0.0541\n",
      "Epoch: 3 / 3, Step: 677 / 2250 Loss: 0.4429\n",
      "Epoch: 3 / 3, Step: 678 / 2250 Loss: 0.4977\n",
      "Epoch: 3 / 3, Step: 679 / 2250 Loss: 0.3220\n",
      "Epoch: 3 / 3, Step: 680 / 2250 Loss: 0.1998\n",
      "Epoch: 3 / 3, Step: 681 / 2250 Loss: 0.2367\n",
      "Epoch: 3 / 3, Step: 682 / 2250 Loss: 0.2138\n",
      "Epoch: 3 / 3, Step: 683 / 2250 Loss: 0.3667\n",
      "Epoch: 3 / 3, Step: 684 / 2250 Loss: 0.3961\n",
      "Epoch: 3 / 3, Step: 685 / 2250 Loss: 0.2680\n",
      "Epoch: 3 / 3, Step: 686 / 2250 Loss: 0.0625\n",
      "Epoch: 3 / 3, Step: 687 / 2250 Loss: 0.2302\n",
      "Epoch: 3 / 3, Step: 688 / 2250 Loss: 0.2185\n",
      "Epoch: 3 / 3, Step: 689 / 2250 Loss: 0.1664\n",
      "Epoch: 3 / 3, Step: 690 / 2250 Loss: 0.2130\n",
      "Epoch: 3 / 3, Step: 691 / 2250 Loss: 0.1991\n",
      "Epoch: 3 / 3, Step: 692 / 2250 Loss: 0.3911\n",
      "Epoch: 3 / 3, Step: 693 / 2250 Loss: 0.2190\n",
      "Epoch: 3 / 3, Step: 694 / 2250 Loss: 0.1366\n",
      "Epoch: 3 / 3, Step: 695 / 2250 Loss: 0.2830\n",
      "Epoch: 3 / 3, Step: 696 / 2250 Loss: 0.4289\n",
      "Epoch: 3 / 3, Step: 697 / 2250 Loss: 0.1434\n",
      "Epoch: 3 / 3, Step: 698 / 2250 Loss: 0.4415\n",
      "Epoch: 3 / 3, Step: 699 / 2250 Loss: 0.3545\n",
      "Epoch: 3 / 3, Step: 700 / 2250 Loss: 0.5179\n",
      "Epoch: 3 / 3, Step: 701 / 2250 Loss: 0.1097\n",
      "Epoch: 3 / 3, Step: 702 / 2250 Loss: 0.3374\n",
      "Epoch: 3 / 3, Step: 703 / 2250 Loss: 0.3359\n",
      "Epoch: 3 / 3, Step: 704 / 2250 Loss: 0.4231\n",
      "Epoch: 3 / 3, Step: 705 / 2250 Loss: 0.3021\n",
      "Epoch: 3 / 3, Step: 706 / 2250 Loss: 0.0671\n",
      "Epoch: 3 / 3, Step: 707 / 2250 Loss: 0.4526\n",
      "Epoch: 3 / 3, Step: 708 / 2250 Loss: 0.3004\n",
      "Epoch: 3 / 3, Step: 709 / 2250 Loss: 0.3073\n",
      "Epoch: 3 / 3, Step: 710 / 2250 Loss: 0.2607\n",
      "Epoch: 3 / 3, Step: 711 / 2250 Loss: 0.2776\n",
      "Epoch: 3 / 3, Step: 712 / 2250 Loss: 0.1598\n",
      "Epoch: 3 / 3, Step: 713 / 2250 Loss: 0.3361\n",
      "Epoch: 3 / 3, Step: 714 / 2250 Loss: 0.1956\n",
      "Epoch: 3 / 3, Step: 715 / 2250 Loss: 0.2108\n",
      "Epoch: 3 / 3, Step: 716 / 2250 Loss: 0.1732\n",
      "Epoch: 3 / 3, Step: 717 / 2250 Loss: 0.3559\n",
      "Epoch: 3 / 3, Step: 718 / 2250 Loss: 0.2273\n",
      "Epoch: 3 / 3, Step: 719 / 2250 Loss: 0.1321\n",
      "Epoch: 3 / 3, Step: 720 / 2250 Loss: 0.2158\n",
      "Epoch: 3 / 3, Step: 721 / 2250 Loss: 0.3456\n",
      "Epoch: 3 / 3, Step: 722 / 2250 Loss: 0.1256\n",
      "Epoch: 3 / 3, Step: 723 / 2250 Loss: 0.1919\n",
      "Epoch: 3 / 3, Step: 724 / 2250 Loss: 0.3519\n",
      "Epoch: 3 / 3, Step: 725 / 2250 Loss: 0.1364\n",
      "Epoch: 3 / 3, Step: 726 / 2250 Loss: 0.2441\n",
      "Epoch: 3 / 3, Step: 727 / 2250 Loss: 0.2409\n",
      "Epoch: 3 / 3, Step: 728 / 2250 Loss: 0.1012\n",
      "Epoch: 3 / 3, Step: 729 / 2250 Loss: 0.2733\n",
      "Epoch: 3 / 3, Step: 730 / 2250 Loss: 0.4170\n",
      "Epoch: 3 / 3, Step: 731 / 2250 Loss: 0.2161\n",
      "Epoch: 3 / 3, Step: 732 / 2250 Loss: 0.2412\n",
      "Epoch: 3 / 3, Step: 733 / 2250 Loss: 0.3629\n",
      "Epoch: 3 / 3, Step: 734 / 2250 Loss: 0.2494\n",
      "Epoch: 3 / 3, Step: 735 / 2250 Loss: 0.3928\n",
      "Epoch: 3 / 3, Step: 736 / 2250 Loss: 0.2230\n",
      "Epoch: 3 / 3, Step: 737 / 2250 Loss: 0.1783\n",
      "Epoch: 3 / 3, Step: 738 / 2250 Loss: 0.1530\n",
      "Epoch: 3 / 3, Step: 739 / 2250 Loss: 0.1088\n",
      "Epoch: 3 / 3, Step: 740 / 2250 Loss: 0.3095\n",
      "Epoch: 3 / 3, Step: 741 / 2250 Loss: 0.1889\n",
      "Epoch: 3 / 3, Step: 742 / 2250 Loss: 0.3145\n",
      "Epoch: 3 / 3, Step: 743 / 2250 Loss: 0.4118\n",
      "Epoch: 3 / 3, Step: 744 / 2250 Loss: 0.1809\n",
      "Epoch: 3 / 3, Step: 745 / 2250 Loss: 0.3229\n",
      "Epoch: 3 / 3, Step: 746 / 2250 Loss: 0.1540\n",
      "Epoch: 3 / 3, Step: 747 / 2250 Loss: 0.1847\n",
      "Epoch: 3 / 3, Step: 748 / 2250 Loss: 0.1974\n",
      "Epoch: 3 / 3, Step: 749 / 2250 Loss: 0.1541\n",
      "Epoch: 3 / 3, Step: 750 / 2250 Loss: 0.2125\n",
      "Epoch: 3 / 3, Step: 751 / 2250 Loss: 0.3015\n",
      "Epoch: 3 / 3, Step: 752 / 2250 Loss: 0.5427\n",
      "Epoch: 3 / 3, Step: 753 / 2250 Loss: 0.1198\n",
      "Epoch: 3 / 3, Step: 754 / 2250 Loss: 0.2719\n",
      "Epoch: 3 / 3, Step: 755 / 2250 Loss: 0.1633\n",
      "Epoch: 3 / 3, Step: 756 / 2250 Loss: 0.2436\n",
      "Epoch: 3 / 3, Step: 757 / 2250 Loss: 0.2768\n",
      "Epoch: 3 / 3, Step: 758 / 2250 Loss: 0.0644\n",
      "Epoch: 3 / 3, Step: 759 / 2250 Loss: 0.2733\n",
      "Epoch: 3 / 3, Step: 760 / 2250 Loss: 0.1929\n",
      "Epoch: 3 / 3, Step: 761 / 2250 Loss: 0.1105\n",
      "Epoch: 3 / 3, Step: 762 / 2250 Loss: 0.1371\n",
      "Epoch: 3 / 3, Step: 763 / 2250 Loss: 0.3487\n",
      "Epoch: 3 / 3, Step: 764 / 2250 Loss: 0.2935\n",
      "Epoch: 3 / 3, Step: 765 / 2250 Loss: 0.4277\n",
      "Epoch: 3 / 3, Step: 766 / 2250 Loss: 0.4352\n",
      "Epoch: 3 / 3, Step: 767 / 2250 Loss: 0.1688\n",
      "Epoch: 3 / 3, Step: 768 / 2250 Loss: 0.1217\n",
      "Epoch: 3 / 3, Step: 769 / 2250 Loss: 0.2853\n",
      "Epoch: 3 / 3, Step: 770 / 2250 Loss: 0.2876\n",
      "Epoch: 3 / 3, Step: 771 / 2250 Loss: 0.3475\n",
      "Epoch: 3 / 3, Step: 772 / 2250 Loss: 0.2616\n",
      "Epoch: 3 / 3, Step: 773 / 2250 Loss: 0.1133\n",
      "Epoch: 3 / 3, Step: 774 / 2250 Loss: 0.1263\n",
      "Epoch: 3 / 3, Step: 775 / 2250 Loss: 0.4020\n",
      "Epoch: 3 / 3, Step: 776 / 2250 Loss: 0.1097\n",
      "Epoch: 3 / 3, Step: 777 / 2250 Loss: 0.1869\n",
      "Epoch: 3 / 3, Step: 778 / 2250 Loss: 0.1178\n",
      "Epoch: 3 / 3, Step: 779 / 2250 Loss: 0.4983\n",
      "Epoch: 3 / 3, Step: 780 / 2250 Loss: 0.1101\n",
      "Epoch: 3 / 3, Step: 781 / 2250 Loss: 0.1976\n",
      "Epoch: 3 / 3, Step: 782 / 2250 Loss: 0.3546\n",
      "Epoch: 3 / 3, Step: 783 / 2250 Loss: 0.5004\n",
      "Epoch: 3 / 3, Step: 784 / 2250 Loss: 0.3030\n",
      "Epoch: 3 / 3, Step: 785 / 2250 Loss: 0.1682\n",
      "Epoch: 3 / 3, Step: 786 / 2250 Loss: 0.3994\n",
      "Epoch: 3 / 3, Step: 787 / 2250 Loss: 0.1443\n",
      "Epoch: 3 / 3, Step: 788 / 2250 Loss: 0.1877\n",
      "Epoch: 3 / 3, Step: 789 / 2250 Loss: 0.2854\n",
      "Epoch: 3 / 3, Step: 790 / 2250 Loss: 0.3510\n",
      "Epoch: 3 / 3, Step: 791 / 2250 Loss: 0.2170\n",
      "Epoch: 3 / 3, Step: 792 / 2250 Loss: 0.0851\n",
      "Epoch: 3 / 3, Step: 793 / 2250 Loss: 0.3482\n",
      "Epoch: 3 / 3, Step: 794 / 2250 Loss: 0.2173\n",
      "Epoch: 3 / 3, Step: 795 / 2250 Loss: 0.3268\n",
      "Epoch: 3 / 3, Step: 796 / 2250 Loss: 0.1707\n",
      "Epoch: 3 / 3, Step: 797 / 2250 Loss: 0.1101\n",
      "Epoch: 3 / 3, Step: 798 / 2250 Loss: 0.4377\n",
      "Epoch: 3 / 3, Step: 799 / 2250 Loss: 0.2044\n",
      "Epoch: 3 / 3, Step: 800 / 2250 Loss: 0.2321\n",
      "Epoch: 3 / 3, Step: 801 / 2250 Loss: 0.2625\n",
      "Epoch: 3 / 3, Step: 802 / 2250 Loss: 0.3014\n",
      "Epoch: 3 / 3, Step: 803 / 2250 Loss: 0.2737\n",
      "Epoch: 3 / 3, Step: 804 / 2250 Loss: 0.1365\n",
      "Epoch: 3 / 3, Step: 805 / 2250 Loss: 0.5213\n",
      "Epoch: 3 / 3, Step: 806 / 2250 Loss: 0.0967\n",
      "Epoch: 3 / 3, Step: 807 / 2250 Loss: 0.1333\n",
      "Epoch: 3 / 3, Step: 808 / 2250 Loss: 0.3353\n",
      "Epoch: 3 / 3, Step: 809 / 2250 Loss: 0.3299\n",
      "Epoch: 3 / 3, Step: 810 / 2250 Loss: 0.2413\n",
      "Epoch: 3 / 3, Step: 811 / 2250 Loss: 0.1771\n",
      "Epoch: 3 / 3, Step: 812 / 2250 Loss: 0.2861\n",
      "Epoch: 3 / 3, Step: 813 / 2250 Loss: 0.1629\n",
      "Epoch: 3 / 3, Step: 814 / 2250 Loss: 0.3634\n",
      "Epoch: 3 / 3, Step: 815 / 2250 Loss: 0.2014\n",
      "Epoch: 3 / 3, Step: 816 / 2250 Loss: 0.4954\n",
      "Epoch: 3 / 3, Step: 817 / 2250 Loss: 0.2125\n",
      "Epoch: 3 / 3, Step: 818 / 2250 Loss: 0.1528\n",
      "Epoch: 3 / 3, Step: 819 / 2250 Loss: 0.2179\n",
      "Epoch: 3 / 3, Step: 820 / 2250 Loss: 0.1708\n",
      "Epoch: 3 / 3, Step: 821 / 2250 Loss: 0.2036\n",
      "Epoch: 3 / 3, Step: 822 / 2250 Loss: 0.2690\n",
      "Epoch: 3 / 3, Step: 823 / 2250 Loss: 0.1670\n",
      "Epoch: 3 / 3, Step: 824 / 2250 Loss: 0.1500\n",
      "Epoch: 3 / 3, Step: 825 / 2250 Loss: 0.5164\n",
      "Epoch: 3 / 3, Step: 826 / 2250 Loss: 0.0462\n",
      "Epoch: 3 / 3, Step: 827 / 2250 Loss: 0.1736\n",
      "Epoch: 3 / 3, Step: 828 / 2250 Loss: 0.2805\n",
      "Epoch: 3 / 3, Step: 829 / 2250 Loss: 0.2756\n",
      "Epoch: 3 / 3, Step: 830 / 2250 Loss: 0.1283\n",
      "Epoch: 3 / 3, Step: 831 / 2250 Loss: 0.0906\n",
      "Epoch: 3 / 3, Step: 832 / 2250 Loss: 0.2185\n",
      "Epoch: 3 / 3, Step: 833 / 2250 Loss: 0.1794\n",
      "Epoch: 3 / 3, Step: 834 / 2250 Loss: 0.1382\n",
      "Epoch: 3 / 3, Step: 835 / 2250 Loss: 0.4049\n",
      "Epoch: 3 / 3, Step: 836 / 2250 Loss: 0.4127\n",
      "Epoch: 3 / 3, Step: 837 / 2250 Loss: 0.2591\n",
      "Epoch: 3 / 3, Step: 838 / 2250 Loss: 0.1412\n",
      "Epoch: 3 / 3, Step: 839 / 2250 Loss: 0.0892\n",
      "Epoch: 3 / 3, Step: 840 / 2250 Loss: 0.2605\n",
      "Epoch: 3 / 3, Step: 841 / 2250 Loss: 0.1753\n",
      "Epoch: 3 / 3, Step: 842 / 2250 Loss: 0.1910\n",
      "Epoch: 3 / 3, Step: 843 / 2250 Loss: 0.3274\n",
      "Epoch: 3 / 3, Step: 844 / 2250 Loss: 0.3765\n",
      "Epoch: 3 / 3, Step: 845 / 2250 Loss: 0.2524\n",
      "Epoch: 3 / 3, Step: 846 / 2250 Loss: 0.2555\n",
      "Epoch: 3 / 3, Step: 847 / 2250 Loss: 0.3237\n",
      "Epoch: 3 / 3, Step: 848 / 2250 Loss: 0.1452\n",
      "Epoch: 3 / 3, Step: 849 / 2250 Loss: 0.5554\n",
      "Epoch: 3 / 3, Step: 850 / 2250 Loss: 0.2966\n",
      "Epoch: 3 / 3, Step: 851 / 2250 Loss: 0.2303\n",
      "Epoch: 3 / 3, Step: 852 / 2250 Loss: 0.2369\n",
      "Epoch: 3 / 3, Step: 853 / 2250 Loss: 0.1663\n",
      "Epoch: 3 / 3, Step: 854 / 2250 Loss: 0.4618\n",
      "Epoch: 3 / 3, Step: 855 / 2250 Loss: 0.0905\n",
      "Epoch: 3 / 3, Step: 856 / 2250 Loss: 0.2021\n",
      "Epoch: 3 / 3, Step: 857 / 2250 Loss: 0.2315\n",
      "Epoch: 3 / 3, Step: 858 / 2250 Loss: 0.1741\n",
      "Epoch: 3 / 3, Step: 859 / 2250 Loss: 0.1942\n",
      "Epoch: 3 / 3, Step: 860 / 2250 Loss: 0.1161\n",
      "Epoch: 3 / 3, Step: 861 / 2250 Loss: 0.2260\n",
      "Epoch: 3 / 3, Step: 862 / 2250 Loss: 0.2927\n",
      "Epoch: 3 / 3, Step: 863 / 2250 Loss: 0.4541\n",
      "Epoch: 3 / 3, Step: 864 / 2250 Loss: 0.1020\n",
      "Epoch: 3 / 3, Step: 865 / 2250 Loss: 0.2744\n",
      "Epoch: 3 / 3, Step: 866 / 2250 Loss: 0.3156\n",
      "Epoch: 3 / 3, Step: 867 / 2250 Loss: 0.3247\n",
      "Epoch: 3 / 3, Step: 868 / 2250 Loss: 0.2373\n",
      "Epoch: 3 / 3, Step: 869 / 2250 Loss: 0.1146\n",
      "Epoch: 3 / 3, Step: 870 / 2250 Loss: 0.3110\n",
      "Epoch: 3 / 3, Step: 871 / 2250 Loss: 0.2598\n",
      "Epoch: 3 / 3, Step: 872 / 2250 Loss: 0.3710\n",
      "Epoch: 3 / 3, Step: 873 / 2250 Loss: 0.0743\n",
      "Epoch: 3 / 3, Step: 874 / 2250 Loss: 0.2305\n",
      "Epoch: 3 / 3, Step: 875 / 2250 Loss: 0.1691\n",
      "Epoch: 3 / 3, Step: 876 / 2250 Loss: 0.3447\n",
      "Epoch: 3 / 3, Step: 877 / 2250 Loss: 0.1481\n",
      "Epoch: 3 / 3, Step: 878 / 2250 Loss: 0.1786\n",
      "Epoch: 3 / 3, Step: 879 / 2250 Loss: 0.1910\n",
      "Epoch: 3 / 3, Step: 880 / 2250 Loss: 0.4718\n",
      "Epoch: 3 / 3, Step: 881 / 2250 Loss: 0.2791\n",
      "Epoch: 3 / 3, Step: 882 / 2250 Loss: 0.2564\n",
      "Epoch: 3 / 3, Step: 883 / 2250 Loss: 0.0959\n",
      "Epoch: 3 / 3, Step: 884 / 2250 Loss: 0.1081\n",
      "Epoch: 3 / 3, Step: 885 / 2250 Loss: 0.1840\n",
      "Epoch: 3 / 3, Step: 886 / 2250 Loss: 0.3332\n",
      "Epoch: 3 / 3, Step: 887 / 2250 Loss: 0.1125\n",
      "Epoch: 3 / 3, Step: 888 / 2250 Loss: 0.1461\n",
      "Epoch: 3 / 3, Step: 889 / 2250 Loss: 0.2159\n",
      "Epoch: 3 / 3, Step: 890 / 2250 Loss: 0.4193\n",
      "Epoch: 3 / 3, Step: 891 / 2250 Loss: 0.1002\n",
      "Epoch: 3 / 3, Step: 892 / 2250 Loss: 0.3351\n",
      "Epoch: 3 / 3, Step: 893 / 2250 Loss: 0.2275\n",
      "Epoch: 3 / 3, Step: 894 / 2250 Loss: 0.1211\n",
      "Epoch: 3 / 3, Step: 895 / 2250 Loss: 0.3940\n",
      "Epoch: 3 / 3, Step: 896 / 2250 Loss: 0.3004\n",
      "Epoch: 3 / 3, Step: 897 / 2250 Loss: 0.4259\n",
      "Epoch: 3 / 3, Step: 898 / 2250 Loss: 0.0681\n",
      "Epoch: 3 / 3, Step: 899 / 2250 Loss: 0.1400\n",
      "Epoch: 3 / 3, Step: 900 / 2250 Loss: 0.3475\n",
      "Epoch: 3 / 3, Step: 901 / 2250 Loss: 0.0917\n",
      "Epoch: 3 / 3, Step: 902 / 2250 Loss: 0.1266\n",
      "Epoch: 3 / 3, Step: 903 / 2250 Loss: 0.2286\n",
      "Epoch: 3 / 3, Step: 904 / 2250 Loss: 0.3349\n",
      "Epoch: 3 / 3, Step: 905 / 2250 Loss: 0.1260\n",
      "Epoch: 3 / 3, Step: 906 / 2250 Loss: 0.1534\n",
      "Epoch: 3 / 3, Step: 907 / 2250 Loss: 0.3285\n",
      "Epoch: 3 / 3, Step: 908 / 2250 Loss: 0.0767\n",
      "Epoch: 3 / 3, Step: 909 / 2250 Loss: 0.2097\n",
      "Epoch: 3 / 3, Step: 910 / 2250 Loss: 0.1062\n",
      "Epoch: 3 / 3, Step: 911 / 2250 Loss: 0.2935\n",
      "Epoch: 3 / 3, Step: 912 / 2250 Loss: 0.3388\n",
      "Epoch: 3 / 3, Step: 913 / 2250 Loss: 0.4346\n",
      "Epoch: 3 / 3, Step: 914 / 2250 Loss: 0.1045\n",
      "Epoch: 3 / 3, Step: 915 / 2250 Loss: 0.0394\n",
      "Epoch: 3 / 3, Step: 916 / 2250 Loss: 0.2593\n",
      "Epoch: 3 / 3, Step: 917 / 2250 Loss: 0.2082\n",
      "Epoch: 3 / 3, Step: 918 / 2250 Loss: 0.2288\n",
      "Epoch: 3 / 3, Step: 919 / 2250 Loss: 0.1052\n",
      "Epoch: 3 / 3, Step: 920 / 2250 Loss: 0.1905\n",
      "Epoch: 3 / 3, Step: 921 / 2250 Loss: 0.3567\n",
      "Epoch: 3 / 3, Step: 922 / 2250 Loss: 0.3718\n",
      "Epoch: 3 / 3, Step: 923 / 2250 Loss: 0.1271\n",
      "Epoch: 3 / 3, Step: 924 / 2250 Loss: 0.2185\n",
      "Epoch: 3 / 3, Step: 925 / 2250 Loss: 0.3345\n",
      "Epoch: 3 / 3, Step: 926 / 2250 Loss: 0.3220\n",
      "Epoch: 3 / 3, Step: 927 / 2250 Loss: 0.0773\n",
      "Epoch: 3 / 3, Step: 928 / 2250 Loss: 0.3533\n",
      "Epoch: 3 / 3, Step: 929 / 2250 Loss: 0.1907\n",
      "Epoch: 3 / 3, Step: 930 / 2250 Loss: 0.0834\n",
      "Epoch: 3 / 3, Step: 931 / 2250 Loss: 0.1651\n",
      "Epoch: 3 / 3, Step: 932 / 2250 Loss: 0.2483\n",
      "Epoch: 3 / 3, Step: 933 / 2250 Loss: 0.2103\n",
      "Epoch: 3 / 3, Step: 934 / 2250 Loss: 0.4353\n",
      "Epoch: 3 / 3, Step: 935 / 2250 Loss: 0.3167\n",
      "Epoch: 3 / 3, Step: 936 / 2250 Loss: 0.2315\n",
      "Epoch: 3 / 3, Step: 937 / 2250 Loss: 0.3177\n",
      "Epoch: 3 / 3, Step: 938 / 2250 Loss: 0.3453\n",
      "Epoch: 3 / 3, Step: 939 / 2250 Loss: 0.0515\n",
      "Epoch: 3 / 3, Step: 940 / 2250 Loss: 0.4037\n",
      "Epoch: 3 / 3, Step: 941 / 2250 Loss: 0.3534\n",
      "Epoch: 3 / 3, Step: 942 / 2250 Loss: 0.3063\n",
      "Epoch: 3 / 3, Step: 943 / 2250 Loss: 0.3348\n",
      "Epoch: 3 / 3, Step: 944 / 2250 Loss: 0.1965\n",
      "Epoch: 3 / 3, Step: 945 / 2250 Loss: 0.0874\n",
      "Epoch: 3 / 3, Step: 946 / 2250 Loss: 0.1981\n",
      "Epoch: 3 / 3, Step: 947 / 2250 Loss: 0.1998\n",
      "Epoch: 3 / 3, Step: 948 / 2250 Loss: 0.2863\n",
      "Epoch: 3 / 3, Step: 949 / 2250 Loss: 0.2728\n",
      "Epoch: 3 / 3, Step: 950 / 2250 Loss: 0.4589\n",
      "Epoch: 3 / 3, Step: 951 / 2250 Loss: 0.3911\n",
      "Epoch: 3 / 3, Step: 952 / 2250 Loss: 0.3036\n",
      "Epoch: 3 / 3, Step: 953 / 2250 Loss: 0.1115\n",
      "Epoch: 3 / 3, Step: 954 / 2250 Loss: 0.2568\n",
      "Epoch: 3 / 3, Step: 955 / 2250 Loss: 0.1459\n",
      "Epoch: 3 / 3, Step: 956 / 2250 Loss: 0.1247\n",
      "Epoch: 3 / 3, Step: 957 / 2250 Loss: 0.1107\n",
      "Epoch: 3 / 3, Step: 958 / 2250 Loss: 0.1226\n",
      "Epoch: 3 / 3, Step: 959 / 2250 Loss: 0.1512\n",
      "Epoch: 3 / 3, Step: 960 / 2250 Loss: 0.2669\n",
      "Epoch: 3 / 3, Step: 961 / 2250 Loss: 0.3147\n",
      "Epoch: 3 / 3, Step: 962 / 2250 Loss: 0.1248\n",
      "Epoch: 3 / 3, Step: 963 / 2250 Loss: 0.4370\n",
      "Epoch: 3 / 3, Step: 964 / 2250 Loss: 0.2718\n",
      "Epoch: 3 / 3, Step: 965 / 2250 Loss: 0.2391\n",
      "Epoch: 3 / 3, Step: 966 / 2250 Loss: 0.1331\n",
      "Epoch: 3 / 3, Step: 967 / 2250 Loss: 0.1670\n",
      "Epoch: 3 / 3, Step: 968 / 2250 Loss: 0.3353\n",
      "Epoch: 3 / 3, Step: 969 / 2250 Loss: 0.3508\n",
      "Epoch: 3 / 3, Step: 970 / 2250 Loss: 0.1875\n",
      "Epoch: 3 / 3, Step: 971 / 2250 Loss: 0.2985\n",
      "Epoch: 3 / 3, Step: 972 / 2250 Loss: 0.1356\n",
      "Epoch: 3 / 3, Step: 973 / 2250 Loss: 0.1469\n",
      "Epoch: 3 / 3, Step: 974 / 2250 Loss: 0.2931\n",
      "Epoch: 3 / 3, Step: 975 / 2250 Loss: 0.1563\n",
      "Epoch: 3 / 3, Step: 976 / 2250 Loss: 0.0689\n",
      "Epoch: 3 / 3, Step: 977 / 2250 Loss: 0.2532\n",
      "Epoch: 3 / 3, Step: 978 / 2250 Loss: 0.7005\n",
      "Epoch: 3 / 3, Step: 979 / 2250 Loss: 0.1197\n",
      "Epoch: 3 / 3, Step: 980 / 2250 Loss: 0.0850\n",
      "Epoch: 3 / 3, Step: 981 / 2250 Loss: 0.3300\n",
      "Epoch: 3 / 3, Step: 982 / 2250 Loss: 0.1553\n",
      "Epoch: 3 / 3, Step: 983 / 2250 Loss: 0.2138\n",
      "Epoch: 3 / 3, Step: 984 / 2250 Loss: 0.1979\n",
      "Epoch: 3 / 3, Step: 985 / 2250 Loss: 0.2508\n",
      "Epoch: 3 / 3, Step: 986 / 2250 Loss: 0.1729\n",
      "Epoch: 3 / 3, Step: 987 / 2250 Loss: 0.0831\n",
      "Epoch: 3 / 3, Step: 988 / 2250 Loss: 0.1099\n",
      "Epoch: 3 / 3, Step: 989 / 2250 Loss: 0.0885\n",
      "Epoch: 3 / 3, Step: 990 / 2250 Loss: 0.4230\n",
      "Epoch: 3 / 3, Step: 991 / 2250 Loss: 0.1026\n",
      "Epoch: 3 / 3, Step: 992 / 2250 Loss: 0.2728\n",
      "Epoch: 3 / 3, Step: 993 / 2250 Loss: 0.3138\n",
      "Epoch: 3 / 3, Step: 994 / 2250 Loss: 0.5845\n",
      "Epoch: 3 / 3, Step: 995 / 2250 Loss: 0.2863\n",
      "Epoch: 3 / 3, Step: 996 / 2250 Loss: 0.2232\n",
      "Epoch: 3 / 3, Step: 997 / 2250 Loss: 0.2046\n",
      "Epoch: 3 / 3, Step: 998 / 2250 Loss: 0.0979\n",
      "Epoch: 3 / 3, Step: 999 / 2250 Loss: 0.4430\n",
      "Epoch: 3 / 3, Step: 1000 / 2250 Loss: 0.2366\n",
      "Epoch: 3 / 3, Step: 1001 / 2250 Loss: 0.3808\n",
      "Epoch: 3 / 3, Step: 1002 / 2250 Loss: 0.2462\n",
      "Epoch: 3 / 3, Step: 1003 / 2250 Loss: 0.1533\n",
      "Epoch: 3 / 3, Step: 1004 / 2250 Loss: 0.4153\n",
      "Epoch: 3 / 3, Step: 1005 / 2250 Loss: 0.2285\n",
      "Epoch: 3 / 3, Step: 1006 / 2250 Loss: 0.5012\n",
      "Epoch: 3 / 3, Step: 1007 / 2250 Loss: 0.2246\n",
      "Epoch: 3 / 3, Step: 1008 / 2250 Loss: 0.2463\n",
      "Epoch: 3 / 3, Step: 1009 / 2250 Loss: 0.2096\n",
      "Epoch: 3 / 3, Step: 1010 / 2250 Loss: 0.3155\n",
      "Epoch: 3 / 3, Step: 1011 / 2250 Loss: 0.1638\n",
      "Epoch: 3 / 3, Step: 1012 / 2250 Loss: 0.3768\n",
      "Epoch: 3 / 3, Step: 1013 / 2250 Loss: 0.3544\n",
      "Epoch: 3 / 3, Step: 1014 / 2250 Loss: 0.2750\n",
      "Epoch: 3 / 3, Step: 1015 / 2250 Loss: 0.1580\n",
      "Epoch: 3 / 3, Step: 1016 / 2250 Loss: 0.1573\n",
      "Epoch: 3 / 3, Step: 1017 / 2250 Loss: 0.0501\n",
      "Epoch: 3 / 3, Step: 1018 / 2250 Loss: 0.3624\n",
      "Epoch: 3 / 3, Step: 1019 / 2250 Loss: 0.2531\n",
      "Epoch: 3 / 3, Step: 1020 / 2250 Loss: 0.1320\n",
      "Epoch: 3 / 3, Step: 1021 / 2250 Loss: 0.2693\n",
      "Epoch: 3 / 3, Step: 1022 / 2250 Loss: 0.2573\n",
      "Epoch: 3 / 3, Step: 1023 / 2250 Loss: 0.2767\n",
      "Epoch: 3 / 3, Step: 1024 / 2250 Loss: 0.2501\n",
      "Epoch: 3 / 3, Step: 1025 / 2250 Loss: 0.1009\n",
      "Epoch: 3 / 3, Step: 1026 / 2250 Loss: 0.6331\n",
      "Epoch: 3 / 3, Step: 1027 / 2250 Loss: 0.1936\n",
      "Epoch: 3 / 3, Step: 1028 / 2250 Loss: 0.1449\n",
      "Epoch: 3 / 3, Step: 1029 / 2250 Loss: 0.0559\n",
      "Epoch: 3 / 3, Step: 1030 / 2250 Loss: 0.1217\n",
      "Epoch: 3 / 3, Step: 1031 / 2250 Loss: 0.2728\n",
      "Epoch: 3 / 3, Step: 1032 / 2250 Loss: 0.2976\n",
      "Epoch: 3 / 3, Step: 1033 / 2250 Loss: 0.2349\n",
      "Epoch: 3 / 3, Step: 1034 / 2250 Loss: 0.2323\n",
      "Epoch: 3 / 3, Step: 1035 / 2250 Loss: 0.2740\n",
      "Epoch: 3 / 3, Step: 1036 / 2250 Loss: 0.4051\n",
      "Epoch: 3 / 3, Step: 1037 / 2250 Loss: 0.1841\n",
      "Epoch: 3 / 3, Step: 1038 / 2250 Loss: 0.1544\n",
      "Epoch: 3 / 3, Step: 1039 / 2250 Loss: 0.4677\n",
      "Epoch: 3 / 3, Step: 1040 / 2250 Loss: 0.1593\n",
      "Epoch: 3 / 3, Step: 1041 / 2250 Loss: 0.1076\n",
      "Epoch: 3 / 3, Step: 1042 / 2250 Loss: 0.1790\n",
      "Epoch: 3 / 3, Step: 1043 / 2250 Loss: 0.3053\n",
      "Epoch: 3 / 3, Step: 1044 / 2250 Loss: 0.2223\n",
      "Epoch: 3 / 3, Step: 1045 / 2250 Loss: 0.3666\n",
      "Epoch: 3 / 3, Step: 1046 / 2250 Loss: 0.0979\n",
      "Epoch: 3 / 3, Step: 1047 / 2250 Loss: 0.1459\n",
      "Epoch: 3 / 3, Step: 1048 / 2250 Loss: 0.1290\n",
      "Epoch: 3 / 3, Step: 1049 / 2250 Loss: 0.2397\n",
      "Epoch: 3 / 3, Step: 1050 / 2250 Loss: 0.1362\n",
      "Epoch: 3 / 3, Step: 1051 / 2250 Loss: 0.2007\n",
      "Epoch: 3 / 3, Step: 1052 / 2250 Loss: 0.3751\n",
      "Epoch: 3 / 3, Step: 1053 / 2250 Loss: 0.3444\n",
      "Epoch: 3 / 3, Step: 1054 / 2250 Loss: 0.4761\n",
      "Epoch: 3 / 3, Step: 1055 / 2250 Loss: 0.1151\n",
      "Epoch: 3 / 3, Step: 1056 / 2250 Loss: 0.3523\n",
      "Epoch: 3 / 3, Step: 1057 / 2250 Loss: 0.3131\n",
      "Epoch: 3 / 3, Step: 1058 / 2250 Loss: 0.2323\n",
      "Epoch: 3 / 3, Step: 1059 / 2250 Loss: 0.2520\n",
      "Epoch: 3 / 3, Step: 1060 / 2250 Loss: 0.0806\n",
      "Epoch: 3 / 3, Step: 1061 / 2250 Loss: 0.2671\n",
      "Epoch: 3 / 3, Step: 1062 / 2250 Loss: 0.2398\n",
      "Epoch: 3 / 3, Step: 1063 / 2250 Loss: 0.1974\n",
      "Epoch: 3 / 3, Step: 1064 / 2250 Loss: 0.0982\n",
      "Epoch: 3 / 3, Step: 1065 / 2250 Loss: 0.3302\n",
      "Epoch: 3 / 3, Step: 1066 / 2250 Loss: 0.4808\n",
      "Epoch: 3 / 3, Step: 1067 / 2250 Loss: 0.0881\n",
      "Epoch: 3 / 3, Step: 1068 / 2250 Loss: 0.2943\n",
      "Epoch: 3 / 3, Step: 1069 / 2250 Loss: 0.2319\n",
      "Epoch: 3 / 3, Step: 1070 / 2250 Loss: 0.3730\n",
      "Epoch: 3 / 3, Step: 1071 / 2250 Loss: 0.2359\n",
      "Epoch: 3 / 3, Step: 1072 / 2250 Loss: 0.2445\n",
      "Epoch: 3 / 3, Step: 1073 / 2250 Loss: 0.1208\n",
      "Epoch: 3 / 3, Step: 1074 / 2250 Loss: 0.2728\n",
      "Epoch: 3 / 3, Step: 1075 / 2250 Loss: 0.3290\n",
      "Epoch: 3 / 3, Step: 1076 / 2250 Loss: 0.1439\n",
      "Epoch: 3 / 3, Step: 1077 / 2250 Loss: 0.2752\n",
      "Epoch: 3 / 3, Step: 1078 / 2250 Loss: 0.1781\n",
      "Epoch: 3 / 3, Step: 1079 / 2250 Loss: 0.3324\n",
      "Epoch: 3 / 3, Step: 1080 / 2250 Loss: 0.3790\n",
      "Epoch: 3 / 3, Step: 1081 / 2250 Loss: 0.4798\n",
      "Epoch: 3 / 3, Step: 1082 / 2250 Loss: 0.2576\n",
      "Epoch: 3 / 3, Step: 1083 / 2250 Loss: 0.1805\n",
      "Epoch: 3 / 3, Step: 1084 / 2250 Loss: 0.1441\n",
      "Epoch: 3 / 3, Step: 1085 / 2250 Loss: 0.3351\n",
      "Epoch: 3 / 3, Step: 1086 / 2250 Loss: 0.1339\n",
      "Epoch: 3 / 3, Step: 1087 / 2250 Loss: 0.1023\n",
      "Epoch: 3 / 3, Step: 1088 / 2250 Loss: 0.4807\n",
      "Epoch: 3 / 3, Step: 1089 / 2250 Loss: 0.2543\n",
      "Epoch: 3 / 3, Step: 1090 / 2250 Loss: 0.3516\n",
      "Epoch: 3 / 3, Step: 1091 / 2250 Loss: 0.2072\n",
      "Epoch: 3 / 3, Step: 1092 / 2250 Loss: 0.1736\n",
      "Epoch: 3 / 3, Step: 1093 / 2250 Loss: 0.3901\n",
      "Epoch: 3 / 3, Step: 1094 / 2250 Loss: 0.3571\n",
      "Epoch: 3 / 3, Step: 1095 / 2250 Loss: 0.3958\n",
      "Epoch: 3 / 3, Step: 1096 / 2250 Loss: 0.2713\n",
      "Epoch: 3 / 3, Step: 1097 / 2250 Loss: 0.2964\n",
      "Epoch: 3 / 3, Step: 1098 / 2250 Loss: 0.1828\n",
      "Epoch: 3 / 3, Step: 1099 / 2250 Loss: 0.1165\n",
      "Epoch: 3 / 3, Step: 1100 / 2250 Loss: 0.2643\n",
      "Epoch: 3 / 3, Step: 1101 / 2250 Loss: 0.2481\n",
      "Epoch: 3 / 3, Step: 1102 / 2250 Loss: 0.3105\n",
      "Epoch: 3 / 3, Step: 1103 / 2250 Loss: 0.1811\n",
      "Epoch: 3 / 3, Step: 1104 / 2250 Loss: 0.3779\n",
      "Epoch: 3 / 3, Step: 1105 / 2250 Loss: 0.1606\n",
      "Epoch: 3 / 3, Step: 1106 / 2250 Loss: 0.1528\n",
      "Epoch: 3 / 3, Step: 1107 / 2250 Loss: 0.1974\n",
      "Epoch: 3 / 3, Step: 1108 / 2250 Loss: 0.3367\n",
      "Epoch: 3 / 3, Step: 1109 / 2250 Loss: 0.1511\n",
      "Epoch: 3 / 3, Step: 1110 / 2250 Loss: 0.2673\n",
      "Epoch: 3 / 3, Step: 1111 / 2250 Loss: 0.2540\n",
      "Epoch: 3 / 3, Step: 1112 / 2250 Loss: 0.2685\n",
      "Epoch: 3 / 3, Step: 1113 / 2250 Loss: 0.2046\n",
      "Epoch: 3 / 3, Step: 1114 / 2250 Loss: 0.3261\n",
      "Epoch: 3 / 3, Step: 1115 / 2250 Loss: 0.4914\n",
      "Epoch: 3 / 3, Step: 1116 / 2250 Loss: 0.1991\n",
      "Epoch: 3 / 3, Step: 1117 / 2250 Loss: 0.3738\n",
      "Epoch: 3 / 3, Step: 1118 / 2250 Loss: 0.2409\n",
      "Epoch: 3 / 3, Step: 1119 / 2250 Loss: 0.1856\n",
      "Epoch: 3 / 3, Step: 1120 / 2250 Loss: 0.3475\n",
      "Epoch: 3 / 3, Step: 1121 / 2250 Loss: 0.0890\n",
      "Epoch: 3 / 3, Step: 1122 / 2250 Loss: 0.3382\n",
      "Epoch: 3 / 3, Step: 1123 / 2250 Loss: 0.1741\n",
      "Epoch: 3 / 3, Step: 1124 / 2250 Loss: 0.0966\n",
      "Epoch: 3 / 3, Step: 1125 / 2250 Loss: 0.2263\n",
      "Epoch: 3 / 3, Step: 1126 / 2250 Loss: 0.1259\n",
      "Epoch: 3 / 3, Step: 1127 / 2250 Loss: 0.1960\n",
      "Epoch: 3 / 3, Step: 1128 / 2250 Loss: 0.4059\n",
      "Epoch: 3 / 3, Step: 1129 / 2250 Loss: 0.5324\n",
      "Epoch: 3 / 3, Step: 1130 / 2250 Loss: 0.2384\n",
      "Epoch: 3 / 3, Step: 1131 / 2250 Loss: 0.1629\n",
      "Epoch: 3 / 3, Step: 1132 / 2250 Loss: 0.2754\n",
      "Epoch: 3 / 3, Step: 1133 / 2250 Loss: 0.1045\n",
      "Epoch: 3 / 3, Step: 1134 / 2250 Loss: 0.1920\n",
      "Epoch: 3 / 3, Step: 1135 / 2250 Loss: 0.2207\n",
      "Epoch: 3 / 3, Step: 1136 / 2250 Loss: 0.4222\n",
      "Epoch: 3 / 3, Step: 1137 / 2250 Loss: 0.2731\n",
      "Epoch: 3 / 3, Step: 1138 / 2250 Loss: 0.1476\n",
      "Epoch: 3 / 3, Step: 1139 / 2250 Loss: 0.1163\n",
      "Epoch: 3 / 3, Step: 1140 / 2250 Loss: 0.3426\n",
      "Epoch: 3 / 3, Step: 1141 / 2250 Loss: 0.2205\n",
      "Epoch: 3 / 3, Step: 1142 / 2250 Loss: 0.3522\n",
      "Epoch: 3 / 3, Step: 1143 / 2250 Loss: 0.1745\n",
      "Epoch: 3 / 3, Step: 1144 / 2250 Loss: 0.1255\n",
      "Epoch: 3 / 3, Step: 1145 / 2250 Loss: 0.2948\n",
      "Epoch: 3 / 3, Step: 1146 / 2250 Loss: 0.2096\n",
      "Epoch: 3 / 3, Step: 1147 / 2250 Loss: 0.3060\n",
      "Epoch: 3 / 3, Step: 1148 / 2250 Loss: 0.2271\n",
      "Epoch: 3 / 3, Step: 1149 / 2250 Loss: 0.1370\n",
      "Epoch: 3 / 3, Step: 1150 / 2250 Loss: 0.0534\n",
      "Epoch: 3 / 3, Step: 1151 / 2250 Loss: 0.3696\n",
      "Epoch: 3 / 3, Step: 1152 / 2250 Loss: 0.4569\n",
      "Epoch: 3 / 3, Step: 1153 / 2250 Loss: 0.2743\n",
      "Epoch: 3 / 3, Step: 1154 / 2250 Loss: 0.2367\n",
      "Epoch: 3 / 3, Step: 1155 / 2250 Loss: 0.2480\n",
      "Epoch: 3 / 3, Step: 1156 / 2250 Loss: 0.1479\n",
      "Epoch: 3 / 3, Step: 1157 / 2250 Loss: 0.2418\n",
      "Epoch: 3 / 3, Step: 1158 / 2250 Loss: 0.1841\n",
      "Epoch: 3 / 3, Step: 1159 / 2250 Loss: 0.0785\n",
      "Epoch: 3 / 3, Step: 1160 / 2250 Loss: 0.2653\n",
      "Epoch: 3 / 3, Step: 1161 / 2250 Loss: 0.1830\n",
      "Epoch: 3 / 3, Step: 1162 / 2250 Loss: 0.3185\n",
      "Epoch: 3 / 3, Step: 1163 / 2250 Loss: 0.2145\n",
      "Epoch: 3 / 3, Step: 1164 / 2250 Loss: 0.2116\n",
      "Epoch: 3 / 3, Step: 1165 / 2250 Loss: 0.1063\n",
      "Epoch: 3 / 3, Step: 1166 / 2250 Loss: 0.1091\n",
      "Epoch: 3 / 3, Step: 1167 / 2250 Loss: 0.1175\n",
      "Epoch: 3 / 3, Step: 1168 / 2250 Loss: 0.0905\n",
      "Epoch: 3 / 3, Step: 1169 / 2250 Loss: 0.1619\n",
      "Epoch: 3 / 3, Step: 1170 / 2250 Loss: 0.2178\n",
      "Epoch: 3 / 3, Step: 1171 / 2250 Loss: 0.0962\n",
      "Epoch: 3 / 3, Step: 1172 / 2250 Loss: 0.3251\n",
      "Epoch: 3 / 3, Step: 1173 / 2250 Loss: 0.3314\n",
      "Epoch: 3 / 3, Step: 1174 / 2250 Loss: 0.1070\n",
      "Epoch: 3 / 3, Step: 1175 / 2250 Loss: 0.4739\n",
      "Epoch: 3 / 3, Step: 1176 / 2250 Loss: 0.2070\n",
      "Epoch: 3 / 3, Step: 1177 / 2250 Loss: 0.2314\n",
      "Epoch: 3 / 3, Step: 1178 / 2250 Loss: 0.2526\n",
      "Epoch: 3 / 3, Step: 1179 / 2250 Loss: 0.2743\n",
      "Epoch: 3 / 3, Step: 1180 / 2250 Loss: 0.2604\n",
      "Epoch: 3 / 3, Step: 1181 / 2250 Loss: 0.0971\n",
      "Epoch: 3 / 3, Step: 1182 / 2250 Loss: 0.4471\n",
      "Epoch: 3 / 3, Step: 1183 / 2250 Loss: 0.1750\n",
      "Epoch: 3 / 3, Step: 1184 / 2250 Loss: 0.2148\n",
      "Epoch: 3 / 3, Step: 1185 / 2250 Loss: 0.4746\n",
      "Epoch: 3 / 3, Step: 1186 / 2250 Loss: 0.1777\n",
      "Epoch: 3 / 3, Step: 1187 / 2250 Loss: 0.3455\n",
      "Epoch: 3 / 3, Step: 1188 / 2250 Loss: 0.1561\n",
      "Epoch: 3 / 3, Step: 1189 / 2250 Loss: 0.1973\n",
      "Epoch: 3 / 3, Step: 1190 / 2250 Loss: 0.2901\n",
      "Epoch: 3 / 3, Step: 1191 / 2250 Loss: 0.1772\n",
      "Epoch: 3 / 3, Step: 1192 / 2250 Loss: 0.1393\n",
      "Epoch: 3 / 3, Step: 1193 / 2250 Loss: 0.1412\n",
      "Epoch: 3 / 3, Step: 1194 / 2250 Loss: 0.3157\n",
      "Epoch: 3 / 3, Step: 1195 / 2250 Loss: 0.1142\n",
      "Epoch: 3 / 3, Step: 1196 / 2250 Loss: 0.0995\n",
      "Epoch: 3 / 3, Step: 1197 / 2250 Loss: 0.1102\n",
      "Epoch: 3 / 3, Step: 1198 / 2250 Loss: 0.2573\n",
      "Epoch: 3 / 3, Step: 1199 / 2250 Loss: 0.3807\n",
      "Epoch: 3 / 3, Step: 1200 / 2250 Loss: 0.3022\n",
      "Epoch: 3 / 3, Step: 1201 / 2250 Loss: 0.2084\n",
      "Epoch: 3 / 3, Step: 1202 / 2250 Loss: 0.1057\n",
      "Epoch: 3 / 3, Step: 1203 / 2250 Loss: 0.1362\n",
      "Epoch: 3 / 3, Step: 1204 / 2250 Loss: 0.1734\n",
      "Epoch: 3 / 3, Step: 1205 / 2250 Loss: 0.2144\n",
      "Epoch: 3 / 3, Step: 1206 / 2250 Loss: 0.0817\n",
      "Epoch: 3 / 3, Step: 1207 / 2250 Loss: 0.2414\n",
      "Epoch: 3 / 3, Step: 1208 / 2250 Loss: 0.1981\n",
      "Epoch: 3 / 3, Step: 1209 / 2250 Loss: 0.2041\n",
      "Epoch: 3 / 3, Step: 1210 / 2250 Loss: 0.1547\n",
      "Epoch: 3 / 3, Step: 1211 / 2250 Loss: 0.4353\n",
      "Epoch: 3 / 3, Step: 1212 / 2250 Loss: 0.3271\n",
      "Epoch: 3 / 3, Step: 1213 / 2250 Loss: 0.2642\n",
      "Epoch: 3 / 3, Step: 1214 / 2250 Loss: 0.2501\n",
      "Epoch: 3 / 3, Step: 1215 / 2250 Loss: 0.2258\n",
      "Epoch: 3 / 3, Step: 1216 / 2250 Loss: 0.0701\n",
      "Epoch: 3 / 3, Step: 1217 / 2250 Loss: 0.3282\n",
      "Epoch: 3 / 3, Step: 1218 / 2250 Loss: 0.2189\n",
      "Epoch: 3 / 3, Step: 1219 / 2250 Loss: 0.3522\n",
      "Epoch: 3 / 3, Step: 1220 / 2250 Loss: 0.1720\n",
      "Epoch: 3 / 3, Step: 1221 / 2250 Loss: 0.3583\n",
      "Epoch: 3 / 3, Step: 1222 / 2250 Loss: 0.3247\n",
      "Epoch: 3 / 3, Step: 1223 / 2250 Loss: 0.2819\n",
      "Epoch: 3 / 3, Step: 1224 / 2250 Loss: 0.2474\n",
      "Epoch: 3 / 3, Step: 1225 / 2250 Loss: 0.1276\n",
      "Epoch: 3 / 3, Step: 1226 / 2250 Loss: 0.2316\n",
      "Epoch: 3 / 3, Step: 1227 / 2250 Loss: 0.2720\n",
      "Epoch: 3 / 3, Step: 1228 / 2250 Loss: 0.3006\n",
      "Epoch: 3 / 3, Step: 1229 / 2250 Loss: 0.2217\n",
      "Epoch: 3 / 3, Step: 1230 / 2250 Loss: 0.1023\n",
      "Epoch: 3 / 3, Step: 1231 / 2250 Loss: 0.2398\n",
      "Epoch: 3 / 3, Step: 1232 / 2250 Loss: 0.2746\n",
      "Epoch: 3 / 3, Step: 1233 / 2250 Loss: 0.2556\n",
      "Epoch: 3 / 3, Step: 1234 / 2250 Loss: 0.2345\n",
      "Epoch: 3 / 3, Step: 1235 / 2250 Loss: 0.1989\n",
      "Epoch: 3 / 3, Step: 1236 / 2250 Loss: 0.4021\n",
      "Epoch: 3 / 3, Step: 1237 / 2250 Loss: 0.3213\n",
      "Epoch: 3 / 3, Step: 1238 / 2250 Loss: 0.0665\n",
      "Epoch: 3 / 3, Step: 1239 / 2250 Loss: 0.4252\n",
      "Epoch: 3 / 3, Step: 1240 / 2250 Loss: 0.4513\n",
      "Epoch: 3 / 3, Step: 1241 / 2250 Loss: 0.4060\n",
      "Epoch: 3 / 3, Step: 1242 / 2250 Loss: 0.2035\n",
      "Epoch: 3 / 3, Step: 1243 / 2250 Loss: 0.1501\n",
      "Epoch: 3 / 3, Step: 1244 / 2250 Loss: 0.1586\n",
      "Epoch: 3 / 3, Step: 1245 / 2250 Loss: 0.1227\n",
      "Epoch: 3 / 3, Step: 1246 / 2250 Loss: 0.1811\n",
      "Epoch: 3 / 3, Step: 1247 / 2250 Loss: 0.3974\n",
      "Epoch: 3 / 3, Step: 1248 / 2250 Loss: 0.2754\n",
      "Epoch: 3 / 3, Step: 1249 / 2250 Loss: 0.2058\n",
      "Epoch: 3 / 3, Step: 1250 / 2250 Loss: 0.0904\n",
      "Epoch: 3 / 3, Step: 1251 / 2250 Loss: 0.3864\n",
      "Epoch: 3 / 3, Step: 1252 / 2250 Loss: 0.2101\n",
      "Epoch: 3 / 3, Step: 1253 / 2250 Loss: 0.2313\n",
      "Epoch: 3 / 3, Step: 1254 / 2250 Loss: 0.1994\n",
      "Epoch: 3 / 3, Step: 1255 / 2250 Loss: 0.1183\n",
      "Epoch: 3 / 3, Step: 1256 / 2250 Loss: 0.1086\n",
      "Epoch: 3 / 3, Step: 1257 / 2250 Loss: 0.0484\n",
      "Epoch: 3 / 3, Step: 1258 / 2250 Loss: 0.3104\n",
      "Epoch: 3 / 3, Step: 1259 / 2250 Loss: 0.4288\n",
      "Epoch: 3 / 3, Step: 1260 / 2250 Loss: 0.3391\n",
      "Epoch: 3 / 3, Step: 1261 / 2250 Loss: 0.2742\n",
      "Epoch: 3 / 3, Step: 1262 / 2250 Loss: 0.2726\n",
      "Epoch: 3 / 3, Step: 1263 / 2250 Loss: 0.3965\n",
      "Epoch: 3 / 3, Step: 1264 / 2250 Loss: 0.2414\n",
      "Epoch: 3 / 3, Step: 1265 / 2250 Loss: 0.4037\n",
      "Epoch: 3 / 3, Step: 1266 / 2250 Loss: 0.1009\n",
      "Epoch: 3 / 3, Step: 1267 / 2250 Loss: 0.2106\n",
      "Epoch: 3 / 3, Step: 1268 / 2250 Loss: 0.1870\n",
      "Epoch: 3 / 3, Step: 1269 / 2250 Loss: 0.2634\n",
      "Epoch: 3 / 3, Step: 1270 / 2250 Loss: 0.3855\n",
      "Epoch: 3 / 3, Step: 1271 / 2250 Loss: 0.3103\n",
      "Epoch: 3 / 3, Step: 1272 / 2250 Loss: 0.1020\n",
      "Epoch: 3 / 3, Step: 1273 / 2250 Loss: 0.3173\n",
      "Epoch: 3 / 3, Step: 1274 / 2250 Loss: 0.2559\n",
      "Epoch: 3 / 3, Step: 1275 / 2250 Loss: 0.1660\n",
      "Epoch: 3 / 3, Step: 1276 / 2250 Loss: 0.3421\n",
      "Epoch: 3 / 3, Step: 1277 / 2250 Loss: 0.1248\n",
      "Epoch: 3 / 3, Step: 1278 / 2250 Loss: 0.1815\n",
      "Epoch: 3 / 3, Step: 1279 / 2250 Loss: 0.1544\n",
      "Epoch: 3 / 3, Step: 1280 / 2250 Loss: 0.3755\n",
      "Epoch: 3 / 3, Step: 1281 / 2250 Loss: 0.2944\n",
      "Epoch: 3 / 3, Step: 1282 / 2250 Loss: 0.1546\n",
      "Epoch: 3 / 3, Step: 1283 / 2250 Loss: 0.3003\n",
      "Epoch: 3 / 3, Step: 1284 / 2250 Loss: 0.0701\n",
      "Epoch: 3 / 3, Step: 1285 / 2250 Loss: 0.3315\n",
      "Epoch: 3 / 3, Step: 1286 / 2250 Loss: 0.2761\n",
      "Epoch: 3 / 3, Step: 1287 / 2250 Loss: 0.2638\n",
      "Epoch: 3 / 3, Step: 1288 / 2250 Loss: 0.1375\n",
      "Epoch: 3 / 3, Step: 1289 / 2250 Loss: 0.2989\n",
      "Epoch: 3 / 3, Step: 1290 / 2250 Loss: 0.1878\n",
      "Epoch: 3 / 3, Step: 1291 / 2250 Loss: 0.1537\n",
      "Epoch: 3 / 3, Step: 1292 / 2250 Loss: 0.1054\n",
      "Epoch: 3 / 3, Step: 1293 / 2250 Loss: 0.1276\n",
      "Epoch: 3 / 3, Step: 1294 / 2250 Loss: 0.1964\n",
      "Epoch: 3 / 3, Step: 1295 / 2250 Loss: 0.1253\n",
      "Epoch: 3 / 3, Step: 1296 / 2250 Loss: 0.3574\n",
      "Epoch: 3 / 3, Step: 1297 / 2250 Loss: 0.2204\n",
      "Epoch: 3 / 3, Step: 1298 / 2250 Loss: 0.0873\n",
      "Epoch: 3 / 3, Step: 1299 / 2250 Loss: 0.2808\n",
      "Epoch: 3 / 3, Step: 1300 / 2250 Loss: 0.1350\n",
      "Epoch: 3 / 3, Step: 1301 / 2250 Loss: 0.1529\n",
      "Epoch: 3 / 3, Step: 1302 / 2250 Loss: 0.4117\n",
      "Epoch: 3 / 3, Step: 1303 / 2250 Loss: 0.1751\n",
      "Epoch: 3 / 3, Step: 1304 / 2250 Loss: 0.0979\n",
      "Epoch: 3 / 3, Step: 1305 / 2250 Loss: 0.2217\n",
      "Epoch: 3 / 3, Step: 1306 / 2250 Loss: 0.3725\n",
      "Epoch: 3 / 3, Step: 1307 / 2250 Loss: 0.3096\n",
      "Epoch: 3 / 3, Step: 1308 / 2250 Loss: 0.2451\n",
      "Epoch: 3 / 3, Step: 1309 / 2250 Loss: 0.2727\n",
      "Epoch: 3 / 3, Step: 1310 / 2250 Loss: 0.2511\n",
      "Epoch: 3 / 3, Step: 1311 / 2250 Loss: 0.4909\n",
      "Epoch: 3 / 3, Step: 1312 / 2250 Loss: 0.2544\n",
      "Epoch: 3 / 3, Step: 1313 / 2250 Loss: 0.2202\n",
      "Epoch: 3 / 3, Step: 1314 / 2250 Loss: 0.3202\n",
      "Epoch: 3 / 3, Step: 1315 / 2250 Loss: 0.1383\n",
      "Epoch: 3 / 3, Step: 1316 / 2250 Loss: 0.1266\n",
      "Epoch: 3 / 3, Step: 1317 / 2250 Loss: 0.1395\n",
      "Epoch: 3 / 3, Step: 1318 / 2250 Loss: 0.0653\n",
      "Epoch: 3 / 3, Step: 1319 / 2250 Loss: 0.3439\n",
      "Epoch: 3 / 3, Step: 1320 / 2250 Loss: 0.3580\n",
      "Epoch: 3 / 3, Step: 1321 / 2250 Loss: 0.2041\n",
      "Epoch: 3 / 3, Step: 1322 / 2250 Loss: 0.3374\n",
      "Epoch: 3 / 3, Step: 1323 / 2250 Loss: 0.6789\n",
      "Epoch: 3 / 3, Step: 1324 / 2250 Loss: 0.1198\n",
      "Epoch: 3 / 3, Step: 1325 / 2250 Loss: 0.1685\n",
      "Epoch: 3 / 3, Step: 1326 / 2250 Loss: 0.3172\n",
      "Epoch: 3 / 3, Step: 1327 / 2250 Loss: 0.3395\n",
      "Epoch: 3 / 3, Step: 1328 / 2250 Loss: 0.2309\n",
      "Epoch: 3 / 3, Step: 1329 / 2250 Loss: 0.1388\n",
      "Epoch: 3 / 3, Step: 1330 / 2250 Loss: 0.3609\n",
      "Epoch: 3 / 3, Step: 1331 / 2250 Loss: 0.2529\n",
      "Epoch: 3 / 3, Step: 1332 / 2250 Loss: 0.2334\n",
      "Epoch: 3 / 3, Step: 1333 / 2250 Loss: 0.2579\n",
      "Epoch: 3 / 3, Step: 1334 / 2250 Loss: 0.3159\n",
      "Epoch: 3 / 3, Step: 1335 / 2250 Loss: 0.0767\n",
      "Epoch: 3 / 3, Step: 1336 / 2250 Loss: 0.3489\n",
      "Epoch: 3 / 3, Step: 1337 / 2250 Loss: 0.3803\n",
      "Epoch: 3 / 3, Step: 1338 / 2250 Loss: 0.4323\n",
      "Epoch: 3 / 3, Step: 1339 / 2250 Loss: 0.2179\n",
      "Epoch: 3 / 3, Step: 1340 / 2250 Loss: 0.2543\n",
      "Epoch: 3 / 3, Step: 1341 / 2250 Loss: 0.3630\n",
      "Epoch: 3 / 3, Step: 1342 / 2250 Loss: 0.0767\n",
      "Epoch: 3 / 3, Step: 1343 / 2250 Loss: 0.2218\n",
      "Epoch: 3 / 3, Step: 1344 / 2250 Loss: 0.1233\n",
      "Epoch: 3 / 3, Step: 1345 / 2250 Loss: 0.2546\n",
      "Epoch: 3 / 3, Step: 1346 / 2250 Loss: 0.4246\n",
      "Epoch: 3 / 3, Step: 1347 / 2250 Loss: 0.1184\n",
      "Epoch: 3 / 3, Step: 1348 / 2250 Loss: 0.1626\n",
      "Epoch: 3 / 3, Step: 1349 / 2250 Loss: 0.2964\n",
      "Epoch: 3 / 3, Step: 1350 / 2250 Loss: 0.2386\n",
      "Epoch: 3 / 3, Step: 1351 / 2250 Loss: 0.1038\n",
      "Epoch: 3 / 3, Step: 1352 / 2250 Loss: 0.2180\n",
      "Epoch: 3 / 3, Step: 1353 / 2250 Loss: 0.2639\n",
      "Epoch: 3 / 3, Step: 1354 / 2250 Loss: 0.1327\n",
      "Epoch: 3 / 3, Step: 1355 / 2250 Loss: 0.1675\n",
      "Epoch: 3 / 3, Step: 1356 / 2250 Loss: 0.0887\n",
      "Epoch: 3 / 3, Step: 1357 / 2250 Loss: 0.1894\n",
      "Epoch: 3 / 3, Step: 1358 / 2250 Loss: 0.2923\n",
      "Epoch: 3 / 3, Step: 1359 / 2250 Loss: 0.3490\n",
      "Epoch: 3 / 3, Step: 1360 / 2250 Loss: 0.5193\n",
      "Epoch: 3 / 3, Step: 1361 / 2250 Loss: 0.3110\n",
      "Epoch: 3 / 3, Step: 1362 / 2250 Loss: 0.3499\n",
      "Epoch: 3 / 3, Step: 1363 / 2250 Loss: 0.2229\n",
      "Epoch: 3 / 3, Step: 1364 / 2250 Loss: 0.0769\n",
      "Epoch: 3 / 3, Step: 1365 / 2250 Loss: 0.1484\n",
      "Epoch: 3 / 3, Step: 1366 / 2250 Loss: 0.1159\n",
      "Epoch: 3 / 3, Step: 1367 / 2250 Loss: 0.2818\n",
      "Epoch: 3 / 3, Step: 1368 / 2250 Loss: 0.0720\n",
      "Epoch: 3 / 3, Step: 1369 / 2250 Loss: 0.3420\n",
      "Epoch: 3 / 3, Step: 1370 / 2250 Loss: 0.1275\n",
      "Epoch: 3 / 3, Step: 1371 / 2250 Loss: 0.4430\n",
      "Epoch: 3 / 3, Step: 1372 / 2250 Loss: 0.1372\n",
      "Epoch: 3 / 3, Step: 1373 / 2250 Loss: 0.2036\n",
      "Epoch: 3 / 3, Step: 1374 / 2250 Loss: 0.0707\n",
      "Epoch: 3 / 3, Step: 1375 / 2250 Loss: 0.3351\n",
      "Epoch: 3 / 3, Step: 1376 / 2250 Loss: 0.5728\n",
      "Epoch: 3 / 3, Step: 1377 / 2250 Loss: 0.2675\n",
      "Epoch: 3 / 3, Step: 1378 / 2250 Loss: 0.3576\n",
      "Epoch: 3 / 3, Step: 1379 / 2250 Loss: 0.1286\n",
      "Epoch: 3 / 3, Step: 1380 / 2250 Loss: 0.2673\n",
      "Epoch: 3 / 3, Step: 1381 / 2250 Loss: 0.1961\n",
      "Epoch: 3 / 3, Step: 1382 / 2250 Loss: 0.2361\n",
      "Epoch: 3 / 3, Step: 1383 / 2250 Loss: 0.3191\n",
      "Epoch: 3 / 3, Step: 1384 / 2250 Loss: 0.1805\n",
      "Epoch: 3 / 3, Step: 1385 / 2250 Loss: 0.2648\n",
      "Epoch: 3 / 3, Step: 1386 / 2250 Loss: 0.1561\n",
      "Epoch: 3 / 3, Step: 1387 / 2250 Loss: 0.4596\n",
      "Epoch: 3 / 3, Step: 1388 / 2250 Loss: 0.2202\n",
      "Epoch: 3 / 3, Step: 1389 / 2250 Loss: 0.2035\n",
      "Epoch: 3 / 3, Step: 1390 / 2250 Loss: 0.1618\n",
      "Epoch: 3 / 3, Step: 1391 / 2250 Loss: 0.1982\n",
      "Epoch: 3 / 3, Step: 1392 / 2250 Loss: 0.2379\n",
      "Epoch: 3 / 3, Step: 1393 / 2250 Loss: 0.5263\n",
      "Epoch: 3 / 3, Step: 1394 / 2250 Loss: 0.2092\n",
      "Epoch: 3 / 3, Step: 1395 / 2250 Loss: 0.1592\n",
      "Epoch: 3 / 3, Step: 1396 / 2250 Loss: 0.1699\n",
      "Epoch: 3 / 3, Step: 1397 / 2250 Loss: 0.1286\n",
      "Epoch: 3 / 3, Step: 1398 / 2250 Loss: 0.2004\n",
      "Epoch: 3 / 3, Step: 1399 / 2250 Loss: 0.3761\n",
      "Epoch: 3 / 3, Step: 1400 / 2250 Loss: 0.1836\n",
      "Epoch: 3 / 3, Step: 1401 / 2250 Loss: 0.1573\n",
      "Epoch: 3 / 3, Step: 1402 / 2250 Loss: 0.1085\n",
      "Epoch: 3 / 3, Step: 1403 / 2250 Loss: 0.1342\n",
      "Epoch: 3 / 3, Step: 1404 / 2250 Loss: 0.3482\n",
      "Epoch: 3 / 3, Step: 1405 / 2250 Loss: 0.0985\n",
      "Epoch: 3 / 3, Step: 1406 / 2250 Loss: 0.2596\n",
      "Epoch: 3 / 3, Step: 1407 / 2250 Loss: 0.3530\n",
      "Epoch: 3 / 3, Step: 1408 / 2250 Loss: 0.2623\n",
      "Epoch: 3 / 3, Step: 1409 / 2250 Loss: 0.3077\n",
      "Epoch: 3 / 3, Step: 1410 / 2250 Loss: 0.1619\n",
      "Epoch: 3 / 3, Step: 1411 / 2250 Loss: 0.2899\n",
      "Epoch: 3 / 3, Step: 1412 / 2250 Loss: 0.0679\n",
      "Epoch: 3 / 3, Step: 1413 / 2250 Loss: 0.3923\n",
      "Epoch: 3 / 3, Step: 1414 / 2250 Loss: 0.2582\n",
      "Epoch: 3 / 3, Step: 1415 / 2250 Loss: 0.3071\n",
      "Epoch: 3 / 3, Step: 1416 / 2250 Loss: 0.1749\n",
      "Epoch: 3 / 3, Step: 1417 / 2250 Loss: 0.3082\n",
      "Epoch: 3 / 3, Step: 1418 / 2250 Loss: 0.3714\n",
      "Epoch: 3 / 3, Step: 1419 / 2250 Loss: 0.1004\n",
      "Epoch: 3 / 3, Step: 1420 / 2250 Loss: 0.2581\n",
      "Epoch: 3 / 3, Step: 1421 / 2250 Loss: 0.2009\n",
      "Epoch: 3 / 3, Step: 1422 / 2250 Loss: 0.2576\n",
      "Epoch: 3 / 3, Step: 1423 / 2250 Loss: 0.3574\n",
      "Epoch: 3 / 3, Step: 1424 / 2250 Loss: 0.1821\n",
      "Epoch: 3 / 3, Step: 1425 / 2250 Loss: 0.1613\n",
      "Epoch: 3 / 3, Step: 1426 / 2250 Loss: 0.1481\n",
      "Epoch: 3 / 3, Step: 1427 / 2250 Loss: 0.1330\n",
      "Epoch: 3 / 3, Step: 1428 / 2250 Loss: 0.2913\n",
      "Epoch: 3 / 3, Step: 1429 / 2250 Loss: 0.1763\n",
      "Epoch: 3 / 3, Step: 1430 / 2250 Loss: 0.1527\n",
      "Epoch: 3 / 3, Step: 1431 / 2250 Loss: 0.1460\n",
      "Epoch: 3 / 3, Step: 1432 / 2250 Loss: 0.2094\n",
      "Epoch: 3 / 3, Step: 1433 / 2250 Loss: 0.2301\n",
      "Epoch: 3 / 3, Step: 1434 / 2250 Loss: 0.2379\n",
      "Epoch: 3 / 3, Step: 1435 / 2250 Loss: 0.2596\n",
      "Epoch: 3 / 3, Step: 1436 / 2250 Loss: 0.1605\n",
      "Epoch: 3 / 3, Step: 1437 / 2250 Loss: 0.0714\n",
      "Epoch: 3 / 3, Step: 1438 / 2250 Loss: 0.3064\n",
      "Epoch: 3 / 3, Step: 1439 / 2250 Loss: 0.1391\n",
      "Epoch: 3 / 3, Step: 1440 / 2250 Loss: 0.1983\n",
      "Epoch: 3 / 3, Step: 1441 / 2250 Loss: 0.1860\n",
      "Epoch: 3 / 3, Step: 1442 / 2250 Loss: 0.2294\n",
      "Epoch: 3 / 3, Step: 1443 / 2250 Loss: 0.1756\n",
      "Epoch: 3 / 3, Step: 1444 / 2250 Loss: 0.1568\n",
      "Epoch: 3 / 3, Step: 1445 / 2250 Loss: 0.2049\n",
      "Epoch: 3 / 3, Step: 1446 / 2250 Loss: 0.3309\n",
      "Epoch: 3 / 3, Step: 1447 / 2250 Loss: 0.1040\n",
      "Epoch: 3 / 3, Step: 1448 / 2250 Loss: 0.3122\n",
      "Epoch: 3 / 3, Step: 1449 / 2250 Loss: 0.2486\n",
      "Epoch: 3 / 3, Step: 1450 / 2250 Loss: 0.5273\n",
      "Epoch: 3 / 3, Step: 1451 / 2250 Loss: 0.1151\n",
      "Epoch: 3 / 3, Step: 1452 / 2250 Loss: 0.1787\n",
      "Epoch: 3 / 3, Step: 1453 / 2250 Loss: 0.3103\n",
      "Epoch: 3 / 3, Step: 1454 / 2250 Loss: 0.2278\n",
      "Epoch: 3 / 3, Step: 1455 / 2250 Loss: 0.1758\n",
      "Epoch: 3 / 3, Step: 1456 / 2250 Loss: 0.1663\n",
      "Epoch: 3 / 3, Step: 1457 / 2250 Loss: 0.2613\n",
      "Epoch: 3 / 3, Step: 1458 / 2250 Loss: 0.2659\n",
      "Epoch: 3 / 3, Step: 1459 / 2250 Loss: 0.1831\n",
      "Epoch: 3 / 3, Step: 1460 / 2250 Loss: 0.0709\n",
      "Epoch: 3 / 3, Step: 1461 / 2250 Loss: 0.2045\n",
      "Epoch: 3 / 3, Step: 1462 / 2250 Loss: 0.1311\n",
      "Epoch: 3 / 3, Step: 1463 / 2250 Loss: 0.0682\n",
      "Epoch: 3 / 3, Step: 1464 / 2250 Loss: 0.1382\n",
      "Epoch: 3 / 3, Step: 1465 / 2250 Loss: 0.2469\n",
      "Epoch: 3 / 3, Step: 1466 / 2250 Loss: 0.2606\n",
      "Epoch: 3 / 3, Step: 1467 / 2250 Loss: 0.2235\n",
      "Epoch: 3 / 3, Step: 1468 / 2250 Loss: 0.3108\n",
      "Epoch: 3 / 3, Step: 1469 / 2250 Loss: 0.5770\n",
      "Epoch: 3 / 3, Step: 1470 / 2250 Loss: 0.3480\n",
      "Epoch: 3 / 3, Step: 1471 / 2250 Loss: 0.6099\n",
      "Epoch: 3 / 3, Step: 1472 / 2250 Loss: 0.3203\n",
      "Epoch: 3 / 3, Step: 1473 / 2250 Loss: 0.3237\n",
      "Epoch: 3 / 3, Step: 1474 / 2250 Loss: 0.2136\n",
      "Epoch: 3 / 3, Step: 1475 / 2250 Loss: 0.3662\n",
      "Epoch: 3 / 3, Step: 1476 / 2250 Loss: 0.0903\n",
      "Epoch: 3 / 3, Step: 1477 / 2250 Loss: 0.1593\n",
      "Epoch: 3 / 3, Step: 1478 / 2250 Loss: 0.1709\n",
      "Epoch: 3 / 3, Step: 1479 / 2250 Loss: 0.2284\n",
      "Epoch: 3 / 3, Step: 1480 / 2250 Loss: 0.1521\n",
      "Epoch: 3 / 3, Step: 1481 / 2250 Loss: 0.3805\n",
      "Epoch: 3 / 3, Step: 1482 / 2250 Loss: 0.2528\n",
      "Epoch: 3 / 3, Step: 1483 / 2250 Loss: 0.4375\n",
      "Epoch: 3 / 3, Step: 1484 / 2250 Loss: 0.2282\n",
      "Epoch: 3 / 3, Step: 1485 / 2250 Loss: 0.3651\n",
      "Epoch: 3 / 3, Step: 1486 / 2250 Loss: 0.3218\n",
      "Epoch: 3 / 3, Step: 1487 / 2250 Loss: 0.3493\n",
      "Epoch: 3 / 3, Step: 1488 / 2250 Loss: 0.2879\n",
      "Epoch: 3 / 3, Step: 1489 / 2250 Loss: 0.4363\n",
      "Epoch: 3 / 3, Step: 1490 / 2250 Loss: 0.1666\n",
      "Epoch: 3 / 3, Step: 1491 / 2250 Loss: 0.2353\n",
      "Epoch: 3 / 3, Step: 1492 / 2250 Loss: 0.1528\n",
      "Epoch: 3 / 3, Step: 1493 / 2250 Loss: 0.2886\n",
      "Epoch: 3 / 3, Step: 1494 / 2250 Loss: 0.1534\n",
      "Epoch: 3 / 3, Step: 1495 / 2250 Loss: 0.1105\n",
      "Epoch: 3 / 3, Step: 1496 / 2250 Loss: 0.1704\n",
      "Epoch: 3 / 3, Step: 1497 / 2250 Loss: 0.3960\n",
      "Epoch: 3 / 3, Step: 1498 / 2250 Loss: 0.1112\n",
      "Epoch: 3 / 3, Step: 1499 / 2250 Loss: 0.5784\n",
      "Epoch: 3 / 3, Step: 1500 / 2250 Loss: 0.2089\n",
      "Epoch: 3 / 3, Step: 1501 / 2250 Loss: 0.2504\n",
      "Epoch: 3 / 3, Step: 1502 / 2250 Loss: 0.1839\n",
      "Epoch: 3 / 3, Step: 1503 / 2250 Loss: 0.3139\n",
      "Epoch: 3 / 3, Step: 1504 / 2250 Loss: 0.2429\n",
      "Epoch: 3 / 3, Step: 1505 / 2250 Loss: 0.3985\n",
      "Epoch: 3 / 3, Step: 1506 / 2250 Loss: 0.3011\n",
      "Epoch: 3 / 3, Step: 1507 / 2250 Loss: 0.1613\n",
      "Epoch: 3 / 3, Step: 1508 / 2250 Loss: 0.1382\n",
      "Epoch: 3 / 3, Step: 1509 / 2250 Loss: 0.3710\n",
      "Epoch: 3 / 3, Step: 1510 / 2250 Loss: 0.2364\n",
      "Epoch: 3 / 3, Step: 1511 / 2250 Loss: 0.2199\n",
      "Epoch: 3 / 3, Step: 1512 / 2250 Loss: 0.2386\n",
      "Epoch: 3 / 3, Step: 1513 / 2250 Loss: 0.2433\n",
      "Epoch: 3 / 3, Step: 1514 / 2250 Loss: 0.1202\n",
      "Epoch: 3 / 3, Step: 1515 / 2250 Loss: 0.2514\n",
      "Epoch: 3 / 3, Step: 1516 / 2250 Loss: 0.1788\n",
      "Epoch: 3 / 3, Step: 1517 / 2250 Loss: 0.1145\n",
      "Epoch: 3 / 3, Step: 1518 / 2250 Loss: 0.1510\n",
      "Epoch: 3 / 3, Step: 1519 / 2250 Loss: 0.2348\n",
      "Epoch: 3 / 3, Step: 1520 / 2250 Loss: 0.2087\n",
      "Epoch: 3 / 3, Step: 1521 / 2250 Loss: 0.2003\n",
      "Epoch: 3 / 3, Step: 1522 / 2250 Loss: 0.2478\n",
      "Epoch: 3 / 3, Step: 1523 / 2250 Loss: 0.1770\n",
      "Epoch: 3 / 3, Step: 1524 / 2250 Loss: 0.1446\n",
      "Epoch: 3 / 3, Step: 1525 / 2250 Loss: 0.4155\n",
      "Epoch: 3 / 3, Step: 1526 / 2250 Loss: 0.2140\n",
      "Epoch: 3 / 3, Step: 1527 / 2250 Loss: 0.3969\n",
      "Epoch: 3 / 3, Step: 1528 / 2250 Loss: 0.1338\n",
      "Epoch: 3 / 3, Step: 1529 / 2250 Loss: 0.0584\n",
      "Epoch: 3 / 3, Step: 1530 / 2250 Loss: 0.2086\n",
      "Epoch: 3 / 3, Step: 1531 / 2250 Loss: 0.2038\n",
      "Epoch: 3 / 3, Step: 1532 / 2250 Loss: 0.5222\n",
      "Epoch: 3 / 3, Step: 1533 / 2250 Loss: 0.0925\n",
      "Epoch: 3 / 3, Step: 1534 / 2250 Loss: 0.3090\n",
      "Epoch: 3 / 3, Step: 1535 / 2250 Loss: 0.4871\n",
      "Epoch: 3 / 3, Step: 1536 / 2250 Loss: 0.4038\n",
      "Epoch: 3 / 3, Step: 1537 / 2250 Loss: 0.2333\n",
      "Epoch: 3 / 3, Step: 1538 / 2250 Loss: 0.2182\n",
      "Epoch: 3 / 3, Step: 1539 / 2250 Loss: 0.0587\n",
      "Epoch: 3 / 3, Step: 1540 / 2250 Loss: 0.2425\n",
      "Epoch: 3 / 3, Step: 1541 / 2250 Loss: 0.3558\n",
      "Epoch: 3 / 3, Step: 1542 / 2250 Loss: 0.2235\n",
      "Epoch: 3 / 3, Step: 1543 / 2250 Loss: 0.4547\n",
      "Epoch: 3 / 3, Step: 1544 / 2250 Loss: 0.5680\n",
      "Epoch: 3 / 3, Step: 1545 / 2250 Loss: 0.1424\n",
      "Epoch: 3 / 3, Step: 1546 / 2250 Loss: 0.1288\n",
      "Epoch: 3 / 3, Step: 1547 / 2250 Loss: 0.1769\n",
      "Epoch: 3 / 3, Step: 1548 / 2250 Loss: 0.3269\n",
      "Epoch: 3 / 3, Step: 1549 / 2250 Loss: 0.1480\n",
      "Epoch: 3 / 3, Step: 1550 / 2250 Loss: 0.3486\n",
      "Epoch: 3 / 3, Step: 1551 / 2250 Loss: 0.3700\n",
      "Epoch: 3 / 3, Step: 1552 / 2250 Loss: 0.3297\n",
      "Epoch: 3 / 3, Step: 1553 / 2250 Loss: 0.2357\n",
      "Epoch: 3 / 3, Step: 1554 / 2250 Loss: 0.3112\n",
      "Epoch: 3 / 3, Step: 1555 / 2250 Loss: 0.1196\n",
      "Epoch: 3 / 3, Step: 1556 / 2250 Loss: 0.1637\n",
      "Epoch: 3 / 3, Step: 1557 / 2250 Loss: 0.1167\n",
      "Epoch: 3 / 3, Step: 1558 / 2250 Loss: 0.2376\n",
      "Epoch: 3 / 3, Step: 1559 / 2250 Loss: 0.2661\n",
      "Epoch: 3 / 3, Step: 1560 / 2250 Loss: 0.1304\n",
      "Epoch: 3 / 3, Step: 1561 / 2250 Loss: 0.3718\n",
      "Epoch: 3 / 3, Step: 1562 / 2250 Loss: 0.3277\n",
      "Epoch: 3 / 3, Step: 1563 / 2250 Loss: 0.4673\n",
      "Epoch: 3 / 3, Step: 1564 / 2250 Loss: 0.1661\n",
      "Epoch: 3 / 3, Step: 1565 / 2250 Loss: 0.1287\n",
      "Epoch: 3 / 3, Step: 1566 / 2250 Loss: 0.1085\n",
      "Epoch: 3 / 3, Step: 1567 / 2250 Loss: 0.2605\n",
      "Epoch: 3 / 3, Step: 1568 / 2250 Loss: 0.3109\n",
      "Epoch: 3 / 3, Step: 1569 / 2250 Loss: 0.5113\n",
      "Epoch: 3 / 3, Step: 1570 / 2250 Loss: 0.0857\n",
      "Epoch: 3 / 3, Step: 1571 / 2250 Loss: 0.1248\n",
      "Epoch: 3 / 3, Step: 1572 / 2250 Loss: 0.1734\n",
      "Epoch: 3 / 3, Step: 1573 / 2250 Loss: 0.2514\n",
      "Epoch: 3 / 3, Step: 1574 / 2250 Loss: 0.2296\n",
      "Epoch: 3 / 3, Step: 1575 / 2250 Loss: 0.1960\n",
      "Epoch: 3 / 3, Step: 1576 / 2250 Loss: 0.4824\n",
      "Epoch: 3 / 3, Step: 1577 / 2250 Loss: 0.3348\n",
      "Epoch: 3 / 3, Step: 1578 / 2250 Loss: 0.2206\n",
      "Epoch: 3 / 3, Step: 1579 / 2250 Loss: 0.2502\n",
      "Epoch: 3 / 3, Step: 1580 / 2250 Loss: 0.1811\n",
      "Epoch: 3 / 3, Step: 1581 / 2250 Loss: 0.1629\n",
      "Epoch: 3 / 3, Step: 1582 / 2250 Loss: 0.1766\n",
      "Epoch: 3 / 3, Step: 1583 / 2250 Loss: 0.1127\n",
      "Epoch: 3 / 3, Step: 1584 / 2250 Loss: 0.1102\n",
      "Epoch: 3 / 3, Step: 1585 / 2250 Loss: 0.2751\n",
      "Epoch: 3 / 3, Step: 1586 / 2250 Loss: 0.1415\n",
      "Epoch: 3 / 3, Step: 1587 / 2250 Loss: 0.2455\n",
      "Epoch: 3 / 3, Step: 1588 / 2250 Loss: 0.1224\n",
      "Epoch: 3 / 3, Step: 1589 / 2250 Loss: 0.0618\n",
      "Epoch: 3 / 3, Step: 1590 / 2250 Loss: 0.2311\n",
      "Epoch: 3 / 3, Step: 1591 / 2250 Loss: 0.3172\n",
      "Epoch: 3 / 3, Step: 1592 / 2250 Loss: 0.2510\n",
      "Epoch: 3 / 3, Step: 1593 / 2250 Loss: 0.3796\n",
      "Epoch: 3 / 3, Step: 1594 / 2250 Loss: 0.0449\n",
      "Epoch: 3 / 3, Step: 1595 / 2250 Loss: 0.2797\n",
      "Epoch: 3 / 3, Step: 1596 / 2250 Loss: 0.3775\n",
      "Epoch: 3 / 3, Step: 1597 / 2250 Loss: 0.1306\n",
      "Epoch: 3 / 3, Step: 1598 / 2250 Loss: 0.1015\n",
      "Epoch: 3 / 3, Step: 1599 / 2250 Loss: 0.1444\n",
      "Epoch: 3 / 3, Step: 1600 / 2250 Loss: 0.1052\n",
      "Epoch: 3 / 3, Step: 1601 / 2250 Loss: 0.1709\n",
      "Epoch: 3 / 3, Step: 1602 / 2250 Loss: 0.1796\n",
      "Epoch: 3 / 3, Step: 1603 / 2250 Loss: 0.1847\n",
      "Epoch: 3 / 3, Step: 1604 / 2250 Loss: 0.1290\n",
      "Epoch: 3 / 3, Step: 1605 / 2250 Loss: 0.1941\n",
      "Epoch: 3 / 3, Step: 1606 / 2250 Loss: 0.2091\n",
      "Epoch: 3 / 3, Step: 1607 / 2250 Loss: 0.3750\n",
      "Epoch: 3 / 3, Step: 1608 / 2250 Loss: 0.0484\n",
      "Epoch: 3 / 3, Step: 1609 / 2250 Loss: 0.2148\n",
      "Epoch: 3 / 3, Step: 1610 / 2250 Loss: 0.2528\n",
      "Epoch: 3 / 3, Step: 1611 / 2250 Loss: 0.0752\n",
      "Epoch: 3 / 3, Step: 1612 / 2250 Loss: 0.2535\n",
      "Epoch: 3 / 3, Step: 1613 / 2250 Loss: 0.4360\n",
      "Epoch: 3 / 3, Step: 1614 / 2250 Loss: 0.3665\n",
      "Epoch: 3 / 3, Step: 1615 / 2250 Loss: 0.1584\n",
      "Epoch: 3 / 3, Step: 1616 / 2250 Loss: 0.0368\n",
      "Epoch: 3 / 3, Step: 1617 / 2250 Loss: 0.2470\n",
      "Epoch: 3 / 3, Step: 1618 / 2250 Loss: 0.2369\n",
      "Epoch: 3 / 3, Step: 1619 / 2250 Loss: 0.1704\n",
      "Epoch: 3 / 3, Step: 1620 / 2250 Loss: 0.1467\n",
      "Epoch: 3 / 3, Step: 1621 / 2250 Loss: 0.1667\n",
      "Epoch: 3 / 3, Step: 1622 / 2250 Loss: 0.3702\n",
      "Epoch: 3 / 3, Step: 1623 / 2250 Loss: 0.1106\n",
      "Epoch: 3 / 3, Step: 1624 / 2250 Loss: 0.2031\n",
      "Epoch: 3 / 3, Step: 1625 / 2250 Loss: 0.3945\n",
      "Epoch: 3 / 3, Step: 1626 / 2250 Loss: 0.3102\n",
      "Epoch: 3 / 3, Step: 1627 / 2250 Loss: 0.5438\n",
      "Epoch: 3 / 3, Step: 1628 / 2250 Loss: 0.1900\n",
      "Epoch: 3 / 3, Step: 1629 / 2250 Loss: 0.1760\n",
      "Epoch: 3 / 3, Step: 1630 / 2250 Loss: 0.1996\n",
      "Epoch: 3 / 3, Step: 1631 / 2250 Loss: 0.1264\n",
      "Epoch: 3 / 3, Step: 1632 / 2250 Loss: 0.3252\n",
      "Epoch: 3 / 3, Step: 1633 / 2250 Loss: 0.3813\n",
      "Epoch: 3 / 3, Step: 1634 / 2250 Loss: 0.1672\n",
      "Epoch: 3 / 3, Step: 1635 / 2250 Loss: 0.2960\n",
      "Epoch: 3 / 3, Step: 1636 / 2250 Loss: 0.1374\n",
      "Epoch: 3 / 3, Step: 1637 / 2250 Loss: 0.0708\n",
      "Epoch: 3 / 3, Step: 1638 / 2250 Loss: 0.0410\n",
      "Epoch: 3 / 3, Step: 1639 / 2250 Loss: 0.2678\n",
      "Epoch: 3 / 3, Step: 1640 / 2250 Loss: 0.2207\n",
      "Epoch: 3 / 3, Step: 1641 / 2250 Loss: 0.3081\n",
      "Epoch: 3 / 3, Step: 1642 / 2250 Loss: 0.0837\n",
      "Epoch: 3 / 3, Step: 1643 / 2250 Loss: 0.0942\n",
      "Epoch: 3 / 3, Step: 1644 / 2250 Loss: 0.2855\n",
      "Epoch: 3 / 3, Step: 1645 / 2250 Loss: 0.4716\n",
      "Epoch: 3 / 3, Step: 1646 / 2250 Loss: 0.2956\n",
      "Epoch: 3 / 3, Step: 1647 / 2250 Loss: 0.1174\n",
      "Epoch: 3 / 3, Step: 1648 / 2250 Loss: 0.0817\n",
      "Epoch: 3 / 3, Step: 1649 / 2250 Loss: 0.2519\n",
      "Epoch: 3 / 3, Step: 1650 / 2250 Loss: 0.1337\n",
      "Epoch: 3 / 3, Step: 1651 / 2250 Loss: 0.5254\n",
      "Epoch: 3 / 3, Step: 1652 / 2250 Loss: 0.2895\n",
      "Epoch: 3 / 3, Step: 1653 / 2250 Loss: 0.3871\n",
      "Epoch: 3 / 3, Step: 1654 / 2250 Loss: 0.1762\n",
      "Epoch: 3 / 3, Step: 1655 / 2250 Loss: 0.0441\n",
      "Epoch: 3 / 3, Step: 1656 / 2250 Loss: 0.2934\n",
      "Epoch: 3 / 3, Step: 1657 / 2250 Loss: 0.3670\n",
      "Epoch: 3 / 3, Step: 1658 / 2250 Loss: 0.1951\n",
      "Epoch: 3 / 3, Step: 1659 / 2250 Loss: 0.3770\n",
      "Epoch: 3 / 3, Step: 1660 / 2250 Loss: 0.2896\n",
      "Epoch: 3 / 3, Step: 1661 / 2250 Loss: 0.5297\n",
      "Epoch: 3 / 3, Step: 1662 / 2250 Loss: 0.3008\n",
      "Epoch: 3 / 3, Step: 1663 / 2250 Loss: 0.1404\n",
      "Epoch: 3 / 3, Step: 1664 / 2250 Loss: 0.4943\n",
      "Epoch: 3 / 3, Step: 1665 / 2250 Loss: 0.3675\n",
      "Epoch: 3 / 3, Step: 1666 / 2250 Loss: 0.2638\n",
      "Epoch: 3 / 3, Step: 1667 / 2250 Loss: 0.1850\n",
      "Epoch: 3 / 3, Step: 1668 / 2250 Loss: 0.2312\n",
      "Epoch: 3 / 3, Step: 1669 / 2250 Loss: 0.1050\n",
      "Epoch: 3 / 3, Step: 1670 / 2250 Loss: 0.2522\n",
      "Epoch: 3 / 3, Step: 1671 / 2250 Loss: 0.3136\n",
      "Epoch: 3 / 3, Step: 1672 / 2250 Loss: 0.3020\n",
      "Epoch: 3 / 3, Step: 1673 / 2250 Loss: 0.1455\n",
      "Epoch: 3 / 3, Step: 1674 / 2250 Loss: 0.4215\n",
      "Epoch: 3 / 3, Step: 1675 / 2250 Loss: 0.3484\n",
      "Epoch: 3 / 3, Step: 1676 / 2250 Loss: 0.1569\n",
      "Epoch: 3 / 3, Step: 1677 / 2250 Loss: 0.2049\n",
      "Epoch: 3 / 3, Step: 1678 / 2250 Loss: 0.1774\n",
      "Epoch: 3 / 3, Step: 1679 / 2250 Loss: 0.2554\n",
      "Epoch: 3 / 3, Step: 1680 / 2250 Loss: 0.2161\n",
      "Epoch: 3 / 3, Step: 1681 / 2250 Loss: 0.1832\n",
      "Epoch: 3 / 3, Step: 1682 / 2250 Loss: 0.1980\n",
      "Epoch: 3 / 3, Step: 1683 / 2250 Loss: 0.2059\n",
      "Epoch: 3 / 3, Step: 1684 / 2250 Loss: 0.1510\n",
      "Epoch: 3 / 3, Step: 1685 / 2250 Loss: 0.2074\n",
      "Epoch: 3 / 3, Step: 1686 / 2250 Loss: 0.1556\n",
      "Epoch: 3 / 3, Step: 1687 / 2250 Loss: 0.3301\n",
      "Epoch: 3 / 3, Step: 1688 / 2250 Loss: 0.2850\n",
      "Epoch: 3 / 3, Step: 1689 / 2250 Loss: 0.3732\n",
      "Epoch: 3 / 3, Step: 1690 / 2250 Loss: 0.1241\n",
      "Epoch: 3 / 3, Step: 1691 / 2250 Loss: 0.2266\n",
      "Epoch: 3 / 3, Step: 1692 / 2250 Loss: 0.1301\n",
      "Epoch: 3 / 3, Step: 1693 / 2250 Loss: 0.3039\n",
      "Epoch: 3 / 3, Step: 1694 / 2250 Loss: 0.3838\n",
      "Epoch: 3 / 3, Step: 1695 / 2250 Loss: 0.0957\n",
      "Epoch: 3 / 3, Step: 1696 / 2250 Loss: 0.2228\n",
      "Epoch: 3 / 3, Step: 1697 / 2250 Loss: 0.1702\n",
      "Epoch: 3 / 3, Step: 1698 / 2250 Loss: 0.1631\n",
      "Epoch: 3 / 3, Step: 1699 / 2250 Loss: 0.3789\n",
      "Epoch: 3 / 3, Step: 1700 / 2250 Loss: 0.0688\n",
      "Epoch: 3 / 3, Step: 1701 / 2250 Loss: 0.1950\n",
      "Epoch: 3 / 3, Step: 1702 / 2250 Loss: 0.2399\n",
      "Epoch: 3 / 3, Step: 1703 / 2250 Loss: 0.1487\n",
      "Epoch: 3 / 3, Step: 1704 / 2250 Loss: 0.1204\n",
      "Epoch: 3 / 3, Step: 1705 / 2250 Loss: 0.2317\n",
      "Epoch: 3 / 3, Step: 1706 / 2250 Loss: 0.2113\n",
      "Epoch: 3 / 3, Step: 1707 / 2250 Loss: 0.2451\n",
      "Epoch: 3 / 3, Step: 1708 / 2250 Loss: 0.1623\n",
      "Epoch: 3 / 3, Step: 1709 / 2250 Loss: 0.1151\n",
      "Epoch: 3 / 3, Step: 1710 / 2250 Loss: 0.1866\n",
      "Epoch: 3 / 3, Step: 1711 / 2250 Loss: 0.1341\n",
      "Epoch: 3 / 3, Step: 1712 / 2250 Loss: 0.2155\n",
      "Epoch: 3 / 3, Step: 1713 / 2250 Loss: 0.2029\n",
      "Epoch: 3 / 3, Step: 1714 / 2250 Loss: 0.1934\n",
      "Epoch: 3 / 3, Step: 1715 / 2250 Loss: 0.2638\n",
      "Epoch: 3 / 3, Step: 1716 / 2250 Loss: 0.3207\n",
      "Epoch: 3 / 3, Step: 1717 / 2250 Loss: 0.3142\n",
      "Epoch: 3 / 3, Step: 1718 / 2250 Loss: 0.2231\n",
      "Epoch: 3 / 3, Step: 1719 / 2250 Loss: 0.1195\n",
      "Epoch: 3 / 3, Step: 1720 / 2250 Loss: 0.4052\n",
      "Epoch: 3 / 3, Step: 1721 / 2250 Loss: 0.3633\n",
      "Epoch: 3 / 3, Step: 1722 / 2250 Loss: 0.3023\n",
      "Epoch: 3 / 3, Step: 1723 / 2250 Loss: 0.2758\n",
      "Epoch: 3 / 3, Step: 1724 / 2250 Loss: 0.0597\n",
      "Epoch: 3 / 3, Step: 1725 / 2250 Loss: 0.1194\n",
      "Epoch: 3 / 3, Step: 1726 / 2250 Loss: 0.3293\n",
      "Epoch: 3 / 3, Step: 1727 / 2250 Loss: 0.2782\n",
      "Epoch: 3 / 3, Step: 1728 / 2250 Loss: 0.1158\n",
      "Epoch: 3 / 3, Step: 1729 / 2250 Loss: 0.1733\n",
      "Epoch: 3 / 3, Step: 1730 / 2250 Loss: 0.5507\n",
      "Epoch: 3 / 3, Step: 1731 / 2250 Loss: 0.3779\n",
      "Epoch: 3 / 3, Step: 1732 / 2250 Loss: 0.3011\n",
      "Epoch: 3 / 3, Step: 1733 / 2250 Loss: 0.2240\n",
      "Epoch: 3 / 3, Step: 1734 / 2250 Loss: 0.3756\n",
      "Epoch: 3 / 3, Step: 1735 / 2250 Loss: 0.1282\n",
      "Epoch: 3 / 3, Step: 1736 / 2250 Loss: 0.1478\n",
      "Epoch: 3 / 3, Step: 1737 / 2250 Loss: 0.3435\n",
      "Epoch: 3 / 3, Step: 1738 / 2250 Loss: 0.1631\n",
      "Epoch: 3 / 3, Step: 1739 / 2250 Loss: 0.1830\n",
      "Epoch: 3 / 3, Step: 1740 / 2250 Loss: 0.1762\n",
      "Epoch: 3 / 3, Step: 1741 / 2250 Loss: 0.2695\n",
      "Epoch: 3 / 3, Step: 1742 / 2250 Loss: 0.4048\n",
      "Epoch: 3 / 3, Step: 1743 / 2250 Loss: 0.1031\n",
      "Epoch: 3 / 3, Step: 1744 / 2250 Loss: 0.2554\n",
      "Epoch: 3 / 3, Step: 1745 / 2250 Loss: 0.1660\n",
      "Epoch: 3 / 3, Step: 1746 / 2250 Loss: 0.1410\n",
      "Epoch: 3 / 3, Step: 1747 / 2250 Loss: 0.2903\n",
      "Epoch: 3 / 3, Step: 1748 / 2250 Loss: 0.1296\n",
      "Epoch: 3 / 3, Step: 1749 / 2250 Loss: 0.2754\n",
      "Epoch: 3 / 3, Step: 1750 / 2250 Loss: 0.2111\n",
      "Epoch: 3 / 3, Step: 1751 / 2250 Loss: 0.4237\n",
      "Epoch: 3 / 3, Step: 1752 / 2250 Loss: 0.0651\n",
      "Epoch: 3 / 3, Step: 1753 / 2250 Loss: 0.3557\n",
      "Epoch: 3 / 3, Step: 1754 / 2250 Loss: 0.0789\n",
      "Epoch: 3 / 3, Step: 1755 / 2250 Loss: 0.2075\n",
      "Epoch: 3 / 3, Step: 1756 / 2250 Loss: 0.5109\n",
      "Epoch: 3 / 3, Step: 1757 / 2250 Loss: 0.0978\n",
      "Epoch: 3 / 3, Step: 1758 / 2250 Loss: 0.2136\n",
      "Epoch: 3 / 3, Step: 1759 / 2250 Loss: 0.1941\n",
      "Epoch: 3 / 3, Step: 1760 / 2250 Loss: 0.0727\n",
      "Epoch: 3 / 3, Step: 1761 / 2250 Loss: 0.1308\n",
      "Epoch: 3 / 3, Step: 1762 / 2250 Loss: 0.1814\n",
      "Epoch: 3 / 3, Step: 1763 / 2250 Loss: 0.3862\n",
      "Epoch: 3 / 3, Step: 1764 / 2250 Loss: 0.1770\n",
      "Epoch: 3 / 3, Step: 1765 / 2250 Loss: 0.2302\n",
      "Epoch: 3 / 3, Step: 1766 / 2250 Loss: 0.2740\n",
      "Epoch: 3 / 3, Step: 1767 / 2250 Loss: 0.1395\n",
      "Epoch: 3 / 3, Step: 1768 / 2250 Loss: 0.3207\n",
      "Epoch: 3 / 3, Step: 1769 / 2250 Loss: 0.3405\n",
      "Epoch: 3 / 3, Step: 1770 / 2250 Loss: 0.3391\n",
      "Epoch: 3 / 3, Step: 1771 / 2250 Loss: 0.3069\n",
      "Epoch: 3 / 3, Step: 1772 / 2250 Loss: 0.2540\n",
      "Epoch: 3 / 3, Step: 1773 / 2250 Loss: 0.4478\n",
      "Epoch: 3 / 3, Step: 1774 / 2250 Loss: 0.1600\n",
      "Epoch: 3 / 3, Step: 1775 / 2250 Loss: 0.2225\n",
      "Epoch: 3 / 3, Step: 1776 / 2250 Loss: 0.0985\n",
      "Epoch: 3 / 3, Step: 1777 / 2250 Loss: 0.3642\n",
      "Epoch: 3 / 3, Step: 1778 / 2250 Loss: 0.1556\n",
      "Epoch: 3 / 3, Step: 1779 / 2250 Loss: 0.2948\n",
      "Epoch: 3 / 3, Step: 1780 / 2250 Loss: 0.3087\n",
      "Epoch: 3 / 3, Step: 1781 / 2250 Loss: 0.2608\n",
      "Epoch: 3 / 3, Step: 1782 / 2250 Loss: 0.1061\n",
      "Epoch: 3 / 3, Step: 1783 / 2250 Loss: 0.1173\n",
      "Epoch: 3 / 3, Step: 1784 / 2250 Loss: 0.1465\n",
      "Epoch: 3 / 3, Step: 1785 / 2250 Loss: 0.2573\n",
      "Epoch: 3 / 3, Step: 1786 / 2250 Loss: 0.4110\n",
      "Epoch: 3 / 3, Step: 1787 / 2250 Loss: 0.0989\n",
      "Epoch: 3 / 3, Step: 1788 / 2250 Loss: 0.0945\n",
      "Epoch: 3 / 3, Step: 1789 / 2250 Loss: 0.4293\n",
      "Epoch: 3 / 3, Step: 1790 / 2250 Loss: 0.1925\n",
      "Epoch: 3 / 3, Step: 1791 / 2250 Loss: 0.0945\n",
      "Epoch: 3 / 3, Step: 1792 / 2250 Loss: 0.2024\n",
      "Epoch: 3 / 3, Step: 1793 / 2250 Loss: 0.1511\n",
      "Epoch: 3 / 3, Step: 1794 / 2250 Loss: 0.2936\n",
      "Epoch: 3 / 3, Step: 1795 / 2250 Loss: 0.3190\n",
      "Epoch: 3 / 3, Step: 1796 / 2250 Loss: 0.2339\n",
      "Epoch: 3 / 3, Step: 1797 / 2250 Loss: 0.1781\n",
      "Epoch: 3 / 3, Step: 1798 / 2250 Loss: 0.1305\n",
      "Epoch: 3 / 3, Step: 1799 / 2250 Loss: 0.3042\n",
      "Epoch: 3 / 3, Step: 1800 / 2250 Loss: 0.1261\n",
      "Epoch: 3 / 3, Step: 1801 / 2250 Loss: 0.0553\n",
      "Epoch: 3 / 3, Step: 1802 / 2250 Loss: 0.1710\n",
      "Epoch: 3 / 3, Step: 1803 / 2250 Loss: 0.0714\n",
      "Epoch: 3 / 3, Step: 1804 / 2250 Loss: 0.5169\n",
      "Epoch: 3 / 3, Step: 1805 / 2250 Loss: 0.2914\n",
      "Epoch: 3 / 3, Step: 1806 / 2250 Loss: 0.0355\n",
      "Epoch: 3 / 3, Step: 1807 / 2250 Loss: 0.0987\n",
      "Epoch: 3 / 3, Step: 1808 / 2250 Loss: 0.1105\n",
      "Epoch: 3 / 3, Step: 1809 / 2250 Loss: 0.1023\n",
      "Epoch: 3 / 3, Step: 1810 / 2250 Loss: 0.2280\n",
      "Epoch: 3 / 3, Step: 1811 / 2250 Loss: 0.1471\n",
      "Epoch: 3 / 3, Step: 1812 / 2250 Loss: 0.2926\n",
      "Epoch: 3 / 3, Step: 1813 / 2250 Loss: 0.2205\n",
      "Epoch: 3 / 3, Step: 1814 / 2250 Loss: 0.4420\n",
      "Epoch: 3 / 3, Step: 1815 / 2250 Loss: 0.3493\n",
      "Epoch: 3 / 3, Step: 1816 / 2250 Loss: 0.1006\n",
      "Epoch: 3 / 3, Step: 1817 / 2250 Loss: 0.2356\n",
      "Epoch: 3 / 3, Step: 1818 / 2250 Loss: 0.1865\n",
      "Epoch: 3 / 3, Step: 1819 / 2250 Loss: 0.1898\n",
      "Epoch: 3 / 3, Step: 1820 / 2250 Loss: 0.1170\n",
      "Epoch: 3 / 3, Step: 1821 / 2250 Loss: 0.1526\n",
      "Epoch: 3 / 3, Step: 1822 / 2250 Loss: 0.0630\n",
      "Epoch: 3 / 3, Step: 1823 / 2250 Loss: 0.0619\n",
      "Epoch: 3 / 3, Step: 1824 / 2250 Loss: 0.1576\n",
      "Epoch: 3 / 3, Step: 1825 / 2250 Loss: 0.2718\n",
      "Epoch: 3 / 3, Step: 1826 / 2250 Loss: 0.1203\n",
      "Epoch: 3 / 3, Step: 1827 / 2250 Loss: 0.3452\n",
      "Epoch: 3 / 3, Step: 1828 / 2250 Loss: 0.1961\n",
      "Epoch: 3 / 3, Step: 1829 / 2250 Loss: 0.3237\n",
      "Epoch: 3 / 3, Step: 1830 / 2250 Loss: 0.2114\n",
      "Epoch: 3 / 3, Step: 1831 / 2250 Loss: 0.3478\n",
      "Epoch: 3 / 3, Step: 1832 / 2250 Loss: 0.4413\n",
      "Epoch: 3 / 3, Step: 1833 / 2250 Loss: 0.1655\n",
      "Epoch: 3 / 3, Step: 1834 / 2250 Loss: 0.3479\n",
      "Epoch: 3 / 3, Step: 1835 / 2250 Loss: 0.1247\n",
      "Epoch: 3 / 3, Step: 1836 / 2250 Loss: 0.1285\n",
      "Epoch: 3 / 3, Step: 1837 / 2250 Loss: 0.3653\n",
      "Epoch: 3 / 3, Step: 1838 / 2250 Loss: 0.3536\n",
      "Epoch: 3 / 3, Step: 1839 / 2250 Loss: 0.1376\n",
      "Epoch: 3 / 3, Step: 1840 / 2250 Loss: 0.1465\n",
      "Epoch: 3 / 3, Step: 1841 / 2250 Loss: 0.2181\n",
      "Epoch: 3 / 3, Step: 1842 / 2250 Loss: 0.1168\n",
      "Epoch: 3 / 3, Step: 1843 / 2250 Loss: 0.1826\n",
      "Epoch: 3 / 3, Step: 1844 / 2250 Loss: 0.1682\n",
      "Epoch: 3 / 3, Step: 1845 / 2250 Loss: 0.1843\n",
      "Epoch: 3 / 3, Step: 1846 / 2250 Loss: 0.2001\n",
      "Epoch: 3 / 3, Step: 1847 / 2250 Loss: 0.1741\n",
      "Epoch: 3 / 3, Step: 1848 / 2250 Loss: 0.1214\n",
      "Epoch: 3 / 3, Step: 1849 / 2250 Loss: 0.6411\n",
      "Epoch: 3 / 3, Step: 1850 / 2250 Loss: 0.4617\n",
      "Epoch: 3 / 3, Step: 1851 / 2250 Loss: 0.1691\n",
      "Epoch: 3 / 3, Step: 1852 / 2250 Loss: 0.1314\n",
      "Epoch: 3 / 3, Step: 1853 / 2250 Loss: 0.0609\n",
      "Epoch: 3 / 3, Step: 1854 / 2250 Loss: 0.1023\n",
      "Epoch: 3 / 3, Step: 1855 / 2250 Loss: 0.2140\n",
      "Epoch: 3 / 3, Step: 1856 / 2250 Loss: 0.1947\n",
      "Epoch: 3 / 3, Step: 1857 / 2250 Loss: 0.2057\n",
      "Epoch: 3 / 3, Step: 1858 / 2250 Loss: 0.1016\n",
      "Epoch: 3 / 3, Step: 1859 / 2250 Loss: 0.1007\n",
      "Epoch: 3 / 3, Step: 1860 / 2250 Loss: 0.0418\n",
      "Epoch: 3 / 3, Step: 1861 / 2250 Loss: 0.2108\n",
      "Epoch: 3 / 3, Step: 1862 / 2250 Loss: 0.4147\n",
      "Epoch: 3 / 3, Step: 1863 / 2250 Loss: 0.4388\n",
      "Epoch: 3 / 3, Step: 1864 / 2250 Loss: 0.1098\n",
      "Epoch: 3 / 3, Step: 1865 / 2250 Loss: 0.2256\n",
      "Epoch: 3 / 3, Step: 1866 / 2250 Loss: 0.3151\n",
      "Epoch: 3 / 3, Step: 1867 / 2250 Loss: 0.4599\n",
      "Epoch: 3 / 3, Step: 1868 / 2250 Loss: 0.1893\n",
      "Epoch: 3 / 3, Step: 1869 / 2250 Loss: 0.4185\n",
      "Epoch: 3 / 3, Step: 1870 / 2250 Loss: 0.2085\n",
      "Epoch: 3 / 3, Step: 1871 / 2250 Loss: 0.0763\n",
      "Epoch: 3 / 3, Step: 1872 / 2250 Loss: 0.1178\n",
      "Epoch: 3 / 3, Step: 1873 / 2250 Loss: 0.2388\n",
      "Epoch: 3 / 3, Step: 1874 / 2250 Loss: 0.1954\n",
      "Epoch: 3 / 3, Step: 1875 / 2250 Loss: 0.3734\n",
      "Epoch: 3 / 3, Step: 1876 / 2250 Loss: 0.1770\n",
      "Epoch: 3 / 3, Step: 1877 / 2250 Loss: 0.1410\n",
      "Epoch: 3 / 3, Step: 1878 / 2250 Loss: 0.0888\n",
      "Epoch: 3 / 3, Step: 1879 / 2250 Loss: 0.2064\n",
      "Epoch: 3 / 3, Step: 1880 / 2250 Loss: 0.0743\n",
      "Epoch: 3 / 3, Step: 1881 / 2250 Loss: 0.1783\n",
      "Epoch: 3 / 3, Step: 1882 / 2250 Loss: 0.4154\n",
      "Epoch: 3 / 3, Step: 1883 / 2250 Loss: 0.1496\n",
      "Epoch: 3 / 3, Step: 1884 / 2250 Loss: 0.2083\n",
      "Epoch: 3 / 3, Step: 1885 / 2250 Loss: 0.1704\n",
      "Epoch: 3 / 3, Step: 1886 / 2250 Loss: 0.2829\n",
      "Epoch: 3 / 3, Step: 1887 / 2250 Loss: 0.3645\n",
      "Epoch: 3 / 3, Step: 1888 / 2250 Loss: 0.1889\n",
      "Epoch: 3 / 3, Step: 1889 / 2250 Loss: 0.4241\n",
      "Epoch: 3 / 3, Step: 1890 / 2250 Loss: 0.2251\n",
      "Epoch: 3 / 3, Step: 1891 / 2250 Loss: 0.2011\n",
      "Epoch: 3 / 3, Step: 1892 / 2250 Loss: 0.1111\n",
      "Epoch: 3 / 3, Step: 1893 / 2250 Loss: 0.1909\n",
      "Epoch: 3 / 3, Step: 1894 / 2250 Loss: 0.0504\n",
      "Epoch: 3 / 3, Step: 1895 / 2250 Loss: 0.1115\n",
      "Epoch: 3 / 3, Step: 1896 / 2250 Loss: 0.1965\n",
      "Epoch: 3 / 3, Step: 1897 / 2250 Loss: 0.1385\n",
      "Epoch: 3 / 3, Step: 1898 / 2250 Loss: 0.1848\n",
      "Epoch: 3 / 3, Step: 1899 / 2250 Loss: 0.1571\n",
      "Epoch: 3 / 3, Step: 1900 / 2250 Loss: 0.1107\n",
      "Epoch: 3 / 3, Step: 1901 / 2250 Loss: 0.3575\n",
      "Epoch: 3 / 3, Step: 1902 / 2250 Loss: 0.4503\n",
      "Epoch: 3 / 3, Step: 1903 / 2250 Loss: 0.1237\n",
      "Epoch: 3 / 3, Step: 1904 / 2250 Loss: 0.0499\n",
      "Epoch: 3 / 3, Step: 1905 / 2250 Loss: 0.0607\n",
      "Epoch: 3 / 3, Step: 1906 / 2250 Loss: 0.1320\n",
      "Epoch: 3 / 3, Step: 1907 / 2250 Loss: 0.2644\n",
      "Epoch: 3 / 3, Step: 1908 / 2250 Loss: 0.3367\n",
      "Epoch: 3 / 3, Step: 1909 / 2250 Loss: 0.1864\n",
      "Epoch: 3 / 3, Step: 1910 / 2250 Loss: 0.2995\n",
      "Epoch: 3 / 3, Step: 1911 / 2250 Loss: 0.1847\n",
      "Epoch: 3 / 3, Step: 1912 / 2250 Loss: 0.1569\n",
      "Epoch: 3 / 3, Step: 1913 / 2250 Loss: 0.2776\n",
      "Epoch: 3 / 3, Step: 1914 / 2250 Loss: 0.0975\n",
      "Epoch: 3 / 3, Step: 1915 / 2250 Loss: 0.1836\n",
      "Epoch: 3 / 3, Step: 1916 / 2250 Loss: 0.1247\n",
      "Epoch: 3 / 3, Step: 1917 / 2250 Loss: 0.0990\n",
      "Epoch: 3 / 3, Step: 1918 / 2250 Loss: 0.1468\n",
      "Epoch: 3 / 3, Step: 1919 / 2250 Loss: 0.1275\n",
      "Epoch: 3 / 3, Step: 1920 / 2250 Loss: 0.4913\n",
      "Epoch: 3 / 3, Step: 1921 / 2250 Loss: 0.1421\n",
      "Epoch: 3 / 3, Step: 1922 / 2250 Loss: 0.2045\n",
      "Epoch: 3 / 3, Step: 1923 / 2250 Loss: 0.2186\n",
      "Epoch: 3 / 3, Step: 1924 / 2250 Loss: 0.2362\n",
      "Epoch: 3 / 3, Step: 1925 / 2250 Loss: 0.1544\n",
      "Epoch: 3 / 3, Step: 1926 / 2250 Loss: 0.2891\n",
      "Epoch: 3 / 3, Step: 1927 / 2250 Loss: 0.3118\n",
      "Epoch: 3 / 3, Step: 1928 / 2250 Loss: 0.1857\n",
      "Epoch: 3 / 3, Step: 1929 / 2250 Loss: 0.3100\n",
      "Epoch: 3 / 3, Step: 1930 / 2250 Loss: 0.1855\n",
      "Epoch: 3 / 3, Step: 1931 / 2250 Loss: 0.1496\n",
      "Epoch: 3 / 3, Step: 1932 / 2250 Loss: 0.2403\n",
      "Epoch: 3 / 3, Step: 1933 / 2250 Loss: 0.0893\n",
      "Epoch: 3 / 3, Step: 1934 / 2250 Loss: 0.1232\n",
      "Epoch: 3 / 3, Step: 1935 / 2250 Loss: 0.0944\n",
      "Epoch: 3 / 3, Step: 1936 / 2250 Loss: 0.2826\n",
      "Epoch: 3 / 3, Step: 1937 / 2250 Loss: 0.1132\n",
      "Epoch: 3 / 3, Step: 1938 / 2250 Loss: 0.1592\n",
      "Epoch: 3 / 3, Step: 1939 / 2250 Loss: 0.4295\n",
      "Epoch: 3 / 3, Step: 1940 / 2250 Loss: 0.5412\n",
      "Epoch: 3 / 3, Step: 1941 / 2250 Loss: 0.2714\n",
      "Epoch: 3 / 3, Step: 1942 / 2250 Loss: 0.3159\n",
      "Epoch: 3 / 3, Step: 1943 / 2250 Loss: 0.4110\n",
      "Epoch: 3 / 3, Step: 1944 / 2250 Loss: 0.2317\n",
      "Epoch: 3 / 3, Step: 1945 / 2250 Loss: 0.1352\n",
      "Epoch: 3 / 3, Step: 1946 / 2250 Loss: 0.3002\n",
      "Epoch: 3 / 3, Step: 1947 / 2250 Loss: 0.1980\n",
      "Epoch: 3 / 3, Step: 1948 / 2250 Loss: 0.4914\n",
      "Epoch: 3 / 3, Step: 1949 / 2250 Loss: 0.2066\n",
      "Epoch: 3 / 3, Step: 1950 / 2250 Loss: 0.0944\n",
      "Epoch: 3 / 3, Step: 1951 / 2250 Loss: 0.2128\n",
      "Epoch: 3 / 3, Step: 1952 / 2250 Loss: 0.3171\n",
      "Epoch: 3 / 3, Step: 1953 / 2250 Loss: 0.2149\n",
      "Epoch: 3 / 3, Step: 1954 / 2250 Loss: 0.3363\n",
      "Epoch: 3 / 3, Step: 1955 / 2250 Loss: 0.2326\n",
      "Epoch: 3 / 3, Step: 1956 / 2250 Loss: 0.1995\n",
      "Epoch: 3 / 3, Step: 1957 / 2250 Loss: 0.1184\n",
      "Epoch: 3 / 3, Step: 1958 / 2250 Loss: 0.3699\n",
      "Epoch: 3 / 3, Step: 1959 / 2250 Loss: 0.1780\n",
      "Epoch: 3 / 3, Step: 1960 / 2250 Loss: 0.1777\n",
      "Epoch: 3 / 3, Step: 1961 / 2250 Loss: 0.1589\n",
      "Epoch: 3 / 3, Step: 1962 / 2250 Loss: 0.0374\n",
      "Epoch: 3 / 3, Step: 1963 / 2250 Loss: 0.1521\n",
      "Epoch: 3 / 3, Step: 1964 / 2250 Loss: 0.4473\n",
      "Epoch: 3 / 3, Step: 1965 / 2250 Loss: 0.1895\n",
      "Epoch: 3 / 3, Step: 1966 / 2250 Loss: 0.2090\n",
      "Epoch: 3 / 3, Step: 1967 / 2250 Loss: 0.2784\n",
      "Epoch: 3 / 3, Step: 1968 / 2250 Loss: 0.2659\n",
      "Epoch: 3 / 3, Step: 1969 / 2250 Loss: 0.2168\n",
      "Epoch: 3 / 3, Step: 1970 / 2250 Loss: 0.1885\n",
      "Epoch: 3 / 3, Step: 1971 / 2250 Loss: 0.2886\n",
      "Epoch: 3 / 3, Step: 1972 / 2250 Loss: 0.2004\n",
      "Epoch: 3 / 3, Step: 1973 / 2250 Loss: 0.1615\n",
      "Epoch: 3 / 3, Step: 1974 / 2250 Loss: 0.2327\n",
      "Epoch: 3 / 3, Step: 1975 / 2250 Loss: 0.1183\n",
      "Epoch: 3 / 3, Step: 1976 / 2250 Loss: 0.1479\n",
      "Epoch: 3 / 3, Step: 1977 / 2250 Loss: 0.2540\n",
      "Epoch: 3 / 3, Step: 1978 / 2250 Loss: 0.4188\n",
      "Epoch: 3 / 3, Step: 1979 / 2250 Loss: 0.1824\n",
      "Epoch: 3 / 3, Step: 1980 / 2250 Loss: 0.1220\n",
      "Epoch: 3 / 3, Step: 1981 / 2250 Loss: 0.1554\n",
      "Epoch: 3 / 3, Step: 1982 / 2250 Loss: 0.2170\n",
      "Epoch: 3 / 3, Step: 1983 / 2250 Loss: 0.2373\n",
      "Epoch: 3 / 3, Step: 1984 / 2250 Loss: 0.2854\n",
      "Epoch: 3 / 3, Step: 1985 / 2250 Loss: 0.1545\n",
      "Epoch: 3 / 3, Step: 1986 / 2250 Loss: 0.1585\n",
      "Epoch: 3 / 3, Step: 1987 / 2250 Loss: 0.1630\n",
      "Epoch: 3 / 3, Step: 1988 / 2250 Loss: 0.6778\n",
      "Epoch: 3 / 3, Step: 1989 / 2250 Loss: 0.2566\n",
      "Epoch: 3 / 3, Step: 1990 / 2250 Loss: 0.2744\n",
      "Epoch: 3 / 3, Step: 1991 / 2250 Loss: 0.2441\n",
      "Epoch: 3 / 3, Step: 1992 / 2250 Loss: 0.1566\n",
      "Epoch: 3 / 3, Step: 1993 / 2250 Loss: 0.4112\n",
      "Epoch: 3 / 3, Step: 1994 / 2250 Loss: 0.1207\n",
      "Epoch: 3 / 3, Step: 1995 / 2250 Loss: 0.2591\n",
      "Epoch: 3 / 3, Step: 1996 / 2250 Loss: 0.4063\n",
      "Epoch: 3 / 3, Step: 1997 / 2250 Loss: 0.1471\n",
      "Epoch: 3 / 3, Step: 1998 / 2250 Loss: 0.1704\n",
      "Epoch: 3 / 3, Step: 1999 / 2250 Loss: 0.1597\n",
      "Epoch: 3 / 3, Step: 2000 / 2250 Loss: 0.1072\n",
      "Epoch: 3 / 3, Step: 2001 / 2250 Loss: 0.2004\n",
      "Epoch: 3 / 3, Step: 2002 / 2250 Loss: 0.2849\n",
      "Epoch: 3 / 3, Step: 2003 / 2250 Loss: 0.1035\n",
      "Epoch: 3 / 3, Step: 2004 / 2250 Loss: 0.4029\n",
      "Epoch: 3 / 3, Step: 2005 / 2250 Loss: 0.1444\n",
      "Epoch: 3 / 3, Step: 2006 / 2250 Loss: 0.1707\n",
      "Epoch: 3 / 3, Step: 2007 / 2250 Loss: 0.2329\n",
      "Epoch: 3 / 3, Step: 2008 / 2250 Loss: 0.3898\n",
      "Epoch: 3 / 3, Step: 2009 / 2250 Loss: 0.1638\n",
      "Epoch: 3 / 3, Step: 2010 / 2250 Loss: 0.1899\n",
      "Epoch: 3 / 3, Step: 2011 / 2250 Loss: 0.1769\n",
      "Epoch: 3 / 3, Step: 2012 / 2250 Loss: 0.2204\n",
      "Epoch: 3 / 3, Step: 2013 / 2250 Loss: 0.1548\n",
      "Epoch: 3 / 3, Step: 2014 / 2250 Loss: 0.2738\n",
      "Epoch: 3 / 3, Step: 2015 / 2250 Loss: 0.5057\n",
      "Epoch: 3 / 3, Step: 2016 / 2250 Loss: 0.2971\n",
      "Epoch: 3 / 3, Step: 2017 / 2250 Loss: 0.1657\n",
      "Epoch: 3 / 3, Step: 2018 / 2250 Loss: 0.1939\n",
      "Epoch: 3 / 3, Step: 2019 / 2250 Loss: 0.3230\n",
      "Epoch: 3 / 3, Step: 2020 / 2250 Loss: 0.3781\n",
      "Epoch: 3 / 3, Step: 2021 / 2250 Loss: 0.1797\n",
      "Epoch: 3 / 3, Step: 2022 / 2250 Loss: 0.2095\n",
      "Epoch: 3 / 3, Step: 2023 / 2250 Loss: 0.1039\n",
      "Epoch: 3 / 3, Step: 2024 / 2250 Loss: 0.3244\n",
      "Epoch: 3 / 3, Step: 2025 / 2250 Loss: 0.4653\n",
      "Epoch: 3 / 3, Step: 2026 / 2250 Loss: 0.1940\n",
      "Epoch: 3 / 3, Step: 2027 / 2250 Loss: 0.2940\n",
      "Epoch: 3 / 3, Step: 2028 / 2250 Loss: 0.2576\n",
      "Epoch: 3 / 3, Step: 2029 / 2250 Loss: 0.4313\n",
      "Epoch: 3 / 3, Step: 2030 / 2250 Loss: 0.3570\n",
      "Epoch: 3 / 3, Step: 2031 / 2250 Loss: 0.1200\n",
      "Epoch: 3 / 3, Step: 2032 / 2250 Loss: 0.3982\n",
      "Epoch: 3 / 3, Step: 2033 / 2250 Loss: 0.3360\n",
      "Epoch: 3 / 3, Step: 2034 / 2250 Loss: 0.1590\n",
      "Epoch: 3 / 3, Step: 2035 / 2250 Loss: 0.3436\n",
      "Epoch: 3 / 3, Step: 2036 / 2250 Loss: 0.2658\n",
      "Epoch: 3 / 3, Step: 2037 / 2250 Loss: 0.1721\n",
      "Epoch: 3 / 3, Step: 2038 / 2250 Loss: 0.3815\n",
      "Epoch: 3 / 3, Step: 2039 / 2250 Loss: 0.2932\n",
      "Epoch: 3 / 3, Step: 2040 / 2250 Loss: 0.1066\n",
      "Epoch: 3 / 3, Step: 2041 / 2250 Loss: 0.3196\n",
      "Epoch: 3 / 3, Step: 2042 / 2250 Loss: 0.1331\n",
      "Epoch: 3 / 3, Step: 2043 / 2250 Loss: 0.2450\n",
      "Epoch: 3 / 3, Step: 2044 / 2250 Loss: 0.2076\n",
      "Epoch: 3 / 3, Step: 2045 / 2250 Loss: 0.1271\n",
      "Epoch: 3 / 3, Step: 2046 / 2250 Loss: 0.1579\n",
      "Epoch: 3 / 3, Step: 2047 / 2250 Loss: 0.1433\n",
      "Epoch: 3 / 3, Step: 2048 / 2250 Loss: 0.1375\n",
      "Epoch: 3 / 3, Step: 2049 / 2250 Loss: 0.2478\n",
      "Epoch: 3 / 3, Step: 2050 / 2250 Loss: 0.2157\n",
      "Epoch: 3 / 3, Step: 2051 / 2250 Loss: 0.2495\n",
      "Epoch: 3 / 3, Step: 2052 / 2250 Loss: 0.0466\n",
      "Epoch: 3 / 3, Step: 2053 / 2250 Loss: 0.1842\n",
      "Epoch: 3 / 3, Step: 2054 / 2250 Loss: 0.1676\n",
      "Epoch: 3 / 3, Step: 2055 / 2250 Loss: 0.2284\n",
      "Epoch: 3 / 3, Step: 2056 / 2250 Loss: 0.3711\n",
      "Epoch: 3 / 3, Step: 2057 / 2250 Loss: 0.2032\n",
      "Epoch: 3 / 3, Step: 2058 / 2250 Loss: 0.2521\n",
      "Epoch: 3 / 3, Step: 2059 / 2250 Loss: 0.1264\n",
      "Epoch: 3 / 3, Step: 2060 / 2250 Loss: 0.2367\n",
      "Epoch: 3 / 3, Step: 2061 / 2250 Loss: 0.2130\n",
      "Epoch: 3 / 3, Step: 2062 / 2250 Loss: 0.3111\n",
      "Epoch: 3 / 3, Step: 2063 / 2250 Loss: 0.2205\n",
      "Epoch: 3 / 3, Step: 2064 / 2250 Loss: 0.1948\n",
      "Epoch: 3 / 3, Step: 2065 / 2250 Loss: 0.2202\n",
      "Epoch: 3 / 3, Step: 2066 / 2250 Loss: 0.3032\n",
      "Epoch: 3 / 3, Step: 2067 / 2250 Loss: 0.1707\n",
      "Epoch: 3 / 3, Step: 2068 / 2250 Loss: 0.0923\n",
      "Epoch: 3 / 3, Step: 2069 / 2250 Loss: 0.1542\n",
      "Epoch: 3 / 3, Step: 2070 / 2250 Loss: 0.3113\n",
      "Epoch: 3 / 3, Step: 2071 / 2250 Loss: 0.4294\n",
      "Epoch: 3 / 3, Step: 2072 / 2250 Loss: 0.1365\n",
      "Epoch: 3 / 3, Step: 2073 / 2250 Loss: 0.1062\n",
      "Epoch: 3 / 3, Step: 2074 / 2250 Loss: 0.1262\n",
      "Epoch: 3 / 3, Step: 2075 / 2250 Loss: 0.2685\n",
      "Epoch: 3 / 3, Step: 2076 / 2250 Loss: 0.2237\n",
      "Epoch: 3 / 3, Step: 2077 / 2250 Loss: 0.0919\n",
      "Epoch: 3 / 3, Step: 2078 / 2250 Loss: 0.2085\n",
      "Epoch: 3 / 3, Step: 2079 / 2250 Loss: 0.2101\n",
      "Epoch: 3 / 3, Step: 2080 / 2250 Loss: 0.4959\n",
      "Epoch: 3 / 3, Step: 2081 / 2250 Loss: 0.1776\n",
      "Epoch: 3 / 3, Step: 2082 / 2250 Loss: 0.0950\n",
      "Epoch: 3 / 3, Step: 2083 / 2250 Loss: 0.2142\n",
      "Epoch: 3 / 3, Step: 2084 / 2250 Loss: 0.2726\n",
      "Epoch: 3 / 3, Step: 2085 / 2250 Loss: 0.1097\n",
      "Epoch: 3 / 3, Step: 2086 / 2250 Loss: 0.1478\n",
      "Epoch: 3 / 3, Step: 2087 / 2250 Loss: 0.3252\n",
      "Epoch: 3 / 3, Step: 2088 / 2250 Loss: 0.2334\n",
      "Epoch: 3 / 3, Step: 2089 / 2250 Loss: 0.5752\n",
      "Epoch: 3 / 3, Step: 2090 / 2250 Loss: 0.1459\n",
      "Epoch: 3 / 3, Step: 2091 / 2250 Loss: 0.2177\n",
      "Epoch: 3 / 3, Step: 2092 / 2250 Loss: 0.1792\n",
      "Epoch: 3 / 3, Step: 2093 / 2250 Loss: 0.3319\n",
      "Epoch: 3 / 3, Step: 2094 / 2250 Loss: 0.1641\n",
      "Epoch: 3 / 3, Step: 2095 / 2250 Loss: 0.2060\n",
      "Epoch: 3 / 3, Step: 2096 / 2250 Loss: 0.3981\n",
      "Epoch: 3 / 3, Step: 2097 / 2250 Loss: 0.1154\n",
      "Epoch: 3 / 3, Step: 2098 / 2250 Loss: 0.3865\n",
      "Epoch: 3 / 3, Step: 2099 / 2250 Loss: 0.0931\n",
      "Epoch: 3 / 3, Step: 2100 / 2250 Loss: 0.1641\n",
      "Epoch: 3 / 3, Step: 2101 / 2250 Loss: 0.0607\n",
      "Epoch: 3 / 3, Step: 2102 / 2250 Loss: 0.1385\n",
      "Epoch: 3 / 3, Step: 2103 / 2250 Loss: 0.0986\n",
      "Epoch: 3 / 3, Step: 2104 / 2250 Loss: 0.3187\n",
      "Epoch: 3 / 3, Step: 2105 / 2250 Loss: 0.0947\n",
      "Epoch: 3 / 3, Step: 2106 / 2250 Loss: 0.3401\n",
      "Epoch: 3 / 3, Step: 2107 / 2250 Loss: 0.3387\n",
      "Epoch: 3 / 3, Step: 2108 / 2250 Loss: 0.3526\n",
      "Epoch: 3 / 3, Step: 2109 / 2250 Loss: 0.1202\n",
      "Epoch: 3 / 3, Step: 2110 / 2250 Loss: 0.1701\n",
      "Epoch: 3 / 3, Step: 2111 / 2250 Loss: 0.2394\n",
      "Epoch: 3 / 3, Step: 2112 / 2250 Loss: 0.1699\n",
      "Epoch: 3 / 3, Step: 2113 / 2250 Loss: 0.2446\n",
      "Epoch: 3 / 3, Step: 2114 / 2250 Loss: 0.1833\n",
      "Epoch: 3 / 3, Step: 2115 / 2250 Loss: 0.1083\n",
      "Epoch: 3 / 3, Step: 2116 / 2250 Loss: 0.3854\n",
      "Epoch: 3 / 3, Step: 2117 / 2250 Loss: 0.2575\n",
      "Epoch: 3 / 3, Step: 2118 / 2250 Loss: 0.1124\n",
      "Epoch: 3 / 3, Step: 2119 / 2250 Loss: 0.1039\n",
      "Epoch: 3 / 3, Step: 2120 / 2250 Loss: 0.3339\n",
      "Epoch: 3 / 3, Step: 2121 / 2250 Loss: 0.3099\n",
      "Epoch: 3 / 3, Step: 2122 / 2250 Loss: 0.2221\n",
      "Epoch: 3 / 3, Step: 2123 / 2250 Loss: 0.1059\n",
      "Epoch: 3 / 3, Step: 2124 / 2250 Loss: 0.1658\n",
      "Epoch: 3 / 3, Step: 2125 / 2250 Loss: 0.1856\n",
      "Epoch: 3 / 3, Step: 2126 / 2250 Loss: 0.0969\n",
      "Epoch: 3 / 3, Step: 2127 / 2250 Loss: 0.1644\n",
      "Epoch: 3 / 3, Step: 2128 / 2250 Loss: 0.2385\n",
      "Epoch: 3 / 3, Step: 2129 / 2250 Loss: 0.5176\n",
      "Epoch: 3 / 3, Step: 2130 / 2250 Loss: 0.2430\n",
      "Epoch: 3 / 3, Step: 2131 / 2250 Loss: 0.2095\n",
      "Epoch: 3 / 3, Step: 2132 / 2250 Loss: 0.4415\n",
      "Epoch: 3 / 3, Step: 2133 / 2250 Loss: 0.2506\n",
      "Epoch: 3 / 3, Step: 2134 / 2250 Loss: 0.1346\n",
      "Epoch: 3 / 3, Step: 2135 / 2250 Loss: 0.3008\n",
      "Epoch: 3 / 3, Step: 2136 / 2250 Loss: 0.0745\n",
      "Epoch: 3 / 3, Step: 2137 / 2250 Loss: 0.3828\n",
      "Epoch: 3 / 3, Step: 2138 / 2250 Loss: 0.1743\n",
      "Epoch: 3 / 3, Step: 2139 / 2250 Loss: 0.2193\n",
      "Epoch: 3 / 3, Step: 2140 / 2250 Loss: 0.2196\n",
      "Epoch: 3 / 3, Step: 2141 / 2250 Loss: 0.0585\n",
      "Epoch: 3 / 3, Step: 2142 / 2250 Loss: 0.2539\n",
      "Epoch: 3 / 3, Step: 2143 / 2250 Loss: 0.2540\n",
      "Epoch: 3 / 3, Step: 2144 / 2250 Loss: 0.2348\n",
      "Epoch: 3 / 3, Step: 2145 / 2250 Loss: 0.2149\n",
      "Epoch: 3 / 3, Step: 2146 / 2250 Loss: 0.4758\n",
      "Epoch: 3 / 3, Step: 2147 / 2250 Loss: 0.2888\n",
      "Epoch: 3 / 3, Step: 2148 / 2250 Loss: 0.3556\n",
      "Epoch: 3 / 3, Step: 2149 / 2250 Loss: 0.1429\n",
      "Epoch: 3 / 3, Step: 2150 / 2250 Loss: 0.3232\n",
      "Epoch: 3 / 3, Step: 2151 / 2250 Loss: 0.3289\n",
      "Epoch: 3 / 3, Step: 2152 / 2250 Loss: 0.0797\n",
      "Epoch: 3 / 3, Step: 2153 / 2250 Loss: 0.0926\n",
      "Epoch: 3 / 3, Step: 2154 / 2250 Loss: 0.1591\n",
      "Epoch: 3 / 3, Step: 2155 / 2250 Loss: 0.1536\n",
      "Epoch: 3 / 3, Step: 2156 / 2250 Loss: 0.2504\n",
      "Epoch: 3 / 3, Step: 2157 / 2250 Loss: 0.2660\n",
      "Epoch: 3 / 3, Step: 2158 / 2250 Loss: 0.3824\n",
      "Epoch: 3 / 3, Step: 2159 / 2250 Loss: 0.2689\n",
      "Epoch: 3 / 3, Step: 2160 / 2250 Loss: 0.1776\n",
      "Epoch: 3 / 3, Step: 2161 / 2250 Loss: 0.1889\n",
      "Epoch: 3 / 3, Step: 2162 / 2250 Loss: 0.2316\n",
      "Epoch: 3 / 3, Step: 2163 / 2250 Loss: 0.1887\n",
      "Epoch: 3 / 3, Step: 2164 / 2250 Loss: 0.2853\n",
      "Epoch: 3 / 3, Step: 2165 / 2250 Loss: 0.2724\n",
      "Epoch: 3 / 3, Step: 2166 / 2250 Loss: 0.1462\n",
      "Epoch: 3 / 3, Step: 2167 / 2250 Loss: 0.1194\n",
      "Epoch: 3 / 3, Step: 2168 / 2250 Loss: 0.1142\n",
      "Epoch: 3 / 3, Step: 2169 / 2250 Loss: 0.1392\n",
      "Epoch: 3 / 3, Step: 2170 / 2250 Loss: 0.2096\n",
      "Epoch: 3 / 3, Step: 2171 / 2250 Loss: 0.3118\n",
      "Epoch: 3 / 3, Step: 2172 / 2250 Loss: 0.1449\n",
      "Epoch: 3 / 3, Step: 2173 / 2250 Loss: 0.4739\n",
      "Epoch: 3 / 3, Step: 2174 / 2250 Loss: 0.2291\n",
      "Epoch: 3 / 3, Step: 2175 / 2250 Loss: 0.2645\n",
      "Epoch: 3 / 3, Step: 2176 / 2250 Loss: 0.4225\n",
      "Epoch: 3 / 3, Step: 2177 / 2250 Loss: 0.2464\n",
      "Epoch: 3 / 3, Step: 2178 / 2250 Loss: 0.2659\n",
      "Epoch: 3 / 3, Step: 2179 / 2250 Loss: 0.2714\n",
      "Epoch: 3 / 3, Step: 2180 / 2250 Loss: 0.1480\n",
      "Epoch: 3 / 3, Step: 2181 / 2250 Loss: 0.3107\n",
      "Epoch: 3 / 3, Step: 2182 / 2250 Loss: 0.2627\n",
      "Epoch: 3 / 3, Step: 2183 / 2250 Loss: 0.1719\n",
      "Epoch: 3 / 3, Step: 2184 / 2250 Loss: 0.2953\n",
      "Epoch: 3 / 3, Step: 2185 / 2250 Loss: 0.2210\n",
      "Epoch: 3 / 3, Step: 2186 / 2250 Loss: 0.2088\n",
      "Epoch: 3 / 3, Step: 2187 / 2250 Loss: 0.2958\n",
      "Epoch: 3 / 3, Step: 2188 / 2250 Loss: 0.2367\n",
      "Epoch: 3 / 3, Step: 2189 / 2250 Loss: 0.3682\n",
      "Epoch: 3 / 3, Step: 2190 / 2250 Loss: 0.1996\n",
      "Epoch: 3 / 3, Step: 2191 / 2250 Loss: 0.1631\n",
      "Epoch: 3 / 3, Step: 2192 / 2250 Loss: 0.2668\n",
      "Epoch: 3 / 3, Step: 2193 / 2250 Loss: 0.2833\n",
      "Epoch: 3 / 3, Step: 2194 / 2250 Loss: 0.1645\n",
      "Epoch: 3 / 3, Step: 2195 / 2250 Loss: 0.2487\n",
      "Epoch: 3 / 3, Step: 2196 / 2250 Loss: 0.0807\n",
      "Epoch: 3 / 3, Step: 2197 / 2250 Loss: 0.3182\n",
      "Epoch: 3 / 3, Step: 2198 / 2250 Loss: 0.1566\n",
      "Epoch: 3 / 3, Step: 2199 / 2250 Loss: 0.1065\n",
      "Epoch: 3 / 3, Step: 2200 / 2250 Loss: 0.4262\n",
      "Epoch: 3 / 3, Step: 2201 / 2250 Loss: 0.2556\n",
      "Epoch: 3 / 3, Step: 2202 / 2250 Loss: 0.2056\n",
      "Epoch: 3 / 3, Step: 2203 / 2250 Loss: 0.1268\n",
      "Epoch: 3 / 3, Step: 2204 / 2250 Loss: 0.1779\n",
      "Epoch: 3 / 3, Step: 2205 / 2250 Loss: 0.2228\n",
      "Epoch: 3 / 3, Step: 2206 / 2250 Loss: 0.1412\n",
      "Epoch: 3 / 3, Step: 2207 / 2250 Loss: 0.2583\n",
      "Epoch: 3 / 3, Step: 2208 / 2250 Loss: 0.3099\n",
      "Epoch: 3 / 3, Step: 2209 / 2250 Loss: 0.2337\n",
      "Epoch: 3 / 3, Step: 2210 / 2250 Loss: 0.2234\n",
      "Epoch: 3 / 3, Step: 2211 / 2250 Loss: 0.1897\n",
      "Epoch: 3 / 3, Step: 2212 / 2250 Loss: 0.2019\n",
      "Epoch: 3 / 3, Step: 2213 / 2250 Loss: 0.5751\n",
      "Epoch: 3 / 3, Step: 2214 / 2250 Loss: 0.2047\n",
      "Epoch: 3 / 3, Step: 2215 / 2250 Loss: 0.2858\n",
      "Epoch: 3 / 3, Step: 2216 / 2250 Loss: 0.2613\n",
      "Epoch: 3 / 3, Step: 2217 / 2250 Loss: 0.1610\n",
      "Epoch: 3 / 3, Step: 2218 / 2250 Loss: 0.1926\n",
      "Epoch: 3 / 3, Step: 2219 / 2250 Loss: 0.2339\n",
      "Epoch: 3 / 3, Step: 2220 / 2250 Loss: 0.1963\n",
      "Epoch: 3 / 3, Step: 2221 / 2250 Loss: 0.2038\n",
      "Epoch: 3 / 3, Step: 2222 / 2250 Loss: 0.4510\n",
      "Epoch: 3 / 3, Step: 2223 / 2250 Loss: 0.2156\n",
      "Epoch: 3 / 3, Step: 2224 / 2250 Loss: 0.1462\n",
      "Epoch: 3 / 3, Step: 2225 / 2250 Loss: 0.3179\n",
      "Epoch: 3 / 3, Step: 2226 / 2250 Loss: 0.2292\n",
      "Epoch: 3 / 3, Step: 2227 / 2250 Loss: 0.1253\n",
      "Epoch: 3 / 3, Step: 2228 / 2250 Loss: 0.1834\n",
      "Epoch: 3 / 3, Step: 2229 / 2250 Loss: 0.1454\n",
      "Epoch: 3 / 3, Step: 2230 / 2250 Loss: 0.1193\n",
      "Epoch: 3 / 3, Step: 2231 / 2250 Loss: 0.2396\n",
      "Epoch: 3 / 3, Step: 2232 / 2250 Loss: 0.1023\n",
      "Epoch: 3 / 3, Step: 2233 / 2250 Loss: 0.1142\n",
      "Epoch: 3 / 3, Step: 2234 / 2250 Loss: 0.5265\n",
      "Epoch: 3 / 3, Step: 2235 / 2250 Loss: 0.2281\n",
      "Epoch: 3 / 3, Step: 2236 / 2250 Loss: 0.0694\n",
      "Epoch: 3 / 3, Step: 2237 / 2250 Loss: 0.2352\n",
      "Epoch: 3 / 3, Step: 2238 / 2250 Loss: 0.2760\n",
      "Epoch: 3 / 3, Step: 2239 / 2250 Loss: 0.2088\n",
      "Epoch: 3 / 3, Step: 2240 / 2250 Loss: 0.4476\n",
      "Epoch: 3 / 3, Step: 2241 / 2250 Loss: 0.2017\n",
      "Epoch: 3 / 3, Step: 2242 / 2250 Loss: 0.3828\n",
      "Epoch: 3 / 3, Step: 2243 / 2250 Loss: 0.1195\n",
      "Epoch: 3 / 3, Step: 2244 / 2250 Loss: 0.3048\n",
      "Epoch: 3 / 3, Step: 2245 / 2250 Loss: 0.2276\n",
      "Epoch: 3 / 3, Step: 2246 / 2250 Loss: 0.1879\n",
      "Epoch: 3 / 3, Step: 2247 / 2250 Loss: 0.3747\n",
      "Epoch: 3 / 3, Step: 2248 / 2250 Loss: 0.3057\n",
      "Epoch: 3 / 3, Step: 2249 / 2250 Loss: 0.1773\n",
      "Time for training:  2395.7499873638153\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "loss_total = 0\n",
    "model.train()\n",
    "start = time.time()\n",
    "for i in range(3):\n",
    "    for j, data in enumerate(train_dataloader):\n",
    "        inputs = {'input_ids': data[0].cuda(), \n",
    "                      'attention_mask': data[1].cuda(), \n",
    "                      'labels': data[2].cuda()}\n",
    "        output = model(**inputs)\n",
    "        loss = output[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Epoch: {} / {}, Step: {} / {} Loss: {:.4f}\".format(i+1, 3, j, len(train_dataloader),\n",
    "                                                                      loss))\n",
    "print(\"Time for training: \", time.time()-start)\n",
    "torch.save(model.state_dict(), 'distilmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12f96e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for inference:  62.421045541763306\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('distilmodel.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "loss_total = 0\n",
    "results = defaultdict(dict)\n",
    "predictions, true_vals = [], []\n",
    "start = time.time()\n",
    "for j, data in enumerate(test_dataloader):\n",
    "    inputs = {'input_ids': data[0].cuda(), \n",
    "              'attention_mask': data[1].cuda(), \n",
    "              'labels': data[2].cuda()}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "    loss = output[0]\n",
    "    logits = output[1]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    labels = inputs['labels'].cpu().numpy()\n",
    "    loss_total += loss.item()\n",
    "    predictions.append(logits)\n",
    "    true_vals.append(labels)\n",
    "print(\"Time for inference: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6fd611a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_vals = np.concatenate(true_vals, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15777acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.85      0.84      0.85      3267\n",
      "         pos       0.68      0.79      0.73      1256\n",
      "         neu       0.94      0.93      0.94     13477\n",
      "\n",
      "    accuracy                           0.91     18000\n",
      "   macro avg       0.83      0.86      0.84     18000\n",
      "weighted avg       0.91      0.91      0.91     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds_flat = np.argmax(predictions, axis = 1).flatten()\n",
    "labels_flat = true_vals.flatten()\n",
    "target_names = ['neg', 'pos', 'neu']\n",
    "print(classification_report(labels_flat, preds_flat, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4923f6",
   "metadata": {},
   "source": [
    "Although DistilBERT is a lighter version of BERT, it is giving less accuracy on our dataset. The accuracy given by BERT is 97% while that by DistilBERT is 91%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4471f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e536cda",
   "metadata": {},
   "source": [
    "# Evaluation on Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f1248",
   "metadata": {},
   "source": [
    "The dataset on which we worked above has been divided into three parts [2]. After analysing the performance of different Machine Learning algorithms on the huge dataset, let's analyze the performace of the dataset on individual parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66056a6e",
   "metadata": {},
   "source": [
    "Let's start on Part A. Part A of this dataset contains tweets related to government actions against COVID-19. The evaluation of this part (preprocessing and processing) is same as that of the entire dataset as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdaa111a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    22949\n",
       "neg     5083\n",
       "pos     1968\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_senti = pd.read_csv(\"COVIDSenti-main/COVIDSenti-A.csv\")\n",
    "covid_senti[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a139f",
   "metadata": {},
   "source": [
    "It contains 22949 samples marked as neutral, 5083 as positive and 1968 as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd26003",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e45b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "import string\n",
    "\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "covid_senti['tokenized_tweet'] = [simple_preprocess(line, deacc=True) for line in covid_senti['tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[word.replace('\\n', '') for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[word.replace('#', '') for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[word.lower() for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[word.translate(table) for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[''.join(filter(lambda x: not word.startswith('https'), word)) for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[''.join(filter(lambda x: not word.startswith('@'), word)) for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "# print(covid_senti['tokenized_tweet'].sample(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a37e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /s/chopin/a/grad/sanket96/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "covid_senti['lemmatized_tweet'] = [[wordnet_lemmatizer.lemmatize(word) for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "# print(covid_senti['lemmatized_tweet'].sample(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08b9c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(covid_senti)) < 0.8\n",
    "covid_senti_train = covid_senti[mask]\n",
    "covid_senti_test = covid_senti[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabb1f8a",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d296485e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.00      0.00      0.00      1037\n",
      "         pos       0.00      0.00      0.00       382\n",
      "         neu       0.76      1.00      0.86      4526\n",
      "\n",
      "    accuracy                           0.76      5945\n",
      "   macro avg       0.25      0.33      0.29      5945\n",
      "weighted avg       0.58      0.76      0.66      5945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayes():\n",
    "\n",
    "    def __init__(self):\n",
    "        # be sure to use the right class_dict for each data set\n",
    "        self.class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "        # self.class_dict = {'action': 0, 'comedy': 1}\n",
    "        self.feature_dict = {}\n",
    "        self.prior = np.zeros(len(self.class_dict))\n",
    "        self.likelihood = None\n",
    "    '''\n",
    "    Trains a multinomial Naive Bayes classifier on a training set.\n",
    "    Specifically, fills in self.prior and self.likelihood such that:\n",
    "    self.prior[class] = log(P(class))\n",
    "    self.likelihood[class][feature] = log(P(feature|class))\n",
    "    '''\n",
    "    def train(self, train_set):\n",
    "        self.feature_dict = self.select_features(train_set)\n",
    "        # iterate over training documents\n",
    "        self.likelihood = np.zeros((len(self.class_dict), len(self.feature_dict)))\n",
    "        doc_per_class = {}\n",
    "        word_count = {}\n",
    "        total_words_per_class = {}\n",
    "        vocabulary = set()\n",
    "        for index, row in train_set.iterrows():\n",
    "            class_name = row['label']\n",
    "            if (class_name in self.class_dict):\n",
    "                doc_per_class[class_name] = 1 + doc_per_class.get(class_name, 0)\n",
    "                    # collect class counts and feature counts\n",
    "                data = row['lemmatized_tweet']\n",
    "                for word in data:\n",
    "                    vocabulary.add(word)\n",
    "                    word_count[(word, class_name)] = 1 + word_count.get((word, class_name), 0)\n",
    "        # normalize counts to probabilities, and take logs\n",
    "        for class_name in self.class_dict:\n",
    "            counts = [v for k, v in word_count.items() if k[1] == class_name]\n",
    "            total_words_per_class[class_name] = sum(counts)\n",
    "        for word in self.feature_dict:\n",
    "            for class_name in self.class_dict:\n",
    "                self.likelihood[self.class_dict.get(class_name)][self.feature_dict.get(word)] = np.log(((word_count.get((word,\n",
    "                                                class_name), 0) + 1)/(total_words_per_class[class_name] + len(vocabulary))))\n",
    "        for class_name in self.class_dict:\n",
    "            self.prior[self.class_dict[class_name]] = np.log((doc_per_class[class_name] / sum(doc_per_class.values())))\n",
    "    '''\n",
    "    Tests the classifier on a development or test set.\n",
    "    Returns a dictionary of filenames mapped to their correct and predicted\n",
    "    classes such that:\n",
    "    results[filename]['correct'] = correct class\n",
    "    results[filename]['predicted'] = predicted class\n",
    "    '''\n",
    "    def test(self, dev_set):\n",
    "        pred_labels = []\n",
    "        true_labels = []\n",
    "        # iterate over testing documents\n",
    "        for index, row in dev_set.iterrows():\n",
    "            class_name = row['label']\n",
    "            # create feature vectors for each document\n",
    "            word_count = {}\n",
    "            true_labels.append(self.class_dict[class_name])\n",
    "            data = str(row['lemmatized_tweet'])\n",
    "            for word in data:\n",
    "                if word in self.feature_dict:\n",
    "                    word_count[word] = 1 + word_count.get(word, 0)\n",
    "            feature_vector = np.zeros((len(self.feature_dict), 1))\n",
    "            for i, word in enumerate(self.feature_dict):\n",
    "                feature_vector[i] = word_count.get(word, 0)\n",
    "            self.prior = np.reshape(self.prior, (self.prior.shape[0], 1))\n",
    "            probability = self.prior + np.matmul(self.likelihood, feature_vector)\n",
    "            pred_labels.append(np.argmax(probability))\n",
    "                # get most likely class\n",
    "        # print(dict(results))\n",
    "        return pred_labels, true_labels\n",
    "\n",
    "    '''\n",
    "    Given results, calculates the following:\n",
    "    Precision, Recall, F1 for each class\n",
    "    Accuracy overall\n",
    "    Also, prints evaluation metrics in readable format.\n",
    "    '''\n",
    "    def evaluate(self, results):\n",
    "        # you may find this helpful\n",
    "        target_names = ['neg', 'pos', 'neu']\n",
    "        print(classification_report(results[1], results[0], target_names=target_names))\n",
    "    '''\n",
    "    Performs feature selection.\n",
    "    Returns a dictionary of features.\n",
    "    '''\n",
    "    def select_features(self, train_set):\n",
    "        # almost any method of feature selection is fine here\n",
    "        doc_per_class = {}\n",
    "        word_count = {}\n",
    "        total_words_per_class = {}\n",
    "        vocabulary = set()\n",
    "        likelihood_ratio = {}\n",
    "        for index, row in train_set.iterrows():\n",
    "            class_name = row['label']\n",
    "            if (class_name in self.class_dict):\n",
    "                doc_per_class[class_name] = 1 + doc_per_class.get(class_name, 0)\n",
    "                    # collect class counts and feature counts\n",
    "                data = row['lemmatized_tweet']\n",
    "                for word in data:\n",
    "                    vocabulary.add(word)\n",
    "                    word_count[(word, class_name)] = 1 + word_count.get((word, class_name), 0)\n",
    "        # normalize counts to probabilities, and take logs\n",
    "        for class_name in self.class_dict:\n",
    "            counts = [v for k, v in word_count.items() if k[1] == class_name]\n",
    "            total_words_per_class[class_name] = sum(counts)\n",
    "        prob_class = np.zeros((3, 1))\n",
    "        for i, class_name in enumerate(self.class_dict):\n",
    "            prob_class[i] = (doc_per_class[class_name] / sum(doc_per_class.values()))\n",
    "        for word in vocabulary:\n",
    "            class_probs = [1] * len(self.class_dict)\n",
    "            for i, class_name in enumerate(self.class_dict):\n",
    "                class_probs[i] = (word_count.get((word,\n",
    "                                      class_name), 0) + 1) / (total_words_per_class[class_name] + len(vocabulary))\n",
    "                class_probs[i] = class_probs[i] / prob_class[i]\n",
    "            likelihood_ratio[word] = (1 / class_probs[0]) * (1 / class_probs[1]) * (1 / class_probs[2])\n",
    "        #likelihood_ratio_pos = dict(sorted(likelihood_ratio.items(), key=lambda item: item[1], reverse=True))\n",
    "        likelihood_ratio = dict(sorted(likelihood_ratio.items(), key=lambda item: item[1]))\n",
    "        words = []\n",
    "        words.extend(list(likelihood_ratio.keys())[:750])\n",
    "        #words.extend(list(likelihood_ratio_pos.keys())[:750])\n",
    "        # for class_name in self.class_dict:\n",
    "        #     self.prior[self.class_dict[class_name]] = np.log((doc_per_class[class_name] / sum(doc_per_class.values())))\n",
    "        features = {}\n",
    "        for i, word in enumerate(words):\n",
    "            features[word] = i\n",
    "        return features\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nb = NaiveBayes()\n",
    "    # make sure these point to the right directories\n",
    "    nb.train(covid_senti_train)\n",
    "    # nb.train('movie_reviews_small/train')\n",
    "    results = nb.test(covid_senti_test)\n",
    "    # results = nb.test('movie_reviews_small/test')\n",
    "    nb.evaluate(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a9a8f0",
   "metadata": {},
   "source": [
    "## Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "63c298ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_116452/623227358.py:111: RuntimeWarning: divide by zero encountered in log\n",
      "  loss += -((y @ np.log(y_hat)) + ((1 - y) @ np.log(1 - y_hat)))\n",
      "/tmp/ipykernel_116452/623227358.py:111: RuntimeWarning: invalid value encountered in matmul\n",
      "  loss += -((y @ np.log(y_hat)) + ((1 - y) @ np.log(1 - y_hat)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: nan\n",
      "Epoch 2 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 11 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 12 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 13 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 14 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 15 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 16 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 17 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 18 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 19 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 20 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 21 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 22 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 23 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 24 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 25 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 26 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 27 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 28 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 29 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 30 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 31 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 32 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 33 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 34 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 35 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 36 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 37 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 38 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 39 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 40 out of 40\n",
      "Average Train Loss: nan\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.00      0.00      0.00      1037\n",
      "         pos       0.06      1.00      0.12       382\n",
      "         neu       0.00      0.00      0.00      4526\n",
      "\n",
      "    accuracy                           0.06      5945\n",
      "   macro avg       0.02      0.33      0.04      5945\n",
      "weighted avg       0.00      0.06      0.01      5945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# CS542 Fall 2021 Programming Assignment 2\n",
    "# Logistic Regression Classifier\n",
    "\n",
    "'''\n",
    "Computes the logistic function.\n",
    "'''\n",
    "\n",
    "\n",
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, n_features=400):\n",
    "        # be sure to use the right class_dict for each data set\n",
    "        self.theta = None\n",
    "        self.n_features = n_features\n",
    "        self.feature_dict = None\n",
    "        self.class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "        # self.class_dict = {'action': 0, 'comedy': 1}\n",
    "        # use of self.feature_dict is optional for this assignment\n",
    "        self.feature_dict = self.select_features(covid_senti_train)\n",
    "\n",
    "    '''\n",
    "    Loads a dataset. Specifically, returns a list of filenames, and dictionaries\n",
    "    of classes and documents such that:\n",
    "    classes[filename] = class of the document\n",
    "    documents[filename] = feature vector for the document (use self.featurize)\n",
    "    '''\n",
    "\n",
    "    def select_features(self, data_set):\n",
    "        feature_count = {}\n",
    "        for index, row in data_set.iterrows():\n",
    "            data = str(row['lemmatized_tweet']).split()\n",
    "            for word in data:\n",
    "                feature_count[word] = 1 + feature_count.get(word, 0)\n",
    "\n",
    "        feature_count = list(dict(sorted(feature_count.items(), key=lambda v: v[1], reverse=True)).keys())[:500]\n",
    "        features = {}\n",
    "\n",
    "        for i, word in enumerate(feature_count):\n",
    "            features[word] = i\n",
    "        return features\n",
    "\n",
    "    def load_data(self, data_set):\n",
    "        filenames = []\n",
    "        classes = dict()\n",
    "        documents = dict()\n",
    "        # iterate over documents\n",
    "        for index, row in data_set.iterrows():\n",
    "            # your code here\n",
    "            # BEGIN STUDENT CODE\n",
    "            # if os.path.isfile(os.path.join(root, name)):\n",
    "            class_name = row['label']\n",
    "            classes[index] = self.class_dict[class_name]\n",
    "            documents[index] = self.featurize(row['lemmatized_tweet'])\n",
    "            # END STUDENT CODE\n",
    "        return classes, documents\n",
    "\n",
    "    '''\n",
    "    Given a document (as a list of words), returns a feature vector.\n",
    "    Note that the last element of the vector, corresponding to the bias, is a\n",
    "    \"dummy feature\" with value 1.\n",
    "    '''\n",
    "\n",
    "    def featurize(self, document):\n",
    "        vector = np.zeros(self.n_features + 1)\n",
    "        # BEGIN STUDENT CODE\n",
    "        for word in document:\n",
    "            if word in self.feature_dict:\n",
    "                if word not in w2v_model.wv.key_to_index:\n",
    "                    vector.extend([0] * 500)\n",
    "                else:\n",
    "                    vector.extend(w2v_model.wv[word])\n",
    "        # END STUDENT CODE\n",
    "        vector[-1] = 1\n",
    "        return vector\n",
    "\n",
    "    '''\n",
    "    Trains a logistic regression classifier on a training set.\n",
    "    '''\n",
    "\n",
    "    def train(self, train_set, batch_size=3, n_epochs=1, eta=0.1):\n",
    "        # if train_set == \"movie_reviews_small/train\":\n",
    "        #     self.feature_dict = {'fast': 0, 'couple': 1, 'shoot': 2, 'fly': 3}\n",
    "        # else:\n",
    "        #     self.feature_dict = self.select_features(train_set)\n",
    "        # self.n_features = len(self.feature_dict)\n",
    "        self.theta = np.zeros(self.n_features + 1)  # weights (and bias)\n",
    "        classes, documents = self.load_data(train_set)\n",
    "        n_minibatches = ceil(len(train_set) / batch_size)\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "            loss = 0\n",
    "            for i in range(n_minibatches):\n",
    "                # list of filenames in minibatch\n",
    "                minibatch = train_set[i * batch_size: (i + 1) * batch_size]\n",
    "                # BEGIN STUDENT CODE\n",
    "                # create and fill in matrix x and vector y\n",
    "                x = np.zeros((len(minibatch), self.n_features + 1))\n",
    "                y = np.zeros(len(minibatch))\n",
    "                k = 0\n",
    "                for j, row in minibatch.iterrows():\n",
    "                    x[k][:] = documents[j]\n",
    "                    y[k] = classes[j]\n",
    "                    k += 1\n",
    "                # compute y_hat\n",
    "                y_hat = sigma(np.dot(x, self.theta))\n",
    "                # update loss\n",
    "                loss += -((y @ np.log(y_hat)) + ((1 - y) @ np.log(1 - y_hat)))\n",
    "                # compute gradient\n",
    "                gradient = np.dot(x.T, np.subtract(y_hat, y)) / len(minibatch)\n",
    "                # update weights (and bias)\n",
    "                self.theta = self.theta - (eta * gradient)\n",
    "                # END STUDENT CODE\n",
    "            loss /= len(train_set)\n",
    "            print(\"Average Train Loss: {}\".format(loss))\n",
    "            # randomize order\n",
    "            #Random(epoch).shuffle(train_set)\n",
    "\n",
    "    '''\n",
    "    Tests the classifier on a development or test set.\n",
    "    Returns a dictionary of filenames mapped to their correct and predicted\n",
    "    classes such that:\n",
    "    results[filename]['correct'] = correct class\n",
    "    results[filename]['predicted'] = predicted class\n",
    "    '''\n",
    "\n",
    "    def test(self, dev_set):\n",
    "        pred_labels = []\n",
    "        true_labels = []\n",
    "        classes, documents = self.load_data(dev_set)\n",
    "        for index, row in dev_set.iterrows():\n",
    "            # BEGIN STUDENT CODE\n",
    "            # get most likely class (recall that P(y=1|x) = y_hat)\n",
    "            true_labels.append(classes[index])\n",
    "            prediction = sigma(np.dot(documents[index], self.theta))\n",
    "            pred_label = 1 if prediction > 0.5 else 0\n",
    "            pred_labels.append(pred_label)\n",
    "            # END STUDENT CODE\n",
    "        return pred_labels, true_labels\n",
    "\n",
    "    '''\n",
    "    Given results, calculates the following:\n",
    "    Precision, Recall, F1 for each class\n",
    "    Accuracy overall\n",
    "    Also, prints evaluation metrics in readable format.\n",
    "    '''\n",
    "\n",
    "    def evaluate(self, results):\n",
    "        # you can copy and paste your code from PA1 here\n",
    "        target_names = ['neg', 'pos', 'neu']\n",
    "        print(classification_report(results[1], results[0], target_names=target_names))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lr = LogisticRegression(n_features=750)\n",
    "    # make sure these point to the right directories\n",
    "    batch_size = [1, 2, 3, 8, 16, 32]\n",
    "    n_epochs = [1, 5, 10, 20, 30, 40]\n",
    "    eta = [0.025, 0.05, 0.1, 0.2, 0.4]\n",
    "\n",
    "    # code for grid search\n",
    "#     for b in batch_size:\n",
    "#         for n in n_epochs:\n",
    "#             for ler in eta:\n",
    "#                 lr.train(covid_senti_train, batch_size=b, n_epochs=n, eta=ler)\n",
    "#                 results = lr.test(covid_senti_test)\n",
    "#                 lr.evaluate(results)\n",
    "#                 print(\"Accuracy is for batch size: \", b, \", n_epochs: \", n, \"eta: \", ler)\n",
    "\n",
    "    # best features from grid search\n",
    "    lr.train(covid_senti_train, batch_size=3, n_epochs=40, eta=0.05)\n",
    "    results = lr.test(covid_senti_test)\n",
    "    # lr.train('movie_reviews_small/train', batch_size=3, n_epochs=1, eta=0.1)\n",
    "    # results = lr.test('movie_reviews_small/test')\n",
    "    lr.evaluate(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfcb51a",
   "metadata": {},
   "source": [
    "## CNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba57742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import gensim\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e41741c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "tweets = list(covid_senti['lemmatized_tweet'].values)\n",
    "tweets.append(['pad'])\n",
    "w2v_model = Word2Vec(tweets, min_count = 1, vector_size = 500, workers = 3, window = 3, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f15c5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>lemmatized_tweet</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coronavirus | Human Coronavirus Types | CDC ht...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[coronavirus, human, coronavirus, types, cdc, ...</td>\n",
       "      <td>[coronavirus, human, coronavirus, type, cdc, ,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@shehryar_taseer That‚Äôs üíØ true , \\nCorona...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[shehryartaseer, that, aos, uiø, true, corona,...</td>\n",
       "      <td>[shehryartaseer, that, aos, uiø, true, corona,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TLDR: Not SARS, possibly new coronavirus. Diff...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[tldr, not, sars, possibly, new, coronavirus, ...</td>\n",
       "      <td>[tldr, not, sars, possibly, new, coronavirus, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Disease outbreak news from the WHO: Middle Eas...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[disease, outbreak, news, from, the, who, midd...</td>\n",
       "      <td>[disease, outbreak, news, from, the, who, midd...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>China - Media: WSJ says sources tell them myst...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[china, media, wsj, says, sources, tell, them,...</td>\n",
       "      <td>[china, medium, wsj, say, source, tell, them, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>CDC: Re-test confirms Westerdam cruise ship pa...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[cdc, re, test, confirms, westerdam, cruise, s...</td>\n",
       "      <td>[cdc, re, test, confirms, westerdam, cruise, s...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>Two doctors die of coronavirus within 24 hours...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[two, doctors, die, of, coronavirus, within, h...</td>\n",
       "      <td>[two, doctor, die, of, coronavirus, within, ho...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>BEIJING - The lockdown of Guo Jing's neighbour...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[beijing, the, lockdown, of, guo, jing, neighb...</td>\n",
       "      <td>[beijing, the, lockdown, of, guo, jing, neighb...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>#CoronavirusOutbreak in #Balochistan !!\\n#CPEC...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[in, balochistan, cpec, route, to, spread, cor...</td>\n",
       "      <td>[in, balochistan, cpec, route, to, spread, cor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>The Australian dollar has hit a fresh decade l...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[the, australian, dollar, has, hit, fresh, dec...</td>\n",
       "      <td>[the, australian, dollar, ha, hit, fresh, deca...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet label  \\\n",
       "0      Coronavirus | Human Coronavirus Types | CDC ht...   neu   \n",
       "1      @shehryar_taseer That‚Äôs üíØ true , \\nCorona...   neu   \n",
       "2      TLDR: Not SARS, possibly new coronavirus. Diff...   neg   \n",
       "3      Disease outbreak news from the WHO: Middle Eas...   neu   \n",
       "4      China - Media: WSJ says sources tell them myst...   neu   \n",
       "...                                                  ...   ...   \n",
       "29995  CDC: Re-test confirms Westerdam cruise ship pa...   neu   \n",
       "29996  Two doctors die of coronavirus within 24 hours...   neu   \n",
       "29997  BEIJING - The lockdown of Guo Jing's neighbour...   neu   \n",
       "29998  #CoronavirusOutbreak in #Balochistan !!\\n#CPEC...   neu   \n",
       "29999  The Australian dollar has hit a fresh decade l...   neu   \n",
       "\n",
       "                                         tokenized_tweet  \\\n",
       "0      [coronavirus, human, coronavirus, types, cdc, ...   \n",
       "1      [shehryartaseer, that, aos, uiø, true, corona,...   \n",
       "2      [tldr, not, sars, possibly, new, coronavirus, ...   \n",
       "3      [disease, outbreak, news, from, the, who, midd...   \n",
       "4      [china, media, wsj, says, sources, tell, them,...   \n",
       "...                                                  ...   \n",
       "29995  [cdc, re, test, confirms, westerdam, cruise, s...   \n",
       "29996  [two, doctors, die, of, coronavirus, within, h...   \n",
       "29997  [beijing, the, lockdown, of, guo, jing, neighb...   \n",
       "29998  [in, balochistan, cpec, route, to, spread, cor...   \n",
       "29999  [the, australian, dollar, has, hit, fresh, dec...   \n",
       "\n",
       "                                        lemmatized_tweet  label_num  \n",
       "0      [coronavirus, human, coronavirus, type, cdc, ,...          2  \n",
       "1      [shehryartaseer, that, aos, uiø, true, corona,...          2  \n",
       "2      [tldr, not, sars, possibly, new, coronavirus, ...          0  \n",
       "3      [disease, outbreak, news, from, the, who, midd...          2  \n",
       "4      [china, medium, wsj, say, source, tell, them, ...          2  \n",
       "...                                                  ...        ...  \n",
       "29995  [cdc, re, test, confirms, westerdam, cruise, s...          2  \n",
       "29996  [two, doctor, die, of, coronavirus, within, ho...          2  \n",
       "29997  [beijing, the, lockdown, of, guo, jing, neighb...          2  \n",
       "29998  [in, balochistan, cpec, route, to, spread, cor...          2  \n",
       "29999  [the, australian, dollar, ha, hit, fresh, deca...          2  \n",
       "\n",
       "[30000 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "covid_senti['label_num'] = covid_senti.label.replace(class_dict)\n",
    "covid_senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cd519c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = covid_senti['lemmatized_tweet'].map(len).max()\n",
    "def make_word_2_vec(sentence):\n",
    "    padding_idx = w2v_model.wv.key_to_index['pad']\n",
    "    padded_X = [padding_idx for i in range(max_len)]\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        if word not in w2v_model.wv.key_to_index:\n",
    "            padded_X[i] = 0\n",
    "            print(word)\n",
    "        else:\n",
    "            padded_X[i] = w2v_model.wv.key_to_index[word]\n",
    "        i += 1\n",
    "    return torch.tensor(padded_X, dtype=torch.long).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "abf67315",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 500\n",
    "NUM_FILTERS = 10\n",
    "\n",
    "class CnnTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, window_sizes=(1,2,3,5)):\n",
    "        super(CnnTextClassifier, self).__init__()\n",
    "        weights = w2v_model.wv\n",
    "        # With pretrained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors),\n",
    "                                                      padding_idx=w2v_model.wv.key_to_index['pad'])\n",
    "        # Without pretrained embeddings\n",
    "        # self.embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "                                   nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE],\n",
    "                                             padding=(window_size - 1, 0))\n",
    "                                   for window_size in window_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Apply a convolution + max_pool layer for each window size\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        xs = []\n",
    "        for conv in self.convs:\n",
    "            x2 = torch.tanh(conv(x))\n",
    "            x2 = torch.squeeze(x2, -1)\n",
    "            x2 = F.max_pool1d(x2, x2.size(2))\n",
    "            xs.append(x2)\n",
    "        x = torch.cat(xs, 2)\n",
    "\n",
    "        # FC\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "22d50dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch completed in: 67.5854 seconds\n",
      "1,0.7859940051859021\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Epoch completed in: 68.0643 seconds\n",
      "2,0.7855748417495914\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Epoch completed in: 70.3885 seconds\n",
      "3,0.7855748417347242\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 3\n",
    "VOCAB_SIZE = len(w2v_model.wv.key_to_index)\n",
    "\n",
    "cnn_model = CnnTextClassifier(vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\n",
    "# cnn_model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "num_epochs = 3\n",
    "\n",
    "# Open the file for writing loss\n",
    "class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "loss_file_name = 'cnn_class_big_loss_with_padding.csv'\n",
    "losses = []\n",
    "cnn_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch \" + str(epoch + 1))\n",
    "    train_loss = 0\n",
    "    for index, row in covid_senti_train.iterrows():\n",
    "        # Clearing the accumulated gradients\n",
    "        cnn_model.zero_grad()\n",
    "\n",
    "        # Make the bag of words vector for stemmed tokens \n",
    "        bow_vec = make_word_2_vec(row['lemmatized_tweet'])\n",
    "       \n",
    "        # Forward pass to get output\n",
    "        probs = cnn_model(bow_vec)\n",
    "\n",
    "        # Get the target label\n",
    "        target = torch.tensor([class_dict[row['label']]], dtype=torch.long)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = loss_function(probs, target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # if index == 0:\n",
    "    #     continue\n",
    "    print(\"Epoch completed in: %.4f seconds\" % (time.time()-start_time))\n",
    "    print(str((epoch+1)) + \",\" + str(train_loss / len(covid_senti_train)))\n",
    "    print('\\n')\n",
    "    train_loss = 0\n",
    "\n",
    "torch.save(cnn_model, 'cnn_big_model_500_with_paddingA.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d59bdefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.00      0.00      0.00         0\n",
      "         pos       0.00      0.00      0.00         0\n",
      "         neu       1.00      0.76      0.86      5945\n",
      "\n",
      "    accuracy                           0.76      5945\n",
      "   macro avg       0.33      0.25      0.29      5945\n",
      "weighted avg       1.00      0.76      0.86      5945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions = []\n",
    "correct = []\n",
    "cnn_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = defaultdict(dict)\n",
    "    for index, row in covid_senti_test.iterrows():\n",
    "        bow_vec = make_word_2_vec(row['lemmatized_tweet'])\n",
    "        probs = cnn_model(bow_vec)\n",
    "        correct.append(class_dict[row['label']])\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        predictions.append(predicted.numpy()[0])\n",
    "target_names = ['neg', 'pos', 'neu']\n",
    "print(classification_report(predictions, correct, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ef5f6",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a99eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(covid_senti.index.values, \n",
    "                                                    covid_senti.label.values, test_size=0.2,\n",
    "                                                   stratify=covid_senti.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cb97ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {'neg': 0, 'pos': 1, 'neu': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41a1a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_senti['data_type'] = ['not_set'] * covid_senti.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eee0b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_senti.loc[X_train, 'data_type'] = 'train'\n",
    "covid_senti.loc[X_test, 'data_type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96283602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9fbcaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e096e593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/a/grad/sanket96/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2322: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='train'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')\n",
    "\n",
    "encoded_data_test = tokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='test'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b943029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(covid_senti[covid_senti.data_type == 'train'].label_num.values)\n",
    "\n",
    "#validation set\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(covid_senti[covid_senti.data_type == 'test'].label_num.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bc8c662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(class_dict),\n",
    "                                                     output_attentions = False,\n",
    "                                                      output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51e3096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbb86bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train,labels_train)\n",
    "\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80fa3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86f6bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01e8e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 3, Step: 0 / 750 Loss: 1.4978\n",
      "Epoch: 1 / 3, Step: 1 / 750 Loss: 1.4497\n",
      "Epoch: 1 / 3, Step: 2 / 750 Loss: 1.3034\n",
      "Epoch: 1 / 3, Step: 3 / 750 Loss: 1.3531\n",
      "Epoch: 1 / 3, Step: 4 / 750 Loss: 1.3236\n",
      "Epoch: 1 / 3, Step: 5 / 750 Loss: 1.2706\n",
      "Epoch: 1 / 3, Step: 6 / 750 Loss: 1.2097\n",
      "Epoch: 1 / 3, Step: 7 / 750 Loss: 1.1642\n",
      "Epoch: 1 / 3, Step: 8 / 750 Loss: 1.1235\n",
      "Epoch: 1 / 3, Step: 9 / 750 Loss: 1.0776\n",
      "Epoch: 1 / 3, Step: 10 / 750 Loss: 1.0305\n",
      "Epoch: 1 / 3, Step: 11 / 750 Loss: 1.0590\n",
      "Epoch: 1 / 3, Step: 12 / 750 Loss: 0.9862\n",
      "Epoch: 1 / 3, Step: 13 / 750 Loss: 0.9032\n",
      "Epoch: 1 / 3, Step: 14 / 750 Loss: 0.7150\n",
      "Epoch: 1 / 3, Step: 15 / 750 Loss: 0.8478\n",
      "Epoch: 1 / 3, Step: 16 / 750 Loss: 0.6535\n",
      "Epoch: 1 / 3, Step: 17 / 750 Loss: 0.6589\n",
      "Epoch: 1 / 3, Step: 18 / 750 Loss: 0.6021\n",
      "Epoch: 1 / 3, Step: 19 / 750 Loss: 0.7397\n",
      "Epoch: 1 / 3, Step: 20 / 750 Loss: 0.5786\n",
      "Epoch: 1 / 3, Step: 21 / 750 Loss: 0.5841\n",
      "Epoch: 1 / 3, Step: 22 / 750 Loss: 0.7222\n",
      "Epoch: 1 / 3, Step: 23 / 750 Loss: 0.6692\n",
      "Epoch: 1 / 3, Step: 24 / 750 Loss: 0.8888\n",
      "Epoch: 1 / 3, Step: 25 / 750 Loss: 0.6459\n",
      "Epoch: 1 / 3, Step: 26 / 750 Loss: 0.7034\n",
      "Epoch: 1 / 3, Step: 27 / 750 Loss: 0.7436\n",
      "Epoch: 1 / 3, Step: 28 / 750 Loss: 0.6241\n",
      "Epoch: 1 / 3, Step: 29 / 750 Loss: 1.1799\n",
      "Epoch: 1 / 3, Step: 30 / 750 Loss: 0.6677\n",
      "Epoch: 1 / 3, Step: 31 / 750 Loss: 0.6262\n",
      "Epoch: 1 / 3, Step: 32 / 750 Loss: 0.7246\n",
      "Epoch: 1 / 3, Step: 33 / 750 Loss: 0.5188\n",
      "Epoch: 1 / 3, Step: 34 / 750 Loss: 0.9133\n",
      "Epoch: 1 / 3, Step: 35 / 750 Loss: 0.7648\n",
      "Epoch: 1 / 3, Step: 36 / 750 Loss: 0.7470\n",
      "Epoch: 1 / 3, Step: 37 / 750 Loss: 0.9750\n",
      "Epoch: 1 / 3, Step: 38 / 750 Loss: 0.5141\n",
      "Epoch: 1 / 3, Step: 39 / 750 Loss: 0.5607\n",
      "Epoch: 1 / 3, Step: 40 / 750 Loss: 0.8056\n",
      "Epoch: 1 / 3, Step: 41 / 750 Loss: 0.7355\n",
      "Epoch: 1 / 3, Step: 42 / 750 Loss: 0.6393\n",
      "Epoch: 1 / 3, Step: 43 / 750 Loss: 0.6201\n",
      "Epoch: 1 / 3, Step: 44 / 750 Loss: 0.5630\n",
      "Epoch: 1 / 3, Step: 45 / 750 Loss: 0.8844\n",
      "Epoch: 1 / 3, Step: 46 / 750 Loss: 0.4317\n",
      "Epoch: 1 / 3, Step: 47 / 750 Loss: 0.6145\n",
      "Epoch: 1 / 3, Step: 48 / 750 Loss: 0.7155\n",
      "Epoch: 1 / 3, Step: 49 / 750 Loss: 0.8411\n",
      "Epoch: 1 / 3, Step: 50 / 750 Loss: 0.7284\n",
      "Epoch: 1 / 3, Step: 51 / 750 Loss: 0.3960\n",
      "Epoch: 1 / 3, Step: 52 / 750 Loss: 0.5798\n",
      "Epoch: 1 / 3, Step: 53 / 750 Loss: 0.5124\n",
      "Epoch: 1 / 3, Step: 54 / 750 Loss: 0.6768\n",
      "Epoch: 1 / 3, Step: 55 / 750 Loss: 1.0003\n",
      "Epoch: 1 / 3, Step: 56 / 750 Loss: 0.6705\n",
      "Epoch: 1 / 3, Step: 57 / 750 Loss: 0.7079\n",
      "Epoch: 1 / 3, Step: 58 / 750 Loss: 0.6168\n",
      "Epoch: 1 / 3, Step: 59 / 750 Loss: 0.5902\n",
      "Epoch: 1 / 3, Step: 60 / 750 Loss: 1.0575\n",
      "Epoch: 1 / 3, Step: 61 / 750 Loss: 0.7421\n",
      "Epoch: 1 / 3, Step: 62 / 750 Loss: 0.6409\n",
      "Epoch: 1 / 3, Step: 63 / 750 Loss: 0.7316\n",
      "Epoch: 1 / 3, Step: 64 / 750 Loss: 0.6151\n",
      "Epoch: 1 / 3, Step: 65 / 750 Loss: 0.5537\n",
      "Epoch: 1 / 3, Step: 66 / 750 Loss: 0.6513\n",
      "Epoch: 1 / 3, Step: 67 / 750 Loss: 0.6420\n",
      "Epoch: 1 / 3, Step: 68 / 750 Loss: 0.5547\n",
      "Epoch: 1 / 3, Step: 69 / 750 Loss: 0.6251\n",
      "Epoch: 1 / 3, Step: 70 / 750 Loss: 0.5439\n",
      "Epoch: 1 / 3, Step: 71 / 750 Loss: 0.5444\n",
      "Epoch: 1 / 3, Step: 72 / 750 Loss: 0.6697\n",
      "Epoch: 1 / 3, Step: 73 / 750 Loss: 0.5247\n",
      "Epoch: 1 / 3, Step: 74 / 750 Loss: 0.7414\n",
      "Epoch: 1 / 3, Step: 75 / 750 Loss: 0.6129\n",
      "Epoch: 1 / 3, Step: 76 / 750 Loss: 0.7909\n",
      "Epoch: 1 / 3, Step: 77 / 750 Loss: 0.6264\n",
      "Epoch: 1 / 3, Step: 78 / 750 Loss: 0.5723\n",
      "Epoch: 1 / 3, Step: 79 / 750 Loss: 0.6268\n",
      "Epoch: 1 / 3, Step: 80 / 750 Loss: 0.4890\n",
      "Epoch: 1 / 3, Step: 81 / 750 Loss: 0.6410\n",
      "Epoch: 1 / 3, Step: 82 / 750 Loss: 0.7669\n",
      "Epoch: 1 / 3, Step: 83 / 750 Loss: 0.8970\n",
      "Epoch: 1 / 3, Step: 84 / 750 Loss: 0.7907\n",
      "Epoch: 1 / 3, Step: 85 / 750 Loss: 0.8090\n",
      "Epoch: 1 / 3, Step: 86 / 750 Loss: 0.8933\n",
      "Epoch: 1 / 3, Step: 87 / 750 Loss: 0.5997\n",
      "Epoch: 1 / 3, Step: 88 / 750 Loss: 0.3455\n",
      "Epoch: 1 / 3, Step: 89 / 750 Loss: 0.5919\n",
      "Epoch: 1 / 3, Step: 90 / 750 Loss: 0.5778\n",
      "Epoch: 1 / 3, Step: 91 / 750 Loss: 0.5925\n",
      "Epoch: 1 / 3, Step: 92 / 750 Loss: 0.6029\n",
      "Epoch: 1 / 3, Step: 93 / 750 Loss: 0.9430\n",
      "Epoch: 1 / 3, Step: 94 / 750 Loss: 0.8020\n",
      "Epoch: 1 / 3, Step: 95 / 750 Loss: 0.7864\n",
      "Epoch: 1 / 3, Step: 96 / 750 Loss: 0.5254\n",
      "Epoch: 1 / 3, Step: 97 / 750 Loss: 0.9031\n",
      "Epoch: 1 / 3, Step: 98 / 750 Loss: 0.8135\n",
      "Epoch: 1 / 3, Step: 99 / 750 Loss: 0.6803\n",
      "Epoch: 1 / 3, Step: 100 / 750 Loss: 0.7031\n",
      "Epoch: 1 / 3, Step: 101 / 750 Loss: 0.5997\n",
      "Epoch: 1 / 3, Step: 102 / 750 Loss: 0.5855\n",
      "Epoch: 1 / 3, Step: 103 / 750 Loss: 0.7100\n",
      "Epoch: 1 / 3, Step: 104 / 750 Loss: 0.7428\n",
      "Epoch: 1 / 3, Step: 105 / 750 Loss: 0.6857\n",
      "Epoch: 1 / 3, Step: 106 / 750 Loss: 0.4586\n",
      "Epoch: 1 / 3, Step: 107 / 750 Loss: 0.4991\n",
      "Epoch: 1 / 3, Step: 108 / 750 Loss: 0.6269\n",
      "Epoch: 1 / 3, Step: 109 / 750 Loss: 0.7274\n",
      "Epoch: 1 / 3, Step: 110 / 750 Loss: 0.5017\n",
      "Epoch: 1 / 3, Step: 111 / 750 Loss: 0.7599\n",
      "Epoch: 1 / 3, Step: 112 / 750 Loss: 0.7142\n",
      "Epoch: 1 / 3, Step: 113 / 750 Loss: 0.6044\n",
      "Epoch: 1 / 3, Step: 114 / 750 Loss: 0.8823\n",
      "Epoch: 1 / 3, Step: 115 / 750 Loss: 1.0087\n",
      "Epoch: 1 / 3, Step: 116 / 750 Loss: 0.5831\n",
      "Epoch: 1 / 3, Step: 117 / 750 Loss: 0.6578\n",
      "Epoch: 1 / 3, Step: 118 / 750 Loss: 0.7068\n",
      "Epoch: 1 / 3, Step: 119 / 750 Loss: 0.5849\n",
      "Epoch: 1 / 3, Step: 120 / 750 Loss: 0.5811\n",
      "Epoch: 1 / 3, Step: 121 / 750 Loss: 0.6470\n",
      "Epoch: 1 / 3, Step: 122 / 750 Loss: 0.4160\n",
      "Epoch: 1 / 3, Step: 123 / 750 Loss: 0.9556\n",
      "Epoch: 1 / 3, Step: 124 / 750 Loss: 0.6259\n",
      "Epoch: 1 / 3, Step: 125 / 750 Loss: 0.7975\n",
      "Epoch: 1 / 3, Step: 126 / 750 Loss: 0.6263\n",
      "Epoch: 1 / 3, Step: 127 / 750 Loss: 0.5872\n",
      "Epoch: 1 / 3, Step: 128 / 750 Loss: 0.5641\n",
      "Epoch: 1 / 3, Step: 129 / 750 Loss: 0.8686\n",
      "Epoch: 1 / 3, Step: 130 / 750 Loss: 0.4923\n",
      "Epoch: 1 / 3, Step: 131 / 750 Loss: 0.5025\n",
      "Epoch: 1 / 3, Step: 132 / 750 Loss: 0.6107\n",
      "Epoch: 1 / 3, Step: 133 / 750 Loss: 0.5950\n",
      "Epoch: 1 / 3, Step: 134 / 750 Loss: 0.6869\n",
      "Epoch: 1 / 3, Step: 135 / 750 Loss: 0.8444\n",
      "Epoch: 1 / 3, Step: 136 / 750 Loss: 0.5381\n",
      "Epoch: 1 / 3, Step: 137 / 750 Loss: 0.7112\n",
      "Epoch: 1 / 3, Step: 138 / 750 Loss: 0.6995\n",
      "Epoch: 1 / 3, Step: 139 / 750 Loss: 0.4923\n",
      "Epoch: 1 / 3, Step: 140 / 750 Loss: 0.3910\n",
      "Epoch: 1 / 3, Step: 141 / 750 Loss: 0.5190\n",
      "Epoch: 1 / 3, Step: 142 / 750 Loss: 0.6392\n",
      "Epoch: 1 / 3, Step: 143 / 750 Loss: 0.5759\n",
      "Epoch: 1 / 3, Step: 144 / 750 Loss: 0.5087\n",
      "Epoch: 1 / 3, Step: 145 / 750 Loss: 0.6690\n",
      "Epoch: 1 / 3, Step: 146 / 750 Loss: 0.8046\n",
      "Epoch: 1 / 3, Step: 147 / 750 Loss: 0.8064\n",
      "Epoch: 1 / 3, Step: 148 / 750 Loss: 0.6297\n",
      "Epoch: 1 / 3, Step: 149 / 750 Loss: 0.7561\n",
      "Epoch: 1 / 3, Step: 150 / 750 Loss: 0.5400\n",
      "Epoch: 1 / 3, Step: 151 / 750 Loss: 0.7638\n",
      "Epoch: 1 / 3, Step: 152 / 750 Loss: 0.4613\n",
      "Epoch: 1 / 3, Step: 153 / 750 Loss: 0.6225\n",
      "Epoch: 1 / 3, Step: 154 / 750 Loss: 0.6266\n",
      "Epoch: 1 / 3, Step: 155 / 750 Loss: 0.7573\n",
      "Epoch: 1 / 3, Step: 156 / 750 Loss: 0.7875\n",
      "Epoch: 1 / 3, Step: 157 / 750 Loss: 0.5653\n",
      "Epoch: 1 / 3, Step: 158 / 750 Loss: 0.6243\n",
      "Epoch: 1 / 3, Step: 159 / 750 Loss: 0.9017\n",
      "Epoch: 1 / 3, Step: 160 / 750 Loss: 0.6475\n",
      "Epoch: 1 / 3, Step: 161 / 750 Loss: 0.6911\n",
      "Epoch: 1 / 3, Step: 162 / 750 Loss: 0.3792\n",
      "Epoch: 1 / 3, Step: 163 / 750 Loss: 0.6918\n",
      "Epoch: 1 / 3, Step: 164 / 750 Loss: 0.6856\n",
      "Epoch: 1 / 3, Step: 165 / 750 Loss: 0.7657\n",
      "Epoch: 1 / 3, Step: 166 / 750 Loss: 0.7145\n",
      "Epoch: 1 / 3, Step: 167 / 750 Loss: 0.8267\n",
      "Epoch: 1 / 3, Step: 168 / 750 Loss: 0.5032\n",
      "Epoch: 1 / 3, Step: 169 / 750 Loss: 0.7524\n",
      "Epoch: 1 / 3, Step: 170 / 750 Loss: 0.8293\n",
      "Epoch: 1 / 3, Step: 171 / 750 Loss: 0.7183\n",
      "Epoch: 1 / 3, Step: 172 / 750 Loss: 0.6118\n",
      "Epoch: 1 / 3, Step: 173 / 750 Loss: 0.6329\n",
      "Epoch: 1 / 3, Step: 174 / 750 Loss: 0.7867\n",
      "Epoch: 1 / 3, Step: 175 / 750 Loss: 0.6535\n",
      "Epoch: 1 / 3, Step: 176 / 750 Loss: 0.8305\n",
      "Epoch: 1 / 3, Step: 177 / 750 Loss: 0.5912\n",
      "Epoch: 1 / 3, Step: 178 / 750 Loss: 0.6944\n",
      "Epoch: 1 / 3, Step: 179 / 750 Loss: 0.5253\n",
      "Epoch: 1 / 3, Step: 180 / 750 Loss: 0.6765\n",
      "Epoch: 1 / 3, Step: 181 / 750 Loss: 0.4955\n",
      "Epoch: 1 / 3, Step: 182 / 750 Loss: 0.3533\n",
      "Epoch: 1 / 3, Step: 183 / 750 Loss: 0.4236\n",
      "Epoch: 1 / 3, Step: 184 / 750 Loss: 0.4318\n",
      "Epoch: 1 / 3, Step: 185 / 750 Loss: 0.4771\n",
      "Epoch: 1 / 3, Step: 186 / 750 Loss: 0.7204\n",
      "Epoch: 1 / 3, Step: 187 / 750 Loss: 0.6361\n",
      "Epoch: 1 / 3, Step: 188 / 750 Loss: 0.4437\n",
      "Epoch: 1 / 3, Step: 189 / 750 Loss: 0.7366\n",
      "Epoch: 1 / 3, Step: 190 / 750 Loss: 0.5458\n",
      "Epoch: 1 / 3, Step: 191 / 750 Loss: 0.6190\n",
      "Epoch: 1 / 3, Step: 192 / 750 Loss: 0.4459\n",
      "Epoch: 1 / 3, Step: 193 / 750 Loss: 1.0292\n",
      "Epoch: 1 / 3, Step: 194 / 750 Loss: 0.7648\n",
      "Epoch: 1 / 3, Step: 195 / 750 Loss: 1.2006\n",
      "Epoch: 1 / 3, Step: 196 / 750 Loss: 0.6064\n",
      "Epoch: 1 / 3, Step: 197 / 750 Loss: 0.5237\n",
      "Epoch: 1 / 3, Step: 198 / 750 Loss: 0.4227\n",
      "Epoch: 1 / 3, Step: 199 / 750 Loss: 0.7219\n",
      "Epoch: 1 / 3, Step: 200 / 750 Loss: 0.3920\n",
      "Epoch: 1 / 3, Step: 201 / 750 Loss: 0.7751\n",
      "Epoch: 1 / 3, Step: 202 / 750 Loss: 0.7335\n",
      "Epoch: 1 / 3, Step: 203 / 750 Loss: 0.5680\n",
      "Epoch: 1 / 3, Step: 204 / 750 Loss: 0.9287\n",
      "Epoch: 1 / 3, Step: 205 / 750 Loss: 0.7453\n",
      "Epoch: 1 / 3, Step: 206 / 750 Loss: 0.8983\n",
      "Epoch: 1 / 3, Step: 207 / 750 Loss: 0.8068\n",
      "Epoch: 1 / 3, Step: 208 / 750 Loss: 0.7513\n",
      "Epoch: 1 / 3, Step: 209 / 750 Loss: 0.7350\n",
      "Epoch: 1 / 3, Step: 210 / 750 Loss: 0.6287\n",
      "Epoch: 1 / 3, Step: 211 / 750 Loss: 0.5391\n",
      "Epoch: 1 / 3, Step: 212 / 750 Loss: 0.7753\n",
      "Epoch: 1 / 3, Step: 213 / 750 Loss: 0.8869\n",
      "Epoch: 1 / 3, Step: 214 / 750 Loss: 0.8361\n",
      "Epoch: 1 / 3, Step: 215 / 750 Loss: 0.6678\n",
      "Epoch: 1 / 3, Step: 216 / 750 Loss: 0.5803\n",
      "Epoch: 1 / 3, Step: 217 / 750 Loss: 0.4633\n",
      "Epoch: 1 / 3, Step: 218 / 750 Loss: 0.7528\n",
      "Epoch: 1 / 3, Step: 219 / 750 Loss: 0.6304\n",
      "Epoch: 1 / 3, Step: 220 / 750 Loss: 0.6889\n",
      "Epoch: 1 / 3, Step: 221 / 750 Loss: 0.9058\n",
      "Epoch: 1 / 3, Step: 222 / 750 Loss: 0.6636\n",
      "Epoch: 1 / 3, Step: 223 / 750 Loss: 0.5285\n",
      "Epoch: 1 / 3, Step: 224 / 750 Loss: 0.4944\n",
      "Epoch: 1 / 3, Step: 225 / 750 Loss: 0.7249\n",
      "Epoch: 1 / 3, Step: 226 / 750 Loss: 0.4414\n",
      "Epoch: 1 / 3, Step: 227 / 750 Loss: 1.0700\n",
      "Epoch: 1 / 3, Step: 228 / 750 Loss: 0.7035\n",
      "Epoch: 1 / 3, Step: 229 / 750 Loss: 0.5574\n",
      "Epoch: 1 / 3, Step: 230 / 750 Loss: 0.5567\n",
      "Epoch: 1 / 3, Step: 231 / 750 Loss: 0.5988\n",
      "Epoch: 1 / 3, Step: 232 / 750 Loss: 0.8406\n",
      "Epoch: 1 / 3, Step: 233 / 750 Loss: 0.4800\n",
      "Epoch: 1 / 3, Step: 234 / 750 Loss: 0.7255\n",
      "Epoch: 1 / 3, Step: 235 / 750 Loss: 0.6958\n",
      "Epoch: 1 / 3, Step: 236 / 750 Loss: 0.4362\n",
      "Epoch: 1 / 3, Step: 237 / 750 Loss: 0.9133\n",
      "Epoch: 1 / 3, Step: 238 / 750 Loss: 0.9451\n",
      "Epoch: 1 / 3, Step: 239 / 750 Loss: 0.6402\n",
      "Epoch: 1 / 3, Step: 240 / 750 Loss: 0.6539\n",
      "Epoch: 1 / 3, Step: 241 / 750 Loss: 0.6343\n",
      "Epoch: 1 / 3, Step: 242 / 750 Loss: 0.6206\n",
      "Epoch: 1 / 3, Step: 243 / 750 Loss: 0.5881\n",
      "Epoch: 1 / 3, Step: 244 / 750 Loss: 1.0366\n",
      "Epoch: 1 / 3, Step: 245 / 750 Loss: 0.7201\n",
      "Epoch: 1 / 3, Step: 246 / 750 Loss: 0.4736\n",
      "Epoch: 1 / 3, Step: 247 / 750 Loss: 0.5236\n",
      "Epoch: 1 / 3, Step: 248 / 750 Loss: 0.8950\n",
      "Epoch: 1 / 3, Step: 249 / 750 Loss: 0.7056\n",
      "Epoch: 1 / 3, Step: 250 / 750 Loss: 0.5360\n",
      "Epoch: 1 / 3, Step: 251 / 750 Loss: 0.4717\n",
      "Epoch: 1 / 3, Step: 252 / 750 Loss: 0.7728\n",
      "Epoch: 1 / 3, Step: 253 / 750 Loss: 0.7868\n",
      "Epoch: 1 / 3, Step: 254 / 750 Loss: 0.8537\n",
      "Epoch: 1 / 3, Step: 255 / 750 Loss: 0.7448\n",
      "Epoch: 1 / 3, Step: 256 / 750 Loss: 0.6081\n",
      "Epoch: 1 / 3, Step: 257 / 750 Loss: 0.6624\n",
      "Epoch: 1 / 3, Step: 258 / 750 Loss: 0.6592\n",
      "Epoch: 1 / 3, Step: 259 / 750 Loss: 0.6954\n",
      "Epoch: 1 / 3, Step: 260 / 750 Loss: 0.5864\n",
      "Epoch: 1 / 3, Step: 261 / 750 Loss: 0.6419\n",
      "Epoch: 1 / 3, Step: 262 / 750 Loss: 0.6408\n",
      "Epoch: 1 / 3, Step: 263 / 750 Loss: 0.9053\n",
      "Epoch: 1 / 3, Step: 264 / 750 Loss: 0.7376\n",
      "Epoch: 1 / 3, Step: 265 / 750 Loss: 0.8172\n",
      "Epoch: 1 / 3, Step: 266 / 750 Loss: 0.7914\n",
      "Epoch: 1 / 3, Step: 267 / 750 Loss: 0.7571\n",
      "Epoch: 1 / 3, Step: 268 / 750 Loss: 0.9199\n",
      "Epoch: 1 / 3, Step: 269 / 750 Loss: 0.7434\n",
      "Epoch: 1 / 3, Step: 270 / 750 Loss: 0.8135\n",
      "Epoch: 1 / 3, Step: 271 / 750 Loss: 0.6604\n",
      "Epoch: 1 / 3, Step: 272 / 750 Loss: 0.9064\n",
      "Epoch: 1 / 3, Step: 273 / 750 Loss: 0.5160\n",
      "Epoch: 1 / 3, Step: 274 / 750 Loss: 0.5950\n",
      "Epoch: 1 / 3, Step: 275 / 750 Loss: 0.5659\n",
      "Epoch: 1 / 3, Step: 276 / 750 Loss: 0.6971\n",
      "Epoch: 1 / 3, Step: 277 / 750 Loss: 0.6170\n",
      "Epoch: 1 / 3, Step: 278 / 750 Loss: 0.7433\n",
      "Epoch: 1 / 3, Step: 279 / 750 Loss: 0.7858\n",
      "Epoch: 1 / 3, Step: 280 / 750 Loss: 0.5783\n",
      "Epoch: 1 / 3, Step: 281 / 750 Loss: 0.6748\n",
      "Epoch: 1 / 3, Step: 282 / 750 Loss: 0.6729\n",
      "Epoch: 1 / 3, Step: 283 / 750 Loss: 0.5417\n",
      "Epoch: 1 / 3, Step: 284 / 750 Loss: 0.5851\n",
      "Epoch: 1 / 3, Step: 285 / 750 Loss: 0.6055\n",
      "Epoch: 1 / 3, Step: 286 / 750 Loss: 0.4180\n",
      "Epoch: 1 / 3, Step: 287 / 750 Loss: 0.5622\n",
      "Epoch: 1 / 3, Step: 288 / 750 Loss: 0.7172\n",
      "Epoch: 1 / 3, Step: 289 / 750 Loss: 0.4286\n",
      "Epoch: 1 / 3, Step: 290 / 750 Loss: 1.0921\n",
      "Epoch: 1 / 3, Step: 291 / 750 Loss: 0.2189\n",
      "Epoch: 1 / 3, Step: 292 / 750 Loss: 0.5965\n",
      "Epoch: 1 / 3, Step: 293 / 750 Loss: 0.5972\n",
      "Epoch: 1 / 3, Step: 294 / 750 Loss: 0.7506\n",
      "Epoch: 1 / 3, Step: 295 / 750 Loss: 0.5761\n",
      "Epoch: 1 / 3, Step: 296 / 750 Loss: 0.3085\n",
      "Epoch: 1 / 3, Step: 297 / 750 Loss: 0.5372\n",
      "Epoch: 1 / 3, Step: 298 / 750 Loss: 0.4491\n",
      "Epoch: 1 / 3, Step: 299 / 750 Loss: 0.8498\n",
      "Epoch: 1 / 3, Step: 300 / 750 Loss: 0.6750\n",
      "Epoch: 1 / 3, Step: 301 / 750 Loss: 0.5834\n",
      "Epoch: 1 / 3, Step: 302 / 750 Loss: 0.5180\n",
      "Epoch: 1 / 3, Step: 303 / 750 Loss: 0.5356\n",
      "Epoch: 1 / 3, Step: 304 / 750 Loss: 0.5698\n",
      "Epoch: 1 / 3, Step: 305 / 750 Loss: 0.4833\n",
      "Epoch: 1 / 3, Step: 306 / 750 Loss: 0.6287\n",
      "Epoch: 1 / 3, Step: 307 / 750 Loss: 0.7025\n",
      "Epoch: 1 / 3, Step: 308 / 750 Loss: 0.7675\n",
      "Epoch: 1 / 3, Step: 309 / 750 Loss: 0.5956\n",
      "Epoch: 1 / 3, Step: 310 / 750 Loss: 0.6618\n",
      "Epoch: 1 / 3, Step: 311 / 750 Loss: 0.4607\n",
      "Epoch: 1 / 3, Step: 312 / 750 Loss: 0.5031\n",
      "Epoch: 1 / 3, Step: 313 / 750 Loss: 0.6725\n",
      "Epoch: 1 / 3, Step: 314 / 750 Loss: 0.5749\n",
      "Epoch: 1 / 3, Step: 315 / 750 Loss: 0.6991\n",
      "Epoch: 1 / 3, Step: 316 / 750 Loss: 0.5595\n",
      "Epoch: 1 / 3, Step: 317 / 750 Loss: 0.5696\n",
      "Epoch: 1 / 3, Step: 318 / 750 Loss: 0.3919\n",
      "Epoch: 1 / 3, Step: 319 / 750 Loss: 0.6053\n",
      "Epoch: 1 / 3, Step: 320 / 750 Loss: 0.7793\n",
      "Epoch: 1 / 3, Step: 321 / 750 Loss: 0.7397\n",
      "Epoch: 1 / 3, Step: 322 / 750 Loss: 0.6178\n",
      "Epoch: 1 / 3, Step: 323 / 750 Loss: 0.4994\n",
      "Epoch: 1 / 3, Step: 324 / 750 Loss: 0.6970\n",
      "Epoch: 1 / 3, Step: 325 / 750 Loss: 0.6847\n",
      "Epoch: 1 / 3, Step: 326 / 750 Loss: 0.6785\n",
      "Epoch: 1 / 3, Step: 327 / 750 Loss: 0.6167\n",
      "Epoch: 1 / 3, Step: 328 / 750 Loss: 0.6203\n",
      "Epoch: 1 / 3, Step: 329 / 750 Loss: 0.8651\n",
      "Epoch: 1 / 3, Step: 330 / 750 Loss: 0.5947\n",
      "Epoch: 1 / 3, Step: 331 / 750 Loss: 0.7847\n",
      "Epoch: 1 / 3, Step: 332 / 750 Loss: 0.5206\n",
      "Epoch: 1 / 3, Step: 333 / 750 Loss: 0.3017\n",
      "Epoch: 1 / 3, Step: 334 / 750 Loss: 0.4408\n",
      "Epoch: 1 / 3, Step: 335 / 750 Loss: 0.6084\n",
      "Epoch: 1 / 3, Step: 336 / 750 Loss: 0.6994\n",
      "Epoch: 1 / 3, Step: 337 / 750 Loss: 0.5179\n",
      "Epoch: 1 / 3, Step: 338 / 750 Loss: 0.4680\n",
      "Epoch: 1 / 3, Step: 339 / 750 Loss: 0.4205\n",
      "Epoch: 1 / 3, Step: 340 / 750 Loss: 0.8362\n",
      "Epoch: 1 / 3, Step: 341 / 750 Loss: 0.6584\n",
      "Epoch: 1 / 3, Step: 342 / 750 Loss: 0.3870\n",
      "Epoch: 1 / 3, Step: 343 / 750 Loss: 0.6576\n",
      "Epoch: 1 / 3, Step: 344 / 750 Loss: 0.6773\n",
      "Epoch: 1 / 3, Step: 345 / 750 Loss: 0.5730\n",
      "Epoch: 1 / 3, Step: 346 / 750 Loss: 0.6257\n",
      "Epoch: 1 / 3, Step: 347 / 750 Loss: 0.8914\n",
      "Epoch: 1 / 3, Step: 348 / 750 Loss: 0.7003\n",
      "Epoch: 1 / 3, Step: 349 / 750 Loss: 0.3162\n",
      "Epoch: 1 / 3, Step: 350 / 750 Loss: 0.6947\n",
      "Epoch: 1 / 3, Step: 351 / 750 Loss: 0.6117\n",
      "Epoch: 1 / 3, Step: 352 / 750 Loss: 0.5903\n",
      "Epoch: 1 / 3, Step: 353 / 750 Loss: 0.3817\n",
      "Epoch: 1 / 3, Step: 354 / 750 Loss: 0.3671\n",
      "Epoch: 1 / 3, Step: 355 / 750 Loss: 0.4670\n",
      "Epoch: 1 / 3, Step: 356 / 750 Loss: 0.6052\n",
      "Epoch: 1 / 3, Step: 357 / 750 Loss: 0.5555\n",
      "Epoch: 1 / 3, Step: 358 / 750 Loss: 0.6696\n",
      "Epoch: 1 / 3, Step: 359 / 750 Loss: 0.4982\n",
      "Epoch: 1 / 3, Step: 360 / 750 Loss: 0.3756\n",
      "Epoch: 1 / 3, Step: 361 / 750 Loss: 0.5399\n",
      "Epoch: 1 / 3, Step: 362 / 750 Loss: 0.8426\n",
      "Epoch: 1 / 3, Step: 363 / 750 Loss: 0.5392\n",
      "Epoch: 1 / 3, Step: 364 / 750 Loss: 0.3608\n",
      "Epoch: 1 / 3, Step: 365 / 750 Loss: 0.6939\n",
      "Epoch: 1 / 3, Step: 366 / 750 Loss: 0.4897\n",
      "Epoch: 1 / 3, Step: 367 / 750 Loss: 0.6554\n",
      "Epoch: 1 / 3, Step: 368 / 750 Loss: 0.6050\n",
      "Epoch: 1 / 3, Step: 369 / 750 Loss: 0.5867\n",
      "Epoch: 1 / 3, Step: 370 / 750 Loss: 0.6866\n",
      "Epoch: 1 / 3, Step: 371 / 750 Loss: 0.3384\n",
      "Epoch: 1 / 3, Step: 372 / 750 Loss: 0.5387\n",
      "Epoch: 1 / 3, Step: 373 / 750 Loss: 0.6469\n",
      "Epoch: 1 / 3, Step: 374 / 750 Loss: 0.7682\n",
      "Epoch: 1 / 3, Step: 375 / 750 Loss: 0.5430\n",
      "Epoch: 1 / 3, Step: 376 / 750 Loss: 0.3240\n",
      "Epoch: 1 / 3, Step: 377 / 750 Loss: 0.8254\n",
      "Epoch: 1 / 3, Step: 378 / 750 Loss: 0.6518\n",
      "Epoch: 1 / 3, Step: 379 / 750 Loss: 0.5463\n",
      "Epoch: 1 / 3, Step: 380 / 750 Loss: 0.7149\n",
      "Epoch: 1 / 3, Step: 381 / 750 Loss: 0.4243\n",
      "Epoch: 1 / 3, Step: 382 / 750 Loss: 0.6012\n",
      "Epoch: 1 / 3, Step: 383 / 750 Loss: 0.5347\n",
      "Epoch: 1 / 3, Step: 384 / 750 Loss: 0.7783\n",
      "Epoch: 1 / 3, Step: 385 / 750 Loss: 0.9346\n",
      "Epoch: 1 / 3, Step: 386 / 750 Loss: 0.2870\n",
      "Epoch: 1 / 3, Step: 387 / 750 Loss: 0.4948\n",
      "Epoch: 1 / 3, Step: 388 / 750 Loss: 0.4643\n",
      "Epoch: 1 / 3, Step: 389 / 750 Loss: 0.5481\n",
      "Epoch: 1 / 3, Step: 390 / 750 Loss: 0.5860\n",
      "Epoch: 1 / 3, Step: 391 / 750 Loss: 0.7930\n",
      "Epoch: 1 / 3, Step: 392 / 750 Loss: 0.4479\n",
      "Epoch: 1 / 3, Step: 393 / 750 Loss: 0.5698\n",
      "Epoch: 1 / 3, Step: 394 / 750 Loss: 0.6836\n",
      "Epoch: 1 / 3, Step: 395 / 750 Loss: 0.6020\n",
      "Epoch: 1 / 3, Step: 396 / 750 Loss: 0.6529\n",
      "Epoch: 1 / 3, Step: 397 / 750 Loss: 0.5006\n",
      "Epoch: 1 / 3, Step: 398 / 750 Loss: 0.7111\n",
      "Epoch: 1 / 3, Step: 399 / 750 Loss: 0.4759\n",
      "Epoch: 1 / 3, Step: 400 / 750 Loss: 0.8520\n",
      "Epoch: 1 / 3, Step: 401 / 750 Loss: 0.3659\n",
      "Epoch: 1 / 3, Step: 402 / 750 Loss: 0.5150\n",
      "Epoch: 1 / 3, Step: 403 / 750 Loss: 0.4833\n",
      "Epoch: 1 / 3, Step: 404 / 750 Loss: 0.4157\n",
      "Epoch: 1 / 3, Step: 405 / 750 Loss: 0.3256\n",
      "Epoch: 1 / 3, Step: 406 / 750 Loss: 0.3929\n",
      "Epoch: 1 / 3, Step: 407 / 750 Loss: 0.6312\n",
      "Epoch: 1 / 3, Step: 408 / 750 Loss: 0.4803\n",
      "Epoch: 1 / 3, Step: 409 / 750 Loss: 0.5324\n",
      "Epoch: 1 / 3, Step: 410 / 750 Loss: 0.4723\n",
      "Epoch: 1 / 3, Step: 411 / 750 Loss: 0.7163\n",
      "Epoch: 1 / 3, Step: 412 / 750 Loss: 0.5467\n",
      "Epoch: 1 / 3, Step: 413 / 750 Loss: 0.6324\n",
      "Epoch: 1 / 3, Step: 414 / 750 Loss: 0.4639\n",
      "Epoch: 1 / 3, Step: 415 / 750 Loss: 0.4281\n",
      "Epoch: 1 / 3, Step: 416 / 750 Loss: 0.5038\n",
      "Epoch: 1 / 3, Step: 417 / 750 Loss: 0.5690\n",
      "Epoch: 1 / 3, Step: 418 / 750 Loss: 0.5363\n",
      "Epoch: 1 / 3, Step: 419 / 750 Loss: 0.3405\n",
      "Epoch: 1 / 3, Step: 420 / 750 Loss: 0.4333\n",
      "Epoch: 1 / 3, Step: 421 / 750 Loss: 0.5329\n",
      "Epoch: 1 / 3, Step: 422 / 750 Loss: 0.4276\n",
      "Epoch: 1 / 3, Step: 423 / 750 Loss: 0.7617\n",
      "Epoch: 1 / 3, Step: 424 / 750 Loss: 0.4734\n",
      "Epoch: 1 / 3, Step: 425 / 750 Loss: 0.6038\n",
      "Epoch: 1 / 3, Step: 426 / 750 Loss: 0.4286\n",
      "Epoch: 1 / 3, Step: 427 / 750 Loss: 0.7956\n",
      "Epoch: 1 / 3, Step: 428 / 750 Loss: 0.5764\n",
      "Epoch: 1 / 3, Step: 429 / 750 Loss: 0.3373\n",
      "Epoch: 1 / 3, Step: 430 / 750 Loss: 0.2615\n",
      "Epoch: 1 / 3, Step: 431 / 750 Loss: 0.4217\n",
      "Epoch: 1 / 3, Step: 432 / 750 Loss: 0.3706\n",
      "Epoch: 1 / 3, Step: 433 / 750 Loss: 0.4258\n",
      "Epoch: 1 / 3, Step: 434 / 750 Loss: 0.6754\n",
      "Epoch: 1 / 3, Step: 435 / 750 Loss: 0.5917\n",
      "Epoch: 1 / 3, Step: 436 / 750 Loss: 0.3881\n",
      "Epoch: 1 / 3, Step: 437 / 750 Loss: 0.4691\n",
      "Epoch: 1 / 3, Step: 438 / 750 Loss: 0.6236\n",
      "Epoch: 1 / 3, Step: 439 / 750 Loss: 0.4727\n",
      "Epoch: 1 / 3, Step: 440 / 750 Loss: 0.3514\n",
      "Epoch: 1 / 3, Step: 441 / 750 Loss: 0.3871\n",
      "Epoch: 1 / 3, Step: 442 / 750 Loss: 0.5479\n",
      "Epoch: 1 / 3, Step: 443 / 750 Loss: 0.2912\n",
      "Epoch: 1 / 3, Step: 444 / 750 Loss: 0.4809\n",
      "Epoch: 1 / 3, Step: 445 / 750 Loss: 0.5134\n",
      "Epoch: 1 / 3, Step: 446 / 750 Loss: 0.6659\n",
      "Epoch: 1 / 3, Step: 447 / 750 Loss: 0.5245\n",
      "Epoch: 1 / 3, Step: 448 / 750 Loss: 0.7880\n",
      "Epoch: 1 / 3, Step: 449 / 750 Loss: 0.6736\n",
      "Epoch: 1 / 3, Step: 450 / 750 Loss: 0.5716\n",
      "Epoch: 1 / 3, Step: 451 / 750 Loss: 0.7566\n",
      "Epoch: 1 / 3, Step: 452 / 750 Loss: 0.6668\n",
      "Epoch: 1 / 3, Step: 453 / 750 Loss: 0.4305\n",
      "Epoch: 1 / 3, Step: 454 / 750 Loss: 0.6627\n",
      "Epoch: 1 / 3, Step: 455 / 750 Loss: 0.5788\n",
      "Epoch: 1 / 3, Step: 456 / 750 Loss: 0.5605\n",
      "Epoch: 1 / 3, Step: 457 / 750 Loss: 0.5193\n",
      "Epoch: 1 / 3, Step: 458 / 750 Loss: 0.5075\n",
      "Epoch: 1 / 3, Step: 459 / 750 Loss: 0.5134\n",
      "Epoch: 1 / 3, Step: 460 / 750 Loss: 0.5409\n",
      "Epoch: 1 / 3, Step: 461 / 750 Loss: 0.3894\n",
      "Epoch: 1 / 3, Step: 462 / 750 Loss: 0.5492\n",
      "Epoch: 1 / 3, Step: 463 / 750 Loss: 0.5316\n",
      "Epoch: 1 / 3, Step: 464 / 750 Loss: 0.4849\n",
      "Epoch: 1 / 3, Step: 465 / 750 Loss: 0.5077\n",
      "Epoch: 1 / 3, Step: 466 / 750 Loss: 0.4362\n",
      "Epoch: 1 / 3, Step: 467 / 750 Loss: 0.6687\n",
      "Epoch: 1 / 3, Step: 468 / 750 Loss: 0.5599\n",
      "Epoch: 1 / 3, Step: 469 / 750 Loss: 0.5802\n",
      "Epoch: 1 / 3, Step: 470 / 750 Loss: 0.4778\n",
      "Epoch: 1 / 3, Step: 471 / 750 Loss: 0.4717\n",
      "Epoch: 1 / 3, Step: 472 / 750 Loss: 0.3438\n",
      "Epoch: 1 / 3, Step: 473 / 750 Loss: 0.6345\n",
      "Epoch: 1 / 3, Step: 474 / 750 Loss: 0.2877\n",
      "Epoch: 1 / 3, Step: 475 / 750 Loss: 0.3920\n",
      "Epoch: 1 / 3, Step: 476 / 750 Loss: 0.4039\n",
      "Epoch: 1 / 3, Step: 477 / 750 Loss: 0.5153\n",
      "Epoch: 1 / 3, Step: 478 / 750 Loss: 0.4018\n",
      "Epoch: 1 / 3, Step: 479 / 750 Loss: 0.5108\n",
      "Epoch: 1 / 3, Step: 480 / 750 Loss: 0.6349\n",
      "Epoch: 1 / 3, Step: 481 / 750 Loss: 0.4734\n",
      "Epoch: 1 / 3, Step: 482 / 750 Loss: 0.6588\n",
      "Epoch: 1 / 3, Step: 483 / 750 Loss: 0.4604\n",
      "Epoch: 1 / 3, Step: 484 / 750 Loss: 0.5076\n",
      "Epoch: 1 / 3, Step: 485 / 750 Loss: 0.4825\n",
      "Epoch: 1 / 3, Step: 486 / 750 Loss: 0.6592\n",
      "Epoch: 1 / 3, Step: 487 / 750 Loss: 0.5003\n",
      "Epoch: 1 / 3, Step: 488 / 750 Loss: 0.3878\n",
      "Epoch: 1 / 3, Step: 489 / 750 Loss: 0.3633\n",
      "Epoch: 1 / 3, Step: 490 / 750 Loss: 0.4158\n",
      "Epoch: 1 / 3, Step: 491 / 750 Loss: 0.5719\n",
      "Epoch: 1 / 3, Step: 492 / 750 Loss: 0.4704\n",
      "Epoch: 1 / 3, Step: 493 / 750 Loss: 0.3186\n",
      "Epoch: 1 / 3, Step: 494 / 750 Loss: 0.3619\n",
      "Epoch: 1 / 3, Step: 495 / 750 Loss: 0.7051\n",
      "Epoch: 1 / 3, Step: 496 / 750 Loss: 0.3374\n",
      "Epoch: 1 / 3, Step: 497 / 750 Loss: 1.1221\n",
      "Epoch: 1 / 3, Step: 498 / 750 Loss: 0.6591\n",
      "Epoch: 1 / 3, Step: 499 / 750 Loss: 0.3325\n",
      "Epoch: 1 / 3, Step: 500 / 750 Loss: 0.3895\n",
      "Epoch: 1 / 3, Step: 501 / 750 Loss: 0.5819\n",
      "Epoch: 1 / 3, Step: 502 / 750 Loss: 0.6028\n",
      "Epoch: 1 / 3, Step: 503 / 750 Loss: 0.4255\n",
      "Epoch: 1 / 3, Step: 504 / 750 Loss: 0.6728\n",
      "Epoch: 1 / 3, Step: 505 / 750 Loss: 0.4899\n",
      "Epoch: 1 / 3, Step: 506 / 750 Loss: 0.4644\n",
      "Epoch: 1 / 3, Step: 507 / 750 Loss: 0.3995\n",
      "Epoch: 1 / 3, Step: 508 / 750 Loss: 0.3426\n",
      "Epoch: 1 / 3, Step: 509 / 750 Loss: 0.3884\n",
      "Epoch: 1 / 3, Step: 510 / 750 Loss: 0.2823\n",
      "Epoch: 1 / 3, Step: 511 / 750 Loss: 0.6628\n",
      "Epoch: 1 / 3, Step: 512 / 750 Loss: 0.4694\n",
      "Epoch: 1 / 3, Step: 513 / 750 Loss: 0.5359\n",
      "Epoch: 1 / 3, Step: 514 / 750 Loss: 0.5356\n",
      "Epoch: 1 / 3, Step: 515 / 750 Loss: 0.4198\n",
      "Epoch: 1 / 3, Step: 516 / 750 Loss: 0.5049\n",
      "Epoch: 1 / 3, Step: 517 / 750 Loss: 0.5498\n",
      "Epoch: 1 / 3, Step: 518 / 750 Loss: 0.4240\n",
      "Epoch: 1 / 3, Step: 519 / 750 Loss: 0.6908\n",
      "Epoch: 1 / 3, Step: 520 / 750 Loss: 0.4335\n",
      "Epoch: 1 / 3, Step: 521 / 750 Loss: 0.5397\n",
      "Epoch: 1 / 3, Step: 522 / 750 Loss: 0.5270\n",
      "Epoch: 1 / 3, Step: 523 / 750 Loss: 0.7699\n",
      "Epoch: 1 / 3, Step: 524 / 750 Loss: 0.5293\n",
      "Epoch: 1 / 3, Step: 525 / 750 Loss: 0.4926\n",
      "Epoch: 1 / 3, Step: 526 / 750 Loss: 0.4979\n",
      "Epoch: 1 / 3, Step: 527 / 750 Loss: 0.5619\n",
      "Epoch: 1 / 3, Step: 528 / 750 Loss: 0.5027\n",
      "Epoch: 1 / 3, Step: 529 / 750 Loss: 0.5399\n",
      "Epoch: 1 / 3, Step: 530 / 750 Loss: 0.5044\n",
      "Epoch: 1 / 3, Step: 531 / 750 Loss: 0.4072\n",
      "Epoch: 1 / 3, Step: 532 / 750 Loss: 0.4104\n",
      "Epoch: 1 / 3, Step: 533 / 750 Loss: 0.6445\n",
      "Epoch: 1 / 3, Step: 534 / 750 Loss: 0.3498\n",
      "Epoch: 1 / 3, Step: 535 / 750 Loss: 0.4555\n",
      "Epoch: 1 / 3, Step: 536 / 750 Loss: 0.6644\n",
      "Epoch: 1 / 3, Step: 537 / 750 Loss: 0.4532\n",
      "Epoch: 1 / 3, Step: 538 / 750 Loss: 0.5296\n",
      "Epoch: 1 / 3, Step: 539 / 750 Loss: 0.3439\n",
      "Epoch: 1 / 3, Step: 540 / 750 Loss: 0.4655\n",
      "Epoch: 1 / 3, Step: 541 / 750 Loss: 0.4087\n",
      "Epoch: 1 / 3, Step: 542 / 750 Loss: 0.5801\n",
      "Epoch: 1 / 3, Step: 543 / 750 Loss: 0.4151\n",
      "Epoch: 1 / 3, Step: 544 / 750 Loss: 0.3092\n",
      "Epoch: 1 / 3, Step: 545 / 750 Loss: 0.6345\n",
      "Epoch: 1 / 3, Step: 546 / 750 Loss: 0.2553\n",
      "Epoch: 1 / 3, Step: 547 / 750 Loss: 0.4867\n",
      "Epoch: 1 / 3, Step: 548 / 750 Loss: 0.2672\n",
      "Epoch: 1 / 3, Step: 549 / 750 Loss: 0.4791\n",
      "Epoch: 1 / 3, Step: 550 / 750 Loss: 0.3033\n",
      "Epoch: 1 / 3, Step: 551 / 750 Loss: 0.2746\n",
      "Epoch: 1 / 3, Step: 552 / 750 Loss: 0.5717\n",
      "Epoch: 1 / 3, Step: 553 / 750 Loss: 0.3880\n",
      "Epoch: 1 / 3, Step: 554 / 750 Loss: 0.5529\n",
      "Epoch: 1 / 3, Step: 555 / 750 Loss: 0.3747\n",
      "Epoch: 1 / 3, Step: 556 / 750 Loss: 0.5369\n",
      "Epoch: 1 / 3, Step: 557 / 750 Loss: 0.5497\n",
      "Epoch: 1 / 3, Step: 558 / 750 Loss: 0.1667\n",
      "Epoch: 1 / 3, Step: 559 / 750 Loss: 0.6528\n",
      "Epoch: 1 / 3, Step: 560 / 750 Loss: 0.3743\n",
      "Epoch: 1 / 3, Step: 561 / 750 Loss: 0.5027\n",
      "Epoch: 1 / 3, Step: 562 / 750 Loss: 0.4391\n",
      "Epoch: 1 / 3, Step: 563 / 750 Loss: 0.5348\n",
      "Epoch: 1 / 3, Step: 564 / 750 Loss: 0.3662\n",
      "Epoch: 1 / 3, Step: 565 / 750 Loss: 0.4375\n",
      "Epoch: 1 / 3, Step: 566 / 750 Loss: 0.4420\n",
      "Epoch: 1 / 3, Step: 567 / 750 Loss: 0.7229\n",
      "Epoch: 1 / 3, Step: 568 / 750 Loss: 0.3159\n",
      "Epoch: 1 / 3, Step: 569 / 750 Loss: 0.3352\n",
      "Epoch: 1 / 3, Step: 570 / 750 Loss: 0.4454\n",
      "Epoch: 1 / 3, Step: 571 / 750 Loss: 0.2925\n",
      "Epoch: 1 / 3, Step: 572 / 750 Loss: 0.6505\n",
      "Epoch: 1 / 3, Step: 573 / 750 Loss: 0.4271\n",
      "Epoch: 1 / 3, Step: 574 / 750 Loss: 0.6670\n",
      "Epoch: 1 / 3, Step: 575 / 750 Loss: 0.5260\n",
      "Epoch: 1 / 3, Step: 576 / 750 Loss: 0.4627\n",
      "Epoch: 1 / 3, Step: 577 / 750 Loss: 0.3770\n",
      "Epoch: 1 / 3, Step: 578 / 750 Loss: 0.5184\n",
      "Epoch: 1 / 3, Step: 579 / 750 Loss: 0.3715\n",
      "Epoch: 1 / 3, Step: 580 / 750 Loss: 0.2801\n",
      "Epoch: 1 / 3, Step: 581 / 750 Loss: 0.3709\n",
      "Epoch: 1 / 3, Step: 582 / 750 Loss: 0.7302\n",
      "Epoch: 1 / 3, Step: 583 / 750 Loss: 0.3924\n",
      "Epoch: 1 / 3, Step: 584 / 750 Loss: 0.3969\n",
      "Epoch: 1 / 3, Step: 585 / 750 Loss: 0.5422\n",
      "Epoch: 1 / 3, Step: 586 / 750 Loss: 0.4533\n",
      "Epoch: 1 / 3, Step: 587 / 750 Loss: 0.3610\n",
      "Epoch: 1 / 3, Step: 588 / 750 Loss: 0.4228\n",
      "Epoch: 1 / 3, Step: 589 / 750 Loss: 0.3555\n",
      "Epoch: 1 / 3, Step: 590 / 750 Loss: 0.8658\n",
      "Epoch: 1 / 3, Step: 591 / 750 Loss: 0.6035\n",
      "Epoch: 1 / 3, Step: 592 / 750 Loss: 0.3454\n",
      "Epoch: 1 / 3, Step: 593 / 750 Loss: 0.3387\n",
      "Epoch: 1 / 3, Step: 594 / 750 Loss: 0.4334\n",
      "Epoch: 1 / 3, Step: 595 / 750 Loss: 0.4400\n",
      "Epoch: 1 / 3, Step: 596 / 750 Loss: 0.5052\n",
      "Epoch: 1 / 3, Step: 597 / 750 Loss: 0.2380\n",
      "Epoch: 1 / 3, Step: 598 / 750 Loss: 0.3779\n",
      "Epoch: 1 / 3, Step: 599 / 750 Loss: 0.3389\n",
      "Epoch: 1 / 3, Step: 600 / 750 Loss: 0.2746\n",
      "Epoch: 1 / 3, Step: 601 / 750 Loss: 0.3970\n",
      "Epoch: 1 / 3, Step: 602 / 750 Loss: 0.5953\n",
      "Epoch: 1 / 3, Step: 603 / 750 Loss: 0.1798\n",
      "Epoch: 1 / 3, Step: 604 / 750 Loss: 0.3403\n",
      "Epoch: 1 / 3, Step: 605 / 750 Loss: 0.4609\n",
      "Epoch: 1 / 3, Step: 606 / 750 Loss: 0.6496\n",
      "Epoch: 1 / 3, Step: 607 / 750 Loss: 0.4091\n",
      "Epoch: 1 / 3, Step: 608 / 750 Loss: 0.7465\n",
      "Epoch: 1 / 3, Step: 609 / 750 Loss: 0.2423\n",
      "Epoch: 1 / 3, Step: 610 / 750 Loss: 0.3963\n",
      "Epoch: 1 / 3, Step: 611 / 750 Loss: 0.3990\n",
      "Epoch: 1 / 3, Step: 612 / 750 Loss: 0.5330\n",
      "Epoch: 1 / 3, Step: 613 / 750 Loss: 0.3946\n",
      "Epoch: 1 / 3, Step: 614 / 750 Loss: 0.4030\n",
      "Epoch: 1 / 3, Step: 615 / 750 Loss: 0.4505\n",
      "Epoch: 1 / 3, Step: 616 / 750 Loss: 0.4217\n",
      "Epoch: 1 / 3, Step: 617 / 750 Loss: 0.4984\n",
      "Epoch: 1 / 3, Step: 618 / 750 Loss: 0.3546\n",
      "Epoch: 1 / 3, Step: 619 / 750 Loss: 0.4476\n",
      "Epoch: 1 / 3, Step: 620 / 750 Loss: 0.2130\n",
      "Epoch: 1 / 3, Step: 621 / 750 Loss: 0.2343\n",
      "Epoch: 1 / 3, Step: 622 / 750 Loss: 0.4198\n",
      "Epoch: 1 / 3, Step: 623 / 750 Loss: 0.2727\n",
      "Epoch: 1 / 3, Step: 624 / 750 Loss: 0.4115\n",
      "Epoch: 1 / 3, Step: 625 / 750 Loss: 0.4715\n",
      "Epoch: 1 / 3, Step: 626 / 750 Loss: 0.4155\n",
      "Epoch: 1 / 3, Step: 627 / 750 Loss: 0.2343\n",
      "Epoch: 1 / 3, Step: 628 / 750 Loss: 0.1950\n",
      "Epoch: 1 / 3, Step: 629 / 750 Loss: 0.5812\n",
      "Epoch: 1 / 3, Step: 630 / 750 Loss: 0.2949\n",
      "Epoch: 1 / 3, Step: 631 / 750 Loss: 0.2963\n",
      "Epoch: 1 / 3, Step: 632 / 750 Loss: 0.3114\n",
      "Epoch: 1 / 3, Step: 633 / 750 Loss: 0.4998\n",
      "Epoch: 1 / 3, Step: 634 / 750 Loss: 0.3384\n",
      "Epoch: 1 / 3, Step: 635 / 750 Loss: 0.3547\n",
      "Epoch: 1 / 3, Step: 636 / 750 Loss: 0.3022\n",
      "Epoch: 1 / 3, Step: 637 / 750 Loss: 0.2641\n",
      "Epoch: 1 / 3, Step: 638 / 750 Loss: 0.4457\n",
      "Epoch: 1 / 3, Step: 639 / 750 Loss: 0.4285\n",
      "Epoch: 1 / 3, Step: 640 / 750 Loss: 0.4431\n",
      "Epoch: 1 / 3, Step: 641 / 750 Loss: 0.3027\n",
      "Epoch: 1 / 3, Step: 642 / 750 Loss: 0.5787\n",
      "Epoch: 1 / 3, Step: 643 / 750 Loss: 0.2416\n",
      "Epoch: 1 / 3, Step: 644 / 750 Loss: 0.5265\n",
      "Epoch: 1 / 3, Step: 645 / 750 Loss: 0.3072\n",
      "Epoch: 1 / 3, Step: 646 / 750 Loss: 0.4525\n",
      "Epoch: 1 / 3, Step: 647 / 750 Loss: 0.6638\n",
      "Epoch: 1 / 3, Step: 648 / 750 Loss: 0.2407\n",
      "Epoch: 1 / 3, Step: 649 / 750 Loss: 0.4261\n",
      "Epoch: 1 / 3, Step: 650 / 750 Loss: 0.5448\n",
      "Epoch: 1 / 3, Step: 651 / 750 Loss: 0.3275\n",
      "Epoch: 1 / 3, Step: 652 / 750 Loss: 0.4511\n",
      "Epoch: 1 / 3, Step: 653 / 750 Loss: 0.4009\n",
      "Epoch: 1 / 3, Step: 654 / 750 Loss: 0.3982\n",
      "Epoch: 1 / 3, Step: 655 / 750 Loss: 0.4120\n",
      "Epoch: 1 / 3, Step: 656 / 750 Loss: 0.3223\n",
      "Epoch: 1 / 3, Step: 657 / 750 Loss: 0.3801\n",
      "Epoch: 1 / 3, Step: 658 / 750 Loss: 0.5523\n",
      "Epoch: 1 / 3, Step: 659 / 750 Loss: 0.3165\n",
      "Epoch: 1 / 3, Step: 660 / 750 Loss: 0.4611\n",
      "Epoch: 1 / 3, Step: 661 / 750 Loss: 0.3333\n",
      "Epoch: 1 / 3, Step: 662 / 750 Loss: 0.3943\n",
      "Epoch: 1 / 3, Step: 663 / 750 Loss: 0.3866\n",
      "Epoch: 1 / 3, Step: 664 / 750 Loss: 0.4705\n",
      "Epoch: 1 / 3, Step: 665 / 750 Loss: 0.3776\n",
      "Epoch: 1 / 3, Step: 666 / 750 Loss: 0.2815\n",
      "Epoch: 1 / 3, Step: 667 / 750 Loss: 0.5077\n",
      "Epoch: 1 / 3, Step: 668 / 750 Loss: 0.2486\n",
      "Epoch: 1 / 3, Step: 669 / 750 Loss: 0.3853\n",
      "Epoch: 1 / 3, Step: 670 / 750 Loss: 0.4605\n",
      "Epoch: 1 / 3, Step: 671 / 750 Loss: 0.3367\n",
      "Epoch: 1 / 3, Step: 672 / 750 Loss: 0.2726\n",
      "Epoch: 1 / 3, Step: 673 / 750 Loss: 0.3268\n",
      "Epoch: 1 / 3, Step: 674 / 750 Loss: 0.6284\n",
      "Epoch: 1 / 3, Step: 675 / 750 Loss: 0.1801\n",
      "Epoch: 1 / 3, Step: 676 / 750 Loss: 0.2366\n",
      "Epoch: 1 / 3, Step: 677 / 750 Loss: 0.4939\n",
      "Epoch: 1 / 3, Step: 678 / 750 Loss: 0.8025\n",
      "Epoch: 1 / 3, Step: 679 / 750 Loss: 0.1574\n",
      "Epoch: 1 / 3, Step: 680 / 750 Loss: 0.3158\n",
      "Epoch: 1 / 3, Step: 681 / 750 Loss: 0.2982\n",
      "Epoch: 1 / 3, Step: 682 / 750 Loss: 0.3961\n",
      "Epoch: 1 / 3, Step: 683 / 750 Loss: 0.4692\n",
      "Epoch: 1 / 3, Step: 684 / 750 Loss: 0.3414\n",
      "Epoch: 1 / 3, Step: 685 / 750 Loss: 0.3396\n",
      "Epoch: 1 / 3, Step: 686 / 750 Loss: 0.1531\n",
      "Epoch: 1 / 3, Step: 687 / 750 Loss: 0.4805\n",
      "Epoch: 1 / 3, Step: 688 / 750 Loss: 0.2488\n",
      "Epoch: 1 / 3, Step: 689 / 750 Loss: 0.2658\n",
      "Epoch: 1 / 3, Step: 690 / 750 Loss: 0.3447\n",
      "Epoch: 1 / 3, Step: 691 / 750 Loss: 0.1535\n",
      "Epoch: 1 / 3, Step: 692 / 750 Loss: 0.1661\n",
      "Epoch: 1 / 3, Step: 693 / 750 Loss: 0.3416\n",
      "Epoch: 1 / 3, Step: 694 / 750 Loss: 0.5253\n",
      "Epoch: 1 / 3, Step: 695 / 750 Loss: 0.4078\n",
      "Epoch: 1 / 3, Step: 696 / 750 Loss: 0.2811\n",
      "Epoch: 1 / 3, Step: 697 / 750 Loss: 0.3834\n",
      "Epoch: 1 / 3, Step: 698 / 750 Loss: 0.3018\n",
      "Epoch: 1 / 3, Step: 699 / 750 Loss: 0.3185\n",
      "Epoch: 1 / 3, Step: 700 / 750 Loss: 0.2732\n",
      "Epoch: 1 / 3, Step: 701 / 750 Loss: 0.4237\n",
      "Epoch: 1 / 3, Step: 702 / 750 Loss: 0.3973\n",
      "Epoch: 1 / 3, Step: 703 / 750 Loss: 0.2822\n",
      "Epoch: 1 / 3, Step: 704 / 750 Loss: 0.2006\n",
      "Epoch: 1 / 3, Step: 705 / 750 Loss: 0.3193\n",
      "Epoch: 1 / 3, Step: 706 / 750 Loss: 0.3100\n",
      "Epoch: 1 / 3, Step: 707 / 750 Loss: 0.3475\n",
      "Epoch: 1 / 3, Step: 708 / 750 Loss: 0.2640\n",
      "Epoch: 1 / 3, Step: 709 / 750 Loss: 0.2636\n",
      "Epoch: 1 / 3, Step: 710 / 750 Loss: 0.3119\n",
      "Epoch: 1 / 3, Step: 711 / 750 Loss: 0.2911\n",
      "Epoch: 1 / 3, Step: 712 / 750 Loss: 0.2281\n",
      "Epoch: 1 / 3, Step: 713 / 750 Loss: 0.2210\n",
      "Epoch: 1 / 3, Step: 714 / 750 Loss: 0.4278\n",
      "Epoch: 1 / 3, Step: 715 / 750 Loss: 0.5630\n",
      "Epoch: 1 / 3, Step: 716 / 750 Loss: 0.3713\n",
      "Epoch: 1 / 3, Step: 717 / 750 Loss: 0.4532\n",
      "Epoch: 1 / 3, Step: 718 / 750 Loss: 0.4856\n",
      "Epoch: 1 / 3, Step: 719 / 750 Loss: 0.1042\n",
      "Epoch: 1 / 3, Step: 720 / 750 Loss: 0.5445\n",
      "Epoch: 1 / 3, Step: 721 / 750 Loss: 0.3989\n",
      "Epoch: 1 / 3, Step: 722 / 750 Loss: 0.2570\n",
      "Epoch: 1 / 3, Step: 723 / 750 Loss: 0.2820\n",
      "Epoch: 1 / 3, Step: 724 / 750 Loss: 0.3319\n",
      "Epoch: 1 / 3, Step: 725 / 750 Loss: 0.4315\n",
      "Epoch: 1 / 3, Step: 726 / 750 Loss: 0.4433\n",
      "Epoch: 1 / 3, Step: 727 / 750 Loss: 0.2728\n",
      "Epoch: 1 / 3, Step: 728 / 750 Loss: 0.5796\n",
      "Epoch: 1 / 3, Step: 729 / 750 Loss: 0.3517\n",
      "Epoch: 1 / 3, Step: 730 / 750 Loss: 0.2369\n",
      "Epoch: 1 / 3, Step: 731 / 750 Loss: 0.4664\n",
      "Epoch: 1 / 3, Step: 732 / 750 Loss: 0.2696\n",
      "Epoch: 1 / 3, Step: 733 / 750 Loss: 0.3636\n",
      "Epoch: 1 / 3, Step: 734 / 750 Loss: 0.2907\n",
      "Epoch: 1 / 3, Step: 735 / 750 Loss: 0.3745\n",
      "Epoch: 1 / 3, Step: 736 / 750 Loss: 0.3968\n",
      "Epoch: 1 / 3, Step: 737 / 750 Loss: 0.2044\n",
      "Epoch: 1 / 3, Step: 738 / 750 Loss: 0.4253\n",
      "Epoch: 1 / 3, Step: 739 / 750 Loss: 0.1688\n",
      "Epoch: 1 / 3, Step: 740 / 750 Loss: 0.4107\n",
      "Epoch: 1 / 3, Step: 741 / 750 Loss: 0.3165\n",
      "Epoch: 1 / 3, Step: 742 / 750 Loss: 0.4776\n",
      "Epoch: 1 / 3, Step: 743 / 750 Loss: 0.4154\n",
      "Epoch: 1 / 3, Step: 744 / 750 Loss: 0.2845\n",
      "Epoch: 1 / 3, Step: 745 / 750 Loss: 0.1718\n",
      "Epoch: 1 / 3, Step: 746 / 750 Loss: 0.5253\n",
      "Epoch: 1 / 3, Step: 747 / 750 Loss: 0.1515\n",
      "Epoch: 1 / 3, Step: 748 / 750 Loss: 0.4082\n",
      "Epoch: 1 / 3, Step: 749 / 750 Loss: 0.2958\n",
      "Epoch: 2 / 3, Step: 0 / 750 Loss: 0.3228\n",
      "Epoch: 2 / 3, Step: 1 / 750 Loss: 0.2924\n",
      "Epoch: 2 / 3, Step: 2 / 750 Loss: 0.4706\n",
      "Epoch: 2 / 3, Step: 3 / 750 Loss: 0.2761\n",
      "Epoch: 2 / 3, Step: 4 / 750 Loss: 0.3349\n",
      "Epoch: 2 / 3, Step: 5 / 750 Loss: 0.1675\n",
      "Epoch: 2 / 3, Step: 6 / 750 Loss: 0.2069\n",
      "Epoch: 2 / 3, Step: 7 / 750 Loss: 0.3241\n",
      "Epoch: 2 / 3, Step: 8 / 750 Loss: 0.3753\n",
      "Epoch: 2 / 3, Step: 9 / 750 Loss: 0.2399\n",
      "Epoch: 2 / 3, Step: 10 / 750 Loss: 0.3608\n",
      "Epoch: 2 / 3, Step: 11 / 750 Loss: 0.3143\n",
      "Epoch: 2 / 3, Step: 12 / 750 Loss: 0.3131\n",
      "Epoch: 2 / 3, Step: 13 / 750 Loss: 0.4168\n",
      "Epoch: 2 / 3, Step: 14 / 750 Loss: 0.3189\n",
      "Epoch: 2 / 3, Step: 15 / 750 Loss: 0.2410\n",
      "Epoch: 2 / 3, Step: 16 / 750 Loss: 0.1514\n",
      "Epoch: 2 / 3, Step: 17 / 750 Loss: 0.3132\n",
      "Epoch: 2 / 3, Step: 18 / 750 Loss: 0.3353\n",
      "Epoch: 2 / 3, Step: 19 / 750 Loss: 0.3748\n",
      "Epoch: 2 / 3, Step: 20 / 750 Loss: 0.2520\n",
      "Epoch: 2 / 3, Step: 21 / 750 Loss: 0.4193\n",
      "Epoch: 2 / 3, Step: 22 / 750 Loss: 0.1599\n",
      "Epoch: 2 / 3, Step: 23 / 750 Loss: 0.1649\n",
      "Epoch: 2 / 3, Step: 24 / 750 Loss: 0.3648\n",
      "Epoch: 2 / 3, Step: 25 / 750 Loss: 0.4594\n",
      "Epoch: 2 / 3, Step: 26 / 750 Loss: 0.4400\n",
      "Epoch: 2 / 3, Step: 27 / 750 Loss: 0.2206\n",
      "Epoch: 2 / 3, Step: 28 / 750 Loss: 0.2700\n",
      "Epoch: 2 / 3, Step: 29 / 750 Loss: 0.1283\n",
      "Epoch: 2 / 3, Step: 30 / 750 Loss: 0.0983\n",
      "Epoch: 2 / 3, Step: 31 / 750 Loss: 0.2847\n",
      "Epoch: 2 / 3, Step: 32 / 750 Loss: 0.4490\n",
      "Epoch: 2 / 3, Step: 33 / 750 Loss: 0.2526\n",
      "Epoch: 2 / 3, Step: 34 / 750 Loss: 0.2847\n",
      "Epoch: 2 / 3, Step: 35 / 750 Loss: 0.2250\n",
      "Epoch: 2 / 3, Step: 36 / 750 Loss: 0.1816\n",
      "Epoch: 2 / 3, Step: 37 / 750 Loss: 0.3118\n",
      "Epoch: 2 / 3, Step: 38 / 750 Loss: 0.5852\n",
      "Epoch: 2 / 3, Step: 39 / 750 Loss: 0.2626\n",
      "Epoch: 2 / 3, Step: 40 / 750 Loss: 0.5024\n",
      "Epoch: 2 / 3, Step: 41 / 750 Loss: 0.2290\n",
      "Epoch: 2 / 3, Step: 42 / 750 Loss: 0.3499\n",
      "Epoch: 2 / 3, Step: 43 / 750 Loss: 0.2503\n",
      "Epoch: 2 / 3, Step: 44 / 750 Loss: 0.2680\n",
      "Epoch: 2 / 3, Step: 45 / 750 Loss: 0.2372\n",
      "Epoch: 2 / 3, Step: 46 / 750 Loss: 0.1138\n",
      "Epoch: 2 / 3, Step: 47 / 750 Loss: 0.4126\n",
      "Epoch: 2 / 3, Step: 48 / 750 Loss: 0.3302\n",
      "Epoch: 2 / 3, Step: 49 / 750 Loss: 0.4330\n",
      "Epoch: 2 / 3, Step: 50 / 750 Loss: 0.1829\n",
      "Epoch: 2 / 3, Step: 51 / 750 Loss: 0.1068\n",
      "Epoch: 2 / 3, Step: 52 / 750 Loss: 0.2352\n",
      "Epoch: 2 / 3, Step: 53 / 750 Loss: 0.3822\n",
      "Epoch: 2 / 3, Step: 54 / 750 Loss: 0.5636\n",
      "Epoch: 2 / 3, Step: 55 / 750 Loss: 0.3256\n",
      "Epoch: 2 / 3, Step: 56 / 750 Loss: 0.3337\n",
      "Epoch: 2 / 3, Step: 57 / 750 Loss: 0.3050\n",
      "Epoch: 2 / 3, Step: 58 / 750 Loss: 0.2895\n",
      "Epoch: 2 / 3, Step: 59 / 750 Loss: 0.3843\n",
      "Epoch: 2 / 3, Step: 60 / 750 Loss: 0.2685\n",
      "Epoch: 2 / 3, Step: 61 / 750 Loss: 0.2612\n",
      "Epoch: 2 / 3, Step: 62 / 750 Loss: 0.2568\n",
      "Epoch: 2 / 3, Step: 63 / 750 Loss: 0.2804\n",
      "Epoch: 2 / 3, Step: 64 / 750 Loss: 0.2416\n",
      "Epoch: 2 / 3, Step: 65 / 750 Loss: 0.2521\n",
      "Epoch: 2 / 3, Step: 66 / 750 Loss: 0.3011\n",
      "Epoch: 2 / 3, Step: 67 / 750 Loss: 0.1955\n",
      "Epoch: 2 / 3, Step: 68 / 750 Loss: 0.3483\n",
      "Epoch: 2 / 3, Step: 69 / 750 Loss: 0.6023\n",
      "Epoch: 2 / 3, Step: 70 / 750 Loss: 0.1678\n",
      "Epoch: 2 / 3, Step: 71 / 750 Loss: 0.1208\n",
      "Epoch: 2 / 3, Step: 72 / 750 Loss: 0.1945\n",
      "Epoch: 2 / 3, Step: 73 / 750 Loss: 0.3029\n",
      "Epoch: 2 / 3, Step: 74 / 750 Loss: 0.4027\n",
      "Epoch: 2 / 3, Step: 75 / 750 Loss: 0.4871\n",
      "Epoch: 2 / 3, Step: 76 / 750 Loss: 0.1595\n",
      "Epoch: 2 / 3, Step: 77 / 750 Loss: 0.3360\n",
      "Epoch: 2 / 3, Step: 78 / 750 Loss: 0.4512\n",
      "Epoch: 2 / 3, Step: 79 / 750 Loss: 0.3136\n",
      "Epoch: 2 / 3, Step: 80 / 750 Loss: 0.1461\n",
      "Epoch: 2 / 3, Step: 81 / 750 Loss: 0.2958\n",
      "Epoch: 2 / 3, Step: 82 / 750 Loss: 0.1492\n",
      "Epoch: 2 / 3, Step: 83 / 750 Loss: 0.3390\n",
      "Epoch: 2 / 3, Step: 84 / 750 Loss: 0.2322\n",
      "Epoch: 2 / 3, Step: 85 / 750 Loss: 0.2733\n",
      "Epoch: 2 / 3, Step: 86 / 750 Loss: 0.4752\n",
      "Epoch: 2 / 3, Step: 87 / 750 Loss: 0.2242\n",
      "Epoch: 2 / 3, Step: 88 / 750 Loss: 0.5285\n",
      "Epoch: 2 / 3, Step: 89 / 750 Loss: 0.2285\n",
      "Epoch: 2 / 3, Step: 90 / 750 Loss: 0.1863\n",
      "Epoch: 2 / 3, Step: 91 / 750 Loss: 0.1288\n",
      "Epoch: 2 / 3, Step: 92 / 750 Loss: 0.2010\n",
      "Epoch: 2 / 3, Step: 93 / 750 Loss: 0.1655\n",
      "Epoch: 2 / 3, Step: 94 / 750 Loss: 0.2726\n",
      "Epoch: 2 / 3, Step: 95 / 750 Loss: 0.2796\n",
      "Epoch: 2 / 3, Step: 96 / 750 Loss: 0.3893\n",
      "Epoch: 2 / 3, Step: 97 / 750 Loss: 0.2155\n",
      "Epoch: 2 / 3, Step: 98 / 750 Loss: 0.1490\n",
      "Epoch: 2 / 3, Step: 99 / 750 Loss: 0.2508\n",
      "Epoch: 2 / 3, Step: 100 / 750 Loss: 0.3128\n",
      "Epoch: 2 / 3, Step: 101 / 750 Loss: 0.5017\n",
      "Epoch: 2 / 3, Step: 102 / 750 Loss: 0.2339\n",
      "Epoch: 2 / 3, Step: 103 / 750 Loss: 0.3750\n",
      "Epoch: 2 / 3, Step: 104 / 750 Loss: 0.3471\n",
      "Epoch: 2 / 3, Step: 105 / 750 Loss: 0.1492\n",
      "Epoch: 2 / 3, Step: 106 / 750 Loss: 0.2170\n",
      "Epoch: 2 / 3, Step: 107 / 750 Loss: 0.1937\n",
      "Epoch: 2 / 3, Step: 108 / 750 Loss: 0.2248\n",
      "Epoch: 2 / 3, Step: 109 / 750 Loss: 0.2926\n",
      "Epoch: 2 / 3, Step: 110 / 750 Loss: 0.4679\n",
      "Epoch: 2 / 3, Step: 111 / 750 Loss: 0.3593\n",
      "Epoch: 2 / 3, Step: 112 / 750 Loss: 0.3135\n",
      "Epoch: 2 / 3, Step: 113 / 750 Loss: 0.2511\n",
      "Epoch: 2 / 3, Step: 114 / 750 Loss: 0.3799\n",
      "Epoch: 2 / 3, Step: 115 / 750 Loss: 0.2022\n",
      "Epoch: 2 / 3, Step: 116 / 750 Loss: 0.3621\n",
      "Epoch: 2 / 3, Step: 117 / 750 Loss: 0.4850\n",
      "Epoch: 2 / 3, Step: 118 / 750 Loss: 0.3930\n",
      "Epoch: 2 / 3, Step: 119 / 750 Loss: 0.6099\n",
      "Epoch: 2 / 3, Step: 120 / 750 Loss: 0.3540\n",
      "Epoch: 2 / 3, Step: 121 / 750 Loss: 0.3403\n",
      "Epoch: 2 / 3, Step: 122 / 750 Loss: 0.2794\n",
      "Epoch: 2 / 3, Step: 123 / 750 Loss: 0.7524\n",
      "Epoch: 2 / 3, Step: 124 / 750 Loss: 0.3603\n",
      "Epoch: 2 / 3, Step: 125 / 750 Loss: 0.3371\n",
      "Epoch: 2 / 3, Step: 126 / 750 Loss: 0.3591\n",
      "Epoch: 2 / 3, Step: 127 / 750 Loss: 0.4312\n",
      "Epoch: 2 / 3, Step: 128 / 750 Loss: 0.3047\n",
      "Epoch: 2 / 3, Step: 129 / 750 Loss: 0.2758\n",
      "Epoch: 2 / 3, Step: 130 / 750 Loss: 0.2757\n",
      "Epoch: 2 / 3, Step: 131 / 750 Loss: 0.2782\n",
      "Epoch: 2 / 3, Step: 132 / 750 Loss: 0.3254\n",
      "Epoch: 2 / 3, Step: 133 / 750 Loss: 0.5510\n",
      "Epoch: 2 / 3, Step: 134 / 750 Loss: 0.4224\n",
      "Epoch: 2 / 3, Step: 135 / 750 Loss: 0.3792\n",
      "Epoch: 2 / 3, Step: 136 / 750 Loss: 0.1953\n",
      "Epoch: 2 / 3, Step: 137 / 750 Loss: 0.4019\n",
      "Epoch: 2 / 3, Step: 138 / 750 Loss: 0.1862\n",
      "Epoch: 2 / 3, Step: 139 / 750 Loss: 0.3755\n",
      "Epoch: 2 / 3, Step: 140 / 750 Loss: 0.2871\n",
      "Epoch: 2 / 3, Step: 141 / 750 Loss: 0.1844\n",
      "Epoch: 2 / 3, Step: 142 / 750 Loss: 0.3697\n",
      "Epoch: 2 / 3, Step: 143 / 750 Loss: 0.6516\n",
      "Epoch: 2 / 3, Step: 144 / 750 Loss: 0.3653\n",
      "Epoch: 2 / 3, Step: 145 / 750 Loss: 0.2262\n",
      "Epoch: 2 / 3, Step: 146 / 750 Loss: 0.1287\n",
      "Epoch: 2 / 3, Step: 147 / 750 Loss: 0.1502\n",
      "Epoch: 2 / 3, Step: 148 / 750 Loss: 0.2654\n",
      "Epoch: 2 / 3, Step: 149 / 750 Loss: 0.1543\n",
      "Epoch: 2 / 3, Step: 150 / 750 Loss: 0.5804\n",
      "Epoch: 2 / 3, Step: 151 / 750 Loss: 0.1056\n",
      "Epoch: 2 / 3, Step: 152 / 750 Loss: 0.5370\n",
      "Epoch: 2 / 3, Step: 153 / 750 Loss: 0.3898\n",
      "Epoch: 2 / 3, Step: 154 / 750 Loss: 0.3768\n",
      "Epoch: 2 / 3, Step: 155 / 750 Loss: 0.2516\n",
      "Epoch: 2 / 3, Step: 156 / 750 Loss: 0.2552\n",
      "Epoch: 2 / 3, Step: 157 / 750 Loss: 0.3856\n",
      "Epoch: 2 / 3, Step: 158 / 750 Loss: 0.2660\n",
      "Epoch: 2 / 3, Step: 159 / 750 Loss: 0.3065\n",
      "Epoch: 2 / 3, Step: 160 / 750 Loss: 0.7216\n",
      "Epoch: 2 / 3, Step: 161 / 750 Loss: 0.3492\n",
      "Epoch: 2 / 3, Step: 162 / 750 Loss: 0.1976\n",
      "Epoch: 2 / 3, Step: 163 / 750 Loss: 0.2555\n",
      "Epoch: 2 / 3, Step: 164 / 750 Loss: 0.4777\n",
      "Epoch: 2 / 3, Step: 165 / 750 Loss: 0.4966\n",
      "Epoch: 2 / 3, Step: 166 / 750 Loss: 0.3998\n",
      "Epoch: 2 / 3, Step: 167 / 750 Loss: 0.2075\n",
      "Epoch: 2 / 3, Step: 168 / 750 Loss: 0.3448\n",
      "Epoch: 2 / 3, Step: 169 / 750 Loss: 0.1790\n",
      "Epoch: 2 / 3, Step: 170 / 750 Loss: 0.4934\n",
      "Epoch: 2 / 3, Step: 171 / 750 Loss: 0.4037\n",
      "Epoch: 2 / 3, Step: 172 / 750 Loss: 0.2801\n",
      "Epoch: 2 / 3, Step: 173 / 750 Loss: 0.5188\n",
      "Epoch: 2 / 3, Step: 174 / 750 Loss: 0.2699\n",
      "Epoch: 2 / 3, Step: 175 / 750 Loss: 0.4154\n",
      "Epoch: 2 / 3, Step: 176 / 750 Loss: 0.3482\n",
      "Epoch: 2 / 3, Step: 177 / 750 Loss: 0.2161\n",
      "Epoch: 2 / 3, Step: 178 / 750 Loss: 0.3325\n",
      "Epoch: 2 / 3, Step: 179 / 750 Loss: 0.3066\n",
      "Epoch: 2 / 3, Step: 180 / 750 Loss: 0.2822\n",
      "Epoch: 2 / 3, Step: 181 / 750 Loss: 0.2945\n",
      "Epoch: 2 / 3, Step: 182 / 750 Loss: 0.2605\n",
      "Epoch: 2 / 3, Step: 183 / 750 Loss: 0.3607\n",
      "Epoch: 2 / 3, Step: 184 / 750 Loss: 0.3038\n",
      "Epoch: 2 / 3, Step: 185 / 750 Loss: 0.4488\n",
      "Epoch: 2 / 3, Step: 186 / 750 Loss: 0.2569\n",
      "Epoch: 2 / 3, Step: 187 / 750 Loss: 0.2027\n",
      "Epoch: 2 / 3, Step: 188 / 750 Loss: 0.2109\n",
      "Epoch: 2 / 3, Step: 189 / 750 Loss: 0.3235\n",
      "Epoch: 2 / 3, Step: 190 / 750 Loss: 0.3784\n",
      "Epoch: 2 / 3, Step: 191 / 750 Loss: 0.0869\n",
      "Epoch: 2 / 3, Step: 192 / 750 Loss: 0.3478\n",
      "Epoch: 2 / 3, Step: 193 / 750 Loss: 0.1924\n",
      "Epoch: 2 / 3, Step: 194 / 750 Loss: 0.2758\n",
      "Epoch: 2 / 3, Step: 195 / 750 Loss: 0.3746\n",
      "Epoch: 2 / 3, Step: 196 / 750 Loss: 0.4517\n",
      "Epoch: 2 / 3, Step: 197 / 750 Loss: 0.3315\n",
      "Epoch: 2 / 3, Step: 198 / 750 Loss: 0.4388\n",
      "Epoch: 2 / 3, Step: 199 / 750 Loss: 0.2015\n",
      "Epoch: 2 / 3, Step: 200 / 750 Loss: 0.2071\n",
      "Epoch: 2 / 3, Step: 201 / 750 Loss: 0.2892\n",
      "Epoch: 2 / 3, Step: 202 / 750 Loss: 0.0972\n",
      "Epoch: 2 / 3, Step: 203 / 750 Loss: 0.3623\n",
      "Epoch: 2 / 3, Step: 204 / 750 Loss: 0.3275\n",
      "Epoch: 2 / 3, Step: 205 / 750 Loss: 0.3379\n",
      "Epoch: 2 / 3, Step: 206 / 750 Loss: 0.3484\n",
      "Epoch: 2 / 3, Step: 207 / 750 Loss: 0.2584\n",
      "Epoch: 2 / 3, Step: 208 / 750 Loss: 0.3493\n",
      "Epoch: 2 / 3, Step: 209 / 750 Loss: 0.4679\n",
      "Epoch: 2 / 3, Step: 210 / 750 Loss: 0.3176\n",
      "Epoch: 2 / 3, Step: 211 / 750 Loss: 0.2969\n",
      "Epoch: 2 / 3, Step: 212 / 750 Loss: 0.5031\n",
      "Epoch: 2 / 3, Step: 213 / 750 Loss: 0.3715\n",
      "Epoch: 2 / 3, Step: 214 / 750 Loss: 0.0641\n",
      "Epoch: 2 / 3, Step: 215 / 750 Loss: 0.5096\n",
      "Epoch: 2 / 3, Step: 216 / 750 Loss: 0.1789\n",
      "Epoch: 2 / 3, Step: 217 / 750 Loss: 0.1220\n",
      "Epoch: 2 / 3, Step: 218 / 750 Loss: 0.3971\n",
      "Epoch: 2 / 3, Step: 219 / 750 Loss: 0.2914\n",
      "Epoch: 2 / 3, Step: 220 / 750 Loss: 0.3224\n",
      "Epoch: 2 / 3, Step: 221 / 750 Loss: 0.3790\n",
      "Epoch: 2 / 3, Step: 222 / 750 Loss: 0.1568\n",
      "Epoch: 2 / 3, Step: 223 / 750 Loss: 0.2038\n",
      "Epoch: 2 / 3, Step: 224 / 750 Loss: 0.4164\n",
      "Epoch: 2 / 3, Step: 225 / 750 Loss: 0.2200\n",
      "Epoch: 2 / 3, Step: 226 / 750 Loss: 0.3077\n",
      "Epoch: 2 / 3, Step: 227 / 750 Loss: 0.3839\n",
      "Epoch: 2 / 3, Step: 228 / 750 Loss: 0.3035\n",
      "Epoch: 2 / 3, Step: 229 / 750 Loss: 0.2230\n",
      "Epoch: 2 / 3, Step: 230 / 750 Loss: 0.1718\n",
      "Epoch: 2 / 3, Step: 231 / 750 Loss: 0.2344\n",
      "Epoch: 2 / 3, Step: 232 / 750 Loss: 0.1423\n",
      "Epoch: 2 / 3, Step: 233 / 750 Loss: 0.2791\n",
      "Epoch: 2 / 3, Step: 234 / 750 Loss: 0.2254\n",
      "Epoch: 2 / 3, Step: 235 / 750 Loss: 0.3465\n",
      "Epoch: 2 / 3, Step: 236 / 750 Loss: 0.4050\n",
      "Epoch: 2 / 3, Step: 237 / 750 Loss: 0.4792\n",
      "Epoch: 2 / 3, Step: 238 / 750 Loss: 0.2494\n",
      "Epoch: 2 / 3, Step: 239 / 750 Loss: 0.2512\n",
      "Epoch: 2 / 3, Step: 240 / 750 Loss: 0.3092\n",
      "Epoch: 2 / 3, Step: 241 / 750 Loss: 0.3174\n",
      "Epoch: 2 / 3, Step: 242 / 750 Loss: 0.3417\n",
      "Epoch: 2 / 3, Step: 243 / 750 Loss: 0.2112\n",
      "Epoch: 2 / 3, Step: 244 / 750 Loss: 0.2804\n",
      "Epoch: 2 / 3, Step: 245 / 750 Loss: 0.3779\n",
      "Epoch: 2 / 3, Step: 246 / 750 Loss: 0.3984\n",
      "Epoch: 2 / 3, Step: 247 / 750 Loss: 0.1846\n",
      "Epoch: 2 / 3, Step: 248 / 750 Loss: 0.1388\n",
      "Epoch: 2 / 3, Step: 249 / 750 Loss: 0.1518\n",
      "Epoch: 2 / 3, Step: 250 / 750 Loss: 0.3609\n",
      "Epoch: 2 / 3, Step: 251 / 750 Loss: 0.1830\n",
      "Epoch: 2 / 3, Step: 252 / 750 Loss: 0.2151\n",
      "Epoch: 2 / 3, Step: 253 / 750 Loss: 0.4078\n",
      "Epoch: 2 / 3, Step: 254 / 750 Loss: 0.6374\n",
      "Epoch: 2 / 3, Step: 255 / 750 Loss: 0.0714\n",
      "Epoch: 2 / 3, Step: 256 / 750 Loss: 0.4300\n",
      "Epoch: 2 / 3, Step: 257 / 750 Loss: 0.3763\n",
      "Epoch: 2 / 3, Step: 258 / 750 Loss: 0.3260\n",
      "Epoch: 2 / 3, Step: 259 / 750 Loss: 0.3154\n",
      "Epoch: 2 / 3, Step: 260 / 750 Loss: 0.3241\n",
      "Epoch: 2 / 3, Step: 261 / 750 Loss: 0.1551\n",
      "Epoch: 2 / 3, Step: 262 / 750 Loss: 0.3251\n",
      "Epoch: 2 / 3, Step: 263 / 750 Loss: 0.2617\n",
      "Epoch: 2 / 3, Step: 264 / 750 Loss: 0.1296\n",
      "Epoch: 2 / 3, Step: 265 / 750 Loss: 0.2972\n",
      "Epoch: 2 / 3, Step: 266 / 750 Loss: 0.3198\n",
      "Epoch: 2 / 3, Step: 267 / 750 Loss: 0.1876\n",
      "Epoch: 2 / 3, Step: 268 / 750 Loss: 0.1660\n",
      "Epoch: 2 / 3, Step: 269 / 750 Loss: 0.2324\n",
      "Epoch: 2 / 3, Step: 270 / 750 Loss: 0.4397\n",
      "Epoch: 2 / 3, Step: 271 / 750 Loss: 0.1216\n",
      "Epoch: 2 / 3, Step: 272 / 750 Loss: 0.3526\n",
      "Epoch: 2 / 3, Step: 273 / 750 Loss: 0.1991\n",
      "Epoch: 2 / 3, Step: 274 / 750 Loss: 0.3517\n",
      "Epoch: 2 / 3, Step: 275 / 750 Loss: 0.3677\n",
      "Epoch: 2 / 3, Step: 276 / 750 Loss: 0.2037\n",
      "Epoch: 2 / 3, Step: 277 / 750 Loss: 0.3076\n",
      "Epoch: 2 / 3, Step: 278 / 750 Loss: 0.2944\n",
      "Epoch: 2 / 3, Step: 279 / 750 Loss: 0.2250\n",
      "Epoch: 2 / 3, Step: 280 / 750 Loss: 0.3224\n",
      "Epoch: 2 / 3, Step: 281 / 750 Loss: 0.4939\n",
      "Epoch: 2 / 3, Step: 282 / 750 Loss: 0.1873\n",
      "Epoch: 2 / 3, Step: 283 / 750 Loss: 0.1745\n",
      "Epoch: 2 / 3, Step: 284 / 750 Loss: 0.1760\n",
      "Epoch: 2 / 3, Step: 285 / 750 Loss: 0.3631\n",
      "Epoch: 2 / 3, Step: 286 / 750 Loss: 0.1277\n",
      "Epoch: 2 / 3, Step: 287 / 750 Loss: 0.1283\n",
      "Epoch: 2 / 3, Step: 288 / 750 Loss: 0.2380\n",
      "Epoch: 2 / 3, Step: 289 / 750 Loss: 0.2368\n",
      "Epoch: 2 / 3, Step: 290 / 750 Loss: 0.0641\n",
      "Epoch: 2 / 3, Step: 291 / 750 Loss: 0.3621\n",
      "Epoch: 2 / 3, Step: 292 / 750 Loss: 0.3066\n",
      "Epoch: 2 / 3, Step: 293 / 750 Loss: 0.2414\n",
      "Epoch: 2 / 3, Step: 294 / 750 Loss: 0.2785\n",
      "Epoch: 2 / 3, Step: 295 / 750 Loss: 0.1407\n",
      "Epoch: 2 / 3, Step: 296 / 750 Loss: 0.3744\n",
      "Epoch: 2 / 3, Step: 297 / 750 Loss: 0.1683\n",
      "Epoch: 2 / 3, Step: 298 / 750 Loss: 0.1056\n",
      "Epoch: 2 / 3, Step: 299 / 750 Loss: 0.4053\n",
      "Epoch: 2 / 3, Step: 300 / 750 Loss: 0.2888\n",
      "Epoch: 2 / 3, Step: 301 / 750 Loss: 0.4878\n",
      "Epoch: 2 / 3, Step: 302 / 750 Loss: 0.2542\n",
      "Epoch: 2 / 3, Step: 303 / 750 Loss: 0.2802\n",
      "Epoch: 2 / 3, Step: 304 / 750 Loss: 0.1865\n",
      "Epoch: 2 / 3, Step: 305 / 750 Loss: 0.2888\n",
      "Epoch: 2 / 3, Step: 306 / 750 Loss: 0.1938\n",
      "Epoch: 2 / 3, Step: 307 / 750 Loss: 0.2450\n",
      "Epoch: 2 / 3, Step: 308 / 750 Loss: 0.3473\n",
      "Epoch: 2 / 3, Step: 309 / 750 Loss: 0.2729\n",
      "Epoch: 2 / 3, Step: 310 / 750 Loss: 0.2411\n",
      "Epoch: 2 / 3, Step: 311 / 750 Loss: 0.2088\n",
      "Epoch: 2 / 3, Step: 312 / 750 Loss: 0.1173\n",
      "Epoch: 2 / 3, Step: 313 / 750 Loss: 0.2266\n",
      "Epoch: 2 / 3, Step: 314 / 750 Loss: 0.1101\n",
      "Epoch: 2 / 3, Step: 315 / 750 Loss: 0.0780\n",
      "Epoch: 2 / 3, Step: 316 / 750 Loss: 0.3217\n",
      "Epoch: 2 / 3, Step: 317 / 750 Loss: 0.2528\n",
      "Epoch: 2 / 3, Step: 318 / 750 Loss: 0.3729\n",
      "Epoch: 2 / 3, Step: 319 / 750 Loss: 0.5839\n",
      "Epoch: 2 / 3, Step: 320 / 750 Loss: 0.2313\n",
      "Epoch: 2 / 3, Step: 321 / 750 Loss: 0.3049\n",
      "Epoch: 2 / 3, Step: 322 / 750 Loss: 0.3073\n",
      "Epoch: 2 / 3, Step: 323 / 750 Loss: 0.3710\n",
      "Epoch: 2 / 3, Step: 324 / 750 Loss: 0.2991\n",
      "Epoch: 2 / 3, Step: 325 / 750 Loss: 0.2102\n",
      "Epoch: 2 / 3, Step: 326 / 750 Loss: 0.1010\n",
      "Epoch: 2 / 3, Step: 327 / 750 Loss: 0.2307\n",
      "Epoch: 2 / 3, Step: 328 / 750 Loss: 0.1305\n",
      "Epoch: 2 / 3, Step: 329 / 750 Loss: 0.2859\n",
      "Epoch: 2 / 3, Step: 330 / 750 Loss: 0.2263\n",
      "Epoch: 2 / 3, Step: 331 / 750 Loss: 0.3608\n",
      "Epoch: 2 / 3, Step: 332 / 750 Loss: 0.2475\n",
      "Epoch: 2 / 3, Step: 333 / 750 Loss: 0.3455\n",
      "Epoch: 2 / 3, Step: 334 / 750 Loss: 0.3535\n",
      "Epoch: 2 / 3, Step: 335 / 750 Loss: 0.0837\n",
      "Epoch: 2 / 3, Step: 336 / 750 Loss: 0.2482\n",
      "Epoch: 2 / 3, Step: 337 / 750 Loss: 0.3891\n",
      "Epoch: 2 / 3, Step: 338 / 750 Loss: 0.2128\n",
      "Epoch: 2 / 3, Step: 339 / 750 Loss: 0.1439\n",
      "Epoch: 2 / 3, Step: 340 / 750 Loss: 0.2591\n",
      "Epoch: 2 / 3, Step: 341 / 750 Loss: 0.3354\n",
      "Epoch: 2 / 3, Step: 342 / 750 Loss: 0.1608\n",
      "Epoch: 2 / 3, Step: 343 / 750 Loss: 0.2411\n",
      "Epoch: 2 / 3, Step: 344 / 750 Loss: 0.2929\n",
      "Epoch: 2 / 3, Step: 345 / 750 Loss: 0.2191\n",
      "Epoch: 2 / 3, Step: 346 / 750 Loss: 0.2063\n",
      "Epoch: 2 / 3, Step: 347 / 750 Loss: 0.1566\n",
      "Epoch: 2 / 3, Step: 348 / 750 Loss: 0.3182\n",
      "Epoch: 2 / 3, Step: 349 / 750 Loss: 0.3616\n",
      "Epoch: 2 / 3, Step: 350 / 750 Loss: 0.1799\n",
      "Epoch: 2 / 3, Step: 351 / 750 Loss: 0.2127\n",
      "Epoch: 2 / 3, Step: 352 / 750 Loss: 0.3682\n",
      "Epoch: 2 / 3, Step: 353 / 750 Loss: 0.3603\n",
      "Epoch: 2 / 3, Step: 354 / 750 Loss: 0.0960\n",
      "Epoch: 2 / 3, Step: 355 / 750 Loss: 0.4052\n",
      "Epoch: 2 / 3, Step: 356 / 750 Loss: 0.2810\n",
      "Epoch: 2 / 3, Step: 357 / 750 Loss: 0.1501\n",
      "Epoch: 2 / 3, Step: 358 / 750 Loss: 0.2266\n",
      "Epoch: 2 / 3, Step: 359 / 750 Loss: 0.3896\n",
      "Epoch: 2 / 3, Step: 360 / 750 Loss: 0.4198\n",
      "Epoch: 2 / 3, Step: 361 / 750 Loss: 0.3436\n",
      "Epoch: 2 / 3, Step: 362 / 750 Loss: 0.1511\n",
      "Epoch: 2 / 3, Step: 363 / 750 Loss: 0.3967\n",
      "Epoch: 2 / 3, Step: 364 / 750 Loss: 0.1234\n",
      "Epoch: 2 / 3, Step: 365 / 750 Loss: 0.2651\n",
      "Epoch: 2 / 3, Step: 366 / 750 Loss: 0.1834\n",
      "Epoch: 2 / 3, Step: 367 / 750 Loss: 0.3947\n",
      "Epoch: 2 / 3, Step: 368 / 750 Loss: 0.3274\n",
      "Epoch: 2 / 3, Step: 369 / 750 Loss: 0.1302\n",
      "Epoch: 2 / 3, Step: 370 / 750 Loss: 0.2072\n",
      "Epoch: 2 / 3, Step: 371 / 750 Loss: 0.3943\n",
      "Epoch: 2 / 3, Step: 372 / 750 Loss: 0.1260\n",
      "Epoch: 2 / 3, Step: 373 / 750 Loss: 0.1156\n",
      "Epoch: 2 / 3, Step: 374 / 750 Loss: 0.1542\n",
      "Epoch: 2 / 3, Step: 375 / 750 Loss: 0.5139\n",
      "Epoch: 2 / 3, Step: 376 / 750 Loss: 0.1763\n",
      "Epoch: 2 / 3, Step: 377 / 750 Loss: 0.2697\n",
      "Epoch: 2 / 3, Step: 378 / 750 Loss: 0.3897\n",
      "Epoch: 2 / 3, Step: 379 / 750 Loss: 0.3735\n",
      "Epoch: 2 / 3, Step: 380 / 750 Loss: 0.2752\n",
      "Epoch: 2 / 3, Step: 381 / 750 Loss: 0.1336\n",
      "Epoch: 2 / 3, Step: 382 / 750 Loss: 0.2269\n",
      "Epoch: 2 / 3, Step: 383 / 750 Loss: 0.1085\n",
      "Epoch: 2 / 3, Step: 384 / 750 Loss: 0.2711\n",
      "Epoch: 2 / 3, Step: 385 / 750 Loss: 0.3848\n",
      "Epoch: 2 / 3, Step: 386 / 750 Loss: 0.3098\n",
      "Epoch: 2 / 3, Step: 387 / 750 Loss: 0.2733\n",
      "Epoch: 2 / 3, Step: 388 / 750 Loss: 0.3729\n",
      "Epoch: 2 / 3, Step: 389 / 750 Loss: 0.2401\n",
      "Epoch: 2 / 3, Step: 390 / 750 Loss: 0.1083\n",
      "Epoch: 2 / 3, Step: 391 / 750 Loss: 0.1886\n",
      "Epoch: 2 / 3, Step: 392 / 750 Loss: 0.1920\n",
      "Epoch: 2 / 3, Step: 393 / 750 Loss: 0.2193\n",
      "Epoch: 2 / 3, Step: 394 / 750 Loss: 0.3832\n",
      "Epoch: 2 / 3, Step: 395 / 750 Loss: 0.1802\n",
      "Epoch: 2 / 3, Step: 396 / 750 Loss: 0.1908\n",
      "Epoch: 2 / 3, Step: 397 / 750 Loss: 0.2250\n",
      "Epoch: 2 / 3, Step: 398 / 750 Loss: 0.3955\n",
      "Epoch: 2 / 3, Step: 399 / 750 Loss: 0.2627\n",
      "Epoch: 2 / 3, Step: 400 / 750 Loss: 0.2336\n",
      "Epoch: 2 / 3, Step: 401 / 750 Loss: 0.1619\n",
      "Epoch: 2 / 3, Step: 402 / 750 Loss: 0.2854\n",
      "Epoch: 2 / 3, Step: 403 / 750 Loss: 0.5260\n",
      "Epoch: 2 / 3, Step: 404 / 750 Loss: 0.3280\n",
      "Epoch: 2 / 3, Step: 405 / 750 Loss: 0.3161\n",
      "Epoch: 2 / 3, Step: 406 / 750 Loss: 0.3148\n",
      "Epoch: 2 / 3, Step: 407 / 750 Loss: 0.2394\n",
      "Epoch: 2 / 3, Step: 408 / 750 Loss: 0.3844\n",
      "Epoch: 2 / 3, Step: 409 / 750 Loss: 0.2708\n",
      "Epoch: 2 / 3, Step: 410 / 750 Loss: 0.4890\n",
      "Epoch: 2 / 3, Step: 411 / 750 Loss: 0.1530\n",
      "Epoch: 2 / 3, Step: 412 / 750 Loss: 0.1821\n",
      "Epoch: 2 / 3, Step: 413 / 750 Loss: 0.1487\n",
      "Epoch: 2 / 3, Step: 414 / 750 Loss: 0.2613\n",
      "Epoch: 2 / 3, Step: 415 / 750 Loss: 0.2433\n",
      "Epoch: 2 / 3, Step: 416 / 750 Loss: 0.1656\n",
      "Epoch: 2 / 3, Step: 417 / 750 Loss: 0.1358\n",
      "Epoch: 2 / 3, Step: 418 / 750 Loss: 0.5543\n",
      "Epoch: 2 / 3, Step: 419 / 750 Loss: 0.4118\n",
      "Epoch: 2 / 3, Step: 420 / 750 Loss: 0.3165\n",
      "Epoch: 2 / 3, Step: 421 / 750 Loss: 0.3162\n",
      "Epoch: 2 / 3, Step: 422 / 750 Loss: 0.1476\n",
      "Epoch: 2 / 3, Step: 423 / 750 Loss: 0.3761\n",
      "Epoch: 2 / 3, Step: 424 / 750 Loss: 0.1653\n",
      "Epoch: 2 / 3, Step: 425 / 750 Loss: 0.2465\n",
      "Epoch: 2 / 3, Step: 426 / 750 Loss: 0.4050\n",
      "Epoch: 2 / 3, Step: 427 / 750 Loss: 0.1470\n",
      "Epoch: 2 / 3, Step: 428 / 750 Loss: 0.0713\n",
      "Epoch: 2 / 3, Step: 429 / 750 Loss: 0.1733\n",
      "Epoch: 2 / 3, Step: 430 / 750 Loss: 0.6025\n",
      "Epoch: 2 / 3, Step: 431 / 750 Loss: 0.2908\n",
      "Epoch: 2 / 3, Step: 432 / 750 Loss: 0.1777\n",
      "Epoch: 2 / 3, Step: 433 / 750 Loss: 0.0638\n",
      "Epoch: 2 / 3, Step: 434 / 750 Loss: 0.2996\n",
      "Epoch: 2 / 3, Step: 435 / 750 Loss: 0.3524\n",
      "Epoch: 2 / 3, Step: 436 / 750 Loss: 0.3250\n",
      "Epoch: 2 / 3, Step: 437 / 750 Loss: 0.2959\n",
      "Epoch: 2 / 3, Step: 438 / 750 Loss: 0.2058\n",
      "Epoch: 2 / 3, Step: 439 / 750 Loss: 0.1459\n",
      "Epoch: 2 / 3, Step: 440 / 750 Loss: 0.1099\n",
      "Epoch: 2 / 3, Step: 441 / 750 Loss: 0.2928\n",
      "Epoch: 2 / 3, Step: 442 / 750 Loss: 0.2484\n",
      "Epoch: 2 / 3, Step: 443 / 750 Loss: 0.0876\n",
      "Epoch: 2 / 3, Step: 444 / 750 Loss: 0.1873\n",
      "Epoch: 2 / 3, Step: 445 / 750 Loss: 0.0609\n",
      "Epoch: 2 / 3, Step: 446 / 750 Loss: 0.2862\n",
      "Epoch: 2 / 3, Step: 447 / 750 Loss: 0.1589\n",
      "Epoch: 2 / 3, Step: 448 / 750 Loss: 0.1542\n",
      "Epoch: 2 / 3, Step: 449 / 750 Loss: 0.2783\n",
      "Epoch: 2 / 3, Step: 450 / 750 Loss: 0.1984\n",
      "Epoch: 2 / 3, Step: 451 / 750 Loss: 0.0838\n",
      "Epoch: 2 / 3, Step: 452 / 750 Loss: 0.2211\n",
      "Epoch: 2 / 3, Step: 453 / 750 Loss: 0.0991\n",
      "Epoch: 2 / 3, Step: 454 / 750 Loss: 0.2504\n",
      "Epoch: 2 / 3, Step: 455 / 750 Loss: 0.5642\n",
      "Epoch: 2 / 3, Step: 456 / 750 Loss: 0.1637\n",
      "Epoch: 2 / 3, Step: 457 / 750 Loss: 0.3559\n",
      "Epoch: 2 / 3, Step: 458 / 750 Loss: 0.1927\n",
      "Epoch: 2 / 3, Step: 459 / 750 Loss: 0.1167\n",
      "Epoch: 2 / 3, Step: 460 / 750 Loss: 0.2599\n",
      "Epoch: 2 / 3, Step: 461 / 750 Loss: 0.1929\n",
      "Epoch: 2 / 3, Step: 462 / 750 Loss: 0.2611\n",
      "Epoch: 2 / 3, Step: 463 / 750 Loss: 0.1876\n",
      "Epoch: 2 / 3, Step: 464 / 750 Loss: 0.2101\n",
      "Epoch: 2 / 3, Step: 465 / 750 Loss: 0.1588\n",
      "Epoch: 2 / 3, Step: 466 / 750 Loss: 0.0934\n",
      "Epoch: 2 / 3, Step: 467 / 750 Loss: 0.3459\n",
      "Epoch: 2 / 3, Step: 468 / 750 Loss: 0.2255\n",
      "Epoch: 2 / 3, Step: 469 / 750 Loss: 0.2557\n",
      "Epoch: 2 / 3, Step: 470 / 750 Loss: 0.2064\n",
      "Epoch: 2 / 3, Step: 471 / 750 Loss: 0.3164\n",
      "Epoch: 2 / 3, Step: 472 / 750 Loss: 0.3363\n",
      "Epoch: 2 / 3, Step: 473 / 750 Loss: 0.2560\n",
      "Epoch: 2 / 3, Step: 474 / 750 Loss: 0.2545\n",
      "Epoch: 2 / 3, Step: 475 / 750 Loss: 0.1133\n",
      "Epoch: 2 / 3, Step: 476 / 750 Loss: 0.2625\n",
      "Epoch: 2 / 3, Step: 477 / 750 Loss: 0.1112\n",
      "Epoch: 2 / 3, Step: 478 / 750 Loss: 0.2480\n",
      "Epoch: 2 / 3, Step: 479 / 750 Loss: 0.0872\n",
      "Epoch: 2 / 3, Step: 480 / 750 Loss: 0.1388\n",
      "Epoch: 2 / 3, Step: 481 / 750 Loss: 0.1364\n",
      "Epoch: 2 / 3, Step: 482 / 750 Loss: 0.5227\n",
      "Epoch: 2 / 3, Step: 483 / 750 Loss: 0.2096\n",
      "Epoch: 2 / 3, Step: 484 / 750 Loss: 0.1110\n",
      "Epoch: 2 / 3, Step: 485 / 750 Loss: 0.1183\n",
      "Epoch: 2 / 3, Step: 486 / 750 Loss: 0.4834\n",
      "Epoch: 2 / 3, Step: 487 / 750 Loss: 0.2657\n",
      "Epoch: 2 / 3, Step: 488 / 750 Loss: 0.1422\n",
      "Epoch: 2 / 3, Step: 489 / 750 Loss: 0.3874\n",
      "Epoch: 2 / 3, Step: 490 / 750 Loss: 0.1590\n",
      "Epoch: 2 / 3, Step: 491 / 750 Loss: 0.3075\n",
      "Epoch: 2 / 3, Step: 492 / 750 Loss: 0.1492\n",
      "Epoch: 2 / 3, Step: 493 / 750 Loss: 0.6212\n",
      "Epoch: 2 / 3, Step: 494 / 750 Loss: 0.1892\n",
      "Epoch: 2 / 3, Step: 495 / 750 Loss: 0.3322\n",
      "Epoch: 2 / 3, Step: 496 / 750 Loss: 0.2138\n",
      "Epoch: 2 / 3, Step: 497 / 750 Loss: 0.4612\n",
      "Epoch: 2 / 3, Step: 498 / 750 Loss: 0.3023\n",
      "Epoch: 2 / 3, Step: 499 / 750 Loss: 0.2576\n",
      "Epoch: 2 / 3, Step: 500 / 750 Loss: 0.1382\n",
      "Epoch: 2 / 3, Step: 501 / 750 Loss: 0.2727\n",
      "Epoch: 2 / 3, Step: 502 / 750 Loss: 0.1416\n",
      "Epoch: 2 / 3, Step: 503 / 750 Loss: 0.3586\n",
      "Epoch: 2 / 3, Step: 504 / 750 Loss: 0.2310\n",
      "Epoch: 2 / 3, Step: 505 / 750 Loss: 0.1657\n",
      "Epoch: 2 / 3, Step: 506 / 750 Loss: 0.1327\n",
      "Epoch: 2 / 3, Step: 507 / 750 Loss: 0.2436\n",
      "Epoch: 2 / 3, Step: 508 / 750 Loss: 0.4963\n",
      "Epoch: 2 / 3, Step: 509 / 750 Loss: 0.4293\n",
      "Epoch: 2 / 3, Step: 510 / 750 Loss: 0.1464\n",
      "Epoch: 2 / 3, Step: 511 / 750 Loss: 0.3998\n",
      "Epoch: 2 / 3, Step: 512 / 750 Loss: 0.1313\n",
      "Epoch: 2 / 3, Step: 513 / 750 Loss: 0.1548\n",
      "Epoch: 2 / 3, Step: 514 / 750 Loss: 0.3221\n",
      "Epoch: 2 / 3, Step: 515 / 750 Loss: 0.2682\n",
      "Epoch: 2 / 3, Step: 516 / 750 Loss: 0.1480\n",
      "Epoch: 2 / 3, Step: 517 / 750 Loss: 0.0983\n",
      "Epoch: 2 / 3, Step: 518 / 750 Loss: 0.1124\n",
      "Epoch: 2 / 3, Step: 519 / 750 Loss: 0.3026\n",
      "Epoch: 2 / 3, Step: 520 / 750 Loss: 0.3416\n",
      "Epoch: 2 / 3, Step: 521 / 750 Loss: 0.2260\n",
      "Epoch: 2 / 3, Step: 522 / 750 Loss: 0.2064\n",
      "Epoch: 2 / 3, Step: 523 / 750 Loss: 0.2653\n",
      "Epoch: 2 / 3, Step: 524 / 750 Loss: 0.2589\n",
      "Epoch: 2 / 3, Step: 525 / 750 Loss: 0.2195\n",
      "Epoch: 2 / 3, Step: 526 / 750 Loss: 0.1448\n",
      "Epoch: 2 / 3, Step: 527 / 750 Loss: 0.1073\n",
      "Epoch: 2 / 3, Step: 528 / 750 Loss: 0.1852\n",
      "Epoch: 2 / 3, Step: 529 / 750 Loss: 0.3821\n",
      "Epoch: 2 / 3, Step: 530 / 750 Loss: 0.1403\n",
      "Epoch: 2 / 3, Step: 531 / 750 Loss: 0.3844\n",
      "Epoch: 2 / 3, Step: 532 / 750 Loss: 0.1445\n",
      "Epoch: 2 / 3, Step: 533 / 750 Loss: 0.2151\n",
      "Epoch: 2 / 3, Step: 534 / 750 Loss: 0.0837\n",
      "Epoch: 2 / 3, Step: 535 / 750 Loss: 0.4127\n",
      "Epoch: 2 / 3, Step: 536 / 750 Loss: 0.3851\n",
      "Epoch: 2 / 3, Step: 537 / 750 Loss: 0.3715\n",
      "Epoch: 2 / 3, Step: 538 / 750 Loss: 0.1736\n",
      "Epoch: 2 / 3, Step: 539 / 750 Loss: 0.3896\n",
      "Epoch: 2 / 3, Step: 540 / 750 Loss: 0.1423\n",
      "Epoch: 2 / 3, Step: 541 / 750 Loss: 0.3714\n",
      "Epoch: 2 / 3, Step: 542 / 750 Loss: 0.3720\n",
      "Epoch: 2 / 3, Step: 543 / 750 Loss: 0.1870\n",
      "Epoch: 2 / 3, Step: 544 / 750 Loss: 0.1806\n",
      "Epoch: 2 / 3, Step: 545 / 750 Loss: 0.4425\n",
      "Epoch: 2 / 3, Step: 546 / 750 Loss: 0.1469\n",
      "Epoch: 2 / 3, Step: 547 / 750 Loss: 0.2329\n",
      "Epoch: 2 / 3, Step: 548 / 750 Loss: 0.1994\n",
      "Epoch: 2 / 3, Step: 549 / 750 Loss: 0.0905\n",
      "Epoch: 2 / 3, Step: 550 / 750 Loss: 0.2010\n",
      "Epoch: 2 / 3, Step: 551 / 750 Loss: 0.3016\n",
      "Epoch: 2 / 3, Step: 552 / 750 Loss: 0.1450\n",
      "Epoch: 2 / 3, Step: 553 / 750 Loss: 0.2564\n",
      "Epoch: 2 / 3, Step: 554 / 750 Loss: 0.2003\n",
      "Epoch: 2 / 3, Step: 555 / 750 Loss: 0.2520\n",
      "Epoch: 2 / 3, Step: 556 / 750 Loss: 0.2470\n",
      "Epoch: 2 / 3, Step: 557 / 750 Loss: 0.3510\n",
      "Epoch: 2 / 3, Step: 558 / 750 Loss: 0.1509\n",
      "Epoch: 2 / 3, Step: 559 / 750 Loss: 0.0752\n",
      "Epoch: 2 / 3, Step: 560 / 750 Loss: 0.1407\n",
      "Epoch: 2 / 3, Step: 561 / 750 Loss: 0.3009\n",
      "Epoch: 2 / 3, Step: 562 / 750 Loss: 0.1459\n",
      "Epoch: 2 / 3, Step: 563 / 750 Loss: 0.0848\n",
      "Epoch: 2 / 3, Step: 564 / 750 Loss: 0.1878\n",
      "Epoch: 2 / 3, Step: 565 / 750 Loss: 0.1635\n",
      "Epoch: 2 / 3, Step: 566 / 750 Loss: 0.2067\n",
      "Epoch: 2 / 3, Step: 567 / 750 Loss: 0.1128\n",
      "Epoch: 2 / 3, Step: 568 / 750 Loss: 0.2737\n",
      "Epoch: 2 / 3, Step: 569 / 750 Loss: 0.3181\n",
      "Epoch: 2 / 3, Step: 570 / 750 Loss: 0.0644\n",
      "Epoch: 2 / 3, Step: 571 / 750 Loss: 0.1862\n",
      "Epoch: 2 / 3, Step: 572 / 750 Loss: 0.3809\n",
      "Epoch: 2 / 3, Step: 573 / 750 Loss: 0.3355\n",
      "Epoch: 2 / 3, Step: 574 / 750 Loss: 0.0998\n",
      "Epoch: 2 / 3, Step: 575 / 750 Loss: 0.4308\n",
      "Epoch: 2 / 3, Step: 576 / 750 Loss: 0.1360\n",
      "Epoch: 2 / 3, Step: 577 / 750 Loss: 0.2502\n",
      "Epoch: 2 / 3, Step: 578 / 750 Loss: 0.4493\n",
      "Epoch: 2 / 3, Step: 579 / 750 Loss: 0.2953\n",
      "Epoch: 2 / 3, Step: 580 / 750 Loss: 0.1848\n",
      "Epoch: 2 / 3, Step: 581 / 750 Loss: 0.2250\n",
      "Epoch: 2 / 3, Step: 582 / 750 Loss: 0.0955\n",
      "Epoch: 2 / 3, Step: 583 / 750 Loss: 0.2259\n",
      "Epoch: 2 / 3, Step: 584 / 750 Loss: 0.1880\n",
      "Epoch: 2 / 3, Step: 585 / 750 Loss: 0.2302\n",
      "Epoch: 2 / 3, Step: 586 / 750 Loss: 0.1845\n",
      "Epoch: 2 / 3, Step: 587 / 750 Loss: 0.2948\n",
      "Epoch: 2 / 3, Step: 588 / 750 Loss: 0.1471\n",
      "Epoch: 2 / 3, Step: 589 / 750 Loss: 0.1600\n",
      "Epoch: 2 / 3, Step: 590 / 750 Loss: 0.2798\n",
      "Epoch: 2 / 3, Step: 591 / 750 Loss: 0.1036\n",
      "Epoch: 2 / 3, Step: 592 / 750 Loss: 0.5057\n",
      "Epoch: 2 / 3, Step: 593 / 750 Loss: 0.1393\n",
      "Epoch: 2 / 3, Step: 594 / 750 Loss: 0.1521\n",
      "Epoch: 2 / 3, Step: 595 / 750 Loss: 0.0894\n",
      "Epoch: 2 / 3, Step: 596 / 750 Loss: 0.0694\n",
      "Epoch: 2 / 3, Step: 597 / 750 Loss: 0.3650\n",
      "Epoch: 2 / 3, Step: 598 / 750 Loss: 0.4339\n",
      "Epoch: 2 / 3, Step: 599 / 750 Loss: 0.2217\n",
      "Epoch: 2 / 3, Step: 600 / 750 Loss: 0.1643\n",
      "Epoch: 2 / 3, Step: 601 / 750 Loss: 0.3017\n",
      "Epoch: 2 / 3, Step: 602 / 750 Loss: 0.2671\n",
      "Epoch: 2 / 3, Step: 603 / 750 Loss: 0.3923\n",
      "Epoch: 2 / 3, Step: 604 / 750 Loss: 0.4979\n",
      "Epoch: 2 / 3, Step: 605 / 750 Loss: 0.1299\n",
      "Epoch: 2 / 3, Step: 606 / 750 Loss: 0.2217\n",
      "Epoch: 2 / 3, Step: 607 / 750 Loss: 0.1733\n",
      "Epoch: 2 / 3, Step: 608 / 750 Loss: 0.3061\n",
      "Epoch: 2 / 3, Step: 609 / 750 Loss: 0.1108\n",
      "Epoch: 2 / 3, Step: 610 / 750 Loss: 0.2609\n",
      "Epoch: 2 / 3, Step: 611 / 750 Loss: 0.1236\n",
      "Epoch: 2 / 3, Step: 612 / 750 Loss: 0.0957\n",
      "Epoch: 2 / 3, Step: 613 / 750 Loss: 0.2822\n",
      "Epoch: 2 / 3, Step: 614 / 750 Loss: 0.2038\n",
      "Epoch: 2 / 3, Step: 615 / 750 Loss: 0.1617\n",
      "Epoch: 2 / 3, Step: 616 / 750 Loss: 0.1726\n",
      "Epoch: 2 / 3, Step: 617 / 750 Loss: 0.1516\n",
      "Epoch: 2 / 3, Step: 618 / 750 Loss: 0.4829\n",
      "Epoch: 2 / 3, Step: 619 / 750 Loss: 0.2686\n",
      "Epoch: 2 / 3, Step: 620 / 750 Loss: 0.2132\n",
      "Epoch: 2 / 3, Step: 621 / 750 Loss: 0.0739\n",
      "Epoch: 2 / 3, Step: 622 / 750 Loss: 0.1953\n",
      "Epoch: 2 / 3, Step: 623 / 750 Loss: 0.2837\n",
      "Epoch: 2 / 3, Step: 624 / 750 Loss: 0.2032\n",
      "Epoch: 2 / 3, Step: 625 / 750 Loss: 0.2008\n",
      "Epoch: 2 / 3, Step: 626 / 750 Loss: 0.4223\n",
      "Epoch: 2 / 3, Step: 627 / 750 Loss: 0.4030\n",
      "Epoch: 2 / 3, Step: 628 / 750 Loss: 0.2069\n",
      "Epoch: 2 / 3, Step: 629 / 750 Loss: 0.3751\n",
      "Epoch: 2 / 3, Step: 630 / 750 Loss: 0.2250\n",
      "Epoch: 2 / 3, Step: 631 / 750 Loss: 0.0868\n",
      "Epoch: 2 / 3, Step: 632 / 750 Loss: 0.1859\n",
      "Epoch: 2 / 3, Step: 633 / 750 Loss: 0.1290\n",
      "Epoch: 2 / 3, Step: 634 / 750 Loss: 0.1007\n",
      "Epoch: 2 / 3, Step: 635 / 750 Loss: 0.2054\n",
      "Epoch: 2 / 3, Step: 636 / 750 Loss: 0.2992\n",
      "Epoch: 2 / 3, Step: 637 / 750 Loss: 0.0708\n",
      "Epoch: 2 / 3, Step: 638 / 750 Loss: 0.7571\n",
      "Epoch: 2 / 3, Step: 639 / 750 Loss: 0.1141\n",
      "Epoch: 2 / 3, Step: 640 / 750 Loss: 0.0788\n",
      "Epoch: 2 / 3, Step: 641 / 750 Loss: 0.1295\n",
      "Epoch: 2 / 3, Step: 642 / 750 Loss: 0.2494\n",
      "Epoch: 2 / 3, Step: 643 / 750 Loss: 0.1064\n",
      "Epoch: 2 / 3, Step: 644 / 750 Loss: 0.5168\n",
      "Epoch: 2 / 3, Step: 645 / 750 Loss: 0.5688\n",
      "Epoch: 2 / 3, Step: 646 / 750 Loss: 0.2184\n",
      "Epoch: 2 / 3, Step: 647 / 750 Loss: 0.2231\n",
      "Epoch: 2 / 3, Step: 648 / 750 Loss: 0.0900\n",
      "Epoch: 2 / 3, Step: 649 / 750 Loss: 0.1039\n",
      "Epoch: 2 / 3, Step: 650 / 750 Loss: 0.3619\n",
      "Epoch: 2 / 3, Step: 651 / 750 Loss: 0.2109\n",
      "Epoch: 2 / 3, Step: 652 / 750 Loss: 0.1695\n",
      "Epoch: 2 / 3, Step: 653 / 750 Loss: 0.4969\n",
      "Epoch: 2 / 3, Step: 654 / 750 Loss: 0.1123\n",
      "Epoch: 2 / 3, Step: 655 / 750 Loss: 0.1647\n",
      "Epoch: 2 / 3, Step: 656 / 750 Loss: 0.1599\n",
      "Epoch: 2 / 3, Step: 657 / 750 Loss: 0.2545\n",
      "Epoch: 2 / 3, Step: 658 / 750 Loss: 0.1264\n",
      "Epoch: 2 / 3, Step: 659 / 750 Loss: 0.1707\n",
      "Epoch: 2 / 3, Step: 660 / 750 Loss: 0.1572\n",
      "Epoch: 2 / 3, Step: 661 / 750 Loss: 0.2458\n",
      "Epoch: 2 / 3, Step: 662 / 750 Loss: 0.1953\n",
      "Epoch: 2 / 3, Step: 663 / 750 Loss: 0.1610\n",
      "Epoch: 2 / 3, Step: 664 / 750 Loss: 0.1655\n",
      "Epoch: 2 / 3, Step: 665 / 750 Loss: 0.1307\n",
      "Epoch: 2 / 3, Step: 666 / 750 Loss: 0.5556\n",
      "Epoch: 2 / 3, Step: 667 / 750 Loss: 0.1400\n",
      "Epoch: 2 / 3, Step: 668 / 750 Loss: 0.1812\n",
      "Epoch: 2 / 3, Step: 669 / 750 Loss: 0.2449\n",
      "Epoch: 2 / 3, Step: 670 / 750 Loss: 0.1329\n",
      "Epoch: 2 / 3, Step: 671 / 750 Loss: 0.1015\n",
      "Epoch: 2 / 3, Step: 672 / 750 Loss: 0.1484\n",
      "Epoch: 2 / 3, Step: 673 / 750 Loss: 0.1236\n",
      "Epoch: 2 / 3, Step: 674 / 750 Loss: 0.2718\n",
      "Epoch: 2 / 3, Step: 675 / 750 Loss: 0.2780\n",
      "Epoch: 2 / 3, Step: 676 / 750 Loss: 0.4597\n",
      "Epoch: 2 / 3, Step: 677 / 750 Loss: 0.0667\n",
      "Epoch: 2 / 3, Step: 678 / 750 Loss: 0.2943\n",
      "Epoch: 2 / 3, Step: 679 / 750 Loss: 0.2397\n",
      "Epoch: 2 / 3, Step: 680 / 750 Loss: 0.3518\n",
      "Epoch: 2 / 3, Step: 681 / 750 Loss: 0.3006\n",
      "Epoch: 2 / 3, Step: 682 / 750 Loss: 0.1330\n",
      "Epoch: 2 / 3, Step: 683 / 750 Loss: 0.3003\n",
      "Epoch: 2 / 3, Step: 684 / 750 Loss: 0.2561\n",
      "Epoch: 2 / 3, Step: 685 / 750 Loss: 0.3195\n",
      "Epoch: 2 / 3, Step: 686 / 750 Loss: 0.1721\n",
      "Epoch: 2 / 3, Step: 687 / 750 Loss: 0.1149\n",
      "Epoch: 2 / 3, Step: 688 / 750 Loss: 0.2387\n",
      "Epoch: 2 / 3, Step: 689 / 750 Loss: 0.1567\n",
      "Epoch: 2 / 3, Step: 690 / 750 Loss: 0.3766\n",
      "Epoch: 2 / 3, Step: 691 / 750 Loss: 0.1780\n",
      "Epoch: 2 / 3, Step: 692 / 750 Loss: 0.2171\n",
      "Epoch: 2 / 3, Step: 693 / 750 Loss: 0.1957\n",
      "Epoch: 2 / 3, Step: 694 / 750 Loss: 0.1975\n",
      "Epoch: 2 / 3, Step: 695 / 750 Loss: 0.4780\n",
      "Epoch: 2 / 3, Step: 696 / 750 Loss: 0.2988\n",
      "Epoch: 2 / 3, Step: 697 / 750 Loss: 0.1528\n",
      "Epoch: 2 / 3, Step: 698 / 750 Loss: 0.3651\n",
      "Epoch: 2 / 3, Step: 699 / 750 Loss: 0.1185\n",
      "Epoch: 2 / 3, Step: 700 / 750 Loss: 0.1314\n",
      "Epoch: 2 / 3, Step: 701 / 750 Loss: 0.0679\n",
      "Epoch: 2 / 3, Step: 702 / 750 Loss: 0.1410\n",
      "Epoch: 2 / 3, Step: 703 / 750 Loss: 0.1625\n",
      "Epoch: 2 / 3, Step: 704 / 750 Loss: 0.1608\n",
      "Epoch: 2 / 3, Step: 705 / 750 Loss: 0.4074\n",
      "Epoch: 2 / 3, Step: 706 / 750 Loss: 0.3735\n",
      "Epoch: 2 / 3, Step: 707 / 750 Loss: 0.2065\n",
      "Epoch: 2 / 3, Step: 708 / 750 Loss: 0.1494\n",
      "Epoch: 2 / 3, Step: 709 / 750 Loss: 0.1236\n",
      "Epoch: 2 / 3, Step: 710 / 750 Loss: 0.4156\n",
      "Epoch: 2 / 3, Step: 711 / 750 Loss: 0.2009\n",
      "Epoch: 2 / 3, Step: 712 / 750 Loss: 0.0820\n",
      "Epoch: 2 / 3, Step: 713 / 750 Loss: 0.0767\n",
      "Epoch: 2 / 3, Step: 714 / 750 Loss: 0.0835\n",
      "Epoch: 2 / 3, Step: 715 / 750 Loss: 0.2797\n",
      "Epoch: 2 / 3, Step: 716 / 750 Loss: 0.2228\n",
      "Epoch: 2 / 3, Step: 717 / 750 Loss: 0.1708\n",
      "Epoch: 2 / 3, Step: 718 / 750 Loss: 0.0631\n",
      "Epoch: 2 / 3, Step: 719 / 750 Loss: 0.3057\n",
      "Epoch: 2 / 3, Step: 720 / 750 Loss: 0.1717\n",
      "Epoch: 2 / 3, Step: 721 / 750 Loss: 0.1117\n",
      "Epoch: 2 / 3, Step: 722 / 750 Loss: 0.1024\n",
      "Epoch: 2 / 3, Step: 723 / 750 Loss: 0.0923\n",
      "Epoch: 2 / 3, Step: 724 / 750 Loss: 0.0757\n",
      "Epoch: 2 / 3, Step: 725 / 750 Loss: 0.4742\n",
      "Epoch: 2 / 3, Step: 726 / 750 Loss: 0.2485\n",
      "Epoch: 2 / 3, Step: 727 / 750 Loss: 0.0449\n",
      "Epoch: 2 / 3, Step: 728 / 750 Loss: 0.1006\n",
      "Epoch: 2 / 3, Step: 729 / 750 Loss: 0.2274\n",
      "Epoch: 2 / 3, Step: 730 / 750 Loss: 0.2628\n",
      "Epoch: 2 / 3, Step: 731 / 750 Loss: 0.0489\n",
      "Epoch: 2 / 3, Step: 732 / 750 Loss: 0.2659\n",
      "Epoch: 2 / 3, Step: 733 / 750 Loss: 0.1626\n",
      "Epoch: 2 / 3, Step: 734 / 750 Loss: 0.1479\n",
      "Epoch: 2 / 3, Step: 735 / 750 Loss: 0.3609\n",
      "Epoch: 2 / 3, Step: 736 / 750 Loss: 0.1835\n",
      "Epoch: 2 / 3, Step: 737 / 750 Loss: 0.1771\n",
      "Epoch: 2 / 3, Step: 738 / 750 Loss: 0.2515\n",
      "Epoch: 2 / 3, Step: 739 / 750 Loss: 0.2196\n",
      "Epoch: 2 / 3, Step: 740 / 750 Loss: 0.1206\n",
      "Epoch: 2 / 3, Step: 741 / 750 Loss: 0.0947\n",
      "Epoch: 2 / 3, Step: 742 / 750 Loss: 0.2057\n",
      "Epoch: 2 / 3, Step: 743 / 750 Loss: 0.1860\n",
      "Epoch: 2 / 3, Step: 744 / 750 Loss: 0.1438\n",
      "Epoch: 2 / 3, Step: 745 / 750 Loss: 0.3911\n",
      "Epoch: 2 / 3, Step: 746 / 750 Loss: 0.3199\n",
      "Epoch: 2 / 3, Step: 747 / 750 Loss: 0.1272\n",
      "Epoch: 2 / 3, Step: 748 / 750 Loss: 0.1338\n",
      "Epoch: 2 / 3, Step: 749 / 750 Loss: 0.1341\n",
      "Epoch: 3 / 3, Step: 0 / 750 Loss: 0.1515\n",
      "Epoch: 3 / 3, Step: 1 / 750 Loss: 0.5275\n",
      "Epoch: 3 / 3, Step: 2 / 750 Loss: 0.3391\n",
      "Epoch: 3 / 3, Step: 3 / 750 Loss: 0.2065\n",
      "Epoch: 3 / 3, Step: 4 / 750 Loss: 0.1588\n",
      "Epoch: 3 / 3, Step: 5 / 750 Loss: 0.1439\n",
      "Epoch: 3 / 3, Step: 6 / 750 Loss: 0.1192\n",
      "Epoch: 3 / 3, Step: 7 / 750 Loss: 0.3368\n",
      "Epoch: 3 / 3, Step: 8 / 750 Loss: 0.2142\n",
      "Epoch: 3 / 3, Step: 9 / 750 Loss: 0.1384\n",
      "Epoch: 3 / 3, Step: 10 / 750 Loss: 0.1853\n",
      "Epoch: 3 / 3, Step: 11 / 750 Loss: 0.2843\n",
      "Epoch: 3 / 3, Step: 12 / 750 Loss: 0.1957\n",
      "Epoch: 3 / 3, Step: 13 / 750 Loss: 0.1584\n",
      "Epoch: 3 / 3, Step: 14 / 750 Loss: 0.2115\n",
      "Epoch: 3 / 3, Step: 15 / 750 Loss: 0.0653\n",
      "Epoch: 3 / 3, Step: 16 / 750 Loss: 0.4541\n",
      "Epoch: 3 / 3, Step: 17 / 750 Loss: 0.1477\n",
      "Epoch: 3 / 3, Step: 18 / 750 Loss: 0.1095\n",
      "Epoch: 3 / 3, Step: 19 / 750 Loss: 0.1005\n",
      "Epoch: 3 / 3, Step: 20 / 750 Loss: 0.0972\n",
      "Epoch: 3 / 3, Step: 21 / 750 Loss: 0.2764\n",
      "Epoch: 3 / 3, Step: 22 / 750 Loss: 0.0836\n",
      "Epoch: 3 / 3, Step: 23 / 750 Loss: 0.2105\n",
      "Epoch: 3 / 3, Step: 24 / 750 Loss: 0.2156\n",
      "Epoch: 3 / 3, Step: 25 / 750 Loss: 0.0808\n",
      "Epoch: 3 / 3, Step: 26 / 750 Loss: 0.1185\n",
      "Epoch: 3 / 3, Step: 27 / 750 Loss: 0.1542\n",
      "Epoch: 3 / 3, Step: 28 / 750 Loss: 0.3159\n",
      "Epoch: 3 / 3, Step: 29 / 750 Loss: 0.1090\n",
      "Epoch: 3 / 3, Step: 30 / 750 Loss: 0.0958\n",
      "Epoch: 3 / 3, Step: 31 / 750 Loss: 0.1051\n",
      "Epoch: 3 / 3, Step: 32 / 750 Loss: 0.1206\n",
      "Epoch: 3 / 3, Step: 33 / 750 Loss: 0.0771\n",
      "Epoch: 3 / 3, Step: 34 / 750 Loss: 0.0429\n",
      "Epoch: 3 / 3, Step: 35 / 750 Loss: 0.4441\n",
      "Epoch: 3 / 3, Step: 36 / 750 Loss: 0.2638\n",
      "Epoch: 3 / 3, Step: 37 / 750 Loss: 0.0836\n",
      "Epoch: 3 / 3, Step: 38 / 750 Loss: 0.1254\n",
      "Epoch: 3 / 3, Step: 39 / 750 Loss: 0.1499\n",
      "Epoch: 3 / 3, Step: 40 / 750 Loss: 0.1447\n",
      "Epoch: 3 / 3, Step: 41 / 750 Loss: 0.1408\n",
      "Epoch: 3 / 3, Step: 42 / 750 Loss: 0.1738\n",
      "Epoch: 3 / 3, Step: 43 / 750 Loss: 0.1684\n",
      "Epoch: 3 / 3, Step: 44 / 750 Loss: 0.1827\n",
      "Epoch: 3 / 3, Step: 45 / 750 Loss: 0.2218\n",
      "Epoch: 3 / 3, Step: 46 / 750 Loss: 0.1416\n",
      "Epoch: 3 / 3, Step: 47 / 750 Loss: 0.1555\n",
      "Epoch: 3 / 3, Step: 48 / 750 Loss: 0.3230\n",
      "Epoch: 3 / 3, Step: 49 / 750 Loss: 0.1152\n",
      "Epoch: 3 / 3, Step: 50 / 750 Loss: 0.2856\n",
      "Epoch: 3 / 3, Step: 51 / 750 Loss: 0.2288\n",
      "Epoch: 3 / 3, Step: 52 / 750 Loss: 0.0651\n",
      "Epoch: 3 / 3, Step: 53 / 750 Loss: 0.0910\n",
      "Epoch: 3 / 3, Step: 54 / 750 Loss: 0.0369\n",
      "Epoch: 3 / 3, Step: 55 / 750 Loss: 0.5603\n",
      "Epoch: 3 / 3, Step: 56 / 750 Loss: 0.1360\n",
      "Epoch: 3 / 3, Step: 57 / 750 Loss: 0.1274\n",
      "Epoch: 3 / 3, Step: 58 / 750 Loss: 0.1583\n",
      "Epoch: 3 / 3, Step: 59 / 750 Loss: 0.0521\n",
      "Epoch: 3 / 3, Step: 60 / 750 Loss: 0.1697\n",
      "Epoch: 3 / 3, Step: 61 / 750 Loss: 0.0976\n",
      "Epoch: 3 / 3, Step: 62 / 750 Loss: 0.2453\n",
      "Epoch: 3 / 3, Step: 63 / 750 Loss: 0.4049\n",
      "Epoch: 3 / 3, Step: 64 / 750 Loss: 0.1902\n",
      "Epoch: 3 / 3, Step: 65 / 750 Loss: 0.4579\n",
      "Epoch: 3 / 3, Step: 66 / 750 Loss: 0.0312\n",
      "Epoch: 3 / 3, Step: 67 / 750 Loss: 0.0344\n",
      "Epoch: 3 / 3, Step: 68 / 750 Loss: 0.2594\n",
      "Epoch: 3 / 3, Step: 69 / 750 Loss: 0.1636\n",
      "Epoch: 3 / 3, Step: 70 / 750 Loss: 0.2708\n",
      "Epoch: 3 / 3, Step: 71 / 750 Loss: 0.3555\n",
      "Epoch: 3 / 3, Step: 72 / 750 Loss: 0.1760\n",
      "Epoch: 3 / 3, Step: 73 / 750 Loss: 0.0841\n",
      "Epoch: 3 / 3, Step: 74 / 750 Loss: 0.3372\n",
      "Epoch: 3 / 3, Step: 75 / 750 Loss: 0.1664\n",
      "Epoch: 3 / 3, Step: 76 / 750 Loss: 0.2323\n",
      "Epoch: 3 / 3, Step: 77 / 750 Loss: 0.1108\n",
      "Epoch: 3 / 3, Step: 78 / 750 Loss: 0.1380\n",
      "Epoch: 3 / 3, Step: 79 / 750 Loss: 0.1049\n",
      "Epoch: 3 / 3, Step: 80 / 750 Loss: 0.1351\n",
      "Epoch: 3 / 3, Step: 81 / 750 Loss: 0.2951\n",
      "Epoch: 3 / 3, Step: 82 / 750 Loss: 0.0737\n",
      "Epoch: 3 / 3, Step: 83 / 750 Loss: 0.1049\n",
      "Epoch: 3 / 3, Step: 84 / 750 Loss: 0.2497\n",
      "Epoch: 3 / 3, Step: 85 / 750 Loss: 0.3247\n",
      "Epoch: 3 / 3, Step: 86 / 750 Loss: 0.1180\n",
      "Epoch: 3 / 3, Step: 87 / 750 Loss: 0.1476\n",
      "Epoch: 3 / 3, Step: 88 / 750 Loss: 0.2827\n",
      "Epoch: 3 / 3, Step: 89 / 750 Loss: 0.3145\n",
      "Epoch: 3 / 3, Step: 90 / 750 Loss: 0.1544\n",
      "Epoch: 3 / 3, Step: 91 / 750 Loss: 0.0793\n",
      "Epoch: 3 / 3, Step: 92 / 750 Loss: 0.4032\n",
      "Epoch: 3 / 3, Step: 93 / 750 Loss: 0.2326\n",
      "Epoch: 3 / 3, Step: 94 / 750 Loss: 0.0597\n",
      "Epoch: 3 / 3, Step: 95 / 750 Loss: 0.1509\n",
      "Epoch: 3 / 3, Step: 96 / 750 Loss: 0.0554\n",
      "Epoch: 3 / 3, Step: 97 / 750 Loss: 0.1277\n",
      "Epoch: 3 / 3, Step: 98 / 750 Loss: 0.3078\n",
      "Epoch: 3 / 3, Step: 99 / 750 Loss: 0.1369\n",
      "Epoch: 3 / 3, Step: 100 / 750 Loss: 0.2394\n",
      "Epoch: 3 / 3, Step: 101 / 750 Loss: 0.0685\n",
      "Epoch: 3 / 3, Step: 102 / 750 Loss: 0.0970\n",
      "Epoch: 3 / 3, Step: 103 / 750 Loss: 0.2571\n",
      "Epoch: 3 / 3, Step: 104 / 750 Loss: 0.1708\n",
      "Epoch: 3 / 3, Step: 105 / 750 Loss: 0.1844\n",
      "Epoch: 3 / 3, Step: 106 / 750 Loss: 0.1296\n",
      "Epoch: 3 / 3, Step: 107 / 750 Loss: 0.0825\n",
      "Epoch: 3 / 3, Step: 108 / 750 Loss: 0.2791\n",
      "Epoch: 3 / 3, Step: 109 / 750 Loss: 0.1681\n",
      "Epoch: 3 / 3, Step: 110 / 750 Loss: 0.1670\n",
      "Epoch: 3 / 3, Step: 111 / 750 Loss: 0.1872\n",
      "Epoch: 3 / 3, Step: 112 / 750 Loss: 0.0363\n",
      "Epoch: 3 / 3, Step: 113 / 750 Loss: 0.2614\n",
      "Epoch: 3 / 3, Step: 114 / 750 Loss: 0.1211\n",
      "Epoch: 3 / 3, Step: 115 / 750 Loss: 0.0300\n",
      "Epoch: 3 / 3, Step: 116 / 750 Loss: 0.0592\n",
      "Epoch: 3 / 3, Step: 117 / 750 Loss: 0.3451\n",
      "Epoch: 3 / 3, Step: 118 / 750 Loss: 0.1931\n",
      "Epoch: 3 / 3, Step: 119 / 750 Loss: 0.0562\n",
      "Epoch: 3 / 3, Step: 120 / 750 Loss: 0.2470\n",
      "Epoch: 3 / 3, Step: 121 / 750 Loss: 0.0859\n",
      "Epoch: 3 / 3, Step: 122 / 750 Loss: 0.1238\n",
      "Epoch: 3 / 3, Step: 123 / 750 Loss: 0.0802\n",
      "Epoch: 3 / 3, Step: 124 / 750 Loss: 0.1497\n",
      "Epoch: 3 / 3, Step: 125 / 750 Loss: 0.2558\n",
      "Epoch: 3 / 3, Step: 126 / 750 Loss: 0.1078\n",
      "Epoch: 3 / 3, Step: 127 / 750 Loss: 0.1808\n",
      "Epoch: 3 / 3, Step: 128 / 750 Loss: 0.0767\n",
      "Epoch: 3 / 3, Step: 129 / 750 Loss: 0.0847\n",
      "Epoch: 3 / 3, Step: 130 / 750 Loss: 0.2786\n",
      "Epoch: 3 / 3, Step: 131 / 750 Loss: 0.0918\n",
      "Epoch: 3 / 3, Step: 132 / 750 Loss: 0.1335\n",
      "Epoch: 3 / 3, Step: 133 / 750 Loss: 0.0562\n",
      "Epoch: 3 / 3, Step: 134 / 750 Loss: 0.1720\n",
      "Epoch: 3 / 3, Step: 135 / 750 Loss: 0.0666\n",
      "Epoch: 3 / 3, Step: 136 / 750 Loss: 0.1167\n",
      "Epoch: 3 / 3, Step: 137 / 750 Loss: 0.1644\n",
      "Epoch: 3 / 3, Step: 138 / 750 Loss: 0.3457\n",
      "Epoch: 3 / 3, Step: 139 / 750 Loss: 0.1298\n",
      "Epoch: 3 / 3, Step: 140 / 750 Loss: 0.0846\n",
      "Epoch: 3 / 3, Step: 141 / 750 Loss: 0.0300\n",
      "Epoch: 3 / 3, Step: 142 / 750 Loss: 0.1643\n",
      "Epoch: 3 / 3, Step: 143 / 750 Loss: 0.3017\n",
      "Epoch: 3 / 3, Step: 144 / 750 Loss: 0.2911\n",
      "Epoch: 3 / 3, Step: 145 / 750 Loss: 0.0672\n",
      "Epoch: 3 / 3, Step: 146 / 750 Loss: 0.3292\n",
      "Epoch: 3 / 3, Step: 147 / 750 Loss: 0.3002\n",
      "Epoch: 3 / 3, Step: 148 / 750 Loss: 0.1901\n",
      "Epoch: 3 / 3, Step: 149 / 750 Loss: 0.2381\n",
      "Epoch: 3 / 3, Step: 150 / 750 Loss: 0.0600\n",
      "Epoch: 3 / 3, Step: 151 / 750 Loss: 0.0323\n",
      "Epoch: 3 / 3, Step: 152 / 750 Loss: 0.0522\n",
      "Epoch: 3 / 3, Step: 153 / 750 Loss: 0.0994\n",
      "Epoch: 3 / 3, Step: 154 / 750 Loss: 0.0969\n",
      "Epoch: 3 / 3, Step: 155 / 750 Loss: 0.3863\n",
      "Epoch: 3 / 3, Step: 156 / 750 Loss: 0.0794\n",
      "Epoch: 3 / 3, Step: 157 / 750 Loss: 0.0307\n",
      "Epoch: 3 / 3, Step: 158 / 750 Loss: 0.1096\n",
      "Epoch: 3 / 3, Step: 159 / 750 Loss: 0.0337\n",
      "Epoch: 3 / 3, Step: 160 / 750 Loss: 0.0992\n",
      "Epoch: 3 / 3, Step: 161 / 750 Loss: 0.4735\n",
      "Epoch: 3 / 3, Step: 162 / 750 Loss: 0.1159\n",
      "Epoch: 3 / 3, Step: 163 / 750 Loss: 0.0825\n",
      "Epoch: 3 / 3, Step: 164 / 750 Loss: 0.4360\n",
      "Epoch: 3 / 3, Step: 165 / 750 Loss: 0.1553\n",
      "Epoch: 3 / 3, Step: 166 / 750 Loss: 0.3055\n",
      "Epoch: 3 / 3, Step: 167 / 750 Loss: 0.0443\n",
      "Epoch: 3 / 3, Step: 168 / 750 Loss: 0.1599\n",
      "Epoch: 3 / 3, Step: 169 / 750 Loss: 0.1582\n",
      "Epoch: 3 / 3, Step: 170 / 750 Loss: 0.2685\n",
      "Epoch: 3 / 3, Step: 171 / 750 Loss: 0.3959\n",
      "Epoch: 3 / 3, Step: 172 / 750 Loss: 0.1914\n",
      "Epoch: 3 / 3, Step: 173 / 750 Loss: 0.1755\n",
      "Epoch: 3 / 3, Step: 174 / 750 Loss: 0.2754\n",
      "Epoch: 3 / 3, Step: 175 / 750 Loss: 0.1132\n",
      "Epoch: 3 / 3, Step: 176 / 750 Loss: 0.2795\n",
      "Epoch: 3 / 3, Step: 177 / 750 Loss: 0.2327\n",
      "Epoch: 3 / 3, Step: 178 / 750 Loss: 0.0865\n",
      "Epoch: 3 / 3, Step: 179 / 750 Loss: 0.3257\n",
      "Epoch: 3 / 3, Step: 180 / 750 Loss: 0.4506\n",
      "Epoch: 3 / 3, Step: 181 / 750 Loss: 0.0366\n",
      "Epoch: 3 / 3, Step: 182 / 750 Loss: 0.2987\n",
      "Epoch: 3 / 3, Step: 183 / 750 Loss: 0.0934\n",
      "Epoch: 3 / 3, Step: 184 / 750 Loss: 0.0885\n",
      "Epoch: 3 / 3, Step: 185 / 750 Loss: 0.3325\n",
      "Epoch: 3 / 3, Step: 186 / 750 Loss: 0.1406\n",
      "Epoch: 3 / 3, Step: 187 / 750 Loss: 0.2503\n",
      "Epoch: 3 / 3, Step: 188 / 750 Loss: 0.1255\n",
      "Epoch: 3 / 3, Step: 189 / 750 Loss: 0.1763\n",
      "Epoch: 3 / 3, Step: 190 / 750 Loss: 0.2334\n",
      "Epoch: 3 / 3, Step: 191 / 750 Loss: 0.2967\n",
      "Epoch: 3 / 3, Step: 192 / 750 Loss: 0.0582\n",
      "Epoch: 3 / 3, Step: 193 / 750 Loss: 0.1329\n",
      "Epoch: 3 / 3, Step: 194 / 750 Loss: 0.0631\n",
      "Epoch: 3 / 3, Step: 195 / 750 Loss: 0.0484\n",
      "Epoch: 3 / 3, Step: 196 / 750 Loss: 0.1118\n",
      "Epoch: 3 / 3, Step: 197 / 750 Loss: 0.0880\n",
      "Epoch: 3 / 3, Step: 198 / 750 Loss: 0.0750\n",
      "Epoch: 3 / 3, Step: 199 / 750 Loss: 0.0957\n",
      "Epoch: 3 / 3, Step: 200 / 750 Loss: 0.0584\n",
      "Epoch: 3 / 3, Step: 201 / 750 Loss: 0.1454\n",
      "Epoch: 3 / 3, Step: 202 / 750 Loss: 0.1110\n",
      "Epoch: 3 / 3, Step: 203 / 750 Loss: 0.1259\n",
      "Epoch: 3 / 3, Step: 204 / 750 Loss: 0.1180\n",
      "Epoch: 3 / 3, Step: 205 / 750 Loss: 0.2457\n",
      "Epoch: 3 / 3, Step: 206 / 750 Loss: 0.1013\n",
      "Epoch: 3 / 3, Step: 207 / 750 Loss: 0.0228\n",
      "Epoch: 3 / 3, Step: 208 / 750 Loss: 0.0886\n",
      "Epoch: 3 / 3, Step: 209 / 750 Loss: 0.0953\n",
      "Epoch: 3 / 3, Step: 210 / 750 Loss: 0.0405\n",
      "Epoch: 3 / 3, Step: 211 / 750 Loss: 0.0477\n",
      "Epoch: 3 / 3, Step: 212 / 750 Loss: 0.0279\n",
      "Epoch: 3 / 3, Step: 213 / 750 Loss: 0.0633\n",
      "Epoch: 3 / 3, Step: 214 / 750 Loss: 0.1035\n",
      "Epoch: 3 / 3, Step: 215 / 750 Loss: 0.2306\n",
      "Epoch: 3 / 3, Step: 216 / 750 Loss: 0.2382\n",
      "Epoch: 3 / 3, Step: 217 / 750 Loss: 0.0399\n",
      "Epoch: 3 / 3, Step: 218 / 750 Loss: 0.0362\n",
      "Epoch: 3 / 3, Step: 219 / 750 Loss: 0.2594\n",
      "Epoch: 3 / 3, Step: 220 / 750 Loss: 0.4156\n",
      "Epoch: 3 / 3, Step: 221 / 750 Loss: 0.1088\n",
      "Epoch: 3 / 3, Step: 222 / 750 Loss: 0.0587\n",
      "Epoch: 3 / 3, Step: 223 / 750 Loss: 0.0422\n",
      "Epoch: 3 / 3, Step: 224 / 750 Loss: 0.2445\n",
      "Epoch: 3 / 3, Step: 225 / 750 Loss: 0.2026\n",
      "Epoch: 3 / 3, Step: 226 / 750 Loss: 0.5255\n",
      "Epoch: 3 / 3, Step: 227 / 750 Loss: 0.1701\n",
      "Epoch: 3 / 3, Step: 228 / 750 Loss: 0.1318\n",
      "Epoch: 3 / 3, Step: 229 / 750 Loss: 0.1704\n",
      "Epoch: 3 / 3, Step: 230 / 750 Loss: 0.2424\n",
      "Epoch: 3 / 3, Step: 231 / 750 Loss: 0.1172\n",
      "Epoch: 3 / 3, Step: 232 / 750 Loss: 0.2052\n",
      "Epoch: 3 / 3, Step: 233 / 750 Loss: 0.1422\n",
      "Epoch: 3 / 3, Step: 234 / 750 Loss: 0.0654\n",
      "Epoch: 3 / 3, Step: 235 / 750 Loss: 0.1593\n",
      "Epoch: 3 / 3, Step: 236 / 750 Loss: 0.1816\n",
      "Epoch: 3 / 3, Step: 237 / 750 Loss: 0.0360\n",
      "Epoch: 3 / 3, Step: 238 / 750 Loss: 0.0806\n",
      "Epoch: 3 / 3, Step: 239 / 750 Loss: 0.2142\n",
      "Epoch: 3 / 3, Step: 240 / 750 Loss: 0.1302\n",
      "Epoch: 3 / 3, Step: 241 / 750 Loss: 0.0345\n",
      "Epoch: 3 / 3, Step: 242 / 750 Loss: 0.2575\n",
      "Epoch: 3 / 3, Step: 243 / 750 Loss: 0.2488\n",
      "Epoch: 3 / 3, Step: 244 / 750 Loss: 0.1341\n",
      "Epoch: 3 / 3, Step: 245 / 750 Loss: 0.1344\n",
      "Epoch: 3 / 3, Step: 246 / 750 Loss: 0.1913\n",
      "Epoch: 3 / 3, Step: 247 / 750 Loss: 0.0478\n",
      "Epoch: 3 / 3, Step: 248 / 750 Loss: 0.0814\n",
      "Epoch: 3 / 3, Step: 249 / 750 Loss: 0.1611\n",
      "Epoch: 3 / 3, Step: 250 / 750 Loss: 0.1835\n",
      "Epoch: 3 / 3, Step: 251 / 750 Loss: 0.1410\n",
      "Epoch: 3 / 3, Step: 252 / 750 Loss: 0.1987\n",
      "Epoch: 3 / 3, Step: 253 / 750 Loss: 0.0438\n",
      "Epoch: 3 / 3, Step: 254 / 750 Loss: 0.0443\n",
      "Epoch: 3 / 3, Step: 255 / 750 Loss: 0.3254\n",
      "Epoch: 3 / 3, Step: 256 / 750 Loss: 0.0717\n",
      "Epoch: 3 / 3, Step: 257 / 750 Loss: 0.1576\n",
      "Epoch: 3 / 3, Step: 258 / 750 Loss: 0.2114\n",
      "Epoch: 3 / 3, Step: 259 / 750 Loss: 0.1907\n",
      "Epoch: 3 / 3, Step: 260 / 750 Loss: 0.1024\n",
      "Epoch: 3 / 3, Step: 261 / 750 Loss: 0.1079\n",
      "Epoch: 3 / 3, Step: 262 / 750 Loss: 0.2310\n",
      "Epoch: 3 / 3, Step: 263 / 750 Loss: 0.2278\n",
      "Epoch: 3 / 3, Step: 264 / 750 Loss: 0.1446\n",
      "Epoch: 3 / 3, Step: 265 / 750 Loss: 0.1583\n",
      "Epoch: 3 / 3, Step: 266 / 750 Loss: 0.3136\n",
      "Epoch: 3 / 3, Step: 267 / 750 Loss: 0.1075\n",
      "Epoch: 3 / 3, Step: 268 / 750 Loss: 0.2645\n",
      "Epoch: 3 / 3, Step: 269 / 750 Loss: 0.3146\n",
      "Epoch: 3 / 3, Step: 270 / 750 Loss: 0.0875\n",
      "Epoch: 3 / 3, Step: 271 / 750 Loss: 0.2826\n",
      "Epoch: 3 / 3, Step: 272 / 750 Loss: 0.1224\n",
      "Epoch: 3 / 3, Step: 273 / 750 Loss: 0.2422\n",
      "Epoch: 3 / 3, Step: 274 / 750 Loss: 0.1857\n",
      "Epoch: 3 / 3, Step: 275 / 750 Loss: 0.0439\n",
      "Epoch: 3 / 3, Step: 276 / 750 Loss: 0.1023\n",
      "Epoch: 3 / 3, Step: 277 / 750 Loss: 0.2523\n",
      "Epoch: 3 / 3, Step: 278 / 750 Loss: 0.1287\n",
      "Epoch: 3 / 3, Step: 279 / 750 Loss: 0.1768\n",
      "Epoch: 3 / 3, Step: 280 / 750 Loss: 0.1263\n",
      "Epoch: 3 / 3, Step: 281 / 750 Loss: 0.1683\n",
      "Epoch: 3 / 3, Step: 282 / 750 Loss: 0.2582\n",
      "Epoch: 3 / 3, Step: 283 / 750 Loss: 0.0380\n",
      "Epoch: 3 / 3, Step: 284 / 750 Loss: 0.3032\n",
      "Epoch: 3 / 3, Step: 285 / 750 Loss: 0.1975\n",
      "Epoch: 3 / 3, Step: 286 / 750 Loss: 0.1151\n",
      "Epoch: 3 / 3, Step: 287 / 750 Loss: 0.1481\n",
      "Epoch: 3 / 3, Step: 288 / 750 Loss: 0.0537\n",
      "Epoch: 3 / 3, Step: 289 / 750 Loss: 0.0823\n",
      "Epoch: 3 / 3, Step: 290 / 750 Loss: 0.1231\n",
      "Epoch: 3 / 3, Step: 291 / 750 Loss: 0.0724\n",
      "Epoch: 3 / 3, Step: 292 / 750 Loss: 0.0458\n",
      "Epoch: 3 / 3, Step: 293 / 750 Loss: 0.2524\n",
      "Epoch: 3 / 3, Step: 294 / 750 Loss: 0.1068\n",
      "Epoch: 3 / 3, Step: 295 / 750 Loss: 0.1009\n",
      "Epoch: 3 / 3, Step: 296 / 750 Loss: 0.3574\n",
      "Epoch: 3 / 3, Step: 297 / 750 Loss: 0.0900\n",
      "Epoch: 3 / 3, Step: 298 / 750 Loss: 0.0770\n",
      "Epoch: 3 / 3, Step: 299 / 750 Loss: 0.2040\n",
      "Epoch: 3 / 3, Step: 300 / 750 Loss: 0.3009\n",
      "Epoch: 3 / 3, Step: 301 / 750 Loss: 0.0318\n",
      "Epoch: 3 / 3, Step: 302 / 750 Loss: 0.0612\n",
      "Epoch: 3 / 3, Step: 303 / 750 Loss: 0.3041\n",
      "Epoch: 3 / 3, Step: 304 / 750 Loss: 0.1471\n",
      "Epoch: 3 / 3, Step: 305 / 750 Loss: 0.0427\n",
      "Epoch: 3 / 3, Step: 306 / 750 Loss: 0.0930\n",
      "Epoch: 3 / 3, Step: 307 / 750 Loss: 0.1703\n",
      "Epoch: 3 / 3, Step: 308 / 750 Loss: 0.2290\n",
      "Epoch: 3 / 3, Step: 309 / 750 Loss: 0.4613\n",
      "Epoch: 3 / 3, Step: 310 / 750 Loss: 0.2881\n",
      "Epoch: 3 / 3, Step: 311 / 750 Loss: 0.0650\n",
      "Epoch: 3 / 3, Step: 312 / 750 Loss: 0.0628\n",
      "Epoch: 3 / 3, Step: 313 / 750 Loss: 0.0626\n",
      "Epoch: 3 / 3, Step: 314 / 750 Loss: 0.0634\n",
      "Epoch: 3 / 3, Step: 315 / 750 Loss: 0.0499\n",
      "Epoch: 3 / 3, Step: 316 / 750 Loss: 0.3965\n",
      "Epoch: 3 / 3, Step: 317 / 750 Loss: 0.0993\n",
      "Epoch: 3 / 3, Step: 318 / 750 Loss: 0.0957\n",
      "Epoch: 3 / 3, Step: 319 / 750 Loss: 0.1092\n",
      "Epoch: 3 / 3, Step: 320 / 750 Loss: 0.1706\n",
      "Epoch: 3 / 3, Step: 321 / 750 Loss: 0.1559\n",
      "Epoch: 3 / 3, Step: 322 / 750 Loss: 0.0932\n",
      "Epoch: 3 / 3, Step: 323 / 750 Loss: 0.0315\n",
      "Epoch: 3 / 3, Step: 324 / 750 Loss: 0.2611\n",
      "Epoch: 3 / 3, Step: 325 / 750 Loss: 0.0229\n",
      "Epoch: 3 / 3, Step: 326 / 750 Loss: 0.2214\n",
      "Epoch: 3 / 3, Step: 327 / 750 Loss: 0.1605\n",
      "Epoch: 3 / 3, Step: 328 / 750 Loss: 0.0275\n",
      "Epoch: 3 / 3, Step: 329 / 750 Loss: 0.0728\n",
      "Epoch: 3 / 3, Step: 330 / 750 Loss: 0.1243\n",
      "Epoch: 3 / 3, Step: 331 / 750 Loss: 0.1444\n",
      "Epoch: 3 / 3, Step: 332 / 750 Loss: 0.1331\n",
      "Epoch: 3 / 3, Step: 333 / 750 Loss: 0.1031\n",
      "Epoch: 3 / 3, Step: 334 / 750 Loss: 0.0984\n",
      "Epoch: 3 / 3, Step: 335 / 750 Loss: 0.1653\n",
      "Epoch: 3 / 3, Step: 336 / 750 Loss: 0.0851\n",
      "Epoch: 3 / 3, Step: 337 / 750 Loss: 0.2066\n",
      "Epoch: 3 / 3, Step: 338 / 750 Loss: 0.2162\n",
      "Epoch: 3 / 3, Step: 339 / 750 Loss: 0.0312\n",
      "Epoch: 3 / 3, Step: 340 / 750 Loss: 0.0303\n",
      "Epoch: 3 / 3, Step: 341 / 750 Loss: 0.2170\n",
      "Epoch: 3 / 3, Step: 342 / 750 Loss: 0.2386\n",
      "Epoch: 3 / 3, Step: 343 / 750 Loss: 0.0954\n",
      "Epoch: 3 / 3, Step: 344 / 750 Loss: 0.2864\n",
      "Epoch: 3 / 3, Step: 345 / 750 Loss: 0.0861\n",
      "Epoch: 3 / 3, Step: 346 / 750 Loss: 0.1222\n",
      "Epoch: 3 / 3, Step: 347 / 750 Loss: 0.0226\n",
      "Epoch: 3 / 3, Step: 348 / 750 Loss: 0.2503\n",
      "Epoch: 3 / 3, Step: 349 / 750 Loss: 0.3820\n",
      "Epoch: 3 / 3, Step: 350 / 750 Loss: 0.2456\n",
      "Epoch: 3 / 3, Step: 351 / 750 Loss: 0.0896\n",
      "Epoch: 3 / 3, Step: 352 / 750 Loss: 0.5135\n",
      "Epoch: 3 / 3, Step: 353 / 750 Loss: 0.1046\n",
      "Epoch: 3 / 3, Step: 354 / 750 Loss: 0.0837\n",
      "Epoch: 3 / 3, Step: 355 / 750 Loss: 0.2053\n",
      "Epoch: 3 / 3, Step: 356 / 750 Loss: 0.1016\n",
      "Epoch: 3 / 3, Step: 357 / 750 Loss: 0.3019\n",
      "Epoch: 3 / 3, Step: 358 / 750 Loss: 0.0291\n",
      "Epoch: 3 / 3, Step: 359 / 750 Loss: 0.0582\n",
      "Epoch: 3 / 3, Step: 360 / 750 Loss: 0.1073\n",
      "Epoch: 3 / 3, Step: 361 / 750 Loss: 0.3569\n",
      "Epoch: 3 / 3, Step: 362 / 750 Loss: 0.0628\n",
      "Epoch: 3 / 3, Step: 363 / 750 Loss: 0.3440\n",
      "Epoch: 3 / 3, Step: 364 / 750 Loss: 0.1149\n",
      "Epoch: 3 / 3, Step: 365 / 750 Loss: 0.1010\n",
      "Epoch: 3 / 3, Step: 366 / 750 Loss: 0.1825\n",
      "Epoch: 3 / 3, Step: 367 / 750 Loss: 0.2597\n",
      "Epoch: 3 / 3, Step: 368 / 750 Loss: 0.0695\n",
      "Epoch: 3 / 3, Step: 369 / 750 Loss: 0.0864\n",
      "Epoch: 3 / 3, Step: 370 / 750 Loss: 0.1954\n",
      "Epoch: 3 / 3, Step: 371 / 750 Loss: 0.2269\n",
      "Epoch: 3 / 3, Step: 372 / 750 Loss: 0.2136\n",
      "Epoch: 3 / 3, Step: 373 / 750 Loss: 0.1474\n",
      "Epoch: 3 / 3, Step: 374 / 750 Loss: 0.3329\n",
      "Epoch: 3 / 3, Step: 375 / 750 Loss: 0.3245\n",
      "Epoch: 3 / 3, Step: 376 / 750 Loss: 0.0702\n",
      "Epoch: 3 / 3, Step: 377 / 750 Loss: 0.1093\n",
      "Epoch: 3 / 3, Step: 378 / 750 Loss: 0.3694\n",
      "Epoch: 3 / 3, Step: 379 / 750 Loss: 0.1659\n",
      "Epoch: 3 / 3, Step: 380 / 750 Loss: 0.1225\n",
      "Epoch: 3 / 3, Step: 381 / 750 Loss: 0.1591\n",
      "Epoch: 3 / 3, Step: 382 / 750 Loss: 0.1622\n",
      "Epoch: 3 / 3, Step: 383 / 750 Loss: 0.0566\n",
      "Epoch: 3 / 3, Step: 384 / 750 Loss: 0.0559\n",
      "Epoch: 3 / 3, Step: 385 / 750 Loss: 0.1510\n",
      "Epoch: 3 / 3, Step: 386 / 750 Loss: 0.1963\n",
      "Epoch: 3 / 3, Step: 387 / 750 Loss: 0.4429\n",
      "Epoch: 3 / 3, Step: 388 / 750 Loss: 0.1729\n",
      "Epoch: 3 / 3, Step: 389 / 750 Loss: 0.1425\n",
      "Epoch: 3 / 3, Step: 390 / 750 Loss: 0.0892\n",
      "Epoch: 3 / 3, Step: 391 / 750 Loss: 0.0828\n",
      "Epoch: 3 / 3, Step: 392 / 750 Loss: 0.1638\n",
      "Epoch: 3 / 3, Step: 393 / 750 Loss: 0.1451\n",
      "Epoch: 3 / 3, Step: 394 / 750 Loss: 0.0753\n",
      "Epoch: 3 / 3, Step: 395 / 750 Loss: 0.0654\n",
      "Epoch: 3 / 3, Step: 396 / 750 Loss: 0.0526\n",
      "Epoch: 3 / 3, Step: 397 / 750 Loss: 0.0681\n",
      "Epoch: 3 / 3, Step: 398 / 750 Loss: 0.1159\n",
      "Epoch: 3 / 3, Step: 399 / 750 Loss: 0.2798\n",
      "Epoch: 3 / 3, Step: 400 / 750 Loss: 0.0440\n",
      "Epoch: 3 / 3, Step: 401 / 750 Loss: 0.2309\n",
      "Epoch: 3 / 3, Step: 402 / 750 Loss: 0.1592\n",
      "Epoch: 3 / 3, Step: 403 / 750 Loss: 0.1025\n",
      "Epoch: 3 / 3, Step: 404 / 750 Loss: 0.0642\n",
      "Epoch: 3 / 3, Step: 405 / 750 Loss: 0.1658\n",
      "Epoch: 3 / 3, Step: 406 / 750 Loss: 0.0349\n",
      "Epoch: 3 / 3, Step: 407 / 750 Loss: 0.2014\n",
      "Epoch: 3 / 3, Step: 408 / 750 Loss: 0.1856\n",
      "Epoch: 3 / 3, Step: 409 / 750 Loss: 0.3622\n",
      "Epoch: 3 / 3, Step: 410 / 750 Loss: 0.1607\n",
      "Epoch: 3 / 3, Step: 411 / 750 Loss: 0.2930\n",
      "Epoch: 3 / 3, Step: 412 / 750 Loss: 0.1973\n",
      "Epoch: 3 / 3, Step: 413 / 750 Loss: 0.2288\n",
      "Epoch: 3 / 3, Step: 414 / 750 Loss: 0.1261\n",
      "Epoch: 3 / 3, Step: 415 / 750 Loss: 0.0269\n",
      "Epoch: 3 / 3, Step: 416 / 750 Loss: 0.0516\n",
      "Epoch: 3 / 3, Step: 417 / 750 Loss: 0.2661\n",
      "Epoch: 3 / 3, Step: 418 / 750 Loss: 0.0690\n",
      "Epoch: 3 / 3, Step: 419 / 750 Loss: 0.2299\n",
      "Epoch: 3 / 3, Step: 420 / 750 Loss: 0.2924\n",
      "Epoch: 3 / 3, Step: 421 / 750 Loss: 0.1920\n",
      "Epoch: 3 / 3, Step: 422 / 750 Loss: 0.1304\n",
      "Epoch: 3 / 3, Step: 423 / 750 Loss: 0.0506\n",
      "Epoch: 3 / 3, Step: 424 / 750 Loss: 0.1247\n",
      "Epoch: 3 / 3, Step: 425 / 750 Loss: 0.0625\n",
      "Epoch: 3 / 3, Step: 426 / 750 Loss: 0.1468\n",
      "Epoch: 3 / 3, Step: 427 / 750 Loss: 0.2529\n",
      "Epoch: 3 / 3, Step: 428 / 750 Loss: 0.1375\n",
      "Epoch: 3 / 3, Step: 429 / 750 Loss: 0.0629\n",
      "Epoch: 3 / 3, Step: 430 / 750 Loss: 0.2134\n",
      "Epoch: 3 / 3, Step: 431 / 750 Loss: 0.1792\n",
      "Epoch: 3 / 3, Step: 432 / 750 Loss: 0.2709\n",
      "Epoch: 3 / 3, Step: 433 / 750 Loss: 0.0932\n",
      "Epoch: 3 / 3, Step: 434 / 750 Loss: 0.1498\n",
      "Epoch: 3 / 3, Step: 435 / 750 Loss: 0.1951\n",
      "Epoch: 3 / 3, Step: 436 / 750 Loss: 0.3652\n",
      "Epoch: 3 / 3, Step: 437 / 750 Loss: 0.0619\n",
      "Epoch: 3 / 3, Step: 438 / 750 Loss: 0.0487\n",
      "Epoch: 3 / 3, Step: 439 / 750 Loss: 0.0782\n",
      "Epoch: 3 / 3, Step: 440 / 750 Loss: 0.2629\n",
      "Epoch: 3 / 3, Step: 441 / 750 Loss: 0.4513\n",
      "Epoch: 3 / 3, Step: 442 / 750 Loss: 0.0552\n",
      "Epoch: 3 / 3, Step: 443 / 750 Loss: 0.0887\n",
      "Epoch: 3 / 3, Step: 444 / 750 Loss: 0.1361\n",
      "Epoch: 3 / 3, Step: 445 / 750 Loss: 0.1172\n",
      "Epoch: 3 / 3, Step: 446 / 750 Loss: 0.0218\n",
      "Epoch: 3 / 3, Step: 447 / 750 Loss: 0.0352\n",
      "Epoch: 3 / 3, Step: 448 / 750 Loss: 0.2799\n",
      "Epoch: 3 / 3, Step: 449 / 750 Loss: 0.0880\n",
      "Epoch: 3 / 3, Step: 450 / 750 Loss: 0.0790\n",
      "Epoch: 3 / 3, Step: 451 / 750 Loss: 0.0616\n",
      "Epoch: 3 / 3, Step: 452 / 750 Loss: 0.1661\n",
      "Epoch: 3 / 3, Step: 453 / 750 Loss: 0.3120\n",
      "Epoch: 3 / 3, Step: 454 / 750 Loss: 0.3386\n",
      "Epoch: 3 / 3, Step: 455 / 750 Loss: 0.0413\n",
      "Epoch: 3 / 3, Step: 456 / 750 Loss: 0.0354\n",
      "Epoch: 3 / 3, Step: 457 / 750 Loss: 0.0822\n",
      "Epoch: 3 / 3, Step: 458 / 750 Loss: 0.1403\n",
      "Epoch: 3 / 3, Step: 459 / 750 Loss: 0.1635\n",
      "Epoch: 3 / 3, Step: 460 / 750 Loss: 0.0923\n",
      "Epoch: 3 / 3, Step: 461 / 750 Loss: 0.0614\n",
      "Epoch: 3 / 3, Step: 462 / 750 Loss: 0.0936\n",
      "Epoch: 3 / 3, Step: 463 / 750 Loss: 0.2391\n",
      "Epoch: 3 / 3, Step: 464 / 750 Loss: 0.1023\n",
      "Epoch: 3 / 3, Step: 465 / 750 Loss: 0.0523\n",
      "Epoch: 3 / 3, Step: 466 / 750 Loss: 0.0974\n",
      "Epoch: 3 / 3, Step: 467 / 750 Loss: 0.0954\n",
      "Epoch: 3 / 3, Step: 468 / 750 Loss: 0.0774\n",
      "Epoch: 3 / 3, Step: 469 / 750 Loss: 0.0705\n",
      "Epoch: 3 / 3, Step: 470 / 750 Loss: 0.1477\n",
      "Epoch: 3 / 3, Step: 471 / 750 Loss: 0.1466\n",
      "Epoch: 3 / 3, Step: 472 / 750 Loss: 0.0340\n",
      "Epoch: 3 / 3, Step: 473 / 750 Loss: 0.0527\n",
      "Epoch: 3 / 3, Step: 474 / 750 Loss: 0.2498\n",
      "Epoch: 3 / 3, Step: 475 / 750 Loss: 0.0665\n",
      "Epoch: 3 / 3, Step: 476 / 750 Loss: 0.0273\n",
      "Epoch: 3 / 3, Step: 477 / 750 Loss: 0.0746\n",
      "Epoch: 3 / 3, Step: 478 / 750 Loss: 0.0788\n",
      "Epoch: 3 / 3, Step: 479 / 750 Loss: 0.0588\n",
      "Epoch: 3 / 3, Step: 480 / 750 Loss: 0.0427\n",
      "Epoch: 3 / 3, Step: 481 / 750 Loss: 0.2057\n",
      "Epoch: 3 / 3, Step: 482 / 750 Loss: 0.2197\n",
      "Epoch: 3 / 3, Step: 483 / 750 Loss: 0.0548\n",
      "Epoch: 3 / 3, Step: 484 / 750 Loss: 0.3363\n",
      "Epoch: 3 / 3, Step: 485 / 750 Loss: 0.1821\n",
      "Epoch: 3 / 3, Step: 486 / 750 Loss: 0.1435\n",
      "Epoch: 3 / 3, Step: 487 / 750 Loss: 0.2110\n",
      "Epoch: 3 / 3, Step: 488 / 750 Loss: 0.0910\n",
      "Epoch: 3 / 3, Step: 489 / 750 Loss: 0.0935\n",
      "Epoch: 3 / 3, Step: 490 / 750 Loss: 0.1434\n",
      "Epoch: 3 / 3, Step: 491 / 750 Loss: 0.0597\n",
      "Epoch: 3 / 3, Step: 492 / 750 Loss: 0.1940\n",
      "Epoch: 3 / 3, Step: 493 / 750 Loss: 0.0266\n",
      "Epoch: 3 / 3, Step: 494 / 750 Loss: 0.2692\n",
      "Epoch: 3 / 3, Step: 495 / 750 Loss: 0.2101\n",
      "Epoch: 3 / 3, Step: 496 / 750 Loss: 0.1619\n",
      "Epoch: 3 / 3, Step: 497 / 750 Loss: 0.2428\n",
      "Epoch: 3 / 3, Step: 498 / 750 Loss: 0.1847\n",
      "Epoch: 3 / 3, Step: 499 / 750 Loss: 0.1338\n",
      "Epoch: 3 / 3, Step: 500 / 750 Loss: 0.2184\n",
      "Epoch: 3 / 3, Step: 501 / 750 Loss: 0.1343\n",
      "Epoch: 3 / 3, Step: 502 / 750 Loss: 0.2898\n",
      "Epoch: 3 / 3, Step: 503 / 750 Loss: 0.2924\n",
      "Epoch: 3 / 3, Step: 504 / 750 Loss: 0.0832\n",
      "Epoch: 3 / 3, Step: 505 / 750 Loss: 0.0620\n",
      "Epoch: 3 / 3, Step: 506 / 750 Loss: 0.0672\n",
      "Epoch: 3 / 3, Step: 507 / 750 Loss: 0.1723\n",
      "Epoch: 3 / 3, Step: 508 / 750 Loss: 0.3121\n",
      "Epoch: 3 / 3, Step: 509 / 750 Loss: 0.1395\n",
      "Epoch: 3 / 3, Step: 510 / 750 Loss: 0.1644\n",
      "Epoch: 3 / 3, Step: 511 / 750 Loss: 0.2110\n",
      "Epoch: 3 / 3, Step: 512 / 750 Loss: 0.2750\n",
      "Epoch: 3 / 3, Step: 513 / 750 Loss: 0.2161\n",
      "Epoch: 3 / 3, Step: 514 / 750 Loss: 0.0661\n",
      "Epoch: 3 / 3, Step: 515 / 750 Loss: 0.1519\n",
      "Epoch: 3 / 3, Step: 516 / 750 Loss: 0.0981\n",
      "Epoch: 3 / 3, Step: 517 / 750 Loss: 0.0228\n",
      "Epoch: 3 / 3, Step: 518 / 750 Loss: 0.2657\n",
      "Epoch: 3 / 3, Step: 519 / 750 Loss: 0.0359\n",
      "Epoch: 3 / 3, Step: 520 / 750 Loss: 0.0785\n",
      "Epoch: 3 / 3, Step: 521 / 750 Loss: 0.1357\n",
      "Epoch: 3 / 3, Step: 522 / 750 Loss: 0.0847\n",
      "Epoch: 3 / 3, Step: 523 / 750 Loss: 0.0708\n",
      "Epoch: 3 / 3, Step: 524 / 750 Loss: 0.3373\n",
      "Epoch: 3 / 3, Step: 525 / 750 Loss: 0.0403\n",
      "Epoch: 3 / 3, Step: 526 / 750 Loss: 0.2924\n",
      "Epoch: 3 / 3, Step: 527 / 750 Loss: 0.1947\n",
      "Epoch: 3 / 3, Step: 528 / 750 Loss: 0.1152\n",
      "Epoch: 3 / 3, Step: 529 / 750 Loss: 0.2128\n",
      "Epoch: 3 / 3, Step: 530 / 750 Loss: 0.2682\n",
      "Epoch: 3 / 3, Step: 531 / 750 Loss: 0.1251\n",
      "Epoch: 3 / 3, Step: 532 / 750 Loss: 0.1675\n",
      "Epoch: 3 / 3, Step: 533 / 750 Loss: 0.4165\n",
      "Epoch: 3 / 3, Step: 534 / 750 Loss: 0.2193\n",
      "Epoch: 3 / 3, Step: 535 / 750 Loss: 0.1295\n",
      "Epoch: 3 / 3, Step: 536 / 750 Loss: 0.1096\n",
      "Epoch: 3 / 3, Step: 537 / 750 Loss: 0.0488\n",
      "Epoch: 3 / 3, Step: 538 / 750 Loss: 0.1322\n",
      "Epoch: 3 / 3, Step: 539 / 750 Loss: 0.0618\n",
      "Epoch: 3 / 3, Step: 540 / 750 Loss: 0.1346\n",
      "Epoch: 3 / 3, Step: 541 / 750 Loss: 0.0775\n",
      "Epoch: 3 / 3, Step: 542 / 750 Loss: 0.1660\n",
      "Epoch: 3 / 3, Step: 543 / 750 Loss: 0.0337\n",
      "Epoch: 3 / 3, Step: 544 / 750 Loss: 0.1317\n",
      "Epoch: 3 / 3, Step: 545 / 750 Loss: 0.0243\n",
      "Epoch: 3 / 3, Step: 546 / 750 Loss: 0.4363\n",
      "Epoch: 3 / 3, Step: 547 / 750 Loss: 0.2605\n",
      "Epoch: 3 / 3, Step: 548 / 750 Loss: 0.0876\n",
      "Epoch: 3 / 3, Step: 549 / 750 Loss: 0.1079\n",
      "Epoch: 3 / 3, Step: 550 / 750 Loss: 0.1046\n",
      "Epoch: 3 / 3, Step: 551 / 750 Loss: 0.1919\n",
      "Epoch: 3 / 3, Step: 552 / 750 Loss: 0.1348\n",
      "Epoch: 3 / 3, Step: 553 / 750 Loss: 0.1295\n",
      "Epoch: 3 / 3, Step: 554 / 750 Loss: 0.0977\n",
      "Epoch: 3 / 3, Step: 555 / 750 Loss: 0.0829\n",
      "Epoch: 3 / 3, Step: 556 / 750 Loss: 0.1557\n",
      "Epoch: 3 / 3, Step: 557 / 750 Loss: 0.2753\n",
      "Epoch: 3 / 3, Step: 558 / 750 Loss: 0.2357\n",
      "Epoch: 3 / 3, Step: 559 / 750 Loss: 0.3765\n",
      "Epoch: 3 / 3, Step: 560 / 750 Loss: 0.1782\n",
      "Epoch: 3 / 3, Step: 561 / 750 Loss: 0.2216\n",
      "Epoch: 3 / 3, Step: 562 / 750 Loss: 0.1452\n",
      "Epoch: 3 / 3, Step: 563 / 750 Loss: 0.4627\n",
      "Epoch: 3 / 3, Step: 564 / 750 Loss: 0.0419\n",
      "Epoch: 3 / 3, Step: 565 / 750 Loss: 0.1708\n",
      "Epoch: 3 / 3, Step: 566 / 750 Loss: 0.0566\n",
      "Epoch: 3 / 3, Step: 567 / 750 Loss: 0.1829\n",
      "Epoch: 3 / 3, Step: 568 / 750 Loss: 0.1118\n",
      "Epoch: 3 / 3, Step: 569 / 750 Loss: 0.1561\n",
      "Epoch: 3 / 3, Step: 570 / 750 Loss: 0.3381\n",
      "Epoch: 3 / 3, Step: 571 / 750 Loss: 0.0845\n",
      "Epoch: 3 / 3, Step: 572 / 750 Loss: 0.0910\n",
      "Epoch: 3 / 3, Step: 573 / 750 Loss: 0.0296\n",
      "Epoch: 3 / 3, Step: 574 / 750 Loss: 0.2762\n",
      "Epoch: 3 / 3, Step: 575 / 750 Loss: 0.0502\n",
      "Epoch: 3 / 3, Step: 576 / 750 Loss: 0.0895\n",
      "Epoch: 3 / 3, Step: 577 / 750 Loss: 0.3433\n",
      "Epoch: 3 / 3, Step: 578 / 750 Loss: 0.0463\n",
      "Epoch: 3 / 3, Step: 579 / 750 Loss: 0.0336\n",
      "Epoch: 3 / 3, Step: 580 / 750 Loss: 0.0792\n",
      "Epoch: 3 / 3, Step: 581 / 750 Loss: 0.2092\n",
      "Epoch: 3 / 3, Step: 582 / 750 Loss: 0.0306\n",
      "Epoch: 3 / 3, Step: 583 / 750 Loss: 0.3121\n",
      "Epoch: 3 / 3, Step: 584 / 750 Loss: 0.0655\n",
      "Epoch: 3 / 3, Step: 585 / 750 Loss: 0.0240\n",
      "Epoch: 3 / 3, Step: 586 / 750 Loss: 0.0428\n",
      "Epoch: 3 / 3, Step: 587 / 750 Loss: 0.1002\n",
      "Epoch: 3 / 3, Step: 588 / 750 Loss: 0.2042\n",
      "Epoch: 3 / 3, Step: 589 / 750 Loss: 0.0846\n",
      "Epoch: 3 / 3, Step: 590 / 750 Loss: 0.2990\n",
      "Epoch: 3 / 3, Step: 591 / 750 Loss: 0.0426\n",
      "Epoch: 3 / 3, Step: 592 / 750 Loss: 0.1945\n",
      "Epoch: 3 / 3, Step: 593 / 750 Loss: 0.2006\n",
      "Epoch: 3 / 3, Step: 594 / 750 Loss: 0.0754\n",
      "Epoch: 3 / 3, Step: 595 / 750 Loss: 0.1074\n",
      "Epoch: 3 / 3, Step: 596 / 750 Loss: 0.1983\n",
      "Epoch: 3 / 3, Step: 597 / 750 Loss: 0.1006\n",
      "Epoch: 3 / 3, Step: 598 / 750 Loss: 0.1018\n",
      "Epoch: 3 / 3, Step: 599 / 750 Loss: 0.1696\n",
      "Epoch: 3 / 3, Step: 600 / 750 Loss: 0.1480\n",
      "Epoch: 3 / 3, Step: 601 / 750 Loss: 0.2120\n",
      "Epoch: 3 / 3, Step: 602 / 750 Loss: 0.2758\n",
      "Epoch: 3 / 3, Step: 603 / 750 Loss: 0.0718\n",
      "Epoch: 3 / 3, Step: 604 / 750 Loss: 0.2242\n",
      "Epoch: 3 / 3, Step: 605 / 750 Loss: 0.1598\n",
      "Epoch: 3 / 3, Step: 606 / 750 Loss: 0.0562\n",
      "Epoch: 3 / 3, Step: 607 / 750 Loss: 0.1517\n",
      "Epoch: 3 / 3, Step: 608 / 750 Loss: 0.1147\n",
      "Epoch: 3 / 3, Step: 609 / 750 Loss: 0.1798\n",
      "Epoch: 3 / 3, Step: 610 / 750 Loss: 0.1021\n",
      "Epoch: 3 / 3, Step: 611 / 750 Loss: 0.0840\n",
      "Epoch: 3 / 3, Step: 612 / 750 Loss: 0.0542\n",
      "Epoch: 3 / 3, Step: 613 / 750 Loss: 0.1645\n",
      "Epoch: 3 / 3, Step: 614 / 750 Loss: 0.2277\n",
      "Epoch: 3 / 3, Step: 615 / 750 Loss: 0.1589\n",
      "Epoch: 3 / 3, Step: 616 / 750 Loss: 0.0849\n",
      "Epoch: 3 / 3, Step: 617 / 750 Loss: 0.0310\n",
      "Epoch: 3 / 3, Step: 618 / 750 Loss: 0.1520\n",
      "Epoch: 3 / 3, Step: 619 / 750 Loss: 0.1541\n",
      "Epoch: 3 / 3, Step: 620 / 750 Loss: 0.4571\n",
      "Epoch: 3 / 3, Step: 621 / 750 Loss: 0.1689\n",
      "Epoch: 3 / 3, Step: 622 / 750 Loss: 0.0382\n",
      "Epoch: 3 / 3, Step: 623 / 750 Loss: 0.3554\n",
      "Epoch: 3 / 3, Step: 624 / 750 Loss: 0.0225\n",
      "Epoch: 3 / 3, Step: 625 / 750 Loss: 0.3053\n",
      "Epoch: 3 / 3, Step: 626 / 750 Loss: 0.0282\n",
      "Epoch: 3 / 3, Step: 627 / 750 Loss: 0.1838\n",
      "Epoch: 3 / 3, Step: 628 / 750 Loss: 0.3547\n",
      "Epoch: 3 / 3, Step: 629 / 750 Loss: 0.1143\n",
      "Epoch: 3 / 3, Step: 630 / 750 Loss: 0.1548\n",
      "Epoch: 3 / 3, Step: 631 / 750 Loss: 0.1700\n",
      "Epoch: 3 / 3, Step: 632 / 750 Loss: 0.2061\n",
      "Epoch: 3 / 3, Step: 633 / 750 Loss: 0.0798\n",
      "Epoch: 3 / 3, Step: 634 / 750 Loss: 0.0689\n",
      "Epoch: 3 / 3, Step: 635 / 750 Loss: 0.2256\n",
      "Epoch: 3 / 3, Step: 636 / 750 Loss: 0.0957\n",
      "Epoch: 3 / 3, Step: 637 / 750 Loss: 0.0749\n",
      "Epoch: 3 / 3, Step: 638 / 750 Loss: 0.0446\n",
      "Epoch: 3 / 3, Step: 639 / 750 Loss: 0.0879\n",
      "Epoch: 3 / 3, Step: 640 / 750 Loss: 0.1199\n",
      "Epoch: 3 / 3, Step: 641 / 750 Loss: 0.1220\n",
      "Epoch: 3 / 3, Step: 642 / 750 Loss: 0.1982\n",
      "Epoch: 3 / 3, Step: 643 / 750 Loss: 0.0447\n",
      "Epoch: 3 / 3, Step: 644 / 750 Loss: 0.0477\n",
      "Epoch: 3 / 3, Step: 645 / 750 Loss: 0.0475\n",
      "Epoch: 3 / 3, Step: 646 / 750 Loss: 0.1182\n",
      "Epoch: 3 / 3, Step: 647 / 750 Loss: 0.1525\n",
      "Epoch: 3 / 3, Step: 648 / 750 Loss: 0.0583\n",
      "Epoch: 3 / 3, Step: 649 / 750 Loss: 0.1268\n",
      "Epoch: 3 / 3, Step: 650 / 750 Loss: 0.0228\n",
      "Epoch: 3 / 3, Step: 651 / 750 Loss: 0.0850\n",
      "Epoch: 3 / 3, Step: 652 / 750 Loss: 0.3430\n",
      "Epoch: 3 / 3, Step: 653 / 750 Loss: 0.0492\n",
      "Epoch: 3 / 3, Step: 654 / 750 Loss: 0.1882\n",
      "Epoch: 3 / 3, Step: 655 / 750 Loss: 0.0267\n",
      "Epoch: 3 / 3, Step: 656 / 750 Loss: 0.0712\n",
      "Epoch: 3 / 3, Step: 657 / 750 Loss: 0.0522\n",
      "Epoch: 3 / 3, Step: 658 / 750 Loss: 0.0624\n",
      "Epoch: 3 / 3, Step: 659 / 750 Loss: 0.0679\n",
      "Epoch: 3 / 3, Step: 660 / 750 Loss: 0.0768\n",
      "Epoch: 3 / 3, Step: 661 / 750 Loss: 0.1161\n",
      "Epoch: 3 / 3, Step: 662 / 750 Loss: 0.0259\n",
      "Epoch: 3 / 3, Step: 663 / 750 Loss: 0.1761\n",
      "Epoch: 3 / 3, Step: 664 / 750 Loss: 0.0440\n",
      "Epoch: 3 / 3, Step: 665 / 750 Loss: 0.1664\n",
      "Epoch: 3 / 3, Step: 666 / 750 Loss: 0.0536\n",
      "Epoch: 3 / 3, Step: 667 / 750 Loss: 0.0442\n",
      "Epoch: 3 / 3, Step: 668 / 750 Loss: 0.1581\n",
      "Epoch: 3 / 3, Step: 669 / 750 Loss: 0.1847\n",
      "Epoch: 3 / 3, Step: 670 / 750 Loss: 0.0126\n",
      "Epoch: 3 / 3, Step: 671 / 750 Loss: 0.1201\n",
      "Epoch: 3 / 3, Step: 672 / 750 Loss: 0.0209\n",
      "Epoch: 3 / 3, Step: 673 / 750 Loss: 0.0807\n",
      "Epoch: 3 / 3, Step: 674 / 750 Loss: 0.2567\n",
      "Epoch: 3 / 3, Step: 675 / 750 Loss: 0.2060\n",
      "Epoch: 3 / 3, Step: 676 / 750 Loss: 0.1271\n",
      "Epoch: 3 / 3, Step: 677 / 750 Loss: 0.3087\n",
      "Epoch: 3 / 3, Step: 678 / 750 Loss: 0.2648\n",
      "Epoch: 3 / 3, Step: 679 / 750 Loss: 0.1921\n",
      "Epoch: 3 / 3, Step: 680 / 750 Loss: 0.1614\n",
      "Epoch: 3 / 3, Step: 681 / 750 Loss: 0.3643\n",
      "Epoch: 3 / 3, Step: 682 / 750 Loss: 0.0605\n",
      "Epoch: 3 / 3, Step: 683 / 750 Loss: 0.0413\n",
      "Epoch: 3 / 3, Step: 684 / 750 Loss: 0.3705\n",
      "Epoch: 3 / 3, Step: 685 / 750 Loss: 0.1610\n",
      "Epoch: 3 / 3, Step: 686 / 750 Loss: 0.0486\n",
      "Epoch: 3 / 3, Step: 687 / 750 Loss: 0.2886\n",
      "Epoch: 3 / 3, Step: 688 / 750 Loss: 0.1148\n",
      "Epoch: 3 / 3, Step: 689 / 750 Loss: 0.0566\n",
      "Epoch: 3 / 3, Step: 690 / 750 Loss: 0.0909\n",
      "Epoch: 3 / 3, Step: 691 / 750 Loss: 0.0673\n",
      "Epoch: 3 / 3, Step: 692 / 750 Loss: 0.0692\n",
      "Epoch: 3 / 3, Step: 693 / 750 Loss: 0.2388\n",
      "Epoch: 3 / 3, Step: 694 / 750 Loss: 0.1351\n",
      "Epoch: 3 / 3, Step: 695 / 750 Loss: 0.0611\n",
      "Epoch: 3 / 3, Step: 696 / 750 Loss: 0.1853\n",
      "Epoch: 3 / 3, Step: 697 / 750 Loss: 0.0389\n",
      "Epoch: 3 / 3, Step: 698 / 750 Loss: 0.1585\n",
      "Epoch: 3 / 3, Step: 699 / 750 Loss: 0.2154\n",
      "Epoch: 3 / 3, Step: 700 / 750 Loss: 0.0983\n",
      "Epoch: 3 / 3, Step: 701 / 750 Loss: 0.1437\n",
      "Epoch: 3 / 3, Step: 702 / 750 Loss: 0.0942\n",
      "Epoch: 3 / 3, Step: 703 / 750 Loss: 0.1503\n",
      "Epoch: 3 / 3, Step: 704 / 750 Loss: 0.0357\n",
      "Epoch: 3 / 3, Step: 705 / 750 Loss: 0.0997\n",
      "Epoch: 3 / 3, Step: 706 / 750 Loss: 0.1989\n",
      "Epoch: 3 / 3, Step: 707 / 750 Loss: 0.1499\n",
      "Epoch: 3 / 3, Step: 708 / 750 Loss: 0.0526\n",
      "Epoch: 3 / 3, Step: 709 / 750 Loss: 0.1342\n",
      "Epoch: 3 / 3, Step: 710 / 750 Loss: 0.0805\n",
      "Epoch: 3 / 3, Step: 711 / 750 Loss: 0.0703\n",
      "Epoch: 3 / 3, Step: 712 / 750 Loss: 0.0443\n",
      "Epoch: 3 / 3, Step: 713 / 750 Loss: 0.0765\n",
      "Epoch: 3 / 3, Step: 714 / 750 Loss: 0.0401\n",
      "Epoch: 3 / 3, Step: 715 / 750 Loss: 0.1048\n",
      "Epoch: 3 / 3, Step: 716 / 750 Loss: 0.0711\n",
      "Epoch: 3 / 3, Step: 717 / 750 Loss: 0.3681\n",
      "Epoch: 3 / 3, Step: 718 / 750 Loss: 0.1669\n",
      "Epoch: 3 / 3, Step: 719 / 750 Loss: 0.0450\n",
      "Epoch: 3 / 3, Step: 720 / 750 Loss: 0.1169\n",
      "Epoch: 3 / 3, Step: 721 / 750 Loss: 0.0146\n",
      "Epoch: 3 / 3, Step: 722 / 750 Loss: 0.0265\n",
      "Epoch: 3 / 3, Step: 723 / 750 Loss: 0.3408\n",
      "Epoch: 3 / 3, Step: 724 / 750 Loss: 0.0757\n",
      "Epoch: 3 / 3, Step: 725 / 750 Loss: 0.0664\n",
      "Epoch: 3 / 3, Step: 726 / 750 Loss: 0.1971\n",
      "Epoch: 3 / 3, Step: 727 / 750 Loss: 0.2448\n",
      "Epoch: 3 / 3, Step: 728 / 750 Loss: 0.1357\n",
      "Epoch: 3 / 3, Step: 729 / 750 Loss: 0.0885\n",
      "Epoch: 3 / 3, Step: 730 / 750 Loss: 0.2137\n",
      "Epoch: 3 / 3, Step: 731 / 750 Loss: 0.0704\n",
      "Epoch: 3 / 3, Step: 732 / 750 Loss: 0.1285\n",
      "Epoch: 3 / 3, Step: 733 / 750 Loss: 0.1672\n",
      "Epoch: 3 / 3, Step: 734 / 750 Loss: 0.1577\n",
      "Epoch: 3 / 3, Step: 735 / 750 Loss: 0.1211\n",
      "Epoch: 3 / 3, Step: 736 / 750 Loss: 0.0194\n",
      "Epoch: 3 / 3, Step: 737 / 750 Loss: 0.1217\n",
      "Epoch: 3 / 3, Step: 738 / 750 Loss: 0.1518\n",
      "Epoch: 3 / 3, Step: 739 / 750 Loss: 0.0121\n",
      "Epoch: 3 / 3, Step: 740 / 750 Loss: 0.2453\n",
      "Epoch: 3 / 3, Step: 741 / 750 Loss: 0.0671\n",
      "Epoch: 3 / 3, Step: 742 / 750 Loss: 0.0570\n",
      "Epoch: 3 / 3, Step: 743 / 750 Loss: 0.0451\n",
      "Epoch: 3 / 3, Step: 744 / 750 Loss: 0.1273\n",
      "Epoch: 3 / 3, Step: 745 / 750 Loss: 0.1920\n",
      "Epoch: 3 / 3, Step: 746 / 750 Loss: 0.0623\n",
      "Epoch: 3 / 3, Step: 747 / 750 Loss: 0.0592\n",
      "Epoch: 3 / 3, Step: 748 / 750 Loss: 0.0504\n",
      "Epoch: 3 / 3, Step: 749 / 750 Loss: 0.0998\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "loss_total = 0\n",
    "model.train()\n",
    "for i in range(3):\n",
    "    for j, data in enumerate(train_dataloader):\n",
    "        inputs = {'input_ids': data[0].cuda(), \n",
    "                      'attention_mask': data[1].cuda(), \n",
    "                      'labels': data[2].cuda()}\n",
    "        output = model(**inputs)\n",
    "        loss = output[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Epoch: {} / {}, Step: {} / {} Loss: {:.4f}\".format(i+1, 3, j, len(train_dataloader),\n",
    "                                                                      loss))\n",
    "torch.save(model.state_dict(), 'modelA.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "167da4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('modelA.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "loss_total = 0\n",
    "results = defaultdict(dict)\n",
    "predictions, true_vals = [], []\n",
    "for j, data in enumerate(test_dataloader):\n",
    "    inputs = {'input_ids': data[0].cuda(), \n",
    "              'attention_mask': data[1].cuda(), \n",
    "              'labels': data[2].cuda()}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "    loss = output[0]\n",
    "    logits = output[1]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    labels = inputs['labels'].cpu().numpy()\n",
    "    loss_total += loss.item()\n",
    "    predictions.append(logits)\n",
    "    true_vals.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e636c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_vals = np.concatenate(true_vals, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4531c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "preds_flat = np.argmax(predictions, axis = 1).flatten()\n",
    "labels_flat = true_vals.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2de7a277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.88      0.89      0.89      1017\n",
      "         pos       0.86      0.84      0.85       393\n",
      "         neu       0.96      0.96      0.96      4590\n",
      "\n",
      "    accuracy                           0.94      6000\n",
      "   macro avg       0.90      0.90      0.90      6000\n",
      "weighted avg       0.94      0.94      0.94      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['neg', 'pos', 'neu']\n",
    "print(classification_report(labels_flat, preds_flat, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f3919",
   "metadata": {},
   "source": [
    "# DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c136d3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertConfig,DistilBertTokenizer,DistilBertModel\n",
    "distil_berttokenizer = DistilBertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a534cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/a/grad/sanket96/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2322: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = distil_berttokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='train'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')\n",
    "\n",
    "encoded_data_test = distil_berttokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='test'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85165248",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(covid_senti[covid_senti.data_type == 'train'].label_num.values)\n",
    "\n",
    "#validation set\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(covid_senti[covid_senti.data_type == 'test'].label_num.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cc424d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['transformer.layer.7.ffn.lin1.bias', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.9.output_layer_norm.weight', 'pre_classifier.weight', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.11.attention.v_lin.weight', 'classifier.weight', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.0.ffn.lin1.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.1.attention.q_lin.bias', 'pre_classifier.bias', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.0.ffn.lin2.bias', 'embeddings.position_embeddings.weight', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.6.attention.q_lin.weight', 'classifier.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.8.sa_layer_norm.weight', 'embeddings.LayerNorm.weight', 'transformer.layer.5.sa_layer_norm.weight', 'embeddings.word_embeddings.weight', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.4.output_layer_norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(class_dict),\n",
    "                                                     output_attentions = False,\n",
    "                                                      output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85a22875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch import nn, optim\n",
    "\n",
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train,labels_train)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test,labels_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "074579c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7dddaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 3, Step: 0 / 750 Loss: 1.1052\n",
      "Epoch: 1 / 3, Step: 1 / 750 Loss: 0.7276\n",
      "Epoch: 1 / 3, Step: 2 / 750 Loss: 0.9843\n",
      "Epoch: 1 / 3, Step: 3 / 750 Loss: 0.6338\n",
      "Epoch: 1 / 3, Step: 4 / 750 Loss: 0.5480\n",
      "Epoch: 1 / 3, Step: 5 / 750 Loss: 0.6396\n",
      "Epoch: 1 / 3, Step: 6 / 750 Loss: 0.5616\n",
      "Epoch: 1 / 3, Step: 7 / 750 Loss: 0.9645\n",
      "Epoch: 1 / 3, Step: 8 / 750 Loss: 0.6199\n",
      "Epoch: 1 / 3, Step: 9 / 750 Loss: 0.7198\n",
      "Epoch: 1 / 3, Step: 10 / 750 Loss: 0.6688\n",
      "Epoch: 1 / 3, Step: 11 / 750 Loss: 0.7417\n",
      "Epoch: 1 / 3, Step: 12 / 750 Loss: 0.9502\n",
      "Epoch: 1 / 3, Step: 13 / 750 Loss: 0.7326\n",
      "Epoch: 1 / 3, Step: 14 / 750 Loss: 0.6738\n",
      "Epoch: 1 / 3, Step: 15 / 750 Loss: 0.7072\n",
      "Epoch: 1 / 3, Step: 16 / 750 Loss: 0.5473\n",
      "Epoch: 1 / 3, Step: 17 / 750 Loss: 0.6048\n",
      "Epoch: 1 / 3, Step: 18 / 750 Loss: 0.6233\n",
      "Epoch: 1 / 3, Step: 19 / 750 Loss: 0.7719\n",
      "Epoch: 1 / 3, Step: 20 / 750 Loss: 0.6490\n",
      "Epoch: 1 / 3, Step: 21 / 750 Loss: 0.6407\n",
      "Epoch: 1 / 3, Step: 22 / 750 Loss: 0.6419\n",
      "Epoch: 1 / 3, Step: 23 / 750 Loss: 0.6644\n",
      "Epoch: 1 / 3, Step: 24 / 750 Loss: 0.6390\n",
      "Epoch: 1 / 3, Step: 25 / 750 Loss: 0.7519\n",
      "Epoch: 1 / 3, Step: 26 / 750 Loss: 0.8175\n",
      "Epoch: 1 / 3, Step: 27 / 750 Loss: 0.6167\n",
      "Epoch: 1 / 3, Step: 28 / 750 Loss: 0.6230\n",
      "Epoch: 1 / 3, Step: 29 / 750 Loss: 0.8235\n",
      "Epoch: 1 / 3, Step: 30 / 750 Loss: 0.7122\n",
      "Epoch: 1 / 3, Step: 31 / 750 Loss: 0.5559\n",
      "Epoch: 1 / 3, Step: 32 / 750 Loss: 0.6753\n",
      "Epoch: 1 / 3, Step: 33 / 750 Loss: 0.8076\n",
      "Epoch: 1 / 3, Step: 34 / 750 Loss: 0.7902\n",
      "Epoch: 1 / 3, Step: 35 / 750 Loss: 0.6389\n",
      "Epoch: 1 / 3, Step: 36 / 750 Loss: 0.8304\n",
      "Epoch: 1 / 3, Step: 37 / 750 Loss: 0.8349\n",
      "Epoch: 1 / 3, Step: 38 / 750 Loss: 0.8077\n",
      "Epoch: 1 / 3, Step: 39 / 750 Loss: 0.6194\n",
      "Epoch: 1 / 3, Step: 40 / 750 Loss: 0.7356\n",
      "Epoch: 1 / 3, Step: 41 / 750 Loss: 0.9002\n",
      "Epoch: 1 / 3, Step: 42 / 750 Loss: 0.6862\n",
      "Epoch: 1 / 3, Step: 43 / 750 Loss: 0.7487\n",
      "Epoch: 1 / 3, Step: 44 / 750 Loss: 0.6061\n",
      "Epoch: 1 / 3, Step: 45 / 750 Loss: 0.7556\n",
      "Epoch: 1 / 3, Step: 46 / 750 Loss: 0.7986\n",
      "Epoch: 1 / 3, Step: 47 / 750 Loss: 0.7750\n",
      "Epoch: 1 / 3, Step: 48 / 750 Loss: 0.7404\n",
      "Epoch: 1 / 3, Step: 49 / 750 Loss: 0.6062\n",
      "Epoch: 1 / 3, Step: 50 / 750 Loss: 0.8512\n",
      "Epoch: 1 / 3, Step: 51 / 750 Loss: 0.7792\n",
      "Epoch: 1 / 3, Step: 52 / 750 Loss: 0.3671\n",
      "Epoch: 1 / 3, Step: 53 / 750 Loss: 0.5199\n",
      "Epoch: 1 / 3, Step: 54 / 750 Loss: 0.6467\n",
      "Epoch: 1 / 3, Step: 55 / 750 Loss: 0.6042\n",
      "Epoch: 1 / 3, Step: 56 / 750 Loss: 0.8166\n",
      "Epoch: 1 / 3, Step: 57 / 750 Loss: 0.8106\n",
      "Epoch: 1 / 3, Step: 58 / 750 Loss: 0.7760\n",
      "Epoch: 1 / 3, Step: 59 / 750 Loss: 0.5563\n",
      "Epoch: 1 / 3, Step: 60 / 750 Loss: 0.5671\n",
      "Epoch: 1 / 3, Step: 61 / 750 Loss: 0.3762\n",
      "Epoch: 1 / 3, Step: 62 / 750 Loss: 0.4750\n",
      "Epoch: 1 / 3, Step: 63 / 750 Loss: 0.3840\n",
      "Epoch: 1 / 3, Step: 64 / 750 Loss: 0.5904\n",
      "Epoch: 1 / 3, Step: 65 / 750 Loss: 0.4862\n",
      "Epoch: 1 / 3, Step: 66 / 750 Loss: 0.6170\n",
      "Epoch: 1 / 3, Step: 67 / 750 Loss: 0.4228\n",
      "Epoch: 1 / 3, Step: 68 / 750 Loss: 0.8155\n",
      "Epoch: 1 / 3, Step: 69 / 750 Loss: 0.7743\n",
      "Epoch: 1 / 3, Step: 70 / 750 Loss: 0.3526\n",
      "Epoch: 1 / 3, Step: 71 / 750 Loss: 0.6405\n",
      "Epoch: 1 / 3, Step: 72 / 750 Loss: 0.7804\n",
      "Epoch: 1 / 3, Step: 73 / 750 Loss: 0.6698\n",
      "Epoch: 1 / 3, Step: 74 / 750 Loss: 0.4937\n",
      "Epoch: 1 / 3, Step: 75 / 750 Loss: 1.2354\n",
      "Epoch: 1 / 3, Step: 76 / 750 Loss: 0.6675\n",
      "Epoch: 1 / 3, Step: 77 / 750 Loss: 0.6109\n",
      "Epoch: 1 / 3, Step: 78 / 750 Loss: 0.6892\n",
      "Epoch: 1 / 3, Step: 79 / 750 Loss: 0.6622\n",
      "Epoch: 1 / 3, Step: 80 / 750 Loss: 0.6721\n",
      "Epoch: 1 / 3, Step: 81 / 750 Loss: 0.6261\n",
      "Epoch: 1 / 3, Step: 82 / 750 Loss: 0.7371\n",
      "Epoch: 1 / 3, Step: 83 / 750 Loss: 0.9179\n",
      "Epoch: 1 / 3, Step: 84 / 750 Loss: 0.8614\n",
      "Epoch: 1 / 3, Step: 85 / 750 Loss: 0.5887\n",
      "Epoch: 1 / 3, Step: 86 / 750 Loss: 0.6510\n",
      "Epoch: 1 / 3, Step: 87 / 750 Loss: 0.6601\n",
      "Epoch: 1 / 3, Step: 88 / 750 Loss: 0.4202\n",
      "Epoch: 1 / 3, Step: 89 / 750 Loss: 0.8113\n",
      "Epoch: 1 / 3, Step: 90 / 750 Loss: 0.9799\n",
      "Epoch: 1 / 3, Step: 91 / 750 Loss: 0.8386\n",
      "Epoch: 1 / 3, Step: 92 / 750 Loss: 0.6910\n",
      "Epoch: 1 / 3, Step: 93 / 750 Loss: 0.7923\n",
      "Epoch: 1 / 3, Step: 94 / 750 Loss: 0.8618\n",
      "Epoch: 1 / 3, Step: 95 / 750 Loss: 0.8916\n",
      "Epoch: 1 / 3, Step: 96 / 750 Loss: 0.8128\n",
      "Epoch: 1 / 3, Step: 97 / 750 Loss: 0.7352\n",
      "Epoch: 1 / 3, Step: 98 / 750 Loss: 0.6249\n",
      "Epoch: 1 / 3, Step: 99 / 750 Loss: 0.7086\n",
      "Epoch: 1 / 3, Step: 100 / 750 Loss: 0.7943\n",
      "Epoch: 1 / 3, Step: 101 / 750 Loss: 0.9327\n",
      "Epoch: 1 / 3, Step: 102 / 750 Loss: 0.6869\n",
      "Epoch: 1 / 3, Step: 103 / 750 Loss: 0.6899\n",
      "Epoch: 1 / 3, Step: 104 / 750 Loss: 0.5993\n",
      "Epoch: 1 / 3, Step: 105 / 750 Loss: 0.9565\n",
      "Epoch: 1 / 3, Step: 106 / 750 Loss: 0.5300\n",
      "Epoch: 1 / 3, Step: 107 / 750 Loss: 0.7772\n",
      "Epoch: 1 / 3, Step: 108 / 750 Loss: 1.0361\n",
      "Epoch: 1 / 3, Step: 109 / 750 Loss: 0.6187\n",
      "Epoch: 1 / 3, Step: 110 / 750 Loss: 0.9410\n",
      "Epoch: 1 / 3, Step: 111 / 750 Loss: 0.6929\n",
      "Epoch: 1 / 3, Step: 112 / 750 Loss: 0.5858\n",
      "Epoch: 1 / 3, Step: 113 / 750 Loss: 0.5099\n",
      "Epoch: 1 / 3, Step: 114 / 750 Loss: 0.8381\n",
      "Epoch: 1 / 3, Step: 115 / 750 Loss: 0.5798\n",
      "Epoch: 1 / 3, Step: 116 / 750 Loss: 0.7796\n",
      "Epoch: 1 / 3, Step: 117 / 750 Loss: 0.9400\n",
      "Epoch: 1 / 3, Step: 118 / 750 Loss: 0.7006\n",
      "Epoch: 1 / 3, Step: 119 / 750 Loss: 0.6015\n",
      "Epoch: 1 / 3, Step: 120 / 750 Loss: 0.8576\n",
      "Epoch: 1 / 3, Step: 121 / 750 Loss: 0.6908\n",
      "Epoch: 1 / 3, Step: 122 / 750 Loss: 0.7094\n",
      "Epoch: 1 / 3, Step: 123 / 750 Loss: 0.7103\n",
      "Epoch: 1 / 3, Step: 124 / 750 Loss: 0.5468\n",
      "Epoch: 1 / 3, Step: 125 / 750 Loss: 0.6481\n",
      "Epoch: 1 / 3, Step: 126 / 750 Loss: 0.7384\n",
      "Epoch: 1 / 3, Step: 127 / 750 Loss: 0.4670\n",
      "Epoch: 1 / 3, Step: 128 / 750 Loss: 0.4473\n",
      "Epoch: 1 / 3, Step: 129 / 750 Loss: 0.8216\n",
      "Epoch: 1 / 3, Step: 130 / 750 Loss: 0.7266\n",
      "Epoch: 1 / 3, Step: 131 / 750 Loss: 0.4781\n",
      "Epoch: 1 / 3, Step: 132 / 750 Loss: 0.8117\n",
      "Epoch: 1 / 3, Step: 133 / 750 Loss: 0.8144\n",
      "Epoch: 1 / 3, Step: 134 / 750 Loss: 0.6572\n",
      "Epoch: 1 / 3, Step: 135 / 750 Loss: 0.7097\n",
      "Epoch: 1 / 3, Step: 136 / 750 Loss: 0.6938\n",
      "Epoch: 1 / 3, Step: 137 / 750 Loss: 0.3765\n",
      "Epoch: 1 / 3, Step: 138 / 750 Loss: 0.7156\n",
      "Epoch: 1 / 3, Step: 139 / 750 Loss: 0.5308\n",
      "Epoch: 1 / 3, Step: 140 / 750 Loss: 0.7951\n",
      "Epoch: 1 / 3, Step: 141 / 750 Loss: 0.5900\n",
      "Epoch: 1 / 3, Step: 142 / 750 Loss: 0.7296\n",
      "Epoch: 1 / 3, Step: 143 / 750 Loss: 0.6599\n",
      "Epoch: 1 / 3, Step: 144 / 750 Loss: 0.4732\n",
      "Epoch: 1 / 3, Step: 145 / 750 Loss: 0.9691\n",
      "Epoch: 1 / 3, Step: 146 / 750 Loss: 0.5761\n",
      "Epoch: 1 / 3, Step: 147 / 750 Loss: 0.9550\n",
      "Epoch: 1 / 3, Step: 148 / 750 Loss: 0.7011\n",
      "Epoch: 1 / 3, Step: 149 / 750 Loss: 0.6962\n",
      "Epoch: 1 / 3, Step: 150 / 750 Loss: 0.6744\n",
      "Epoch: 1 / 3, Step: 151 / 750 Loss: 0.5066\n",
      "Epoch: 1 / 3, Step: 152 / 750 Loss: 0.6831\n",
      "Epoch: 1 / 3, Step: 153 / 750 Loss: 0.8509\n",
      "Epoch: 1 / 3, Step: 154 / 750 Loss: 0.6897\n",
      "Epoch: 1 / 3, Step: 155 / 750 Loss: 0.5639\n",
      "Epoch: 1 / 3, Step: 156 / 750 Loss: 0.5339\n",
      "Epoch: 1 / 3, Step: 157 / 750 Loss: 0.5531\n",
      "Epoch: 1 / 3, Step: 158 / 750 Loss: 0.7932\n",
      "Epoch: 1 / 3, Step: 159 / 750 Loss: 0.8537\n",
      "Epoch: 1 / 3, Step: 160 / 750 Loss: 0.5880\n",
      "Epoch: 1 / 3, Step: 161 / 750 Loss: 0.7665\n",
      "Epoch: 1 / 3, Step: 162 / 750 Loss: 0.6289\n",
      "Epoch: 1 / 3, Step: 163 / 750 Loss: 0.7158\n",
      "Epoch: 1 / 3, Step: 164 / 750 Loss: 0.5708\n",
      "Epoch: 1 / 3, Step: 165 / 750 Loss: 0.5133\n",
      "Epoch: 1 / 3, Step: 166 / 750 Loss: 0.5757\n",
      "Epoch: 1 / 3, Step: 167 / 750 Loss: 0.8984\n",
      "Epoch: 1 / 3, Step: 168 / 750 Loss: 0.8859\n",
      "Epoch: 1 / 3, Step: 169 / 750 Loss: 0.8606\n",
      "Epoch: 1 / 3, Step: 170 / 750 Loss: 0.7430\n",
      "Epoch: 1 / 3, Step: 171 / 750 Loss: 0.7728\n",
      "Epoch: 1 / 3, Step: 172 / 750 Loss: 0.5909\n",
      "Epoch: 1 / 3, Step: 173 / 750 Loss: 0.7865\n",
      "Epoch: 1 / 3, Step: 174 / 750 Loss: 0.6219\n",
      "Epoch: 1 / 3, Step: 175 / 750 Loss: 0.6636\n",
      "Epoch: 1 / 3, Step: 176 / 750 Loss: 0.6325\n",
      "Epoch: 1 / 3, Step: 177 / 750 Loss: 0.4795\n",
      "Epoch: 1 / 3, Step: 178 / 750 Loss: 0.9057\n",
      "Epoch: 1 / 3, Step: 179 / 750 Loss: 0.7183\n",
      "Epoch: 1 / 3, Step: 180 / 750 Loss: 0.6281\n",
      "Epoch: 1 / 3, Step: 181 / 750 Loss: 0.5226\n",
      "Epoch: 1 / 3, Step: 182 / 750 Loss: 0.4491\n",
      "Epoch: 1 / 3, Step: 183 / 750 Loss: 1.0446\n",
      "Epoch: 1 / 3, Step: 184 / 750 Loss: 0.9665\n",
      "Epoch: 1 / 3, Step: 185 / 750 Loss: 0.7009\n",
      "Epoch: 1 / 3, Step: 186 / 750 Loss: 0.6920\n",
      "Epoch: 1 / 3, Step: 187 / 750 Loss: 0.6611\n",
      "Epoch: 1 / 3, Step: 188 / 750 Loss: 0.8714\n",
      "Epoch: 1 / 3, Step: 189 / 750 Loss: 0.5231\n",
      "Epoch: 1 / 3, Step: 190 / 750 Loss: 0.5770\n",
      "Epoch: 1 / 3, Step: 191 / 750 Loss: 0.5736\n",
      "Epoch: 1 / 3, Step: 192 / 750 Loss: 0.7196\n",
      "Epoch: 1 / 3, Step: 193 / 750 Loss: 0.6309\n",
      "Epoch: 1 / 3, Step: 194 / 750 Loss: 0.5543\n",
      "Epoch: 1 / 3, Step: 195 / 750 Loss: 0.8018\n",
      "Epoch: 1 / 3, Step: 196 / 750 Loss: 0.5388\n",
      "Epoch: 1 / 3, Step: 197 / 750 Loss: 0.7118\n",
      "Epoch: 1 / 3, Step: 198 / 750 Loss: 0.6148\n",
      "Epoch: 1 / 3, Step: 199 / 750 Loss: 0.7767\n",
      "Epoch: 1 / 3, Step: 200 / 750 Loss: 0.7498\n",
      "Epoch: 1 / 3, Step: 201 / 750 Loss: 1.0598\n",
      "Epoch: 1 / 3, Step: 202 / 750 Loss: 0.6526\n",
      "Epoch: 1 / 3, Step: 203 / 750 Loss: 0.5765\n",
      "Epoch: 1 / 3, Step: 204 / 750 Loss: 0.6835\n",
      "Epoch: 1 / 3, Step: 205 / 750 Loss: 0.8617\n",
      "Epoch: 1 / 3, Step: 206 / 750 Loss: 0.7220\n",
      "Epoch: 1 / 3, Step: 207 / 750 Loss: 0.5799\n",
      "Epoch: 1 / 3, Step: 208 / 750 Loss: 0.7634\n",
      "Epoch: 1 / 3, Step: 209 / 750 Loss: 0.4284\n",
      "Epoch: 1 / 3, Step: 210 / 750 Loss: 0.6758\n",
      "Epoch: 1 / 3, Step: 211 / 750 Loss: 0.7410\n",
      "Epoch: 1 / 3, Step: 212 / 750 Loss: 0.4586\n",
      "Epoch: 1 / 3, Step: 213 / 750 Loss: 0.2625\n",
      "Epoch: 1 / 3, Step: 214 / 750 Loss: 0.9063\n",
      "Epoch: 1 / 3, Step: 215 / 750 Loss: 0.6351\n",
      "Epoch: 1 / 3, Step: 216 / 750 Loss: 0.8148\n",
      "Epoch: 1 / 3, Step: 217 / 750 Loss: 0.8457\n",
      "Epoch: 1 / 3, Step: 218 / 750 Loss: 0.7603\n",
      "Epoch: 1 / 3, Step: 219 / 750 Loss: 0.9227\n",
      "Epoch: 1 / 3, Step: 220 / 750 Loss: 0.5474\n",
      "Epoch: 1 / 3, Step: 221 / 750 Loss: 0.7212\n",
      "Epoch: 1 / 3, Step: 222 / 750 Loss: 0.6747\n",
      "Epoch: 1 / 3, Step: 223 / 750 Loss: 0.8806\n",
      "Epoch: 1 / 3, Step: 224 / 750 Loss: 0.3993\n",
      "Epoch: 1 / 3, Step: 225 / 750 Loss: 0.8789\n",
      "Epoch: 1 / 3, Step: 226 / 750 Loss: 0.8870\n",
      "Epoch: 1 / 3, Step: 227 / 750 Loss: 0.5127\n",
      "Epoch: 1 / 3, Step: 228 / 750 Loss: 0.8154\n",
      "Epoch: 1 / 3, Step: 229 / 750 Loss: 0.5645\n",
      "Epoch: 1 / 3, Step: 230 / 750 Loss: 0.6612\n",
      "Epoch: 1 / 3, Step: 231 / 750 Loss: 0.5763\n",
      "Epoch: 1 / 3, Step: 232 / 750 Loss: 0.7085\n",
      "Epoch: 1 / 3, Step: 233 / 750 Loss: 0.4829\n",
      "Epoch: 1 / 3, Step: 234 / 750 Loss: 0.8111\n",
      "Epoch: 1 / 3, Step: 235 / 750 Loss: 0.5695\n",
      "Epoch: 1 / 3, Step: 236 / 750 Loss: 0.6873\n",
      "Epoch: 1 / 3, Step: 237 / 750 Loss: 0.5471\n",
      "Epoch: 1 / 3, Step: 238 / 750 Loss: 0.6180\n",
      "Epoch: 1 / 3, Step: 239 / 750 Loss: 0.6777\n",
      "Epoch: 1 / 3, Step: 240 / 750 Loss: 1.0925\n",
      "Epoch: 1 / 3, Step: 241 / 750 Loss: 0.7599\n",
      "Epoch: 1 / 3, Step: 242 / 750 Loss: 0.4623\n",
      "Epoch: 1 / 3, Step: 243 / 750 Loss: 0.5463\n",
      "Epoch: 1 / 3, Step: 244 / 750 Loss: 0.5466\n",
      "Epoch: 1 / 3, Step: 245 / 750 Loss: 0.5566\n",
      "Epoch: 1 / 3, Step: 246 / 750 Loss: 0.5740\n",
      "Epoch: 1 / 3, Step: 247 / 750 Loss: 0.5981\n",
      "Epoch: 1 / 3, Step: 248 / 750 Loss: 0.5685\n",
      "Epoch: 1 / 3, Step: 249 / 750 Loss: 0.9383\n",
      "Epoch: 1 / 3, Step: 250 / 750 Loss: 0.6778\n",
      "Epoch: 1 / 3, Step: 251 / 750 Loss: 0.5609\n",
      "Epoch: 1 / 3, Step: 252 / 750 Loss: 0.6409\n",
      "Epoch: 1 / 3, Step: 253 / 750 Loss: 0.7770\n",
      "Epoch: 1 / 3, Step: 254 / 750 Loss: 0.3813\n",
      "Epoch: 1 / 3, Step: 255 / 750 Loss: 0.5942\n",
      "Epoch: 1 / 3, Step: 256 / 750 Loss: 0.6199\n",
      "Epoch: 1 / 3, Step: 257 / 750 Loss: 0.4962\n",
      "Epoch: 1 / 3, Step: 258 / 750 Loss: 0.6444\n",
      "Epoch: 1 / 3, Step: 259 / 750 Loss: 0.9470\n",
      "Epoch: 1 / 3, Step: 260 / 750 Loss: 0.8986\n",
      "Epoch: 1 / 3, Step: 261 / 750 Loss: 0.4485\n",
      "Epoch: 1 / 3, Step: 262 / 750 Loss: 0.6495\n",
      "Epoch: 1 / 3, Step: 263 / 750 Loss: 0.7680\n",
      "Epoch: 1 / 3, Step: 264 / 750 Loss: 0.5360\n",
      "Epoch: 1 / 3, Step: 265 / 750 Loss: 0.3719\n",
      "Epoch: 1 / 3, Step: 266 / 750 Loss: 0.6679\n",
      "Epoch: 1 / 3, Step: 267 / 750 Loss: 0.9807\n",
      "Epoch: 1 / 3, Step: 268 / 750 Loss: 0.7133\n",
      "Epoch: 1 / 3, Step: 269 / 750 Loss: 0.7061\n",
      "Epoch: 1 / 3, Step: 270 / 750 Loss: 0.5812\n",
      "Epoch: 1 / 3, Step: 271 / 750 Loss: 0.5741\n",
      "Epoch: 1 / 3, Step: 272 / 750 Loss: 0.6422\n",
      "Epoch: 1 / 3, Step: 273 / 750 Loss: 0.6533\n",
      "Epoch: 1 / 3, Step: 274 / 750 Loss: 0.7354\n",
      "Epoch: 1 / 3, Step: 275 / 750 Loss: 0.8040\n",
      "Epoch: 1 / 3, Step: 276 / 750 Loss: 0.8189\n",
      "Epoch: 1 / 3, Step: 277 / 750 Loss: 0.7695\n",
      "Epoch: 1 / 3, Step: 278 / 750 Loss: 0.6554\n",
      "Epoch: 1 / 3, Step: 279 / 750 Loss: 0.6413\n",
      "Epoch: 1 / 3, Step: 280 / 750 Loss: 0.5718\n",
      "Epoch: 1 / 3, Step: 281 / 750 Loss: 0.6629\n",
      "Epoch: 1 / 3, Step: 282 / 750 Loss: 0.5888\n",
      "Epoch: 1 / 3, Step: 283 / 750 Loss: 0.7452\n",
      "Epoch: 1 / 3, Step: 284 / 750 Loss: 0.8681\n",
      "Epoch: 1 / 3, Step: 285 / 750 Loss: 0.6516\n",
      "Epoch: 1 / 3, Step: 286 / 750 Loss: 0.4385\n",
      "Epoch: 1 / 3, Step: 287 / 750 Loss: 0.7754\n",
      "Epoch: 1 / 3, Step: 288 / 750 Loss: 0.4671\n",
      "Epoch: 1 / 3, Step: 289 / 750 Loss: 0.6066\n",
      "Epoch: 1 / 3, Step: 290 / 750 Loss: 0.8743\n",
      "Epoch: 1 / 3, Step: 291 / 750 Loss: 0.8415\n",
      "Epoch: 1 / 3, Step: 292 / 750 Loss: 0.7329\n",
      "Epoch: 1 / 3, Step: 293 / 750 Loss: 0.6876\n",
      "Epoch: 1 / 3, Step: 294 / 750 Loss: 0.6526\n",
      "Epoch: 1 / 3, Step: 295 / 750 Loss: 0.5755\n",
      "Epoch: 1 / 3, Step: 296 / 750 Loss: 0.6619\n",
      "Epoch: 1 / 3, Step: 297 / 750 Loss: 0.7400\n",
      "Epoch: 1 / 3, Step: 298 / 750 Loss: 0.5294\n",
      "Epoch: 1 / 3, Step: 299 / 750 Loss: 0.9230\n",
      "Epoch: 1 / 3, Step: 300 / 750 Loss: 0.4417\n",
      "Epoch: 1 / 3, Step: 301 / 750 Loss: 0.5863\n",
      "Epoch: 1 / 3, Step: 302 / 750 Loss: 0.6324\n",
      "Epoch: 1 / 3, Step: 303 / 750 Loss: 0.3657\n",
      "Epoch: 1 / 3, Step: 304 / 750 Loss: 0.6569\n",
      "Epoch: 1 / 3, Step: 305 / 750 Loss: 1.0795\n",
      "Epoch: 1 / 3, Step: 306 / 750 Loss: 0.8642\n",
      "Epoch: 1 / 3, Step: 307 / 750 Loss: 0.7369\n",
      "Epoch: 1 / 3, Step: 308 / 750 Loss: 0.7131\n",
      "Epoch: 1 / 3, Step: 309 / 750 Loss: 0.7677\n",
      "Epoch: 1 / 3, Step: 310 / 750 Loss: 0.6286\n",
      "Epoch: 1 / 3, Step: 311 / 750 Loss: 0.5625\n",
      "Epoch: 1 / 3, Step: 312 / 750 Loss: 0.5532\n",
      "Epoch: 1 / 3, Step: 313 / 750 Loss: 0.7691\n",
      "Epoch: 1 / 3, Step: 314 / 750 Loss: 0.7656\n",
      "Epoch: 1 / 3, Step: 315 / 750 Loss: 0.6705\n",
      "Epoch: 1 / 3, Step: 316 / 750 Loss: 0.7781\n",
      "Epoch: 1 / 3, Step: 317 / 750 Loss: 0.6238\n",
      "Epoch: 1 / 3, Step: 318 / 750 Loss: 0.6607\n",
      "Epoch: 1 / 3, Step: 319 / 750 Loss: 0.7300\n",
      "Epoch: 1 / 3, Step: 320 / 750 Loss: 0.7468\n",
      "Epoch: 1 / 3, Step: 321 / 750 Loss: 0.6715\n",
      "Epoch: 1 / 3, Step: 322 / 750 Loss: 0.6217\n",
      "Epoch: 1 / 3, Step: 323 / 750 Loss: 0.8075\n",
      "Epoch: 1 / 3, Step: 324 / 750 Loss: 0.7586\n",
      "Epoch: 1 / 3, Step: 325 / 750 Loss: 0.5847\n",
      "Epoch: 1 / 3, Step: 326 / 750 Loss: 0.5382\n",
      "Epoch: 1 / 3, Step: 327 / 750 Loss: 0.7344\n",
      "Epoch: 1 / 3, Step: 328 / 750 Loss: 0.5694\n",
      "Epoch: 1 / 3, Step: 329 / 750 Loss: 0.5077\n",
      "Epoch: 1 / 3, Step: 330 / 750 Loss: 0.4108\n",
      "Epoch: 1 / 3, Step: 331 / 750 Loss: 0.7702\n",
      "Epoch: 1 / 3, Step: 332 / 750 Loss: 0.6809\n",
      "Epoch: 1 / 3, Step: 333 / 750 Loss: 0.6521\n",
      "Epoch: 1 / 3, Step: 334 / 750 Loss: 0.6396\n",
      "Epoch: 1 / 3, Step: 335 / 750 Loss: 0.6775\n",
      "Epoch: 1 / 3, Step: 336 / 750 Loss: 0.6717\n",
      "Epoch: 1 / 3, Step: 337 / 750 Loss: 0.7901\n",
      "Epoch: 1 / 3, Step: 338 / 750 Loss: 0.5977\n",
      "Epoch: 1 / 3, Step: 339 / 750 Loss: 0.7417\n",
      "Epoch: 1 / 3, Step: 340 / 750 Loss: 0.5376\n",
      "Epoch: 1 / 3, Step: 341 / 750 Loss: 0.5717\n",
      "Epoch: 1 / 3, Step: 342 / 750 Loss: 0.4464\n",
      "Epoch: 1 / 3, Step: 343 / 750 Loss: 0.6413\n",
      "Epoch: 1 / 3, Step: 344 / 750 Loss: 0.6265\n",
      "Epoch: 1 / 3, Step: 345 / 750 Loss: 0.6582\n",
      "Epoch: 1 / 3, Step: 346 / 750 Loss: 0.8279\n",
      "Epoch: 1 / 3, Step: 347 / 750 Loss: 0.5603\n",
      "Epoch: 1 / 3, Step: 348 / 750 Loss: 0.8896\n",
      "Epoch: 1 / 3, Step: 349 / 750 Loss: 0.5161\n",
      "Epoch: 1 / 3, Step: 350 / 750 Loss: 0.7799\n",
      "Epoch: 1 / 3, Step: 351 / 750 Loss: 0.6492\n",
      "Epoch: 1 / 3, Step: 352 / 750 Loss: 0.7823\n",
      "Epoch: 1 / 3, Step: 353 / 750 Loss: 0.9229\n",
      "Epoch: 1 / 3, Step: 354 / 750 Loss: 0.6516\n",
      "Epoch: 1 / 3, Step: 355 / 750 Loss: 0.6176\n",
      "Epoch: 1 / 3, Step: 356 / 750 Loss: 0.6290\n",
      "Epoch: 1 / 3, Step: 357 / 750 Loss: 0.7711\n",
      "Epoch: 1 / 3, Step: 358 / 750 Loss: 0.6164\n",
      "Epoch: 1 / 3, Step: 359 / 750 Loss: 0.5939\n",
      "Epoch: 1 / 3, Step: 360 / 750 Loss: 0.7507\n",
      "Epoch: 1 / 3, Step: 361 / 750 Loss: 0.7717\n",
      "Epoch: 1 / 3, Step: 362 / 750 Loss: 0.4684\n",
      "Epoch: 1 / 3, Step: 363 / 750 Loss: 0.7601\n",
      "Epoch: 1 / 3, Step: 364 / 750 Loss: 0.6868\n",
      "Epoch: 1 / 3, Step: 365 / 750 Loss: 0.8832\n",
      "Epoch: 1 / 3, Step: 366 / 750 Loss: 0.7377\n",
      "Epoch: 1 / 3, Step: 367 / 750 Loss: 0.8602\n",
      "Epoch: 1 / 3, Step: 368 / 750 Loss: 0.5879\n",
      "Epoch: 1 / 3, Step: 369 / 750 Loss: 0.6372\n",
      "Epoch: 1 / 3, Step: 370 / 750 Loss: 0.7918\n",
      "Epoch: 1 / 3, Step: 371 / 750 Loss: 0.5158\n",
      "Epoch: 1 / 3, Step: 372 / 750 Loss: 0.7511\n",
      "Epoch: 1 / 3, Step: 373 / 750 Loss: 0.5979\n",
      "Epoch: 1 / 3, Step: 374 / 750 Loss: 0.6730\n",
      "Epoch: 1 / 3, Step: 375 / 750 Loss: 0.5185\n",
      "Epoch: 1 / 3, Step: 376 / 750 Loss: 0.5940\n",
      "Epoch: 1 / 3, Step: 377 / 750 Loss: 0.9010\n",
      "Epoch: 1 / 3, Step: 378 / 750 Loss: 0.6269\n",
      "Epoch: 1 / 3, Step: 379 / 750 Loss: 0.6815\n",
      "Epoch: 1 / 3, Step: 380 / 750 Loss: 0.7328\n",
      "Epoch: 1 / 3, Step: 381 / 750 Loss: 0.6025\n",
      "Epoch: 1 / 3, Step: 382 / 750 Loss: 0.8382\n",
      "Epoch: 1 / 3, Step: 383 / 750 Loss: 0.5766\n",
      "Epoch: 1 / 3, Step: 384 / 750 Loss: 0.7844\n",
      "Epoch: 1 / 3, Step: 385 / 750 Loss: 0.5886\n",
      "Epoch: 1 / 3, Step: 386 / 750 Loss: 0.5867\n",
      "Epoch: 1 / 3, Step: 387 / 750 Loss: 0.5706\n",
      "Epoch: 1 / 3, Step: 388 / 750 Loss: 0.6488\n",
      "Epoch: 1 / 3, Step: 389 / 750 Loss: 0.6589\n",
      "Epoch: 1 / 3, Step: 390 / 750 Loss: 0.6626\n",
      "Epoch: 1 / 3, Step: 391 / 750 Loss: 0.7912\n",
      "Epoch: 1 / 3, Step: 392 / 750 Loss: 0.7830\n",
      "Epoch: 1 / 3, Step: 393 / 750 Loss: 0.7667\n",
      "Epoch: 1 / 3, Step: 394 / 750 Loss: 0.8591\n",
      "Epoch: 1 / 3, Step: 395 / 750 Loss: 0.6091\n",
      "Epoch: 1 / 3, Step: 396 / 750 Loss: 0.4543\n",
      "Epoch: 1 / 3, Step: 397 / 750 Loss: 0.6571\n",
      "Epoch: 1 / 3, Step: 398 / 750 Loss: 0.7173\n",
      "Epoch: 1 / 3, Step: 399 / 750 Loss: 0.5356\n",
      "Epoch: 1 / 3, Step: 400 / 750 Loss: 0.5578\n",
      "Epoch: 1 / 3, Step: 401 / 750 Loss: 0.6203\n",
      "Epoch: 1 / 3, Step: 402 / 750 Loss: 0.7585\n",
      "Epoch: 1 / 3, Step: 403 / 750 Loss: 0.7974\n",
      "Epoch: 1 / 3, Step: 404 / 750 Loss: 0.8697\n",
      "Epoch: 1 / 3, Step: 405 / 750 Loss: 0.6568\n",
      "Epoch: 1 / 3, Step: 406 / 750 Loss: 0.5451\n",
      "Epoch: 1 / 3, Step: 407 / 750 Loss: 0.5797\n",
      "Epoch: 1 / 3, Step: 408 / 750 Loss: 0.5298\n",
      "Epoch: 1 / 3, Step: 409 / 750 Loss: 0.5230\n",
      "Epoch: 1 / 3, Step: 410 / 750 Loss: 0.6282\n",
      "Epoch: 1 / 3, Step: 411 / 750 Loss: 0.6592\n",
      "Epoch: 1 / 3, Step: 412 / 750 Loss: 0.5878\n",
      "Epoch: 1 / 3, Step: 413 / 750 Loss: 0.6538\n",
      "Epoch: 1 / 3, Step: 414 / 750 Loss: 0.7231\n",
      "Epoch: 1 / 3, Step: 415 / 750 Loss: 0.9177\n",
      "Epoch: 1 / 3, Step: 416 / 750 Loss: 0.8508\n",
      "Epoch: 1 / 3, Step: 417 / 750 Loss: 0.5404\n",
      "Epoch: 1 / 3, Step: 418 / 750 Loss: 0.7114\n",
      "Epoch: 1 / 3, Step: 419 / 750 Loss: 0.6952\n",
      "Epoch: 1 / 3, Step: 420 / 750 Loss: 0.4895\n",
      "Epoch: 1 / 3, Step: 421 / 750 Loss: 0.5885\n",
      "Epoch: 1 / 3, Step: 422 / 750 Loss: 0.5467\n",
      "Epoch: 1 / 3, Step: 423 / 750 Loss: 0.8158\n",
      "Epoch: 1 / 3, Step: 424 / 750 Loss: 0.5638\n",
      "Epoch: 1 / 3, Step: 425 / 750 Loss: 0.4920\n",
      "Epoch: 1 / 3, Step: 426 / 750 Loss: 0.7669\n",
      "Epoch: 1 / 3, Step: 427 / 750 Loss: 0.5712\n",
      "Epoch: 1 / 3, Step: 428 / 750 Loss: 0.3370\n",
      "Epoch: 1 / 3, Step: 429 / 750 Loss: 0.7978\n",
      "Epoch: 1 / 3, Step: 430 / 750 Loss: 0.7355\n",
      "Epoch: 1 / 3, Step: 431 / 750 Loss: 1.2066\n",
      "Epoch: 1 / 3, Step: 432 / 750 Loss: 0.5523\n",
      "Epoch: 1 / 3, Step: 433 / 750 Loss: 0.7002\n",
      "Epoch: 1 / 3, Step: 434 / 750 Loss: 0.9462\n",
      "Epoch: 1 / 3, Step: 435 / 750 Loss: 0.5579\n",
      "Epoch: 1 / 3, Step: 436 / 750 Loss: 0.5865\n",
      "Epoch: 1 / 3, Step: 437 / 750 Loss: 0.7071\n",
      "Epoch: 1 / 3, Step: 438 / 750 Loss: 0.5662\n",
      "Epoch: 1 / 3, Step: 439 / 750 Loss: 0.6084\n",
      "Epoch: 1 / 3, Step: 440 / 750 Loss: 0.8484\n",
      "Epoch: 1 / 3, Step: 441 / 750 Loss: 0.5510\n",
      "Epoch: 1 / 3, Step: 442 / 750 Loss: 0.5769\n",
      "Epoch: 1 / 3, Step: 443 / 750 Loss: 0.7473\n",
      "Epoch: 1 / 3, Step: 444 / 750 Loss: 0.8658\n",
      "Epoch: 1 / 3, Step: 445 / 750 Loss: 0.5502\n",
      "Epoch: 1 / 3, Step: 446 / 750 Loss: 0.7914\n",
      "Epoch: 1 / 3, Step: 447 / 750 Loss: 0.5195\n",
      "Epoch: 1 / 3, Step: 448 / 750 Loss: 0.6513\n",
      "Epoch: 1 / 3, Step: 449 / 750 Loss: 0.9123\n",
      "Epoch: 1 / 3, Step: 450 / 750 Loss: 0.8908\n",
      "Epoch: 1 / 3, Step: 451 / 750 Loss: 0.4798\n",
      "Epoch: 1 / 3, Step: 452 / 750 Loss: 0.7156\n",
      "Epoch: 1 / 3, Step: 453 / 750 Loss: 0.6917\n",
      "Epoch: 1 / 3, Step: 454 / 750 Loss: 0.6751\n",
      "Epoch: 1 / 3, Step: 455 / 750 Loss: 0.7733\n",
      "Epoch: 1 / 3, Step: 456 / 750 Loss: 0.6291\n",
      "Epoch: 1 / 3, Step: 457 / 750 Loss: 0.6881\n",
      "Epoch: 1 / 3, Step: 458 / 750 Loss: 0.7142\n",
      "Epoch: 1 / 3, Step: 459 / 750 Loss: 0.5418\n",
      "Epoch: 1 / 3, Step: 460 / 750 Loss: 0.8476\n",
      "Epoch: 1 / 3, Step: 461 / 750 Loss: 0.6986\n",
      "Epoch: 1 / 3, Step: 462 / 750 Loss: 0.8578\n",
      "Epoch: 1 / 3, Step: 463 / 750 Loss: 0.7430\n",
      "Epoch: 1 / 3, Step: 464 / 750 Loss: 0.4928\n",
      "Epoch: 1 / 3, Step: 465 / 750 Loss: 0.9520\n",
      "Epoch: 1 / 3, Step: 466 / 750 Loss: 0.5042\n",
      "Epoch: 1 / 3, Step: 467 / 750 Loss: 0.5115\n",
      "Epoch: 1 / 3, Step: 468 / 750 Loss: 0.6352\n",
      "Epoch: 1 / 3, Step: 469 / 750 Loss: 0.3821\n",
      "Epoch: 1 / 3, Step: 470 / 750 Loss: 0.5462\n",
      "Epoch: 1 / 3, Step: 471 / 750 Loss: 0.4057\n",
      "Epoch: 1 / 3, Step: 472 / 750 Loss: 0.6223\n",
      "Epoch: 1 / 3, Step: 473 / 750 Loss: 0.6847\n",
      "Epoch: 1 / 3, Step: 474 / 750 Loss: 0.8937\n",
      "Epoch: 1 / 3, Step: 475 / 750 Loss: 0.6034\n",
      "Epoch: 1 / 3, Step: 476 / 750 Loss: 0.9616\n",
      "Epoch: 1 / 3, Step: 477 / 750 Loss: 0.9698\n",
      "Epoch: 1 / 3, Step: 478 / 750 Loss: 0.7244\n",
      "Epoch: 1 / 3, Step: 479 / 750 Loss: 0.6060\n",
      "Epoch: 1 / 3, Step: 480 / 750 Loss: 0.6233\n",
      "Epoch: 1 / 3, Step: 481 / 750 Loss: 0.6509\n",
      "Epoch: 1 / 3, Step: 482 / 750 Loss: 0.7914\n",
      "Epoch: 1 / 3, Step: 483 / 750 Loss: 0.6108\n",
      "Epoch: 1 / 3, Step: 484 / 750 Loss: 0.6827\n",
      "Epoch: 1 / 3, Step: 485 / 750 Loss: 0.5866\n",
      "Epoch: 1 / 3, Step: 486 / 750 Loss: 0.7296\n",
      "Epoch: 1 / 3, Step: 487 / 750 Loss: 0.6172\n",
      "Epoch: 1 / 3, Step: 488 / 750 Loss: 0.6404\n",
      "Epoch: 1 / 3, Step: 489 / 750 Loss: 0.8404\n",
      "Epoch: 1 / 3, Step: 490 / 750 Loss: 0.5099\n",
      "Epoch: 1 / 3, Step: 491 / 750 Loss: 0.8111\n",
      "Epoch: 1 / 3, Step: 492 / 750 Loss: 0.7745\n",
      "Epoch: 1 / 3, Step: 493 / 750 Loss: 0.7254\n",
      "Epoch: 1 / 3, Step: 494 / 750 Loss: 0.7696\n",
      "Epoch: 1 / 3, Step: 495 / 750 Loss: 0.5344\n",
      "Epoch: 1 / 3, Step: 496 / 750 Loss: 0.7554\n",
      "Epoch: 1 / 3, Step: 497 / 750 Loss: 0.7201\n",
      "Epoch: 1 / 3, Step: 498 / 750 Loss: 0.8300\n",
      "Epoch: 1 / 3, Step: 499 / 750 Loss: 0.8136\n",
      "Epoch: 1 / 3, Step: 500 / 750 Loss: 0.6256\n",
      "Epoch: 1 / 3, Step: 501 / 750 Loss: 0.4821\n",
      "Epoch: 1 / 3, Step: 502 / 750 Loss: 0.6143\n",
      "Epoch: 1 / 3, Step: 503 / 750 Loss: 0.6528\n",
      "Epoch: 1 / 3, Step: 504 / 750 Loss: 0.8496\n",
      "Epoch: 1 / 3, Step: 505 / 750 Loss: 0.5638\n",
      "Epoch: 1 / 3, Step: 506 / 750 Loss: 0.8479\n",
      "Epoch: 1 / 3, Step: 507 / 750 Loss: 0.6173\n",
      "Epoch: 1 / 3, Step: 508 / 750 Loss: 0.5222\n",
      "Epoch: 1 / 3, Step: 509 / 750 Loss: 0.6715\n",
      "Epoch: 1 / 3, Step: 510 / 750 Loss: 0.3210\n",
      "Epoch: 1 / 3, Step: 511 / 750 Loss: 0.6059\n",
      "Epoch: 1 / 3, Step: 512 / 750 Loss: 0.3767\n",
      "Epoch: 1 / 3, Step: 513 / 750 Loss: 0.9471\n",
      "Epoch: 1 / 3, Step: 514 / 750 Loss: 0.8023\n",
      "Epoch: 1 / 3, Step: 515 / 750 Loss: 0.5802\n",
      "Epoch: 1 / 3, Step: 516 / 750 Loss: 0.6338\n",
      "Epoch: 1 / 3, Step: 517 / 750 Loss: 0.4879\n",
      "Epoch: 1 / 3, Step: 518 / 750 Loss: 0.5985\n",
      "Epoch: 1 / 3, Step: 519 / 750 Loss: 0.7038\n",
      "Epoch: 1 / 3, Step: 520 / 750 Loss: 0.6993\n",
      "Epoch: 1 / 3, Step: 521 / 750 Loss: 0.4387\n",
      "Epoch: 1 / 3, Step: 522 / 750 Loss: 0.8158\n",
      "Epoch: 1 / 3, Step: 523 / 750 Loss: 0.5429\n",
      "Epoch: 1 / 3, Step: 524 / 750 Loss: 0.6237\n",
      "Epoch: 1 / 3, Step: 525 / 750 Loss: 0.8426\n",
      "Epoch: 1 / 3, Step: 526 / 750 Loss: 0.5032\n",
      "Epoch: 1 / 3, Step: 527 / 750 Loss: 0.5185\n",
      "Epoch: 1 / 3, Step: 528 / 750 Loss: 0.6802\n",
      "Epoch: 1 / 3, Step: 529 / 750 Loss: 0.4832\n",
      "Epoch: 1 / 3, Step: 530 / 750 Loss: 0.9944\n",
      "Epoch: 1 / 3, Step: 531 / 750 Loss: 0.8364\n",
      "Epoch: 1 / 3, Step: 532 / 750 Loss: 0.5442\n",
      "Epoch: 1 / 3, Step: 533 / 750 Loss: 0.5873\n",
      "Epoch: 1 / 3, Step: 534 / 750 Loss: 0.9107\n",
      "Epoch: 1 / 3, Step: 535 / 750 Loss: 0.6926\n",
      "Epoch: 1 / 3, Step: 536 / 750 Loss: 0.6148\n",
      "Epoch: 1 / 3, Step: 537 / 750 Loss: 0.4818\n",
      "Epoch: 1 / 3, Step: 538 / 750 Loss: 0.6152\n",
      "Epoch: 1 / 3, Step: 539 / 750 Loss: 0.4893\n",
      "Epoch: 1 / 3, Step: 540 / 750 Loss: 0.6886\n",
      "Epoch: 1 / 3, Step: 541 / 750 Loss: 0.4561\n",
      "Epoch: 1 / 3, Step: 542 / 750 Loss: 0.4293\n",
      "Epoch: 1 / 3, Step: 543 / 750 Loss: 0.5689\n",
      "Epoch: 1 / 3, Step: 544 / 750 Loss: 0.6724\n",
      "Epoch: 1 / 3, Step: 545 / 750 Loss: 0.6244\n",
      "Epoch: 1 / 3, Step: 546 / 750 Loss: 0.5936\n",
      "Epoch: 1 / 3, Step: 547 / 750 Loss: 0.7807\n",
      "Epoch: 1 / 3, Step: 548 / 750 Loss: 0.7963\n",
      "Epoch: 1 / 3, Step: 549 / 750 Loss: 0.7251\n",
      "Epoch: 1 / 3, Step: 550 / 750 Loss: 0.7095\n",
      "Epoch: 1 / 3, Step: 551 / 750 Loss: 0.7860\n",
      "Epoch: 1 / 3, Step: 552 / 750 Loss: 0.6761\n",
      "Epoch: 1 / 3, Step: 553 / 750 Loss: 0.7273\n",
      "Epoch: 1 / 3, Step: 554 / 750 Loss: 0.7645\n",
      "Epoch: 1 / 3, Step: 555 / 750 Loss: 0.7313\n",
      "Epoch: 1 / 3, Step: 556 / 750 Loss: 0.7365\n",
      "Epoch: 1 / 3, Step: 557 / 750 Loss: 0.7211\n",
      "Epoch: 1 / 3, Step: 558 / 750 Loss: 0.5700\n",
      "Epoch: 1 / 3, Step: 559 / 750 Loss: 0.6409\n",
      "Epoch: 1 / 3, Step: 560 / 750 Loss: 0.4927\n",
      "Epoch: 1 / 3, Step: 561 / 750 Loss: 0.5985\n",
      "Epoch: 1 / 3, Step: 562 / 750 Loss: 0.6425\n",
      "Epoch: 1 / 3, Step: 563 / 750 Loss: 0.8577\n",
      "Epoch: 1 / 3, Step: 564 / 750 Loss: 0.5648\n",
      "Epoch: 1 / 3, Step: 565 / 750 Loss: 0.5634\n",
      "Epoch: 1 / 3, Step: 566 / 750 Loss: 0.9698\n",
      "Epoch: 1 / 3, Step: 567 / 750 Loss: 0.8761\n",
      "Epoch: 1 / 3, Step: 568 / 750 Loss: 0.9950\n",
      "Epoch: 1 / 3, Step: 569 / 750 Loss: 0.5958\n",
      "Epoch: 1 / 3, Step: 570 / 750 Loss: 0.5463\n",
      "Epoch: 1 / 3, Step: 571 / 750 Loss: 0.6692\n",
      "Epoch: 1 / 3, Step: 572 / 750 Loss: 0.7893\n",
      "Epoch: 1 / 3, Step: 573 / 750 Loss: 0.5715\n",
      "Epoch: 1 / 3, Step: 574 / 750 Loss: 0.6194\n",
      "Epoch: 1 / 3, Step: 575 / 750 Loss: 0.6150\n",
      "Epoch: 1 / 3, Step: 576 / 750 Loss: 1.0064\n",
      "Epoch: 1 / 3, Step: 577 / 750 Loss: 0.7910\n",
      "Epoch: 1 / 3, Step: 578 / 750 Loss: 0.4491\n",
      "Epoch: 1 / 3, Step: 579 / 750 Loss: 0.6167\n",
      "Epoch: 1 / 3, Step: 580 / 750 Loss: 0.7024\n",
      "Epoch: 1 / 3, Step: 581 / 750 Loss: 0.6694\n",
      "Epoch: 1 / 3, Step: 582 / 750 Loss: 0.5070\n",
      "Epoch: 1 / 3, Step: 583 / 750 Loss: 0.5957\n",
      "Epoch: 1 / 3, Step: 584 / 750 Loss: 0.4896\n",
      "Epoch: 1 / 3, Step: 585 / 750 Loss: 0.6045\n",
      "Epoch: 1 / 3, Step: 586 / 750 Loss: 0.5085\n",
      "Epoch: 1 / 3, Step: 587 / 750 Loss: 0.6630\n",
      "Epoch: 1 / 3, Step: 588 / 750 Loss: 0.6789\n",
      "Epoch: 1 / 3, Step: 589 / 750 Loss: 0.5742\n",
      "Epoch: 1 / 3, Step: 590 / 750 Loss: 0.4590\n",
      "Epoch: 1 / 3, Step: 591 / 750 Loss: 0.4432\n",
      "Epoch: 1 / 3, Step: 592 / 750 Loss: 0.9855\n",
      "Epoch: 1 / 3, Step: 593 / 750 Loss: 0.6795\n",
      "Epoch: 1 / 3, Step: 594 / 750 Loss: 0.8224\n",
      "Epoch: 1 / 3, Step: 595 / 750 Loss: 0.8944\n",
      "Epoch: 1 / 3, Step: 596 / 750 Loss: 1.0823\n",
      "Epoch: 1 / 3, Step: 597 / 750 Loss: 0.5479\n",
      "Epoch: 1 / 3, Step: 598 / 750 Loss: 0.4691\n",
      "Epoch: 1 / 3, Step: 599 / 750 Loss: 0.7054\n",
      "Epoch: 1 / 3, Step: 600 / 750 Loss: 0.5876\n",
      "Epoch: 1 / 3, Step: 601 / 750 Loss: 0.5155\n",
      "Epoch: 1 / 3, Step: 602 / 750 Loss: 0.7690\n",
      "Epoch: 1 / 3, Step: 603 / 750 Loss: 0.5664\n",
      "Epoch: 1 / 3, Step: 604 / 750 Loss: 0.6158\n",
      "Epoch: 1 / 3, Step: 605 / 750 Loss: 0.7443\n",
      "Epoch: 1 / 3, Step: 606 / 750 Loss: 0.5764\n",
      "Epoch: 1 / 3, Step: 607 / 750 Loss: 0.6968\n",
      "Epoch: 1 / 3, Step: 608 / 750 Loss: 0.7668\n",
      "Epoch: 1 / 3, Step: 609 / 750 Loss: 0.7625\n",
      "Epoch: 1 / 3, Step: 610 / 750 Loss: 0.8842\n",
      "Epoch: 1 / 3, Step: 611 / 750 Loss: 0.7060\n",
      "Epoch: 1 / 3, Step: 612 / 750 Loss: 0.7050\n",
      "Epoch: 1 / 3, Step: 613 / 750 Loss: 0.5444\n",
      "Epoch: 1 / 3, Step: 614 / 750 Loss: 0.7590\n",
      "Epoch: 1 / 3, Step: 615 / 750 Loss: 0.6705\n",
      "Epoch: 1 / 3, Step: 616 / 750 Loss: 0.8076\n",
      "Epoch: 1 / 3, Step: 617 / 750 Loss: 0.4726\n",
      "Epoch: 1 / 3, Step: 618 / 750 Loss: 0.6678\n",
      "Epoch: 1 / 3, Step: 619 / 750 Loss: 0.9276\n",
      "Epoch: 1 / 3, Step: 620 / 750 Loss: 0.6017\n",
      "Epoch: 1 / 3, Step: 621 / 750 Loss: 0.6315\n",
      "Epoch: 1 / 3, Step: 622 / 750 Loss: 0.4600\n",
      "Epoch: 1 / 3, Step: 623 / 750 Loss: 0.6330\n",
      "Epoch: 1 / 3, Step: 624 / 750 Loss: 0.6548\n",
      "Epoch: 1 / 3, Step: 625 / 750 Loss: 0.7371\n",
      "Epoch: 1 / 3, Step: 626 / 750 Loss: 0.8138\n",
      "Epoch: 1 / 3, Step: 627 / 750 Loss: 0.6559\n",
      "Epoch: 1 / 3, Step: 628 / 750 Loss: 0.7386\n",
      "Epoch: 1 / 3, Step: 629 / 750 Loss: 0.6659\n",
      "Epoch: 1 / 3, Step: 630 / 750 Loss: 0.7370\n",
      "Epoch: 1 / 3, Step: 631 / 750 Loss: 0.4572\n",
      "Epoch: 1 / 3, Step: 632 / 750 Loss: 0.5178\n",
      "Epoch: 1 / 3, Step: 633 / 750 Loss: 0.5770\n",
      "Epoch: 1 / 3, Step: 634 / 750 Loss: 0.7590\n",
      "Epoch: 1 / 3, Step: 635 / 750 Loss: 0.3627\n",
      "Epoch: 1 / 3, Step: 636 / 750 Loss: 0.7636\n",
      "Epoch: 1 / 3, Step: 637 / 750 Loss: 0.4394\n",
      "Epoch: 1 / 3, Step: 638 / 750 Loss: 0.4169\n",
      "Epoch: 1 / 3, Step: 639 / 750 Loss: 1.0283\n",
      "Epoch: 1 / 3, Step: 640 / 750 Loss: 0.4604\n",
      "Epoch: 1 / 3, Step: 641 / 750 Loss: 0.5319\n",
      "Epoch: 1 / 3, Step: 642 / 750 Loss: 0.8665\n",
      "Epoch: 1 / 3, Step: 643 / 750 Loss: 0.6579\n",
      "Epoch: 1 / 3, Step: 644 / 750 Loss: 0.6173\n",
      "Epoch: 1 / 3, Step: 645 / 750 Loss: 0.8492\n",
      "Epoch: 1 / 3, Step: 646 / 750 Loss: 0.3272\n",
      "Epoch: 1 / 3, Step: 647 / 750 Loss: 0.7473\n",
      "Epoch: 1 / 3, Step: 648 / 750 Loss: 0.6187\n",
      "Epoch: 1 / 3, Step: 649 / 750 Loss: 0.6043\n",
      "Epoch: 1 / 3, Step: 650 / 750 Loss: 0.5555\n",
      "Epoch: 1 / 3, Step: 651 / 750 Loss: 0.4492\n",
      "Epoch: 1 / 3, Step: 652 / 750 Loss: 0.9630\n",
      "Epoch: 1 / 3, Step: 653 / 750 Loss: 0.6141\n",
      "Epoch: 1 / 3, Step: 654 / 750 Loss: 0.7689\n",
      "Epoch: 1 / 3, Step: 655 / 750 Loss: 0.6504\n",
      "Epoch: 1 / 3, Step: 656 / 750 Loss: 0.8843\n",
      "Epoch: 1 / 3, Step: 657 / 750 Loss: 0.5773\n",
      "Epoch: 1 / 3, Step: 658 / 750 Loss: 0.6277\n",
      "Epoch: 1 / 3, Step: 659 / 750 Loss: 0.4101\n",
      "Epoch: 1 / 3, Step: 660 / 750 Loss: 0.6113\n",
      "Epoch: 1 / 3, Step: 661 / 750 Loss: 0.5654\n",
      "Epoch: 1 / 3, Step: 662 / 750 Loss: 0.7646\n",
      "Epoch: 1 / 3, Step: 663 / 750 Loss: 0.7612\n",
      "Epoch: 1 / 3, Step: 664 / 750 Loss: 0.6789\n",
      "Epoch: 1 / 3, Step: 665 / 750 Loss: 0.9129\n",
      "Epoch: 1 / 3, Step: 666 / 750 Loss: 0.8571\n",
      "Epoch: 1 / 3, Step: 667 / 750 Loss: 0.6681\n",
      "Epoch: 1 / 3, Step: 668 / 750 Loss: 0.8981\n",
      "Epoch: 1 / 3, Step: 669 / 750 Loss: 0.6708\n",
      "Epoch: 1 / 3, Step: 670 / 750 Loss: 0.6015\n",
      "Epoch: 1 / 3, Step: 671 / 750 Loss: 0.5136\n",
      "Epoch: 1 / 3, Step: 672 / 750 Loss: 0.7303\n",
      "Epoch: 1 / 3, Step: 673 / 750 Loss: 0.5711\n",
      "Epoch: 1 / 3, Step: 674 / 750 Loss: 0.6696\n",
      "Epoch: 1 / 3, Step: 675 / 750 Loss: 0.5078\n",
      "Epoch: 1 / 3, Step: 676 / 750 Loss: 0.7148\n",
      "Epoch: 1 / 3, Step: 677 / 750 Loss: 0.2557\n",
      "Epoch: 1 / 3, Step: 678 / 750 Loss: 0.7918\n",
      "Epoch: 1 / 3, Step: 679 / 750 Loss: 0.4574\n",
      "Epoch: 1 / 3, Step: 680 / 750 Loss: 0.8428\n",
      "Epoch: 1 / 3, Step: 681 / 750 Loss: 0.6153\n",
      "Epoch: 1 / 3, Step: 682 / 750 Loss: 0.6198\n",
      "Epoch: 1 / 3, Step: 683 / 750 Loss: 0.5086\n",
      "Epoch: 1 / 3, Step: 684 / 750 Loss: 0.4243\n",
      "Epoch: 1 / 3, Step: 685 / 750 Loss: 0.3950\n",
      "Epoch: 1 / 3, Step: 686 / 750 Loss: 0.7740\n",
      "Epoch: 1 / 3, Step: 687 / 750 Loss: 0.5362\n",
      "Epoch: 1 / 3, Step: 688 / 750 Loss: 0.7580\n",
      "Epoch: 1 / 3, Step: 689 / 750 Loss: 0.7721\n",
      "Epoch: 1 / 3, Step: 690 / 750 Loss: 0.7178\n",
      "Epoch: 1 / 3, Step: 691 / 750 Loss: 0.6972\n",
      "Epoch: 1 / 3, Step: 692 / 750 Loss: 0.5652\n",
      "Epoch: 1 / 3, Step: 693 / 750 Loss: 0.6964\n",
      "Epoch: 1 / 3, Step: 694 / 750 Loss: 0.8477\n",
      "Epoch: 1 / 3, Step: 695 / 750 Loss: 0.6451\n",
      "Epoch: 1 / 3, Step: 696 / 750 Loss: 0.6813\n",
      "Epoch: 1 / 3, Step: 697 / 750 Loss: 0.7344\n",
      "Epoch: 1 / 3, Step: 698 / 750 Loss: 0.7419\n",
      "Epoch: 1 / 3, Step: 699 / 750 Loss: 0.5325\n",
      "Epoch: 1 / 3, Step: 700 / 750 Loss: 0.7914\n",
      "Epoch: 1 / 3, Step: 701 / 750 Loss: 0.6553\n",
      "Epoch: 1 / 3, Step: 702 / 750 Loss: 0.6581\n",
      "Epoch: 1 / 3, Step: 703 / 750 Loss: 0.8474\n",
      "Epoch: 1 / 3, Step: 704 / 750 Loss: 0.7809\n",
      "Epoch: 1 / 3, Step: 705 / 750 Loss: 0.4750\n",
      "Epoch: 1 / 3, Step: 706 / 750 Loss: 0.8221\n",
      "Epoch: 1 / 3, Step: 707 / 750 Loss: 0.4982\n",
      "Epoch: 1 / 3, Step: 708 / 750 Loss: 0.6386\n",
      "Epoch: 1 / 3, Step: 709 / 750 Loss: 0.6398\n",
      "Epoch: 1 / 3, Step: 710 / 750 Loss: 0.6251\n",
      "Epoch: 1 / 3, Step: 711 / 750 Loss: 0.5423\n",
      "Epoch: 1 / 3, Step: 712 / 750 Loss: 0.5532\n",
      "Epoch: 1 / 3, Step: 713 / 750 Loss: 0.5211\n",
      "Epoch: 1 / 3, Step: 714 / 750 Loss: 0.4208\n",
      "Epoch: 1 / 3, Step: 715 / 750 Loss: 0.7305\n",
      "Epoch: 1 / 3, Step: 716 / 750 Loss: 0.4825\n",
      "Epoch: 1 / 3, Step: 717 / 750 Loss: 0.6166\n",
      "Epoch: 1 / 3, Step: 718 / 750 Loss: 0.6332\n",
      "Epoch: 1 / 3, Step: 719 / 750 Loss: 0.9806\n",
      "Epoch: 1 / 3, Step: 720 / 750 Loss: 0.4791\n",
      "Epoch: 1 / 3, Step: 721 / 750 Loss: 0.5560\n",
      "Epoch: 1 / 3, Step: 722 / 750 Loss: 0.4355\n",
      "Epoch: 1 / 3, Step: 723 / 750 Loss: 0.6047\n",
      "Epoch: 1 / 3, Step: 724 / 750 Loss: 0.6880\n",
      "Epoch: 1 / 3, Step: 725 / 750 Loss: 0.9384\n",
      "Epoch: 1 / 3, Step: 726 / 750 Loss: 0.6030\n",
      "Epoch: 1 / 3, Step: 727 / 750 Loss: 0.5273\n",
      "Epoch: 1 / 3, Step: 728 / 750 Loss: 0.5928\n",
      "Epoch: 1 / 3, Step: 729 / 750 Loss: 0.9130\n",
      "Epoch: 1 / 3, Step: 730 / 750 Loss: 0.6575\n",
      "Epoch: 1 / 3, Step: 731 / 750 Loss: 0.6243\n",
      "Epoch: 1 / 3, Step: 732 / 750 Loss: 0.5662\n",
      "Epoch: 1 / 3, Step: 733 / 750 Loss: 0.4925\n",
      "Epoch: 1 / 3, Step: 734 / 750 Loss: 0.5513\n",
      "Epoch: 1 / 3, Step: 735 / 750 Loss: 0.6622\n",
      "Epoch: 1 / 3, Step: 736 / 750 Loss: 0.7243\n",
      "Epoch: 1 / 3, Step: 737 / 750 Loss: 0.5096\n",
      "Epoch: 1 / 3, Step: 738 / 750 Loss: 0.4767\n",
      "Epoch: 1 / 3, Step: 739 / 750 Loss: 0.5013\n",
      "Epoch: 1 / 3, Step: 740 / 750 Loss: 0.4039\n",
      "Epoch: 1 / 3, Step: 741 / 750 Loss: 0.3330\n",
      "Epoch: 1 / 3, Step: 742 / 750 Loss: 1.0121\n",
      "Epoch: 1 / 3, Step: 743 / 750 Loss: 0.7126\n",
      "Epoch: 1 / 3, Step: 744 / 750 Loss: 0.4114\n",
      "Epoch: 1 / 3, Step: 745 / 750 Loss: 0.8276\n",
      "Epoch: 1 / 3, Step: 746 / 750 Loss: 0.6095\n",
      "Epoch: 1 / 3, Step: 747 / 750 Loss: 0.5780\n",
      "Epoch: 1 / 3, Step: 748 / 750 Loss: 0.7778\n",
      "Epoch: 1 / 3, Step: 749 / 750 Loss: 0.5246\n",
      "Epoch: 2 / 3, Step: 0 / 750 Loss: 0.7203\n",
      "Epoch: 2 / 3, Step: 1 / 750 Loss: 0.6278\n",
      "Epoch: 2 / 3, Step: 2 / 750 Loss: 0.7317\n",
      "Epoch: 2 / 3, Step: 3 / 750 Loss: 0.6543\n",
      "Epoch: 2 / 3, Step: 4 / 750 Loss: 0.6790\n",
      "Epoch: 2 / 3, Step: 5 / 750 Loss: 0.4671\n",
      "Epoch: 2 / 3, Step: 6 / 750 Loss: 0.8285\n",
      "Epoch: 2 / 3, Step: 7 / 750 Loss: 0.6954\n",
      "Epoch: 2 / 3, Step: 8 / 750 Loss: 0.6621\n",
      "Epoch: 2 / 3, Step: 9 / 750 Loss: 0.7385\n",
      "Epoch: 2 / 3, Step: 10 / 750 Loss: 0.7246\n",
      "Epoch: 2 / 3, Step: 11 / 750 Loss: 0.6134\n",
      "Epoch: 2 / 3, Step: 12 / 750 Loss: 0.5250\n",
      "Epoch: 2 / 3, Step: 13 / 750 Loss: 0.4929\n",
      "Epoch: 2 / 3, Step: 14 / 750 Loss: 0.6816\n",
      "Epoch: 2 / 3, Step: 15 / 750 Loss: 0.8745\n",
      "Epoch: 2 / 3, Step: 16 / 750 Loss: 0.8370\n",
      "Epoch: 2 / 3, Step: 17 / 750 Loss: 0.5926\n",
      "Epoch: 2 / 3, Step: 18 / 750 Loss: 0.5259\n",
      "Epoch: 2 / 3, Step: 19 / 750 Loss: 0.4664\n",
      "Epoch: 2 / 3, Step: 20 / 750 Loss: 0.8083\n",
      "Epoch: 2 / 3, Step: 21 / 750 Loss: 0.6211\n",
      "Epoch: 2 / 3, Step: 22 / 750 Loss: 0.5635\n",
      "Epoch: 2 / 3, Step: 23 / 750 Loss: 0.5433\n",
      "Epoch: 2 / 3, Step: 24 / 750 Loss: 0.3010\n",
      "Epoch: 2 / 3, Step: 25 / 750 Loss: 0.5494\n",
      "Epoch: 2 / 3, Step: 26 / 750 Loss: 0.5953\n",
      "Epoch: 2 / 3, Step: 27 / 750 Loss: 0.3371\n",
      "Epoch: 2 / 3, Step: 28 / 750 Loss: 0.8903\n",
      "Epoch: 2 / 3, Step: 29 / 750 Loss: 1.2572\n",
      "Epoch: 2 / 3, Step: 30 / 750 Loss: 0.2924\n",
      "Epoch: 2 / 3, Step: 31 / 750 Loss: 0.8683\n",
      "Epoch: 2 / 3, Step: 32 / 750 Loss: 0.8550\n",
      "Epoch: 2 / 3, Step: 33 / 750 Loss: 0.6260\n",
      "Epoch: 2 / 3, Step: 34 / 750 Loss: 0.7225\n",
      "Epoch: 2 / 3, Step: 35 / 750 Loss: 0.7379\n",
      "Epoch: 2 / 3, Step: 36 / 750 Loss: 0.5807\n",
      "Epoch: 2 / 3, Step: 37 / 750 Loss: 0.6656\n",
      "Epoch: 2 / 3, Step: 38 / 750 Loss: 0.6486\n",
      "Epoch: 2 / 3, Step: 39 / 750 Loss: 0.6386\n",
      "Epoch: 2 / 3, Step: 40 / 750 Loss: 0.3739\n",
      "Epoch: 2 / 3, Step: 41 / 750 Loss: 0.4753\n",
      "Epoch: 2 / 3, Step: 42 / 750 Loss: 0.5480\n",
      "Epoch: 2 / 3, Step: 43 / 750 Loss: 0.7612\n",
      "Epoch: 2 / 3, Step: 44 / 750 Loss: 0.7478\n",
      "Epoch: 2 / 3, Step: 45 / 750 Loss: 0.5248\n",
      "Epoch: 2 / 3, Step: 46 / 750 Loss: 0.6055\n",
      "Epoch: 2 / 3, Step: 47 / 750 Loss: 0.3980\n",
      "Epoch: 2 / 3, Step: 48 / 750 Loss: 0.7124\n",
      "Epoch: 2 / 3, Step: 49 / 750 Loss: 0.7365\n",
      "Epoch: 2 / 3, Step: 50 / 750 Loss: 0.7055\n",
      "Epoch: 2 / 3, Step: 51 / 750 Loss: 0.7304\n",
      "Epoch: 2 / 3, Step: 52 / 750 Loss: 0.8577\n",
      "Epoch: 2 / 3, Step: 53 / 750 Loss: 0.5415\n",
      "Epoch: 2 / 3, Step: 54 / 750 Loss: 0.6627\n",
      "Epoch: 2 / 3, Step: 55 / 750 Loss: 0.5206\n",
      "Epoch: 2 / 3, Step: 56 / 750 Loss: 0.6556\n",
      "Epoch: 2 / 3, Step: 57 / 750 Loss: 0.5996\n",
      "Epoch: 2 / 3, Step: 58 / 750 Loss: 0.7433\n",
      "Epoch: 2 / 3, Step: 59 / 750 Loss: 0.7055\n",
      "Epoch: 2 / 3, Step: 60 / 750 Loss: 0.6056\n",
      "Epoch: 2 / 3, Step: 61 / 750 Loss: 0.7135\n",
      "Epoch: 2 / 3, Step: 62 / 750 Loss: 0.5397\n",
      "Epoch: 2 / 3, Step: 63 / 750 Loss: 0.5416\n",
      "Epoch: 2 / 3, Step: 64 / 750 Loss: 0.6657\n",
      "Epoch: 2 / 3, Step: 65 / 750 Loss: 0.5833\n",
      "Epoch: 2 / 3, Step: 66 / 750 Loss: 0.8991\n",
      "Epoch: 2 / 3, Step: 67 / 750 Loss: 0.7099\n",
      "Epoch: 2 / 3, Step: 68 / 750 Loss: 0.4981\n",
      "Epoch: 2 / 3, Step: 69 / 750 Loss: 0.8166\n",
      "Epoch: 2 / 3, Step: 70 / 750 Loss: 0.8914\n",
      "Epoch: 2 / 3, Step: 71 / 750 Loss: 0.8043\n",
      "Epoch: 2 / 3, Step: 72 / 750 Loss: 0.8095\n",
      "Epoch: 2 / 3, Step: 73 / 750 Loss: 0.7023\n",
      "Epoch: 2 / 3, Step: 74 / 750 Loss: 0.6905\n",
      "Epoch: 2 / 3, Step: 75 / 750 Loss: 0.6479\n",
      "Epoch: 2 / 3, Step: 76 / 750 Loss: 0.5919\n",
      "Epoch: 2 / 3, Step: 77 / 750 Loss: 0.5786\n",
      "Epoch: 2 / 3, Step: 78 / 750 Loss: 0.5676\n",
      "Epoch: 2 / 3, Step: 79 / 750 Loss: 0.6403\n",
      "Epoch: 2 / 3, Step: 80 / 750 Loss: 0.4995\n",
      "Epoch: 2 / 3, Step: 81 / 750 Loss: 0.5303\n",
      "Epoch: 2 / 3, Step: 82 / 750 Loss: 0.7945\n",
      "Epoch: 2 / 3, Step: 83 / 750 Loss: 0.6429\n",
      "Epoch: 2 / 3, Step: 84 / 750 Loss: 0.7232\n",
      "Epoch: 2 / 3, Step: 85 / 750 Loss: 0.5248\n",
      "Epoch: 2 / 3, Step: 86 / 750 Loss: 0.6433\n",
      "Epoch: 2 / 3, Step: 87 / 750 Loss: 0.7192\n",
      "Epoch: 2 / 3, Step: 88 / 750 Loss: 0.6080\n",
      "Epoch: 2 / 3, Step: 89 / 750 Loss: 0.6991\n",
      "Epoch: 2 / 3, Step: 90 / 750 Loss: 0.4846\n",
      "Epoch: 2 / 3, Step: 91 / 750 Loss: 0.7636\n",
      "Epoch: 2 / 3, Step: 92 / 750 Loss: 0.6524\n",
      "Epoch: 2 / 3, Step: 93 / 750 Loss: 0.6116\n",
      "Epoch: 2 / 3, Step: 94 / 750 Loss: 0.3405\n",
      "Epoch: 2 / 3, Step: 95 / 750 Loss: 0.5247\n",
      "Epoch: 2 / 3, Step: 96 / 750 Loss: 0.6653\n",
      "Epoch: 2 / 3, Step: 97 / 750 Loss: 0.6540\n",
      "Epoch: 2 / 3, Step: 98 / 750 Loss: 0.3722\n",
      "Epoch: 2 / 3, Step: 99 / 750 Loss: 0.8321\n",
      "Epoch: 2 / 3, Step: 100 / 750 Loss: 0.4333\n",
      "Epoch: 2 / 3, Step: 101 / 750 Loss: 0.7876\n",
      "Epoch: 2 / 3, Step: 102 / 750 Loss: 0.4765\n",
      "Epoch: 2 / 3, Step: 103 / 750 Loss: 0.5869\n",
      "Epoch: 2 / 3, Step: 104 / 750 Loss: 0.7155\n",
      "Epoch: 2 / 3, Step: 105 / 750 Loss: 0.4978\n",
      "Epoch: 2 / 3, Step: 106 / 750 Loss: 0.7658\n",
      "Epoch: 2 / 3, Step: 107 / 750 Loss: 0.4234\n",
      "Epoch: 2 / 3, Step: 108 / 750 Loss: 0.8389\n",
      "Epoch: 2 / 3, Step: 109 / 750 Loss: 0.7764\n",
      "Epoch: 2 / 3, Step: 110 / 750 Loss: 0.4995\n",
      "Epoch: 2 / 3, Step: 111 / 750 Loss: 0.5894\n",
      "Epoch: 2 / 3, Step: 112 / 750 Loss: 0.6140\n",
      "Epoch: 2 / 3, Step: 113 / 750 Loss: 0.7380\n",
      "Epoch: 2 / 3, Step: 114 / 750 Loss: 0.9153\n",
      "Epoch: 2 / 3, Step: 115 / 750 Loss: 0.5760\n",
      "Epoch: 2 / 3, Step: 116 / 750 Loss: 0.5638\n",
      "Epoch: 2 / 3, Step: 117 / 750 Loss: 0.4795\n",
      "Epoch: 2 / 3, Step: 118 / 750 Loss: 0.7120\n",
      "Epoch: 2 / 3, Step: 119 / 750 Loss: 0.5038\n",
      "Epoch: 2 / 3, Step: 120 / 750 Loss: 0.4948\n",
      "Epoch: 2 / 3, Step: 121 / 750 Loss: 0.7059\n",
      "Epoch: 2 / 3, Step: 122 / 750 Loss: 0.5565\n",
      "Epoch: 2 / 3, Step: 123 / 750 Loss: 0.6767\n",
      "Epoch: 2 / 3, Step: 124 / 750 Loss: 0.6061\n",
      "Epoch: 2 / 3, Step: 125 / 750 Loss: 0.4640\n",
      "Epoch: 2 / 3, Step: 126 / 750 Loss: 0.5833\n",
      "Epoch: 2 / 3, Step: 127 / 750 Loss: 0.5364\n",
      "Epoch: 2 / 3, Step: 128 / 750 Loss: 0.7074\n",
      "Epoch: 2 / 3, Step: 129 / 750 Loss: 0.3989\n",
      "Epoch: 2 / 3, Step: 130 / 750 Loss: 0.5408\n",
      "Epoch: 2 / 3, Step: 131 / 750 Loss: 0.5511\n",
      "Epoch: 2 / 3, Step: 132 / 750 Loss: 0.5538\n",
      "Epoch: 2 / 3, Step: 133 / 750 Loss: 0.3537\n",
      "Epoch: 2 / 3, Step: 134 / 750 Loss: 0.5103\n",
      "Epoch: 2 / 3, Step: 135 / 750 Loss: 0.6468\n",
      "Epoch: 2 / 3, Step: 136 / 750 Loss: 0.2549\n",
      "Epoch: 2 / 3, Step: 137 / 750 Loss: 0.3357\n",
      "Epoch: 2 / 3, Step: 138 / 750 Loss: 0.8420\n",
      "Epoch: 2 / 3, Step: 139 / 750 Loss: 1.0929\n",
      "Epoch: 2 / 3, Step: 140 / 750 Loss: 0.4495\n",
      "Epoch: 2 / 3, Step: 141 / 750 Loss: 0.6473\n",
      "Epoch: 2 / 3, Step: 142 / 750 Loss: 0.7636\n",
      "Epoch: 2 / 3, Step: 143 / 750 Loss: 0.4304\n",
      "Epoch: 2 / 3, Step: 144 / 750 Loss: 0.4881\n",
      "Epoch: 2 / 3, Step: 145 / 750 Loss: 0.8515\n",
      "Epoch: 2 / 3, Step: 146 / 750 Loss: 0.9770\n",
      "Epoch: 2 / 3, Step: 147 / 750 Loss: 0.6727\n",
      "Epoch: 2 / 3, Step: 148 / 750 Loss: 0.7056\n",
      "Epoch: 2 / 3, Step: 149 / 750 Loss: 0.6278\n",
      "Epoch: 2 / 3, Step: 150 / 750 Loss: 0.5884\n",
      "Epoch: 2 / 3, Step: 151 / 750 Loss: 0.5434\n",
      "Epoch: 2 / 3, Step: 152 / 750 Loss: 0.8283\n",
      "Epoch: 2 / 3, Step: 153 / 750 Loss: 0.5935\n",
      "Epoch: 2 / 3, Step: 154 / 750 Loss: 0.5562\n",
      "Epoch: 2 / 3, Step: 155 / 750 Loss: 0.7136\n",
      "Epoch: 2 / 3, Step: 156 / 750 Loss: 0.6127\n",
      "Epoch: 2 / 3, Step: 157 / 750 Loss: 0.5427\n",
      "Epoch: 2 / 3, Step: 158 / 750 Loss: 0.6632\n",
      "Epoch: 2 / 3, Step: 159 / 750 Loss: 0.5144\n",
      "Epoch: 2 / 3, Step: 160 / 750 Loss: 0.3194\n",
      "Epoch: 2 / 3, Step: 161 / 750 Loss: 0.5104\n",
      "Epoch: 2 / 3, Step: 162 / 750 Loss: 0.4764\n",
      "Epoch: 2 / 3, Step: 163 / 750 Loss: 0.7495\n",
      "Epoch: 2 / 3, Step: 164 / 750 Loss: 0.5629\n",
      "Epoch: 2 / 3, Step: 165 / 750 Loss: 0.5809\n",
      "Epoch: 2 / 3, Step: 166 / 750 Loss: 0.6979\n",
      "Epoch: 2 / 3, Step: 167 / 750 Loss: 0.5550\n",
      "Epoch: 2 / 3, Step: 168 / 750 Loss: 0.6558\n",
      "Epoch: 2 / 3, Step: 169 / 750 Loss: 0.3880\n",
      "Epoch: 2 / 3, Step: 170 / 750 Loss: 0.6257\n",
      "Epoch: 2 / 3, Step: 171 / 750 Loss: 0.5618\n",
      "Epoch: 2 / 3, Step: 172 / 750 Loss: 0.4418\n",
      "Epoch: 2 / 3, Step: 173 / 750 Loss: 0.4337\n",
      "Epoch: 2 / 3, Step: 174 / 750 Loss: 0.6315\n",
      "Epoch: 2 / 3, Step: 175 / 750 Loss: 0.3234\n",
      "Epoch: 2 / 3, Step: 176 / 750 Loss: 0.3373\n",
      "Epoch: 2 / 3, Step: 177 / 750 Loss: 0.6767\n",
      "Epoch: 2 / 3, Step: 178 / 750 Loss: 0.5120\n",
      "Epoch: 2 / 3, Step: 179 / 750 Loss: 0.4330\n",
      "Epoch: 2 / 3, Step: 180 / 750 Loss: 0.2890\n",
      "Epoch: 2 / 3, Step: 181 / 750 Loss: 0.4883\n",
      "Epoch: 2 / 3, Step: 182 / 750 Loss: 0.8739\n",
      "Epoch: 2 / 3, Step: 183 / 750 Loss: 0.4974\n",
      "Epoch: 2 / 3, Step: 184 / 750 Loss: 0.4936\n",
      "Epoch: 2 / 3, Step: 185 / 750 Loss: 0.5263\n",
      "Epoch: 2 / 3, Step: 186 / 750 Loss: 0.6182\n",
      "Epoch: 2 / 3, Step: 187 / 750 Loss: 0.5027\n",
      "Epoch: 2 / 3, Step: 188 / 750 Loss: 0.4052\n",
      "Epoch: 2 / 3, Step: 189 / 750 Loss: 0.5414\n",
      "Epoch: 2 / 3, Step: 190 / 750 Loss: 0.6031\n",
      "Epoch: 2 / 3, Step: 191 / 750 Loss: 0.4437\n",
      "Epoch: 2 / 3, Step: 192 / 750 Loss: 0.3804\n",
      "Epoch: 2 / 3, Step: 193 / 750 Loss: 0.4895\n",
      "Epoch: 2 / 3, Step: 194 / 750 Loss: 0.3819\n",
      "Epoch: 2 / 3, Step: 195 / 750 Loss: 0.3588\n",
      "Epoch: 2 / 3, Step: 196 / 750 Loss: 0.5267\n",
      "Epoch: 2 / 3, Step: 197 / 750 Loss: 0.4800\n",
      "Epoch: 2 / 3, Step: 198 / 750 Loss: 0.2406\n",
      "Epoch: 2 / 3, Step: 199 / 750 Loss: 0.7298\n",
      "Epoch: 2 / 3, Step: 200 / 750 Loss: 0.5939\n",
      "Epoch: 2 / 3, Step: 201 / 750 Loss: 0.2943\n",
      "Epoch: 2 / 3, Step: 202 / 750 Loss: 0.5236\n",
      "Epoch: 2 / 3, Step: 203 / 750 Loss: 0.4403\n",
      "Epoch: 2 / 3, Step: 204 / 750 Loss: 0.5100\n",
      "Epoch: 2 / 3, Step: 205 / 750 Loss: 0.4621\n",
      "Epoch: 2 / 3, Step: 206 / 750 Loss: 0.3799\n",
      "Epoch: 2 / 3, Step: 207 / 750 Loss: 0.6555\n",
      "Epoch: 2 / 3, Step: 208 / 750 Loss: 0.4574\n",
      "Epoch: 2 / 3, Step: 209 / 750 Loss: 0.5922\n",
      "Epoch: 2 / 3, Step: 210 / 750 Loss: 0.5193\n",
      "Epoch: 2 / 3, Step: 211 / 750 Loss: 0.7907\n",
      "Epoch: 2 / 3, Step: 212 / 750 Loss: 0.4266\n",
      "Epoch: 2 / 3, Step: 213 / 750 Loss: 0.6600\n",
      "Epoch: 2 / 3, Step: 214 / 750 Loss: 0.3285\n",
      "Epoch: 2 / 3, Step: 215 / 750 Loss: 0.5405\n",
      "Epoch: 2 / 3, Step: 216 / 750 Loss: 0.5769\n",
      "Epoch: 2 / 3, Step: 217 / 750 Loss: 0.4634\n",
      "Epoch: 2 / 3, Step: 218 / 750 Loss: 0.5271\n",
      "Epoch: 2 / 3, Step: 219 / 750 Loss: 0.5200\n",
      "Epoch: 2 / 3, Step: 220 / 750 Loss: 0.3169\n",
      "Epoch: 2 / 3, Step: 221 / 750 Loss: 0.5573\n",
      "Epoch: 2 / 3, Step: 222 / 750 Loss: 0.6883\n",
      "Epoch: 2 / 3, Step: 223 / 750 Loss: 0.4724\n",
      "Epoch: 2 / 3, Step: 224 / 750 Loss: 0.5036\n",
      "Epoch: 2 / 3, Step: 225 / 750 Loss: 0.4050\n",
      "Epoch: 2 / 3, Step: 226 / 750 Loss: 0.3533\n",
      "Epoch: 2 / 3, Step: 227 / 750 Loss: 0.5874\n",
      "Epoch: 2 / 3, Step: 228 / 750 Loss: 0.4610\n",
      "Epoch: 2 / 3, Step: 229 / 750 Loss: 0.5213\n",
      "Epoch: 2 / 3, Step: 230 / 750 Loss: 0.4329\n",
      "Epoch: 2 / 3, Step: 231 / 750 Loss: 0.3768\n",
      "Epoch: 2 / 3, Step: 232 / 750 Loss: 0.6344\n",
      "Epoch: 2 / 3, Step: 233 / 750 Loss: 0.2268\n",
      "Epoch: 2 / 3, Step: 234 / 750 Loss: 0.4988\n",
      "Epoch: 2 / 3, Step: 235 / 750 Loss: 0.3560\n",
      "Epoch: 2 / 3, Step: 236 / 750 Loss: 0.6605\n",
      "Epoch: 2 / 3, Step: 237 / 750 Loss: 0.4469\n",
      "Epoch: 2 / 3, Step: 238 / 750 Loss: 0.2954\n",
      "Epoch: 2 / 3, Step: 239 / 750 Loss: 0.3869\n",
      "Epoch: 2 / 3, Step: 240 / 750 Loss: 0.5916\n",
      "Epoch: 2 / 3, Step: 241 / 750 Loss: 0.4030\n",
      "Epoch: 2 / 3, Step: 242 / 750 Loss: 0.4737\n",
      "Epoch: 2 / 3, Step: 243 / 750 Loss: 0.6103\n",
      "Epoch: 2 / 3, Step: 244 / 750 Loss: 0.6053\n",
      "Epoch: 2 / 3, Step: 245 / 750 Loss: 0.4197\n",
      "Epoch: 2 / 3, Step: 246 / 750 Loss: 0.4754\n",
      "Epoch: 2 / 3, Step: 247 / 750 Loss: 0.6971\n",
      "Epoch: 2 / 3, Step: 248 / 750 Loss: 0.4749\n",
      "Epoch: 2 / 3, Step: 249 / 750 Loss: 0.4423\n",
      "Epoch: 2 / 3, Step: 250 / 750 Loss: 0.3449\n",
      "Epoch: 2 / 3, Step: 251 / 750 Loss: 0.5264\n",
      "Epoch: 2 / 3, Step: 252 / 750 Loss: 0.2980\n",
      "Epoch: 2 / 3, Step: 253 / 750 Loss: 0.2990\n",
      "Epoch: 2 / 3, Step: 254 / 750 Loss: 0.5475\n",
      "Epoch: 2 / 3, Step: 255 / 750 Loss: 0.3737\n",
      "Epoch: 2 / 3, Step: 256 / 750 Loss: 0.3543\n",
      "Epoch: 2 / 3, Step: 257 / 750 Loss: 0.2688\n",
      "Epoch: 2 / 3, Step: 258 / 750 Loss: 0.5771\n",
      "Epoch: 2 / 3, Step: 259 / 750 Loss: 0.2623\n",
      "Epoch: 2 / 3, Step: 260 / 750 Loss: 0.5858\n",
      "Epoch: 2 / 3, Step: 261 / 750 Loss: 0.3846\n",
      "Epoch: 2 / 3, Step: 262 / 750 Loss: 0.4846\n",
      "Epoch: 2 / 3, Step: 263 / 750 Loss: 0.6301\n",
      "Epoch: 2 / 3, Step: 264 / 750 Loss: 0.3443\n",
      "Epoch: 2 / 3, Step: 265 / 750 Loss: 0.3091\n",
      "Epoch: 2 / 3, Step: 266 / 750 Loss: 0.4231\n",
      "Epoch: 2 / 3, Step: 267 / 750 Loss: 0.4516\n",
      "Epoch: 2 / 3, Step: 268 / 750 Loss: 0.6337\n",
      "Epoch: 2 / 3, Step: 269 / 750 Loss: 0.5230\n",
      "Epoch: 2 / 3, Step: 270 / 750 Loss: 0.3209\n",
      "Epoch: 2 / 3, Step: 271 / 750 Loss: 0.3262\n",
      "Epoch: 2 / 3, Step: 272 / 750 Loss: 0.3561\n",
      "Epoch: 2 / 3, Step: 273 / 750 Loss: 0.3230\n",
      "Epoch: 2 / 3, Step: 274 / 750 Loss: 0.6576\n",
      "Epoch: 2 / 3, Step: 275 / 750 Loss: 0.4026\n",
      "Epoch: 2 / 3, Step: 276 / 750 Loss: 0.3450\n",
      "Epoch: 2 / 3, Step: 277 / 750 Loss: 0.3514\n",
      "Epoch: 2 / 3, Step: 278 / 750 Loss: 0.3178\n",
      "Epoch: 2 / 3, Step: 279 / 750 Loss: 0.6009\n",
      "Epoch: 2 / 3, Step: 280 / 750 Loss: 0.6083\n",
      "Epoch: 2 / 3, Step: 281 / 750 Loss: 0.2934\n",
      "Epoch: 2 / 3, Step: 282 / 750 Loss: 0.5676\n",
      "Epoch: 2 / 3, Step: 283 / 750 Loss: 0.4071\n",
      "Epoch: 2 / 3, Step: 284 / 750 Loss: 0.3977\n",
      "Epoch: 2 / 3, Step: 285 / 750 Loss: 0.4885\n",
      "Epoch: 2 / 3, Step: 286 / 750 Loss: 0.3436\n",
      "Epoch: 2 / 3, Step: 287 / 750 Loss: 0.6594\n",
      "Epoch: 2 / 3, Step: 288 / 750 Loss: 0.4000\n",
      "Epoch: 2 / 3, Step: 289 / 750 Loss: 0.5007\n",
      "Epoch: 2 / 3, Step: 290 / 750 Loss: 0.3381\n",
      "Epoch: 2 / 3, Step: 291 / 750 Loss: 0.2481\n",
      "Epoch: 2 / 3, Step: 292 / 750 Loss: 0.2293\n",
      "Epoch: 2 / 3, Step: 293 / 750 Loss: 0.6499\n",
      "Epoch: 2 / 3, Step: 294 / 750 Loss: 0.1536\n",
      "Epoch: 2 / 3, Step: 295 / 750 Loss: 0.5371\n",
      "Epoch: 2 / 3, Step: 296 / 750 Loss: 0.5213\n",
      "Epoch: 2 / 3, Step: 297 / 750 Loss: 0.3515\n",
      "Epoch: 2 / 3, Step: 298 / 750 Loss: 0.3615\n",
      "Epoch: 2 / 3, Step: 299 / 750 Loss: 0.4616\n",
      "Epoch: 2 / 3, Step: 300 / 750 Loss: 0.3645\n",
      "Epoch: 2 / 3, Step: 301 / 750 Loss: 0.4641\n",
      "Epoch: 2 / 3, Step: 302 / 750 Loss: 0.4274\n",
      "Epoch: 2 / 3, Step: 303 / 750 Loss: 0.2958\n",
      "Epoch: 2 / 3, Step: 304 / 750 Loss: 0.5064\n",
      "Epoch: 2 / 3, Step: 305 / 750 Loss: 0.5838\n",
      "Epoch: 2 / 3, Step: 306 / 750 Loss: 0.5726\n",
      "Epoch: 2 / 3, Step: 307 / 750 Loss: 0.4651\n",
      "Epoch: 2 / 3, Step: 308 / 750 Loss: 0.2779\n",
      "Epoch: 2 / 3, Step: 309 / 750 Loss: 0.6598\n",
      "Epoch: 2 / 3, Step: 310 / 750 Loss: 0.3778\n",
      "Epoch: 2 / 3, Step: 311 / 750 Loss: 0.3860\n",
      "Epoch: 2 / 3, Step: 312 / 750 Loss: 0.4083\n",
      "Epoch: 2 / 3, Step: 313 / 750 Loss: 0.5088\n",
      "Epoch: 2 / 3, Step: 314 / 750 Loss: 0.3759\n",
      "Epoch: 2 / 3, Step: 315 / 750 Loss: 0.3010\n",
      "Epoch: 2 / 3, Step: 316 / 750 Loss: 0.7430\n",
      "Epoch: 2 / 3, Step: 317 / 750 Loss: 0.3822\n",
      "Epoch: 2 / 3, Step: 318 / 750 Loss: 0.3344\n",
      "Epoch: 2 / 3, Step: 319 / 750 Loss: 0.2903\n",
      "Epoch: 2 / 3, Step: 320 / 750 Loss: 0.4021\n",
      "Epoch: 2 / 3, Step: 321 / 750 Loss: 0.3715\n",
      "Epoch: 2 / 3, Step: 322 / 750 Loss: 0.3617\n",
      "Epoch: 2 / 3, Step: 323 / 750 Loss: 0.3138\n",
      "Epoch: 2 / 3, Step: 324 / 750 Loss: 0.3724\n",
      "Epoch: 2 / 3, Step: 325 / 750 Loss: 0.4354\n",
      "Epoch: 2 / 3, Step: 326 / 750 Loss: 0.3825\n",
      "Epoch: 2 / 3, Step: 327 / 750 Loss: 0.4025\n",
      "Epoch: 2 / 3, Step: 328 / 750 Loss: 0.2310\n",
      "Epoch: 2 / 3, Step: 329 / 750 Loss: 0.3594\n",
      "Epoch: 2 / 3, Step: 330 / 750 Loss: 0.5030\n",
      "Epoch: 2 / 3, Step: 331 / 750 Loss: 0.2938\n",
      "Epoch: 2 / 3, Step: 332 / 750 Loss: 0.4859\n",
      "Epoch: 2 / 3, Step: 333 / 750 Loss: 0.4021\n",
      "Epoch: 2 / 3, Step: 334 / 750 Loss: 0.2398\n",
      "Epoch: 2 / 3, Step: 335 / 750 Loss: 0.5197\n",
      "Epoch: 2 / 3, Step: 336 / 750 Loss: 0.4703\n",
      "Epoch: 2 / 3, Step: 337 / 750 Loss: 0.2129\n",
      "Epoch: 2 / 3, Step: 338 / 750 Loss: 0.3526\n",
      "Epoch: 2 / 3, Step: 339 / 750 Loss: 0.4102\n",
      "Epoch: 2 / 3, Step: 340 / 750 Loss: 0.4917\n",
      "Epoch: 2 / 3, Step: 341 / 750 Loss: 0.6078\n",
      "Epoch: 2 / 3, Step: 342 / 750 Loss: 0.2908\n",
      "Epoch: 2 / 3, Step: 343 / 750 Loss: 0.4412\n",
      "Epoch: 2 / 3, Step: 344 / 750 Loss: 0.2715\n",
      "Epoch: 2 / 3, Step: 345 / 750 Loss: 0.2903\n",
      "Epoch: 2 / 3, Step: 346 / 750 Loss: 0.6825\n",
      "Epoch: 2 / 3, Step: 347 / 750 Loss: 0.6579\n",
      "Epoch: 2 / 3, Step: 348 / 750 Loss: 0.2180\n",
      "Epoch: 2 / 3, Step: 349 / 750 Loss: 0.2621\n",
      "Epoch: 2 / 3, Step: 350 / 750 Loss: 0.4258\n",
      "Epoch: 2 / 3, Step: 351 / 750 Loss: 0.4198\n",
      "Epoch: 2 / 3, Step: 352 / 750 Loss: 0.5844\n",
      "Epoch: 2 / 3, Step: 353 / 750 Loss: 0.2644\n",
      "Epoch: 2 / 3, Step: 354 / 750 Loss: 0.1428\n",
      "Epoch: 2 / 3, Step: 355 / 750 Loss: 0.3331\n",
      "Epoch: 2 / 3, Step: 356 / 750 Loss: 0.4376\n",
      "Epoch: 2 / 3, Step: 357 / 750 Loss: 0.4728\n",
      "Epoch: 2 / 3, Step: 358 / 750 Loss: 0.4151\n",
      "Epoch: 2 / 3, Step: 359 / 750 Loss: 0.4982\n",
      "Epoch: 2 / 3, Step: 360 / 750 Loss: 0.4510\n",
      "Epoch: 2 / 3, Step: 361 / 750 Loss: 0.4962\n",
      "Epoch: 2 / 3, Step: 362 / 750 Loss: 0.6520\n",
      "Epoch: 2 / 3, Step: 363 / 750 Loss: 0.5214\n",
      "Epoch: 2 / 3, Step: 364 / 750 Loss: 0.3478\n",
      "Epoch: 2 / 3, Step: 365 / 750 Loss: 0.4851\n",
      "Epoch: 2 / 3, Step: 366 / 750 Loss: 0.4636\n",
      "Epoch: 2 / 3, Step: 367 / 750 Loss: 0.4274\n",
      "Epoch: 2 / 3, Step: 368 / 750 Loss: 0.4242\n",
      "Epoch: 2 / 3, Step: 369 / 750 Loss: 0.3676\n",
      "Epoch: 2 / 3, Step: 370 / 750 Loss: 0.3155\n",
      "Epoch: 2 / 3, Step: 371 / 750 Loss: 0.4374\n",
      "Epoch: 2 / 3, Step: 372 / 750 Loss: 0.3470\n",
      "Epoch: 2 / 3, Step: 373 / 750 Loss: 0.4557\n",
      "Epoch: 2 / 3, Step: 374 / 750 Loss: 0.4683\n",
      "Epoch: 2 / 3, Step: 375 / 750 Loss: 0.3343\n",
      "Epoch: 2 / 3, Step: 376 / 750 Loss: 0.4360\n",
      "Epoch: 2 / 3, Step: 377 / 750 Loss: 0.3123\n",
      "Epoch: 2 / 3, Step: 378 / 750 Loss: 0.6512\n",
      "Epoch: 2 / 3, Step: 379 / 750 Loss: 0.6108\n",
      "Epoch: 2 / 3, Step: 380 / 750 Loss: 0.4442\n",
      "Epoch: 2 / 3, Step: 381 / 750 Loss: 0.4643\n",
      "Epoch: 2 / 3, Step: 382 / 750 Loss: 0.4429\n",
      "Epoch: 2 / 3, Step: 383 / 750 Loss: 0.3215\n",
      "Epoch: 2 / 3, Step: 384 / 750 Loss: 0.2748\n",
      "Epoch: 2 / 3, Step: 385 / 750 Loss: 0.2193\n",
      "Epoch: 2 / 3, Step: 386 / 750 Loss: 0.5008\n",
      "Epoch: 2 / 3, Step: 387 / 750 Loss: 0.4677\n",
      "Epoch: 2 / 3, Step: 388 / 750 Loss: 0.2473\n",
      "Epoch: 2 / 3, Step: 389 / 750 Loss: 0.5545\n",
      "Epoch: 2 / 3, Step: 390 / 750 Loss: 0.2435\n",
      "Epoch: 2 / 3, Step: 391 / 750 Loss: 0.3351\n",
      "Epoch: 2 / 3, Step: 392 / 750 Loss: 0.2475\n",
      "Epoch: 2 / 3, Step: 393 / 750 Loss: 0.3159\n",
      "Epoch: 2 / 3, Step: 394 / 750 Loss: 0.5693\n",
      "Epoch: 2 / 3, Step: 395 / 750 Loss: 0.4279\n",
      "Epoch: 2 / 3, Step: 396 / 750 Loss: 0.4672\n",
      "Epoch: 2 / 3, Step: 397 / 750 Loss: 0.2990\n",
      "Epoch: 2 / 3, Step: 398 / 750 Loss: 0.6022\n",
      "Epoch: 2 / 3, Step: 399 / 750 Loss: 0.2253\n",
      "Epoch: 2 / 3, Step: 400 / 750 Loss: 0.4656\n",
      "Epoch: 2 / 3, Step: 401 / 750 Loss: 0.5106\n",
      "Epoch: 2 / 3, Step: 402 / 750 Loss: 0.2565\n",
      "Epoch: 2 / 3, Step: 403 / 750 Loss: 0.4693\n",
      "Epoch: 2 / 3, Step: 404 / 750 Loss: 0.4571\n",
      "Epoch: 2 / 3, Step: 405 / 750 Loss: 0.5491\n",
      "Epoch: 2 / 3, Step: 406 / 750 Loss: 0.3533\n",
      "Epoch: 2 / 3, Step: 407 / 750 Loss: 0.3822\n",
      "Epoch: 2 / 3, Step: 408 / 750 Loss: 0.5273\n",
      "Epoch: 2 / 3, Step: 409 / 750 Loss: 0.7942\n",
      "Epoch: 2 / 3, Step: 410 / 750 Loss: 0.2783\n",
      "Epoch: 2 / 3, Step: 411 / 750 Loss: 0.3980\n",
      "Epoch: 2 / 3, Step: 412 / 750 Loss: 0.4968\n",
      "Epoch: 2 / 3, Step: 413 / 750 Loss: 0.4073\n",
      "Epoch: 2 / 3, Step: 414 / 750 Loss: 0.2438\n",
      "Epoch: 2 / 3, Step: 415 / 750 Loss: 0.2783\n",
      "Epoch: 2 / 3, Step: 416 / 750 Loss: 0.3212\n",
      "Epoch: 2 / 3, Step: 417 / 750 Loss: 0.4277\n",
      "Epoch: 2 / 3, Step: 418 / 750 Loss: 0.3531\n",
      "Epoch: 2 / 3, Step: 419 / 750 Loss: 0.5004\n",
      "Epoch: 2 / 3, Step: 420 / 750 Loss: 0.3867\n",
      "Epoch: 2 / 3, Step: 421 / 750 Loss: 0.3147\n",
      "Epoch: 2 / 3, Step: 422 / 750 Loss: 0.3404\n",
      "Epoch: 2 / 3, Step: 423 / 750 Loss: 0.3190\n",
      "Epoch: 2 / 3, Step: 424 / 750 Loss: 0.5227\n",
      "Epoch: 2 / 3, Step: 425 / 750 Loss: 0.4049\n",
      "Epoch: 2 / 3, Step: 426 / 750 Loss: 0.2739\n",
      "Epoch: 2 / 3, Step: 427 / 750 Loss: 0.2244\n",
      "Epoch: 2 / 3, Step: 428 / 750 Loss: 0.4188\n",
      "Epoch: 2 / 3, Step: 429 / 750 Loss: 0.4883\n",
      "Epoch: 2 / 3, Step: 430 / 750 Loss: 0.4467\n",
      "Epoch: 2 / 3, Step: 431 / 750 Loss: 0.1994\n",
      "Epoch: 2 / 3, Step: 432 / 750 Loss: 0.1830\n",
      "Epoch: 2 / 3, Step: 433 / 750 Loss: 0.3808\n",
      "Epoch: 2 / 3, Step: 434 / 750 Loss: 0.3048\n",
      "Epoch: 2 / 3, Step: 435 / 750 Loss: 0.3606\n",
      "Epoch: 2 / 3, Step: 436 / 750 Loss: 0.4061\n",
      "Epoch: 2 / 3, Step: 437 / 750 Loss: 0.2559\n",
      "Epoch: 2 / 3, Step: 438 / 750 Loss: 0.5705\n",
      "Epoch: 2 / 3, Step: 439 / 750 Loss: 0.3373\n",
      "Epoch: 2 / 3, Step: 440 / 750 Loss: 0.8494\n",
      "Epoch: 2 / 3, Step: 441 / 750 Loss: 0.2848\n",
      "Epoch: 2 / 3, Step: 442 / 750 Loss: 0.3551\n",
      "Epoch: 2 / 3, Step: 443 / 750 Loss: 0.5107\n",
      "Epoch: 2 / 3, Step: 444 / 750 Loss: 0.4838\n",
      "Epoch: 2 / 3, Step: 445 / 750 Loss: 0.3794\n",
      "Epoch: 2 / 3, Step: 446 / 750 Loss: 0.5070\n",
      "Epoch: 2 / 3, Step: 447 / 750 Loss: 0.4282\n",
      "Epoch: 2 / 3, Step: 448 / 750 Loss: 0.2507\n",
      "Epoch: 2 / 3, Step: 449 / 750 Loss: 0.3343\n",
      "Epoch: 2 / 3, Step: 450 / 750 Loss: 0.4932\n",
      "Epoch: 2 / 3, Step: 451 / 750 Loss: 0.3657\n",
      "Epoch: 2 / 3, Step: 452 / 750 Loss: 0.3286\n",
      "Epoch: 2 / 3, Step: 453 / 750 Loss: 0.3428\n",
      "Epoch: 2 / 3, Step: 454 / 750 Loss: 0.3468\n",
      "Epoch: 2 / 3, Step: 455 / 750 Loss: 0.3771\n",
      "Epoch: 2 / 3, Step: 456 / 750 Loss: 0.6208\n",
      "Epoch: 2 / 3, Step: 457 / 750 Loss: 0.3105\n",
      "Epoch: 2 / 3, Step: 458 / 750 Loss: 0.3744\n",
      "Epoch: 2 / 3, Step: 459 / 750 Loss: 0.3735\n",
      "Epoch: 2 / 3, Step: 460 / 750 Loss: 0.4599\n",
      "Epoch: 2 / 3, Step: 461 / 750 Loss: 0.3408\n",
      "Epoch: 2 / 3, Step: 462 / 750 Loss: 0.3220\n",
      "Epoch: 2 / 3, Step: 463 / 750 Loss: 0.3284\n",
      "Epoch: 2 / 3, Step: 464 / 750 Loss: 0.3039\n",
      "Epoch: 2 / 3, Step: 465 / 750 Loss: 0.5469\n",
      "Epoch: 2 / 3, Step: 466 / 750 Loss: 0.4188\n",
      "Epoch: 2 / 3, Step: 467 / 750 Loss: 0.2637\n",
      "Epoch: 2 / 3, Step: 468 / 750 Loss: 0.4380\n",
      "Epoch: 2 / 3, Step: 469 / 750 Loss: 0.3950\n",
      "Epoch: 2 / 3, Step: 470 / 750 Loss: 0.7507\n",
      "Epoch: 2 / 3, Step: 471 / 750 Loss: 0.4743\n",
      "Epoch: 2 / 3, Step: 472 / 750 Loss: 0.4457\n",
      "Epoch: 2 / 3, Step: 473 / 750 Loss: 0.5414\n",
      "Epoch: 2 / 3, Step: 474 / 750 Loss: 0.3227\n",
      "Epoch: 2 / 3, Step: 475 / 750 Loss: 0.4193\n",
      "Epoch: 2 / 3, Step: 476 / 750 Loss: 0.6144\n",
      "Epoch: 2 / 3, Step: 477 / 750 Loss: 0.3879\n",
      "Epoch: 2 / 3, Step: 478 / 750 Loss: 0.4064\n",
      "Epoch: 2 / 3, Step: 479 / 750 Loss: 0.3565\n",
      "Epoch: 2 / 3, Step: 480 / 750 Loss: 0.3684\n",
      "Epoch: 2 / 3, Step: 481 / 750 Loss: 0.2889\n",
      "Epoch: 2 / 3, Step: 482 / 750 Loss: 0.3744\n",
      "Epoch: 2 / 3, Step: 483 / 750 Loss: 0.4698\n",
      "Epoch: 2 / 3, Step: 484 / 750 Loss: 0.1986\n",
      "Epoch: 2 / 3, Step: 485 / 750 Loss: 0.4603\n",
      "Epoch: 2 / 3, Step: 486 / 750 Loss: 0.4030\n",
      "Epoch: 2 / 3, Step: 487 / 750 Loss: 0.3278\n",
      "Epoch: 2 / 3, Step: 488 / 750 Loss: 0.5836\n",
      "Epoch: 2 / 3, Step: 489 / 750 Loss: 0.2815\n",
      "Epoch: 2 / 3, Step: 490 / 750 Loss: 0.2240\n",
      "Epoch: 2 / 3, Step: 491 / 750 Loss: 0.2071\n",
      "Epoch: 2 / 3, Step: 492 / 750 Loss: 0.5631\n",
      "Epoch: 2 / 3, Step: 493 / 750 Loss: 0.3462\n",
      "Epoch: 2 / 3, Step: 494 / 750 Loss: 0.6039\n",
      "Epoch: 2 / 3, Step: 495 / 750 Loss: 0.2797\n",
      "Epoch: 2 / 3, Step: 496 / 750 Loss: 0.3353\n",
      "Epoch: 2 / 3, Step: 497 / 750 Loss: 0.6332\n",
      "Epoch: 2 / 3, Step: 498 / 750 Loss: 0.2429\n",
      "Epoch: 2 / 3, Step: 499 / 750 Loss: 0.2190\n",
      "Epoch: 2 / 3, Step: 500 / 750 Loss: 0.3379\n",
      "Epoch: 2 / 3, Step: 501 / 750 Loss: 0.4128\n",
      "Epoch: 2 / 3, Step: 502 / 750 Loss: 0.2528\n",
      "Epoch: 2 / 3, Step: 503 / 750 Loss: 0.3446\n",
      "Epoch: 2 / 3, Step: 504 / 750 Loss: 0.6474\n",
      "Epoch: 2 / 3, Step: 505 / 750 Loss: 0.2614\n",
      "Epoch: 2 / 3, Step: 506 / 750 Loss: 0.2333\n",
      "Epoch: 2 / 3, Step: 507 / 750 Loss: 0.2707\n",
      "Epoch: 2 / 3, Step: 508 / 750 Loss: 0.2292\n",
      "Epoch: 2 / 3, Step: 509 / 750 Loss: 0.3590\n",
      "Epoch: 2 / 3, Step: 510 / 750 Loss: 0.3282\n",
      "Epoch: 2 / 3, Step: 511 / 750 Loss: 0.3551\n",
      "Epoch: 2 / 3, Step: 512 / 750 Loss: 0.4860\n",
      "Epoch: 2 / 3, Step: 513 / 750 Loss: 0.2533\n",
      "Epoch: 2 / 3, Step: 514 / 750 Loss: 0.3271\n",
      "Epoch: 2 / 3, Step: 515 / 750 Loss: 0.2647\n",
      "Epoch: 2 / 3, Step: 516 / 750 Loss: 0.2386\n",
      "Epoch: 2 / 3, Step: 517 / 750 Loss: 0.2477\n",
      "Epoch: 2 / 3, Step: 518 / 750 Loss: 0.2155\n",
      "Epoch: 2 / 3, Step: 519 / 750 Loss: 0.2681\n",
      "Epoch: 2 / 3, Step: 520 / 750 Loss: 0.3656\n",
      "Epoch: 2 / 3, Step: 521 / 750 Loss: 0.3162\n",
      "Epoch: 2 / 3, Step: 522 / 750 Loss: 0.5039\n",
      "Epoch: 2 / 3, Step: 523 / 750 Loss: 0.4758\n",
      "Epoch: 2 / 3, Step: 524 / 750 Loss: 0.3927\n",
      "Epoch: 2 / 3, Step: 525 / 750 Loss: 0.2172\n",
      "Epoch: 2 / 3, Step: 526 / 750 Loss: 0.3775\n",
      "Epoch: 2 / 3, Step: 527 / 750 Loss: 0.2024\n",
      "Epoch: 2 / 3, Step: 528 / 750 Loss: 0.2232\n",
      "Epoch: 2 / 3, Step: 529 / 750 Loss: 0.3278\n",
      "Epoch: 2 / 3, Step: 530 / 750 Loss: 0.4135\n",
      "Epoch: 2 / 3, Step: 531 / 750 Loss: 0.3654\n",
      "Epoch: 2 / 3, Step: 532 / 750 Loss: 0.5295\n",
      "Epoch: 2 / 3, Step: 533 / 750 Loss: 0.3441\n",
      "Epoch: 2 / 3, Step: 534 / 750 Loss: 0.6479\n",
      "Epoch: 2 / 3, Step: 535 / 750 Loss: 0.4847\n",
      "Epoch: 2 / 3, Step: 536 / 750 Loss: 0.3903\n",
      "Epoch: 2 / 3, Step: 537 / 750 Loss: 0.3071\n",
      "Epoch: 2 / 3, Step: 538 / 750 Loss: 0.2586\n",
      "Epoch: 2 / 3, Step: 539 / 750 Loss: 0.6378\n",
      "Epoch: 2 / 3, Step: 540 / 750 Loss: 0.4431\n",
      "Epoch: 2 / 3, Step: 541 / 750 Loss: 0.1742\n",
      "Epoch: 2 / 3, Step: 542 / 750 Loss: 0.3764\n",
      "Epoch: 2 / 3, Step: 543 / 750 Loss: 0.2684\n",
      "Epoch: 2 / 3, Step: 544 / 750 Loss: 0.4152\n",
      "Epoch: 2 / 3, Step: 545 / 750 Loss: 0.7479\n",
      "Epoch: 2 / 3, Step: 546 / 750 Loss: 0.3957\n",
      "Epoch: 2 / 3, Step: 547 / 750 Loss: 0.2113\n",
      "Epoch: 2 / 3, Step: 548 / 750 Loss: 0.2312\n",
      "Epoch: 2 / 3, Step: 549 / 750 Loss: 0.2722\n",
      "Epoch: 2 / 3, Step: 550 / 750 Loss: 0.3769\n",
      "Epoch: 2 / 3, Step: 551 / 750 Loss: 0.3140\n",
      "Epoch: 2 / 3, Step: 552 / 750 Loss: 0.3582\n",
      "Epoch: 2 / 3, Step: 553 / 750 Loss: 0.2406\n",
      "Epoch: 2 / 3, Step: 554 / 750 Loss: 0.1292\n",
      "Epoch: 2 / 3, Step: 555 / 750 Loss: 0.5265\n",
      "Epoch: 2 / 3, Step: 556 / 750 Loss: 0.2689\n",
      "Epoch: 2 / 3, Step: 557 / 750 Loss: 0.3388\n",
      "Epoch: 2 / 3, Step: 558 / 750 Loss: 0.3978\n",
      "Epoch: 2 / 3, Step: 559 / 750 Loss: 0.5904\n",
      "Epoch: 2 / 3, Step: 560 / 750 Loss: 0.2800\n",
      "Epoch: 2 / 3, Step: 561 / 750 Loss: 0.3390\n",
      "Epoch: 2 / 3, Step: 562 / 750 Loss: 0.5173\n",
      "Epoch: 2 / 3, Step: 563 / 750 Loss: 0.2025\n",
      "Epoch: 2 / 3, Step: 564 / 750 Loss: 0.2596\n",
      "Epoch: 2 / 3, Step: 565 / 750 Loss: 0.4070\n",
      "Epoch: 2 / 3, Step: 566 / 750 Loss: 0.2426\n",
      "Epoch: 2 / 3, Step: 567 / 750 Loss: 0.5342\n",
      "Epoch: 2 / 3, Step: 568 / 750 Loss: 0.4375\n",
      "Epoch: 2 / 3, Step: 569 / 750 Loss: 0.5006\n",
      "Epoch: 2 / 3, Step: 570 / 750 Loss: 0.1428\n",
      "Epoch: 2 / 3, Step: 571 / 750 Loss: 0.4205\n",
      "Epoch: 2 / 3, Step: 572 / 750 Loss: 0.3888\n",
      "Epoch: 2 / 3, Step: 573 / 750 Loss: 0.3038\n",
      "Epoch: 2 / 3, Step: 574 / 750 Loss: 0.2467\n",
      "Epoch: 2 / 3, Step: 575 / 750 Loss: 0.2747\n",
      "Epoch: 2 / 3, Step: 576 / 750 Loss: 0.2127\n",
      "Epoch: 2 / 3, Step: 577 / 750 Loss: 0.4386\n",
      "Epoch: 2 / 3, Step: 578 / 750 Loss: 0.5338\n",
      "Epoch: 2 / 3, Step: 579 / 750 Loss: 0.3638\n",
      "Epoch: 2 / 3, Step: 580 / 750 Loss: 0.3752\n",
      "Epoch: 2 / 3, Step: 581 / 750 Loss: 0.5458\n",
      "Epoch: 2 / 3, Step: 582 / 750 Loss: 0.1743\n",
      "Epoch: 2 / 3, Step: 583 / 750 Loss: 0.2504\n",
      "Epoch: 2 / 3, Step: 584 / 750 Loss: 0.3874\n",
      "Epoch: 2 / 3, Step: 585 / 750 Loss: 0.3114\n",
      "Epoch: 2 / 3, Step: 586 / 750 Loss: 0.9318\n",
      "Epoch: 2 / 3, Step: 587 / 750 Loss: 1.0504\n",
      "Epoch: 2 / 3, Step: 588 / 750 Loss: 0.3574\n",
      "Epoch: 2 / 3, Step: 589 / 750 Loss: 0.2264\n",
      "Epoch: 2 / 3, Step: 590 / 750 Loss: 0.1939\n",
      "Epoch: 2 / 3, Step: 591 / 750 Loss: 0.2296\n",
      "Epoch: 2 / 3, Step: 592 / 750 Loss: 0.7724\n",
      "Epoch: 2 / 3, Step: 593 / 750 Loss: 0.5226\n",
      "Epoch: 2 / 3, Step: 594 / 750 Loss: 0.3642\n",
      "Epoch: 2 / 3, Step: 595 / 750 Loss: 0.4994\n",
      "Epoch: 2 / 3, Step: 596 / 750 Loss: 0.6140\n",
      "Epoch: 2 / 3, Step: 597 / 750 Loss: 0.2590\n",
      "Epoch: 2 / 3, Step: 598 / 750 Loss: 0.2379\n",
      "Epoch: 2 / 3, Step: 599 / 750 Loss: 0.5288\n",
      "Epoch: 2 / 3, Step: 600 / 750 Loss: 0.3579\n",
      "Epoch: 2 / 3, Step: 601 / 750 Loss: 0.3302\n",
      "Epoch: 2 / 3, Step: 602 / 750 Loss: 0.3448\n",
      "Epoch: 2 / 3, Step: 603 / 750 Loss: 0.2410\n",
      "Epoch: 2 / 3, Step: 604 / 750 Loss: 0.3052\n",
      "Epoch: 2 / 3, Step: 605 / 750 Loss: 0.4602\n",
      "Epoch: 2 / 3, Step: 606 / 750 Loss: 0.5314\n",
      "Epoch: 2 / 3, Step: 607 / 750 Loss: 0.4323\n",
      "Epoch: 2 / 3, Step: 608 / 750 Loss: 0.3574\n",
      "Epoch: 2 / 3, Step: 609 / 750 Loss: 0.1822\n",
      "Epoch: 2 / 3, Step: 610 / 750 Loss: 0.3263\n",
      "Epoch: 2 / 3, Step: 611 / 750 Loss: 0.2571\n",
      "Epoch: 2 / 3, Step: 612 / 750 Loss: 0.1975\n",
      "Epoch: 2 / 3, Step: 613 / 750 Loss: 0.2581\n",
      "Epoch: 2 / 3, Step: 614 / 750 Loss: 0.4246\n",
      "Epoch: 2 / 3, Step: 615 / 750 Loss: 0.3354\n",
      "Epoch: 2 / 3, Step: 616 / 750 Loss: 0.3617\n",
      "Epoch: 2 / 3, Step: 617 / 750 Loss: 0.4266\n",
      "Epoch: 2 / 3, Step: 618 / 750 Loss: 0.1565\n",
      "Epoch: 2 / 3, Step: 619 / 750 Loss: 0.2226\n",
      "Epoch: 2 / 3, Step: 620 / 750 Loss: 0.3412\n",
      "Epoch: 2 / 3, Step: 621 / 750 Loss: 0.2359\n",
      "Epoch: 2 / 3, Step: 622 / 750 Loss: 0.2556\n",
      "Epoch: 2 / 3, Step: 623 / 750 Loss: 0.0734\n",
      "Epoch: 2 / 3, Step: 624 / 750 Loss: 0.1453\n",
      "Epoch: 2 / 3, Step: 625 / 750 Loss: 0.1613\n",
      "Epoch: 2 / 3, Step: 626 / 750 Loss: 0.3769\n",
      "Epoch: 2 / 3, Step: 627 / 750 Loss: 0.5450\n",
      "Epoch: 2 / 3, Step: 628 / 750 Loss: 0.3683\n",
      "Epoch: 2 / 3, Step: 629 / 750 Loss: 0.3170\n",
      "Epoch: 2 / 3, Step: 630 / 750 Loss: 0.4489\n",
      "Epoch: 2 / 3, Step: 631 / 750 Loss: 0.3454\n",
      "Epoch: 2 / 3, Step: 632 / 750 Loss: 0.7307\n",
      "Epoch: 2 / 3, Step: 633 / 750 Loss: 0.6724\n",
      "Epoch: 2 / 3, Step: 634 / 750 Loss: 0.4864\n",
      "Epoch: 2 / 3, Step: 635 / 750 Loss: 0.4867\n",
      "Epoch: 2 / 3, Step: 636 / 750 Loss: 0.3808\n",
      "Epoch: 2 / 3, Step: 637 / 750 Loss: 0.4969\n",
      "Epoch: 2 / 3, Step: 638 / 750 Loss: 0.6303\n",
      "Epoch: 2 / 3, Step: 639 / 750 Loss: 0.2390\n",
      "Epoch: 2 / 3, Step: 640 / 750 Loss: 0.5613\n",
      "Epoch: 2 / 3, Step: 641 / 750 Loss: 0.4193\n",
      "Epoch: 2 / 3, Step: 642 / 750 Loss: 0.1971\n",
      "Epoch: 2 / 3, Step: 643 / 750 Loss: 0.5942\n",
      "Epoch: 2 / 3, Step: 644 / 750 Loss: 0.4240\n",
      "Epoch: 2 / 3, Step: 645 / 750 Loss: 0.1425\n",
      "Epoch: 2 / 3, Step: 646 / 750 Loss: 0.5446\n",
      "Epoch: 2 / 3, Step: 647 / 750 Loss: 0.4221\n",
      "Epoch: 2 / 3, Step: 648 / 750 Loss: 0.5303\n",
      "Epoch: 2 / 3, Step: 649 / 750 Loss: 0.6240\n",
      "Epoch: 2 / 3, Step: 650 / 750 Loss: 0.3945\n",
      "Epoch: 2 / 3, Step: 651 / 750 Loss: 0.4902\n",
      "Epoch: 2 / 3, Step: 652 / 750 Loss: 0.4973\n",
      "Epoch: 2 / 3, Step: 653 / 750 Loss: 0.3654\n",
      "Epoch: 2 / 3, Step: 654 / 750 Loss: 0.4736\n",
      "Epoch: 2 / 3, Step: 655 / 750 Loss: 0.3367\n",
      "Epoch: 2 / 3, Step: 656 / 750 Loss: 0.5582\n",
      "Epoch: 2 / 3, Step: 657 / 750 Loss: 0.3993\n",
      "Epoch: 2 / 3, Step: 658 / 750 Loss: 0.5421\n",
      "Epoch: 2 / 3, Step: 659 / 750 Loss: 0.2525\n",
      "Epoch: 2 / 3, Step: 660 / 750 Loss: 0.3878\n",
      "Epoch: 2 / 3, Step: 661 / 750 Loss: 0.5254\n",
      "Epoch: 2 / 3, Step: 662 / 750 Loss: 0.5405\n",
      "Epoch: 2 / 3, Step: 663 / 750 Loss: 0.3731\n",
      "Epoch: 2 / 3, Step: 664 / 750 Loss: 0.2375\n",
      "Epoch: 2 / 3, Step: 665 / 750 Loss: 0.2289\n",
      "Epoch: 2 / 3, Step: 666 / 750 Loss: 0.4407\n",
      "Epoch: 2 / 3, Step: 667 / 750 Loss: 0.5014\n",
      "Epoch: 2 / 3, Step: 668 / 750 Loss: 0.4423\n",
      "Epoch: 2 / 3, Step: 669 / 750 Loss: 0.3580\n",
      "Epoch: 2 / 3, Step: 670 / 750 Loss: 0.2856\n",
      "Epoch: 2 / 3, Step: 671 / 750 Loss: 0.4751\n",
      "Epoch: 2 / 3, Step: 672 / 750 Loss: 0.2641\n",
      "Epoch: 2 / 3, Step: 673 / 750 Loss: 0.4026\n",
      "Epoch: 2 / 3, Step: 674 / 750 Loss: 0.1497\n",
      "Epoch: 2 / 3, Step: 675 / 750 Loss: 0.2012\n",
      "Epoch: 2 / 3, Step: 676 / 750 Loss: 0.2521\n",
      "Epoch: 2 / 3, Step: 677 / 750 Loss: 0.3507\n",
      "Epoch: 2 / 3, Step: 678 / 750 Loss: 0.5281\n",
      "Epoch: 2 / 3, Step: 679 / 750 Loss: 0.5274\n",
      "Epoch: 2 / 3, Step: 680 / 750 Loss: 0.3515\n",
      "Epoch: 2 / 3, Step: 681 / 750 Loss: 0.2630\n",
      "Epoch: 2 / 3, Step: 682 / 750 Loss: 0.2443\n",
      "Epoch: 2 / 3, Step: 683 / 750 Loss: 0.3728\n",
      "Epoch: 2 / 3, Step: 684 / 750 Loss: 0.2904\n",
      "Epoch: 2 / 3, Step: 685 / 750 Loss: 0.3973\n",
      "Epoch: 2 / 3, Step: 686 / 750 Loss: 0.4350\n",
      "Epoch: 2 / 3, Step: 687 / 750 Loss: 0.2437\n",
      "Epoch: 2 / 3, Step: 688 / 750 Loss: 0.1992\n",
      "Epoch: 2 / 3, Step: 689 / 750 Loss: 0.3163\n",
      "Epoch: 2 / 3, Step: 690 / 750 Loss: 0.2633\n",
      "Epoch: 2 / 3, Step: 691 / 750 Loss: 0.1468\n",
      "Epoch: 2 / 3, Step: 692 / 750 Loss: 0.1687\n",
      "Epoch: 2 / 3, Step: 693 / 750 Loss: 0.2979\n",
      "Epoch: 2 / 3, Step: 694 / 750 Loss: 0.2373\n",
      "Epoch: 2 / 3, Step: 695 / 750 Loss: 0.3503\n",
      "Epoch: 2 / 3, Step: 696 / 750 Loss: 0.2011\n",
      "Epoch: 2 / 3, Step: 697 / 750 Loss: 0.2226\n",
      "Epoch: 2 / 3, Step: 698 / 750 Loss: 0.4040\n",
      "Epoch: 2 / 3, Step: 699 / 750 Loss: 0.2147\n",
      "Epoch: 2 / 3, Step: 700 / 750 Loss: 0.1748\n",
      "Epoch: 2 / 3, Step: 701 / 750 Loss: 0.3879\n",
      "Epoch: 2 / 3, Step: 702 / 750 Loss: 0.2561\n",
      "Epoch: 2 / 3, Step: 703 / 750 Loss: 0.3333\n",
      "Epoch: 2 / 3, Step: 704 / 750 Loss: 0.3052\n",
      "Epoch: 2 / 3, Step: 705 / 750 Loss: 0.3338\n",
      "Epoch: 2 / 3, Step: 706 / 750 Loss: 0.3365\n",
      "Epoch: 2 / 3, Step: 707 / 750 Loss: 0.5326\n",
      "Epoch: 2 / 3, Step: 708 / 750 Loss: 0.3730\n",
      "Epoch: 2 / 3, Step: 709 / 750 Loss: 0.4548\n",
      "Epoch: 2 / 3, Step: 710 / 750 Loss: 0.1445\n",
      "Epoch: 2 / 3, Step: 711 / 750 Loss: 0.4655\n",
      "Epoch: 2 / 3, Step: 712 / 750 Loss: 0.3446\n",
      "Epoch: 2 / 3, Step: 713 / 750 Loss: 0.2466\n",
      "Epoch: 2 / 3, Step: 714 / 750 Loss: 0.4528\n",
      "Epoch: 2 / 3, Step: 715 / 750 Loss: 0.4201\n",
      "Epoch: 2 / 3, Step: 716 / 750 Loss: 0.2499\n",
      "Epoch: 2 / 3, Step: 717 / 750 Loss: 0.5420\n",
      "Epoch: 2 / 3, Step: 718 / 750 Loss: 0.5340\n",
      "Epoch: 2 / 3, Step: 719 / 750 Loss: 0.5691\n",
      "Epoch: 2 / 3, Step: 720 / 750 Loss: 0.3335\n",
      "Epoch: 2 / 3, Step: 721 / 750 Loss: 0.1965\n",
      "Epoch: 2 / 3, Step: 722 / 750 Loss: 0.2964\n",
      "Epoch: 2 / 3, Step: 723 / 750 Loss: 0.2498\n",
      "Epoch: 2 / 3, Step: 724 / 750 Loss: 0.3913\n",
      "Epoch: 2 / 3, Step: 725 / 750 Loss: 0.4838\n",
      "Epoch: 2 / 3, Step: 726 / 750 Loss: 0.2797\n",
      "Epoch: 2 / 3, Step: 727 / 750 Loss: 0.3570\n",
      "Epoch: 2 / 3, Step: 728 / 750 Loss: 0.2325\n",
      "Epoch: 2 / 3, Step: 729 / 750 Loss: 0.3813\n",
      "Epoch: 2 / 3, Step: 730 / 750 Loss: 0.3886\n",
      "Epoch: 2 / 3, Step: 731 / 750 Loss: 0.5638\n",
      "Epoch: 2 / 3, Step: 732 / 750 Loss: 0.2134\n",
      "Epoch: 2 / 3, Step: 733 / 750 Loss: 0.4127\n",
      "Epoch: 2 / 3, Step: 734 / 750 Loss: 0.4033\n",
      "Epoch: 2 / 3, Step: 735 / 750 Loss: 0.3523\n",
      "Epoch: 2 / 3, Step: 736 / 750 Loss: 0.1850\n",
      "Epoch: 2 / 3, Step: 737 / 750 Loss: 0.1874\n",
      "Epoch: 2 / 3, Step: 738 / 750 Loss: 0.3655\n",
      "Epoch: 2 / 3, Step: 739 / 750 Loss: 0.3453\n",
      "Epoch: 2 / 3, Step: 740 / 750 Loss: 0.1495\n",
      "Epoch: 2 / 3, Step: 741 / 750 Loss: 0.3666\n",
      "Epoch: 2 / 3, Step: 742 / 750 Loss: 0.3694\n",
      "Epoch: 2 / 3, Step: 743 / 750 Loss: 0.2925\n",
      "Epoch: 2 / 3, Step: 744 / 750 Loss: 0.2366\n",
      "Epoch: 2 / 3, Step: 745 / 750 Loss: 0.2514\n",
      "Epoch: 2 / 3, Step: 746 / 750 Loss: 0.2643\n",
      "Epoch: 2 / 3, Step: 747 / 750 Loss: 0.2077\n",
      "Epoch: 2 / 3, Step: 748 / 750 Loss: 0.4855\n",
      "Epoch: 2 / 3, Step: 749 / 750 Loss: 0.1966\n",
      "Epoch: 3 / 3, Step: 0 / 750 Loss: 0.3494\n",
      "Epoch: 3 / 3, Step: 1 / 750 Loss: 0.3993\n",
      "Epoch: 3 / 3, Step: 2 / 750 Loss: 0.1487\n",
      "Epoch: 3 / 3, Step: 3 / 750 Loss: 0.1114\n",
      "Epoch: 3 / 3, Step: 4 / 750 Loss: 0.4160\n",
      "Epoch: 3 / 3, Step: 5 / 750 Loss: 0.1537\n",
      "Epoch: 3 / 3, Step: 6 / 750 Loss: 0.2516\n",
      "Epoch: 3 / 3, Step: 7 / 750 Loss: 0.3437\n",
      "Epoch: 3 / 3, Step: 8 / 750 Loss: 0.2905\n",
      "Epoch: 3 / 3, Step: 9 / 750 Loss: 0.4022\n",
      "Epoch: 3 / 3, Step: 10 / 750 Loss: 0.1950\n",
      "Epoch: 3 / 3, Step: 11 / 750 Loss: 0.3751\n",
      "Epoch: 3 / 3, Step: 12 / 750 Loss: 0.2121\n",
      "Epoch: 3 / 3, Step: 13 / 750 Loss: 0.3129\n",
      "Epoch: 3 / 3, Step: 14 / 750 Loss: 0.1509\n",
      "Epoch: 3 / 3, Step: 15 / 750 Loss: 0.5273\n",
      "Epoch: 3 / 3, Step: 16 / 750 Loss: 0.3101\n",
      "Epoch: 3 / 3, Step: 17 / 750 Loss: 0.1562\n",
      "Epoch: 3 / 3, Step: 18 / 750 Loss: 0.3904\n",
      "Epoch: 3 / 3, Step: 19 / 750 Loss: 0.3408\n",
      "Epoch: 3 / 3, Step: 20 / 750 Loss: 0.2705\n",
      "Epoch: 3 / 3, Step: 21 / 750 Loss: 0.2154\n",
      "Epoch: 3 / 3, Step: 22 / 750 Loss: 0.2250\n",
      "Epoch: 3 / 3, Step: 23 / 750 Loss: 0.1564\n",
      "Epoch: 3 / 3, Step: 24 / 750 Loss: 0.0980\n",
      "Epoch: 3 / 3, Step: 25 / 750 Loss: 0.3852\n",
      "Epoch: 3 / 3, Step: 26 / 750 Loss: 0.2423\n",
      "Epoch: 3 / 3, Step: 27 / 750 Loss: 0.2917\n",
      "Epoch: 3 / 3, Step: 28 / 750 Loss: 0.6765\n",
      "Epoch: 3 / 3, Step: 29 / 750 Loss: 0.2729\n",
      "Epoch: 3 / 3, Step: 30 / 750 Loss: 0.4359\n",
      "Epoch: 3 / 3, Step: 31 / 750 Loss: 0.1820\n",
      "Epoch: 3 / 3, Step: 32 / 750 Loss: 0.4854\n",
      "Epoch: 3 / 3, Step: 33 / 750 Loss: 0.1401\n",
      "Epoch: 3 / 3, Step: 34 / 750 Loss: 0.3827\n",
      "Epoch: 3 / 3, Step: 35 / 750 Loss: 0.5208\n",
      "Epoch: 3 / 3, Step: 36 / 750 Loss: 0.4738\n",
      "Epoch: 3 / 3, Step: 37 / 750 Loss: 0.2995\n",
      "Epoch: 3 / 3, Step: 38 / 750 Loss: 0.3082\n",
      "Epoch: 3 / 3, Step: 39 / 750 Loss: 0.3518\n",
      "Epoch: 3 / 3, Step: 40 / 750 Loss: 0.2327\n",
      "Epoch: 3 / 3, Step: 41 / 750 Loss: 0.2864\n",
      "Epoch: 3 / 3, Step: 42 / 750 Loss: 0.2894\n",
      "Epoch: 3 / 3, Step: 43 / 750 Loss: 0.3862\n",
      "Epoch: 3 / 3, Step: 44 / 750 Loss: 0.3560\n",
      "Epoch: 3 / 3, Step: 45 / 750 Loss: 0.2596\n",
      "Epoch: 3 / 3, Step: 46 / 750 Loss: 0.3964\n",
      "Epoch: 3 / 3, Step: 47 / 750 Loss: 0.4126\n",
      "Epoch: 3 / 3, Step: 48 / 750 Loss: 0.4484\n",
      "Epoch: 3 / 3, Step: 49 / 750 Loss: 0.5161\n",
      "Epoch: 3 / 3, Step: 50 / 750 Loss: 0.2437\n",
      "Epoch: 3 / 3, Step: 51 / 750 Loss: 0.5487\n",
      "Epoch: 3 / 3, Step: 52 / 750 Loss: 0.2526\n",
      "Epoch: 3 / 3, Step: 53 / 750 Loss: 0.3195\n",
      "Epoch: 3 / 3, Step: 54 / 750 Loss: 0.1252\n",
      "Epoch: 3 / 3, Step: 55 / 750 Loss: 0.2047\n",
      "Epoch: 3 / 3, Step: 56 / 750 Loss: 0.3286\n",
      "Epoch: 3 / 3, Step: 57 / 750 Loss: 0.3103\n",
      "Epoch: 3 / 3, Step: 58 / 750 Loss: 0.1656\n",
      "Epoch: 3 / 3, Step: 59 / 750 Loss: 0.1922\n",
      "Epoch: 3 / 3, Step: 60 / 750 Loss: 0.4772\n",
      "Epoch: 3 / 3, Step: 61 / 750 Loss: 0.2461\n",
      "Epoch: 3 / 3, Step: 62 / 750 Loss: 0.1780\n",
      "Epoch: 3 / 3, Step: 63 / 750 Loss: 0.2034\n",
      "Epoch: 3 / 3, Step: 64 / 750 Loss: 0.3969\n",
      "Epoch: 3 / 3, Step: 65 / 750 Loss: 0.3506\n",
      "Epoch: 3 / 3, Step: 66 / 750 Loss: 0.2778\n",
      "Epoch: 3 / 3, Step: 67 / 750 Loss: 0.2290\n",
      "Epoch: 3 / 3, Step: 68 / 750 Loss: 0.3172\n",
      "Epoch: 3 / 3, Step: 69 / 750 Loss: 0.1243\n",
      "Epoch: 3 / 3, Step: 70 / 750 Loss: 0.2622\n",
      "Epoch: 3 / 3, Step: 71 / 750 Loss: 0.2130\n",
      "Epoch: 3 / 3, Step: 72 / 750 Loss: 0.1165\n",
      "Epoch: 3 / 3, Step: 73 / 750 Loss: 0.2589\n",
      "Epoch: 3 / 3, Step: 74 / 750 Loss: 0.3084\n",
      "Epoch: 3 / 3, Step: 75 / 750 Loss: 0.3975\n",
      "Epoch: 3 / 3, Step: 76 / 750 Loss: 0.4747\n",
      "Epoch: 3 / 3, Step: 77 / 750 Loss: 0.1315\n",
      "Epoch: 3 / 3, Step: 78 / 750 Loss: 0.7317\n",
      "Epoch: 3 / 3, Step: 79 / 750 Loss: 0.4600\n",
      "Epoch: 3 / 3, Step: 80 / 750 Loss: 0.2683\n",
      "Epoch: 3 / 3, Step: 81 / 750 Loss: 0.3042\n",
      "Epoch: 3 / 3, Step: 82 / 750 Loss: 0.2992\n",
      "Epoch: 3 / 3, Step: 83 / 750 Loss: 0.1719\n",
      "Epoch: 3 / 3, Step: 84 / 750 Loss: 0.3707\n",
      "Epoch: 3 / 3, Step: 85 / 750 Loss: 0.1725\n",
      "Epoch: 3 / 3, Step: 86 / 750 Loss: 0.1363\n",
      "Epoch: 3 / 3, Step: 87 / 750 Loss: 0.1750\n",
      "Epoch: 3 / 3, Step: 88 / 750 Loss: 0.4513\n",
      "Epoch: 3 / 3, Step: 89 / 750 Loss: 0.3138\n",
      "Epoch: 3 / 3, Step: 90 / 750 Loss: 0.2377\n",
      "Epoch: 3 / 3, Step: 91 / 750 Loss: 0.3531\n",
      "Epoch: 3 / 3, Step: 92 / 750 Loss: 0.4438\n",
      "Epoch: 3 / 3, Step: 93 / 750 Loss: 0.3891\n",
      "Epoch: 3 / 3, Step: 94 / 750 Loss: 0.2307\n",
      "Epoch: 3 / 3, Step: 95 / 750 Loss: 0.2306\n",
      "Epoch: 3 / 3, Step: 96 / 750 Loss: 0.2340\n",
      "Epoch: 3 / 3, Step: 97 / 750 Loss: 0.4211\n",
      "Epoch: 3 / 3, Step: 98 / 750 Loss: 0.3897\n",
      "Epoch: 3 / 3, Step: 99 / 750 Loss: 0.2730\n",
      "Epoch: 3 / 3, Step: 100 / 750 Loss: 0.3113\n",
      "Epoch: 3 / 3, Step: 101 / 750 Loss: 0.4668\n",
      "Epoch: 3 / 3, Step: 102 / 750 Loss: 0.3694\n",
      "Epoch: 3 / 3, Step: 103 / 750 Loss: 0.2724\n",
      "Epoch: 3 / 3, Step: 104 / 750 Loss: 0.3074\n",
      "Epoch: 3 / 3, Step: 105 / 750 Loss: 0.8203\n",
      "Epoch: 3 / 3, Step: 106 / 750 Loss: 0.2462\n",
      "Epoch: 3 / 3, Step: 107 / 750 Loss: 0.3570\n",
      "Epoch: 3 / 3, Step: 108 / 750 Loss: 0.2671\n",
      "Epoch: 3 / 3, Step: 109 / 750 Loss: 0.3855\n",
      "Epoch: 3 / 3, Step: 110 / 750 Loss: 0.2763\n",
      "Epoch: 3 / 3, Step: 111 / 750 Loss: 0.3918\n",
      "Epoch: 3 / 3, Step: 112 / 750 Loss: 0.3038\n",
      "Epoch: 3 / 3, Step: 113 / 750 Loss: 0.2519\n",
      "Epoch: 3 / 3, Step: 114 / 750 Loss: 0.3659\n",
      "Epoch: 3 / 3, Step: 115 / 750 Loss: 0.4926\n",
      "Epoch: 3 / 3, Step: 116 / 750 Loss: 0.1733\n",
      "Epoch: 3 / 3, Step: 117 / 750 Loss: 0.3376\n",
      "Epoch: 3 / 3, Step: 118 / 750 Loss: 0.1885\n",
      "Epoch: 3 / 3, Step: 119 / 750 Loss: 0.2695\n",
      "Epoch: 3 / 3, Step: 120 / 750 Loss: 0.2342\n",
      "Epoch: 3 / 3, Step: 121 / 750 Loss: 0.4883\n",
      "Epoch: 3 / 3, Step: 122 / 750 Loss: 0.4457\n",
      "Epoch: 3 / 3, Step: 123 / 750 Loss: 0.2190\n",
      "Epoch: 3 / 3, Step: 124 / 750 Loss: 0.1921\n",
      "Epoch: 3 / 3, Step: 125 / 750 Loss: 0.2766\n",
      "Epoch: 3 / 3, Step: 126 / 750 Loss: 0.3664\n",
      "Epoch: 3 / 3, Step: 127 / 750 Loss: 0.1744\n",
      "Epoch: 3 / 3, Step: 128 / 750 Loss: 0.1037\n",
      "Epoch: 3 / 3, Step: 129 / 750 Loss: 0.3384\n",
      "Epoch: 3 / 3, Step: 130 / 750 Loss: 0.6690\n",
      "Epoch: 3 / 3, Step: 131 / 750 Loss: 0.1593\n",
      "Epoch: 3 / 3, Step: 132 / 750 Loss: 0.2624\n",
      "Epoch: 3 / 3, Step: 133 / 750 Loss: 0.3362\n",
      "Epoch: 3 / 3, Step: 134 / 750 Loss: 0.3539\n",
      "Epoch: 3 / 3, Step: 135 / 750 Loss: 0.4135\n",
      "Epoch: 3 / 3, Step: 136 / 750 Loss: 0.5962\n",
      "Epoch: 3 / 3, Step: 137 / 750 Loss: 0.2928\n",
      "Epoch: 3 / 3, Step: 138 / 750 Loss: 0.4175\n",
      "Epoch: 3 / 3, Step: 139 / 750 Loss: 0.2726\n",
      "Epoch: 3 / 3, Step: 140 / 750 Loss: 0.5151\n",
      "Epoch: 3 / 3, Step: 141 / 750 Loss: 0.3573\n",
      "Epoch: 3 / 3, Step: 142 / 750 Loss: 0.2911\n",
      "Epoch: 3 / 3, Step: 143 / 750 Loss: 0.3778\n",
      "Epoch: 3 / 3, Step: 144 / 750 Loss: 0.2054\n",
      "Epoch: 3 / 3, Step: 145 / 750 Loss: 0.5043\n",
      "Epoch: 3 / 3, Step: 146 / 750 Loss: 0.2659\n",
      "Epoch: 3 / 3, Step: 147 / 750 Loss: 0.2400\n",
      "Epoch: 3 / 3, Step: 148 / 750 Loss: 0.1564\n",
      "Epoch: 3 / 3, Step: 149 / 750 Loss: 0.1740\n",
      "Epoch: 3 / 3, Step: 150 / 750 Loss: 0.4706\n",
      "Epoch: 3 / 3, Step: 151 / 750 Loss: 0.3781\n",
      "Epoch: 3 / 3, Step: 152 / 750 Loss: 0.2501\n",
      "Epoch: 3 / 3, Step: 153 / 750 Loss: 0.1379\n",
      "Epoch: 3 / 3, Step: 154 / 750 Loss: 0.1718\n",
      "Epoch: 3 / 3, Step: 155 / 750 Loss: 0.4502\n",
      "Epoch: 3 / 3, Step: 156 / 750 Loss: 0.2376\n",
      "Epoch: 3 / 3, Step: 157 / 750 Loss: 0.3175\n",
      "Epoch: 3 / 3, Step: 158 / 750 Loss: 0.2191\n",
      "Epoch: 3 / 3, Step: 159 / 750 Loss: 0.5463\n",
      "Epoch: 3 / 3, Step: 160 / 750 Loss: 0.3630\n",
      "Epoch: 3 / 3, Step: 161 / 750 Loss: 0.4204\n",
      "Epoch: 3 / 3, Step: 162 / 750 Loss: 0.3766\n",
      "Epoch: 3 / 3, Step: 163 / 750 Loss: 0.4492\n",
      "Epoch: 3 / 3, Step: 164 / 750 Loss: 0.2143\n",
      "Epoch: 3 / 3, Step: 165 / 750 Loss: 0.5352\n",
      "Epoch: 3 / 3, Step: 166 / 750 Loss: 0.1473\n",
      "Epoch: 3 / 3, Step: 167 / 750 Loss: 0.2385\n",
      "Epoch: 3 / 3, Step: 168 / 750 Loss: 0.2569\n",
      "Epoch: 3 / 3, Step: 169 / 750 Loss: 0.2712\n",
      "Epoch: 3 / 3, Step: 170 / 750 Loss: 0.2818\n",
      "Epoch: 3 / 3, Step: 171 / 750 Loss: 0.2782\n",
      "Epoch: 3 / 3, Step: 172 / 750 Loss: 0.4646\n",
      "Epoch: 3 / 3, Step: 173 / 750 Loss: 0.4635\n",
      "Epoch: 3 / 3, Step: 174 / 750 Loss: 0.6868\n",
      "Epoch: 3 / 3, Step: 175 / 750 Loss: 0.2441\n",
      "Epoch: 3 / 3, Step: 176 / 750 Loss: 0.2223\n",
      "Epoch: 3 / 3, Step: 177 / 750 Loss: 0.4988\n",
      "Epoch: 3 / 3, Step: 178 / 750 Loss: 0.3470\n",
      "Epoch: 3 / 3, Step: 179 / 750 Loss: 0.1815\n",
      "Epoch: 3 / 3, Step: 180 / 750 Loss: 0.0905\n",
      "Epoch: 3 / 3, Step: 181 / 750 Loss: 0.1099\n",
      "Epoch: 3 / 3, Step: 182 / 750 Loss: 0.5288\n",
      "Epoch: 3 / 3, Step: 183 / 750 Loss: 0.2373\n",
      "Epoch: 3 / 3, Step: 184 / 750 Loss: 0.2563\n",
      "Epoch: 3 / 3, Step: 185 / 750 Loss: 0.6240\n",
      "Epoch: 3 / 3, Step: 186 / 750 Loss: 0.2774\n",
      "Epoch: 3 / 3, Step: 187 / 750 Loss: 0.3428\n",
      "Epoch: 3 / 3, Step: 188 / 750 Loss: 0.1664\n",
      "Epoch: 3 / 3, Step: 189 / 750 Loss: 0.5961\n",
      "Epoch: 3 / 3, Step: 190 / 750 Loss: 0.3060\n",
      "Epoch: 3 / 3, Step: 191 / 750 Loss: 0.3528\n",
      "Epoch: 3 / 3, Step: 192 / 750 Loss: 0.3058\n",
      "Epoch: 3 / 3, Step: 193 / 750 Loss: 0.4502\n",
      "Epoch: 3 / 3, Step: 194 / 750 Loss: 0.3415\n",
      "Epoch: 3 / 3, Step: 195 / 750 Loss: 0.2739\n",
      "Epoch: 3 / 3, Step: 196 / 750 Loss: 0.2472\n",
      "Epoch: 3 / 3, Step: 197 / 750 Loss: 0.1694\n",
      "Epoch: 3 / 3, Step: 198 / 750 Loss: 0.2695\n",
      "Epoch: 3 / 3, Step: 199 / 750 Loss: 0.2637\n",
      "Epoch: 3 / 3, Step: 200 / 750 Loss: 0.3706\n",
      "Epoch: 3 / 3, Step: 201 / 750 Loss: 0.1988\n",
      "Epoch: 3 / 3, Step: 202 / 750 Loss: 0.2440\n",
      "Epoch: 3 / 3, Step: 203 / 750 Loss: 0.3992\n",
      "Epoch: 3 / 3, Step: 204 / 750 Loss: 0.1525\n",
      "Epoch: 3 / 3, Step: 205 / 750 Loss: 0.2703\n",
      "Epoch: 3 / 3, Step: 206 / 750 Loss: 0.1520\n",
      "Epoch: 3 / 3, Step: 207 / 750 Loss: 0.2111\n",
      "Epoch: 3 / 3, Step: 208 / 750 Loss: 0.4232\n",
      "Epoch: 3 / 3, Step: 209 / 750 Loss: 0.3329\n",
      "Epoch: 3 / 3, Step: 210 / 750 Loss: 0.2522\n",
      "Epoch: 3 / 3, Step: 211 / 750 Loss: 0.3664\n",
      "Epoch: 3 / 3, Step: 212 / 750 Loss: 0.1550\n",
      "Epoch: 3 / 3, Step: 213 / 750 Loss: 0.1454\n",
      "Epoch: 3 / 3, Step: 214 / 750 Loss: 0.3205\n",
      "Epoch: 3 / 3, Step: 215 / 750 Loss: 0.3413\n",
      "Epoch: 3 / 3, Step: 216 / 750 Loss: 0.2764\n",
      "Epoch: 3 / 3, Step: 217 / 750 Loss: 0.1052\n",
      "Epoch: 3 / 3, Step: 218 / 750 Loss: 0.3977\n",
      "Epoch: 3 / 3, Step: 219 / 750 Loss: 0.4157\n",
      "Epoch: 3 / 3, Step: 220 / 750 Loss: 0.3936\n",
      "Epoch: 3 / 3, Step: 221 / 750 Loss: 0.2089\n",
      "Epoch: 3 / 3, Step: 222 / 750 Loss: 0.7261\n",
      "Epoch: 3 / 3, Step: 223 / 750 Loss: 0.3182\n",
      "Epoch: 3 / 3, Step: 224 / 750 Loss: 0.1742\n",
      "Epoch: 3 / 3, Step: 225 / 750 Loss: 0.1350\n",
      "Epoch: 3 / 3, Step: 226 / 750 Loss: 0.3247\n",
      "Epoch: 3 / 3, Step: 227 / 750 Loss: 0.3511\n",
      "Epoch: 3 / 3, Step: 228 / 750 Loss: 0.2834\n",
      "Epoch: 3 / 3, Step: 229 / 750 Loss: 0.2993\n",
      "Epoch: 3 / 3, Step: 230 / 750 Loss: 0.5605\n",
      "Epoch: 3 / 3, Step: 231 / 750 Loss: 0.2480\n",
      "Epoch: 3 / 3, Step: 232 / 750 Loss: 0.3223\n",
      "Epoch: 3 / 3, Step: 233 / 750 Loss: 0.1721\n",
      "Epoch: 3 / 3, Step: 234 / 750 Loss: 0.4481\n",
      "Epoch: 3 / 3, Step: 235 / 750 Loss: 0.2205\n",
      "Epoch: 3 / 3, Step: 236 / 750 Loss: 0.2503\n",
      "Epoch: 3 / 3, Step: 237 / 750 Loss: 0.3897\n",
      "Epoch: 3 / 3, Step: 238 / 750 Loss: 0.3225\n",
      "Epoch: 3 / 3, Step: 239 / 750 Loss: 0.3604\n",
      "Epoch: 3 / 3, Step: 240 / 750 Loss: 0.1573\n",
      "Epoch: 3 / 3, Step: 241 / 750 Loss: 0.3867\n",
      "Epoch: 3 / 3, Step: 242 / 750 Loss: 0.4275\n",
      "Epoch: 3 / 3, Step: 243 / 750 Loss: 0.3883\n",
      "Epoch: 3 / 3, Step: 244 / 750 Loss: 0.5380\n",
      "Epoch: 3 / 3, Step: 245 / 750 Loss: 0.3457\n",
      "Epoch: 3 / 3, Step: 246 / 750 Loss: 0.4642\n",
      "Epoch: 3 / 3, Step: 247 / 750 Loss: 0.2612\n",
      "Epoch: 3 / 3, Step: 248 / 750 Loss: 0.2478\n",
      "Epoch: 3 / 3, Step: 249 / 750 Loss: 0.2449\n",
      "Epoch: 3 / 3, Step: 250 / 750 Loss: 0.2704\n",
      "Epoch: 3 / 3, Step: 251 / 750 Loss: 0.2671\n",
      "Epoch: 3 / 3, Step: 252 / 750 Loss: 0.2309\n",
      "Epoch: 3 / 3, Step: 253 / 750 Loss: 0.3879\n",
      "Epoch: 3 / 3, Step: 254 / 750 Loss: 0.2151\n",
      "Epoch: 3 / 3, Step: 255 / 750 Loss: 0.2514\n",
      "Epoch: 3 / 3, Step: 256 / 750 Loss: 0.1343\n",
      "Epoch: 3 / 3, Step: 257 / 750 Loss: 0.3161\n",
      "Epoch: 3 / 3, Step: 258 / 750 Loss: 0.1954\n",
      "Epoch: 3 / 3, Step: 259 / 750 Loss: 0.2070\n",
      "Epoch: 3 / 3, Step: 260 / 750 Loss: 0.1973\n",
      "Epoch: 3 / 3, Step: 261 / 750 Loss: 0.5391\n",
      "Epoch: 3 / 3, Step: 262 / 750 Loss: 0.4648\n",
      "Epoch: 3 / 3, Step: 263 / 750 Loss: 0.2307\n",
      "Epoch: 3 / 3, Step: 264 / 750 Loss: 0.5352\n",
      "Epoch: 3 / 3, Step: 265 / 750 Loss: 0.3665\n",
      "Epoch: 3 / 3, Step: 266 / 750 Loss: 0.3437\n",
      "Epoch: 3 / 3, Step: 267 / 750 Loss: 0.3888\n",
      "Epoch: 3 / 3, Step: 268 / 750 Loss: 0.3827\n",
      "Epoch: 3 / 3, Step: 269 / 750 Loss: 0.3528\n",
      "Epoch: 3 / 3, Step: 270 / 750 Loss: 0.2222\n",
      "Epoch: 3 / 3, Step: 271 / 750 Loss: 0.2176\n",
      "Epoch: 3 / 3, Step: 272 / 750 Loss: 0.1587\n",
      "Epoch: 3 / 3, Step: 273 / 750 Loss: 0.4010\n",
      "Epoch: 3 / 3, Step: 274 / 750 Loss: 0.4260\n",
      "Epoch: 3 / 3, Step: 275 / 750 Loss: 0.2224\n",
      "Epoch: 3 / 3, Step: 276 / 750 Loss: 0.2486\n",
      "Epoch: 3 / 3, Step: 277 / 750 Loss: 0.2951\n",
      "Epoch: 3 / 3, Step: 278 / 750 Loss: 0.2174\n",
      "Epoch: 3 / 3, Step: 279 / 750 Loss: 0.2407\n",
      "Epoch: 3 / 3, Step: 280 / 750 Loss: 0.3125\n",
      "Epoch: 3 / 3, Step: 281 / 750 Loss: 0.2071\n",
      "Epoch: 3 / 3, Step: 282 / 750 Loss: 0.5708\n",
      "Epoch: 3 / 3, Step: 283 / 750 Loss: 0.3837\n",
      "Epoch: 3 / 3, Step: 284 / 750 Loss: 0.3470\n",
      "Epoch: 3 / 3, Step: 285 / 750 Loss: 0.3850\n",
      "Epoch: 3 / 3, Step: 286 / 750 Loss: 0.4422\n",
      "Epoch: 3 / 3, Step: 287 / 750 Loss: 0.3663\n",
      "Epoch: 3 / 3, Step: 288 / 750 Loss: 0.1457\n",
      "Epoch: 3 / 3, Step: 289 / 750 Loss: 0.5080\n",
      "Epoch: 3 / 3, Step: 290 / 750 Loss: 0.3710\n",
      "Epoch: 3 / 3, Step: 291 / 750 Loss: 0.2118\n",
      "Epoch: 3 / 3, Step: 292 / 750 Loss: 0.4300\n",
      "Epoch: 3 / 3, Step: 293 / 750 Loss: 0.3079\n",
      "Epoch: 3 / 3, Step: 294 / 750 Loss: 0.2231\n",
      "Epoch: 3 / 3, Step: 295 / 750 Loss: 0.3575\n",
      "Epoch: 3 / 3, Step: 296 / 750 Loss: 0.4174\n",
      "Epoch: 3 / 3, Step: 297 / 750 Loss: 0.3040\n",
      "Epoch: 3 / 3, Step: 298 / 750 Loss: 0.3621\n",
      "Epoch: 3 / 3, Step: 299 / 750 Loss: 0.2790\n",
      "Epoch: 3 / 3, Step: 300 / 750 Loss: 0.1184\n",
      "Epoch: 3 / 3, Step: 301 / 750 Loss: 0.3798\n",
      "Epoch: 3 / 3, Step: 302 / 750 Loss: 0.2925\n",
      "Epoch: 3 / 3, Step: 303 / 750 Loss: 0.1709\n",
      "Epoch: 3 / 3, Step: 304 / 750 Loss: 0.3443\n",
      "Epoch: 3 / 3, Step: 305 / 750 Loss: 0.1411\n",
      "Epoch: 3 / 3, Step: 306 / 750 Loss: 0.3649\n",
      "Epoch: 3 / 3, Step: 307 / 750 Loss: 0.3253\n",
      "Epoch: 3 / 3, Step: 308 / 750 Loss: 0.2632\n",
      "Epoch: 3 / 3, Step: 309 / 750 Loss: 0.3994\n",
      "Epoch: 3 / 3, Step: 310 / 750 Loss: 0.2339\n",
      "Epoch: 3 / 3, Step: 311 / 750 Loss: 0.2778\n",
      "Epoch: 3 / 3, Step: 312 / 750 Loss: 0.2799\n",
      "Epoch: 3 / 3, Step: 313 / 750 Loss: 0.1368\n",
      "Epoch: 3 / 3, Step: 314 / 750 Loss: 0.2645\n",
      "Epoch: 3 / 3, Step: 315 / 750 Loss: 0.6072\n",
      "Epoch: 3 / 3, Step: 316 / 750 Loss: 0.1740\n",
      "Epoch: 3 / 3, Step: 317 / 750 Loss: 0.3818\n",
      "Epoch: 3 / 3, Step: 318 / 750 Loss: 0.3948\n",
      "Epoch: 3 / 3, Step: 319 / 750 Loss: 0.5954\n",
      "Epoch: 3 / 3, Step: 320 / 750 Loss: 0.5762\n",
      "Epoch: 3 / 3, Step: 321 / 750 Loss: 0.1433\n",
      "Epoch: 3 / 3, Step: 322 / 750 Loss: 0.4996\n",
      "Epoch: 3 / 3, Step: 323 / 750 Loss: 0.3381\n",
      "Epoch: 3 / 3, Step: 324 / 750 Loss: 0.2343\n",
      "Epoch: 3 / 3, Step: 325 / 750 Loss: 0.3979\n",
      "Epoch: 3 / 3, Step: 326 / 750 Loss: 0.3311\n",
      "Epoch: 3 / 3, Step: 327 / 750 Loss: 0.2797\n",
      "Epoch: 3 / 3, Step: 328 / 750 Loss: 0.2576\n",
      "Epoch: 3 / 3, Step: 329 / 750 Loss: 0.4279\n",
      "Epoch: 3 / 3, Step: 330 / 750 Loss: 0.2375\n",
      "Epoch: 3 / 3, Step: 331 / 750 Loss: 0.2278\n",
      "Epoch: 3 / 3, Step: 332 / 750 Loss: 0.2323\n",
      "Epoch: 3 / 3, Step: 333 / 750 Loss: 0.2140\n",
      "Epoch: 3 / 3, Step: 334 / 750 Loss: 0.5981\n",
      "Epoch: 3 / 3, Step: 335 / 750 Loss: 0.2379\n",
      "Epoch: 3 / 3, Step: 336 / 750 Loss: 0.3652\n",
      "Epoch: 3 / 3, Step: 337 / 750 Loss: 0.2972\n",
      "Epoch: 3 / 3, Step: 338 / 750 Loss: 0.1816\n",
      "Epoch: 3 / 3, Step: 339 / 750 Loss: 0.2058\n",
      "Epoch: 3 / 3, Step: 340 / 750 Loss: 0.6774\n",
      "Epoch: 3 / 3, Step: 341 / 750 Loss: 0.1954\n",
      "Epoch: 3 / 3, Step: 342 / 750 Loss: 0.4426\n",
      "Epoch: 3 / 3, Step: 343 / 750 Loss: 0.0874\n",
      "Epoch: 3 / 3, Step: 344 / 750 Loss: 0.3071\n",
      "Epoch: 3 / 3, Step: 345 / 750 Loss: 0.1972\n",
      "Epoch: 3 / 3, Step: 346 / 750 Loss: 0.3057\n",
      "Epoch: 3 / 3, Step: 347 / 750 Loss: 0.2482\n",
      "Epoch: 3 / 3, Step: 348 / 750 Loss: 0.1917\n",
      "Epoch: 3 / 3, Step: 349 / 750 Loss: 0.3875\n",
      "Epoch: 3 / 3, Step: 350 / 750 Loss: 0.1763\n",
      "Epoch: 3 / 3, Step: 351 / 750 Loss: 0.3680\n",
      "Epoch: 3 / 3, Step: 352 / 750 Loss: 0.4874\n",
      "Epoch: 3 / 3, Step: 353 / 750 Loss: 0.2936\n",
      "Epoch: 3 / 3, Step: 354 / 750 Loss: 0.1236\n",
      "Epoch: 3 / 3, Step: 355 / 750 Loss: 0.2748\n",
      "Epoch: 3 / 3, Step: 356 / 750 Loss: 0.2648\n",
      "Epoch: 3 / 3, Step: 357 / 750 Loss: 0.0708\n",
      "Epoch: 3 / 3, Step: 358 / 750 Loss: 0.3834\n",
      "Epoch: 3 / 3, Step: 359 / 750 Loss: 0.1040\n",
      "Epoch: 3 / 3, Step: 360 / 750 Loss: 0.1744\n",
      "Epoch: 3 / 3, Step: 361 / 750 Loss: 0.6188\n",
      "Epoch: 3 / 3, Step: 362 / 750 Loss: 0.3216\n",
      "Epoch: 3 / 3, Step: 363 / 750 Loss: 0.3920\n",
      "Epoch: 3 / 3, Step: 364 / 750 Loss: 0.2404\n",
      "Epoch: 3 / 3, Step: 365 / 750 Loss: 0.5775\n",
      "Epoch: 3 / 3, Step: 366 / 750 Loss: 0.3326\n",
      "Epoch: 3 / 3, Step: 367 / 750 Loss: 0.1838\n",
      "Epoch: 3 / 3, Step: 368 / 750 Loss: 0.1370\n",
      "Epoch: 3 / 3, Step: 369 / 750 Loss: 0.1227\n",
      "Epoch: 3 / 3, Step: 370 / 750 Loss: 0.6688\n",
      "Epoch: 3 / 3, Step: 371 / 750 Loss: 0.2924\n",
      "Epoch: 3 / 3, Step: 372 / 750 Loss: 0.3665\n",
      "Epoch: 3 / 3, Step: 373 / 750 Loss: 0.4824\n",
      "Epoch: 3 / 3, Step: 374 / 750 Loss: 0.1960\n",
      "Epoch: 3 / 3, Step: 375 / 750 Loss: 0.4727\n",
      "Epoch: 3 / 3, Step: 376 / 750 Loss: 0.2976\n",
      "Epoch: 3 / 3, Step: 377 / 750 Loss: 0.3455\n",
      "Epoch: 3 / 3, Step: 378 / 750 Loss: 0.3794\n",
      "Epoch: 3 / 3, Step: 379 / 750 Loss: 0.4519\n",
      "Epoch: 3 / 3, Step: 380 / 750 Loss: 0.2949\n",
      "Epoch: 3 / 3, Step: 381 / 750 Loss: 0.3090\n",
      "Epoch: 3 / 3, Step: 382 / 750 Loss: 0.4836\n",
      "Epoch: 3 / 3, Step: 383 / 750 Loss: 0.2605\n",
      "Epoch: 3 / 3, Step: 384 / 750 Loss: 0.2661\n",
      "Epoch: 3 / 3, Step: 385 / 750 Loss: 0.2405\n",
      "Epoch: 3 / 3, Step: 386 / 750 Loss: 0.4733\n",
      "Epoch: 3 / 3, Step: 387 / 750 Loss: 0.7034\n",
      "Epoch: 3 / 3, Step: 388 / 750 Loss: 0.1725\n",
      "Epoch: 3 / 3, Step: 389 / 750 Loss: 0.4613\n",
      "Epoch: 3 / 3, Step: 390 / 750 Loss: 0.1976\n",
      "Epoch: 3 / 3, Step: 391 / 750 Loss: 0.4529\n",
      "Epoch: 3 / 3, Step: 392 / 750 Loss: 0.4370\n",
      "Epoch: 3 / 3, Step: 393 / 750 Loss: 0.1611\n",
      "Epoch: 3 / 3, Step: 394 / 750 Loss: 0.4044\n",
      "Epoch: 3 / 3, Step: 395 / 750 Loss: 0.4416\n",
      "Epoch: 3 / 3, Step: 396 / 750 Loss: 0.2728\n",
      "Epoch: 3 / 3, Step: 397 / 750 Loss: 0.3168\n",
      "Epoch: 3 / 3, Step: 398 / 750 Loss: 0.2004\n",
      "Epoch: 3 / 3, Step: 399 / 750 Loss: 0.2777\n",
      "Epoch: 3 / 3, Step: 400 / 750 Loss: 0.2768\n",
      "Epoch: 3 / 3, Step: 401 / 750 Loss: 0.1923\n",
      "Epoch: 3 / 3, Step: 402 / 750 Loss: 0.1597\n",
      "Epoch: 3 / 3, Step: 403 / 750 Loss: 0.4612\n",
      "Epoch: 3 / 3, Step: 404 / 750 Loss: 0.3766\n",
      "Epoch: 3 / 3, Step: 405 / 750 Loss: 0.1987\n",
      "Epoch: 3 / 3, Step: 406 / 750 Loss: 0.7181\n",
      "Epoch: 3 / 3, Step: 407 / 750 Loss: 0.2886\n",
      "Epoch: 3 / 3, Step: 408 / 750 Loss: 0.2233\n",
      "Epoch: 3 / 3, Step: 409 / 750 Loss: 0.1835\n",
      "Epoch: 3 / 3, Step: 410 / 750 Loss: 0.4067\n",
      "Epoch: 3 / 3, Step: 411 / 750 Loss: 0.2478\n",
      "Epoch: 3 / 3, Step: 412 / 750 Loss: 0.3780\n",
      "Epoch: 3 / 3, Step: 413 / 750 Loss: 0.3672\n",
      "Epoch: 3 / 3, Step: 414 / 750 Loss: 0.2707\n",
      "Epoch: 3 / 3, Step: 415 / 750 Loss: 0.2957\n",
      "Epoch: 3 / 3, Step: 416 / 750 Loss: 0.0971\n",
      "Epoch: 3 / 3, Step: 417 / 750 Loss: 0.1501\n",
      "Epoch: 3 / 3, Step: 418 / 750 Loss: 0.3985\n",
      "Epoch: 3 / 3, Step: 419 / 750 Loss: 0.6237\n",
      "Epoch: 3 / 3, Step: 420 / 750 Loss: 0.4860\n",
      "Epoch: 3 / 3, Step: 421 / 750 Loss: 0.4120\n",
      "Epoch: 3 / 3, Step: 422 / 750 Loss: 0.6288\n",
      "Epoch: 3 / 3, Step: 423 / 750 Loss: 0.3431\n",
      "Epoch: 3 / 3, Step: 424 / 750 Loss: 0.3086\n",
      "Epoch: 3 / 3, Step: 425 / 750 Loss: 0.2396\n",
      "Epoch: 3 / 3, Step: 426 / 750 Loss: 0.2395\n",
      "Epoch: 3 / 3, Step: 427 / 750 Loss: 0.5466\n",
      "Epoch: 3 / 3, Step: 428 / 750 Loss: 0.2939\n",
      "Epoch: 3 / 3, Step: 429 / 750 Loss: 0.1613\n",
      "Epoch: 3 / 3, Step: 430 / 750 Loss: 0.2417\n",
      "Epoch: 3 / 3, Step: 431 / 750 Loss: 0.3842\n",
      "Epoch: 3 / 3, Step: 432 / 750 Loss: 0.2599\n",
      "Epoch: 3 / 3, Step: 433 / 750 Loss: 0.4379\n",
      "Epoch: 3 / 3, Step: 434 / 750 Loss: 0.3209\n",
      "Epoch: 3 / 3, Step: 435 / 750 Loss: 0.2401\n",
      "Epoch: 3 / 3, Step: 436 / 750 Loss: 0.3084\n",
      "Epoch: 3 / 3, Step: 437 / 750 Loss: 0.2790\n",
      "Epoch: 3 / 3, Step: 438 / 750 Loss: 0.1822\n",
      "Epoch: 3 / 3, Step: 439 / 750 Loss: 0.3775\n",
      "Epoch: 3 / 3, Step: 440 / 750 Loss: 0.4014\n",
      "Epoch: 3 / 3, Step: 441 / 750 Loss: 0.3353\n",
      "Epoch: 3 / 3, Step: 442 / 750 Loss: 0.3018\n",
      "Epoch: 3 / 3, Step: 443 / 750 Loss: 0.1886\n",
      "Epoch: 3 / 3, Step: 444 / 750 Loss: 0.4636\n",
      "Epoch: 3 / 3, Step: 445 / 750 Loss: 0.3952\n",
      "Epoch: 3 / 3, Step: 446 / 750 Loss: 0.1502\n",
      "Epoch: 3 / 3, Step: 447 / 750 Loss: 0.5180\n",
      "Epoch: 3 / 3, Step: 448 / 750 Loss: 0.4520\n",
      "Epoch: 3 / 3, Step: 449 / 750 Loss: 0.3780\n",
      "Epoch: 3 / 3, Step: 450 / 750 Loss: 0.2702\n",
      "Epoch: 3 / 3, Step: 451 / 750 Loss: 0.3948\n",
      "Epoch: 3 / 3, Step: 452 / 750 Loss: 0.3675\n",
      "Epoch: 3 / 3, Step: 453 / 750 Loss: 0.3517\n",
      "Epoch: 3 / 3, Step: 454 / 750 Loss: 0.4139\n",
      "Epoch: 3 / 3, Step: 455 / 750 Loss: 0.4299\n",
      "Epoch: 3 / 3, Step: 456 / 750 Loss: 0.2546\n",
      "Epoch: 3 / 3, Step: 457 / 750 Loss: 0.2300\n",
      "Epoch: 3 / 3, Step: 458 / 750 Loss: 0.1665\n",
      "Epoch: 3 / 3, Step: 459 / 750 Loss: 0.4154\n",
      "Epoch: 3 / 3, Step: 460 / 750 Loss: 0.2098\n",
      "Epoch: 3 / 3, Step: 461 / 750 Loss: 0.1647\n",
      "Epoch: 3 / 3, Step: 462 / 750 Loss: 0.2360\n",
      "Epoch: 3 / 3, Step: 463 / 750 Loss: 0.0767\n",
      "Epoch: 3 / 3, Step: 464 / 750 Loss: 0.0956\n",
      "Epoch: 3 / 3, Step: 465 / 750 Loss: 0.1292\n",
      "Epoch: 3 / 3, Step: 466 / 750 Loss: 0.2411\n",
      "Epoch: 3 / 3, Step: 467 / 750 Loss: 0.3775\n",
      "Epoch: 3 / 3, Step: 468 / 750 Loss: 0.3130\n",
      "Epoch: 3 / 3, Step: 469 / 750 Loss: 0.2849\n",
      "Epoch: 3 / 3, Step: 470 / 750 Loss: 0.3843\n",
      "Epoch: 3 / 3, Step: 471 / 750 Loss: 0.3278\n",
      "Epoch: 3 / 3, Step: 472 / 750 Loss: 0.2997\n",
      "Epoch: 3 / 3, Step: 473 / 750 Loss: 0.4871\n",
      "Epoch: 3 / 3, Step: 474 / 750 Loss: 0.1153\n",
      "Epoch: 3 / 3, Step: 475 / 750 Loss: 0.2371\n",
      "Epoch: 3 / 3, Step: 476 / 750 Loss: 0.4941\n",
      "Epoch: 3 / 3, Step: 477 / 750 Loss: 0.1322\n",
      "Epoch: 3 / 3, Step: 478 / 750 Loss: 0.1597\n",
      "Epoch: 3 / 3, Step: 479 / 750 Loss: 0.2895\n",
      "Epoch: 3 / 3, Step: 480 / 750 Loss: 0.3277\n",
      "Epoch: 3 / 3, Step: 481 / 750 Loss: 0.1200\n",
      "Epoch: 3 / 3, Step: 482 / 750 Loss: 0.4467\n",
      "Epoch: 3 / 3, Step: 483 / 750 Loss: 0.3981\n",
      "Epoch: 3 / 3, Step: 484 / 750 Loss: 0.2078\n",
      "Epoch: 3 / 3, Step: 485 / 750 Loss: 0.4676\n",
      "Epoch: 3 / 3, Step: 486 / 750 Loss: 0.4346\n",
      "Epoch: 3 / 3, Step: 487 / 750 Loss: 0.2176\n",
      "Epoch: 3 / 3, Step: 488 / 750 Loss: 0.4359\n",
      "Epoch: 3 / 3, Step: 489 / 750 Loss: 0.3287\n",
      "Epoch: 3 / 3, Step: 490 / 750 Loss: 0.2682\n",
      "Epoch: 3 / 3, Step: 491 / 750 Loss: 0.1893\n",
      "Epoch: 3 / 3, Step: 492 / 750 Loss: 0.2378\n",
      "Epoch: 3 / 3, Step: 493 / 750 Loss: 0.2735\n",
      "Epoch: 3 / 3, Step: 494 / 750 Loss: 0.3196\n",
      "Epoch: 3 / 3, Step: 495 / 750 Loss: 0.2859\n",
      "Epoch: 3 / 3, Step: 496 / 750 Loss: 0.2831\n",
      "Epoch: 3 / 3, Step: 497 / 750 Loss: 0.2410\n",
      "Epoch: 3 / 3, Step: 498 / 750 Loss: 0.1723\n",
      "Epoch: 3 / 3, Step: 499 / 750 Loss: 0.2274\n",
      "Epoch: 3 / 3, Step: 500 / 750 Loss: 0.2701\n",
      "Epoch: 3 / 3, Step: 501 / 750 Loss: 0.2607\n",
      "Epoch: 3 / 3, Step: 502 / 750 Loss: 0.5260\n",
      "Epoch: 3 / 3, Step: 503 / 750 Loss: 0.1071\n",
      "Epoch: 3 / 3, Step: 504 / 750 Loss: 0.1170\n",
      "Epoch: 3 / 3, Step: 505 / 750 Loss: 0.2777\n",
      "Epoch: 3 / 3, Step: 506 / 750 Loss: 0.5977\n",
      "Epoch: 3 / 3, Step: 507 / 750 Loss: 0.2705\n",
      "Epoch: 3 / 3, Step: 508 / 750 Loss: 0.1949\n",
      "Epoch: 3 / 3, Step: 509 / 750 Loss: 0.2862\n",
      "Epoch: 3 / 3, Step: 510 / 750 Loss: 0.1209\n",
      "Epoch: 3 / 3, Step: 511 / 750 Loss: 0.3161\n",
      "Epoch: 3 / 3, Step: 512 / 750 Loss: 0.2034\n",
      "Epoch: 3 / 3, Step: 513 / 750 Loss: 0.2321\n",
      "Epoch: 3 / 3, Step: 514 / 750 Loss: 0.4522\n",
      "Epoch: 3 / 3, Step: 515 / 750 Loss: 0.5086\n",
      "Epoch: 3 / 3, Step: 516 / 750 Loss: 0.3746\n",
      "Epoch: 3 / 3, Step: 517 / 750 Loss: 0.3039\n",
      "Epoch: 3 / 3, Step: 518 / 750 Loss: 0.3501\n",
      "Epoch: 3 / 3, Step: 519 / 750 Loss: 0.1956\n",
      "Epoch: 3 / 3, Step: 520 / 750 Loss: 0.3176\n",
      "Epoch: 3 / 3, Step: 521 / 750 Loss: 0.3671\n",
      "Epoch: 3 / 3, Step: 522 / 750 Loss: 0.3992\n",
      "Epoch: 3 / 3, Step: 523 / 750 Loss: 0.2765\n",
      "Epoch: 3 / 3, Step: 524 / 750 Loss: 0.2108\n",
      "Epoch: 3 / 3, Step: 525 / 750 Loss: 0.1124\n",
      "Epoch: 3 / 3, Step: 526 / 750 Loss: 0.4241\n",
      "Epoch: 3 / 3, Step: 527 / 750 Loss: 0.3193\n",
      "Epoch: 3 / 3, Step: 528 / 750 Loss: 0.3557\n",
      "Epoch: 3 / 3, Step: 529 / 750 Loss: 0.1079\n",
      "Epoch: 3 / 3, Step: 530 / 750 Loss: 0.3877\n",
      "Epoch: 3 / 3, Step: 531 / 750 Loss: 0.0952\n",
      "Epoch: 3 / 3, Step: 532 / 750 Loss: 0.4711\n",
      "Epoch: 3 / 3, Step: 533 / 750 Loss: 0.3528\n",
      "Epoch: 3 / 3, Step: 534 / 750 Loss: 0.2166\n",
      "Epoch: 3 / 3, Step: 535 / 750 Loss: 0.2556\n",
      "Epoch: 3 / 3, Step: 536 / 750 Loss: 0.2564\n",
      "Epoch: 3 / 3, Step: 537 / 750 Loss: 0.1651\n",
      "Epoch: 3 / 3, Step: 538 / 750 Loss: 0.2870\n",
      "Epoch: 3 / 3, Step: 539 / 750 Loss: 0.3889\n",
      "Epoch: 3 / 3, Step: 540 / 750 Loss: 0.1835\n",
      "Epoch: 3 / 3, Step: 541 / 750 Loss: 0.1279\n",
      "Epoch: 3 / 3, Step: 542 / 750 Loss: 0.3500\n",
      "Epoch: 3 / 3, Step: 543 / 750 Loss: 0.0914\n",
      "Epoch: 3 / 3, Step: 544 / 750 Loss: 0.2013\n",
      "Epoch: 3 / 3, Step: 545 / 750 Loss: 0.3642\n",
      "Epoch: 3 / 3, Step: 546 / 750 Loss: 0.2388\n",
      "Epoch: 3 / 3, Step: 547 / 750 Loss: 0.3784\n",
      "Epoch: 3 / 3, Step: 548 / 750 Loss: 0.3081\n",
      "Epoch: 3 / 3, Step: 549 / 750 Loss: 0.1803\n",
      "Epoch: 3 / 3, Step: 550 / 750 Loss: 0.2786\n",
      "Epoch: 3 / 3, Step: 551 / 750 Loss: 0.1223\n",
      "Epoch: 3 / 3, Step: 552 / 750 Loss: 0.3734\n",
      "Epoch: 3 / 3, Step: 553 / 750 Loss: 0.4529\n",
      "Epoch: 3 / 3, Step: 554 / 750 Loss: 0.4952\n",
      "Epoch: 3 / 3, Step: 555 / 750 Loss: 0.3047\n",
      "Epoch: 3 / 3, Step: 556 / 750 Loss: 0.3782\n",
      "Epoch: 3 / 3, Step: 557 / 750 Loss: 0.4404\n",
      "Epoch: 3 / 3, Step: 558 / 750 Loss: 0.2286\n",
      "Epoch: 3 / 3, Step: 559 / 750 Loss: 0.2112\n",
      "Epoch: 3 / 3, Step: 560 / 750 Loss: 0.2490\n",
      "Epoch: 3 / 3, Step: 561 / 750 Loss: 0.3566\n",
      "Epoch: 3 / 3, Step: 562 / 750 Loss: 0.6347\n",
      "Epoch: 3 / 3, Step: 563 / 750 Loss: 0.5965\n",
      "Epoch: 3 / 3, Step: 564 / 750 Loss: 0.2967\n",
      "Epoch: 3 / 3, Step: 565 / 750 Loss: 0.2537\n",
      "Epoch: 3 / 3, Step: 566 / 750 Loss: 0.5454\n",
      "Epoch: 3 / 3, Step: 567 / 750 Loss: 0.2379\n",
      "Epoch: 3 / 3, Step: 568 / 750 Loss: 0.3635\n",
      "Epoch: 3 / 3, Step: 569 / 750 Loss: 0.2841\n",
      "Epoch: 3 / 3, Step: 570 / 750 Loss: 0.2339\n",
      "Epoch: 3 / 3, Step: 571 / 750 Loss: 0.3526\n",
      "Epoch: 3 / 3, Step: 572 / 750 Loss: 0.3510\n",
      "Epoch: 3 / 3, Step: 573 / 750 Loss: 0.1333\n",
      "Epoch: 3 / 3, Step: 574 / 750 Loss: 0.2918\n",
      "Epoch: 3 / 3, Step: 575 / 750 Loss: 0.2871\n",
      "Epoch: 3 / 3, Step: 576 / 750 Loss: 0.2806\n",
      "Epoch: 3 / 3, Step: 577 / 750 Loss: 0.3179\n",
      "Epoch: 3 / 3, Step: 578 / 750 Loss: 0.2952\n",
      "Epoch: 3 / 3, Step: 579 / 750 Loss: 0.1768\n",
      "Epoch: 3 / 3, Step: 580 / 750 Loss: 0.2960\n",
      "Epoch: 3 / 3, Step: 581 / 750 Loss: 0.4196\n",
      "Epoch: 3 / 3, Step: 582 / 750 Loss: 0.5079\n",
      "Epoch: 3 / 3, Step: 583 / 750 Loss: 0.2808\n",
      "Epoch: 3 / 3, Step: 584 / 750 Loss: 0.1234\n",
      "Epoch: 3 / 3, Step: 585 / 750 Loss: 0.4228\n",
      "Epoch: 3 / 3, Step: 586 / 750 Loss: 0.3732\n",
      "Epoch: 3 / 3, Step: 587 / 750 Loss: 0.3621\n",
      "Epoch: 3 / 3, Step: 588 / 750 Loss: 0.3502\n",
      "Epoch: 3 / 3, Step: 589 / 750 Loss: 0.2618\n",
      "Epoch: 3 / 3, Step: 590 / 750 Loss: 0.3607\n",
      "Epoch: 3 / 3, Step: 591 / 750 Loss: 0.1926\n",
      "Epoch: 3 / 3, Step: 592 / 750 Loss: 0.2680\n",
      "Epoch: 3 / 3, Step: 593 / 750 Loss: 0.3294\n",
      "Epoch: 3 / 3, Step: 594 / 750 Loss: 0.5933\n",
      "Epoch: 3 / 3, Step: 595 / 750 Loss: 0.2516\n",
      "Epoch: 3 / 3, Step: 596 / 750 Loss: 0.1700\n",
      "Epoch: 3 / 3, Step: 597 / 750 Loss: 0.4422\n",
      "Epoch: 3 / 3, Step: 598 / 750 Loss: 0.1518\n",
      "Epoch: 3 / 3, Step: 599 / 750 Loss: 0.3110\n",
      "Epoch: 3 / 3, Step: 600 / 750 Loss: 0.1948\n",
      "Epoch: 3 / 3, Step: 601 / 750 Loss: 0.2722\n",
      "Epoch: 3 / 3, Step: 602 / 750 Loss: 0.4959\n",
      "Epoch: 3 / 3, Step: 603 / 750 Loss: 0.3159\n",
      "Epoch: 3 / 3, Step: 604 / 750 Loss: 0.1979\n",
      "Epoch: 3 / 3, Step: 605 / 750 Loss: 0.1433\n",
      "Epoch: 3 / 3, Step: 606 / 750 Loss: 0.2818\n",
      "Epoch: 3 / 3, Step: 607 / 750 Loss: 0.2118\n",
      "Epoch: 3 / 3, Step: 608 / 750 Loss: 0.1254\n",
      "Epoch: 3 / 3, Step: 609 / 750 Loss: 0.2013\n",
      "Epoch: 3 / 3, Step: 610 / 750 Loss: 0.5156\n",
      "Epoch: 3 / 3, Step: 611 / 750 Loss: 0.0896\n",
      "Epoch: 3 / 3, Step: 612 / 750 Loss: 0.1663\n",
      "Epoch: 3 / 3, Step: 613 / 750 Loss: 0.1920\n",
      "Epoch: 3 / 3, Step: 614 / 750 Loss: 0.2298\n",
      "Epoch: 3 / 3, Step: 615 / 750 Loss: 0.1267\n",
      "Epoch: 3 / 3, Step: 616 / 750 Loss: 0.6311\n",
      "Epoch: 3 / 3, Step: 617 / 750 Loss: 0.4502\n",
      "Epoch: 3 / 3, Step: 618 / 750 Loss: 0.2060\n",
      "Epoch: 3 / 3, Step: 619 / 750 Loss: 0.4336\n",
      "Epoch: 3 / 3, Step: 620 / 750 Loss: 0.4767\n",
      "Epoch: 3 / 3, Step: 621 / 750 Loss: 0.2238\n",
      "Epoch: 3 / 3, Step: 622 / 750 Loss: 0.2459\n",
      "Epoch: 3 / 3, Step: 623 / 750 Loss: 0.2603\n",
      "Epoch: 3 / 3, Step: 624 / 750 Loss: 0.2773\n",
      "Epoch: 3 / 3, Step: 625 / 750 Loss: 0.1636\n",
      "Epoch: 3 / 3, Step: 626 / 750 Loss: 0.3715\n",
      "Epoch: 3 / 3, Step: 627 / 750 Loss: 0.2261\n",
      "Epoch: 3 / 3, Step: 628 / 750 Loss: 0.2187\n",
      "Epoch: 3 / 3, Step: 629 / 750 Loss: 0.2892\n",
      "Epoch: 3 / 3, Step: 630 / 750 Loss: 0.3078\n",
      "Epoch: 3 / 3, Step: 631 / 750 Loss: 0.1703\n",
      "Epoch: 3 / 3, Step: 632 / 750 Loss: 0.1849\n",
      "Epoch: 3 / 3, Step: 633 / 750 Loss: 0.2117\n",
      "Epoch: 3 / 3, Step: 634 / 750 Loss: 0.1657\n",
      "Epoch: 3 / 3, Step: 635 / 750 Loss: 0.1488\n",
      "Epoch: 3 / 3, Step: 636 / 750 Loss: 0.3390\n",
      "Epoch: 3 / 3, Step: 637 / 750 Loss: 0.1017\n",
      "Epoch: 3 / 3, Step: 638 / 750 Loss: 0.1033\n",
      "Epoch: 3 / 3, Step: 639 / 750 Loss: 0.1739\n",
      "Epoch: 3 / 3, Step: 640 / 750 Loss: 0.1971\n",
      "Epoch: 3 / 3, Step: 641 / 750 Loss: 0.2148\n",
      "Epoch: 3 / 3, Step: 642 / 750 Loss: 0.1451\n",
      "Epoch: 3 / 3, Step: 643 / 750 Loss: 0.2913\n",
      "Epoch: 3 / 3, Step: 644 / 750 Loss: 0.0353\n",
      "Epoch: 3 / 3, Step: 645 / 750 Loss: 0.1517\n",
      "Epoch: 3 / 3, Step: 646 / 750 Loss: 0.6420\n",
      "Epoch: 3 / 3, Step: 647 / 750 Loss: 0.3224\n",
      "Epoch: 3 / 3, Step: 648 / 750 Loss: 0.3051\n",
      "Epoch: 3 / 3, Step: 649 / 750 Loss: 0.0658\n",
      "Epoch: 3 / 3, Step: 650 / 750 Loss: 0.1304\n",
      "Epoch: 3 / 3, Step: 651 / 750 Loss: 0.5650\n",
      "Epoch: 3 / 3, Step: 652 / 750 Loss: 0.2130\n",
      "Epoch: 3 / 3, Step: 653 / 750 Loss: 0.3284\n",
      "Epoch: 3 / 3, Step: 654 / 750 Loss: 0.3144\n",
      "Epoch: 3 / 3, Step: 655 / 750 Loss: 0.4993\n",
      "Epoch: 3 / 3, Step: 656 / 750 Loss: 0.1659\n",
      "Epoch: 3 / 3, Step: 657 / 750 Loss: 0.1716\n",
      "Epoch: 3 / 3, Step: 658 / 750 Loss: 0.3974\n",
      "Epoch: 3 / 3, Step: 659 / 750 Loss: 0.2769\n",
      "Epoch: 3 / 3, Step: 660 / 750 Loss: 0.4274\n",
      "Epoch: 3 / 3, Step: 661 / 750 Loss: 0.3457\n",
      "Epoch: 3 / 3, Step: 662 / 750 Loss: 0.6597\n",
      "Epoch: 3 / 3, Step: 663 / 750 Loss: 0.1176\n",
      "Epoch: 3 / 3, Step: 664 / 750 Loss: 0.2155\n",
      "Epoch: 3 / 3, Step: 665 / 750 Loss: 0.4653\n",
      "Epoch: 3 / 3, Step: 666 / 750 Loss: 0.3588\n",
      "Epoch: 3 / 3, Step: 667 / 750 Loss: 0.1587\n",
      "Epoch: 3 / 3, Step: 668 / 750 Loss: 0.1477\n",
      "Epoch: 3 / 3, Step: 669 / 750 Loss: 0.6046\n",
      "Epoch: 3 / 3, Step: 670 / 750 Loss: 0.2998\n",
      "Epoch: 3 / 3, Step: 671 / 750 Loss: 0.1182\n",
      "Epoch: 3 / 3, Step: 672 / 750 Loss: 0.4131\n",
      "Epoch: 3 / 3, Step: 673 / 750 Loss: 0.3016\n",
      "Epoch: 3 / 3, Step: 674 / 750 Loss: 0.0971\n",
      "Epoch: 3 / 3, Step: 675 / 750 Loss: 0.3709\n",
      "Epoch: 3 / 3, Step: 676 / 750 Loss: 0.2214\n",
      "Epoch: 3 / 3, Step: 677 / 750 Loss: 0.4992\n",
      "Epoch: 3 / 3, Step: 678 / 750 Loss: 0.4120\n",
      "Epoch: 3 / 3, Step: 679 / 750 Loss: 0.1585\n",
      "Epoch: 3 / 3, Step: 680 / 750 Loss: 0.3289\n",
      "Epoch: 3 / 3, Step: 681 / 750 Loss: 0.4372\n",
      "Epoch: 3 / 3, Step: 682 / 750 Loss: 0.4014\n",
      "Epoch: 3 / 3, Step: 683 / 750 Loss: 0.4617\n",
      "Epoch: 3 / 3, Step: 684 / 750 Loss: 0.1875\n",
      "Epoch: 3 / 3, Step: 685 / 750 Loss: 0.3941\n",
      "Epoch: 3 / 3, Step: 686 / 750 Loss: 0.6608\n",
      "Epoch: 3 / 3, Step: 687 / 750 Loss: 0.1544\n",
      "Epoch: 3 / 3, Step: 688 / 750 Loss: 0.2993\n",
      "Epoch: 3 / 3, Step: 689 / 750 Loss: 0.2186\n",
      "Epoch: 3 / 3, Step: 690 / 750 Loss: 0.4599\n",
      "Epoch: 3 / 3, Step: 691 / 750 Loss: 0.1036\n",
      "Epoch: 3 / 3, Step: 692 / 750 Loss: 0.2008\n",
      "Epoch: 3 / 3, Step: 693 / 750 Loss: 0.1202\n",
      "Epoch: 3 / 3, Step: 694 / 750 Loss: 0.3764\n",
      "Epoch: 3 / 3, Step: 695 / 750 Loss: 0.3543\n",
      "Epoch: 3 / 3, Step: 696 / 750 Loss: 0.3157\n",
      "Epoch: 3 / 3, Step: 697 / 750 Loss: 0.4816\n",
      "Epoch: 3 / 3, Step: 698 / 750 Loss: 0.4541\n",
      "Epoch: 3 / 3, Step: 699 / 750 Loss: 0.2091\n",
      "Epoch: 3 / 3, Step: 700 / 750 Loss: 0.3402\n",
      "Epoch: 3 / 3, Step: 701 / 750 Loss: 0.2630\n",
      "Epoch: 3 / 3, Step: 702 / 750 Loss: 0.3080\n",
      "Epoch: 3 / 3, Step: 703 / 750 Loss: 0.2938\n",
      "Epoch: 3 / 3, Step: 704 / 750 Loss: 0.3891\n",
      "Epoch: 3 / 3, Step: 705 / 750 Loss: 0.3613\n",
      "Epoch: 3 / 3, Step: 706 / 750 Loss: 0.2584\n",
      "Epoch: 3 / 3, Step: 707 / 750 Loss: 0.3142\n",
      "Epoch: 3 / 3, Step: 708 / 750 Loss: 0.3690\n",
      "Epoch: 3 / 3, Step: 709 / 750 Loss: 0.1209\n",
      "Epoch: 3 / 3, Step: 710 / 750 Loss: 0.1584\n",
      "Epoch: 3 / 3, Step: 711 / 750 Loss: 0.3345\n",
      "Epoch: 3 / 3, Step: 712 / 750 Loss: 0.3083\n",
      "Epoch: 3 / 3, Step: 713 / 750 Loss: 0.3247\n",
      "Epoch: 3 / 3, Step: 714 / 750 Loss: 0.3311\n",
      "Epoch: 3 / 3, Step: 715 / 750 Loss: 0.2915\n",
      "Epoch: 3 / 3, Step: 716 / 750 Loss: 0.3408\n",
      "Epoch: 3 / 3, Step: 717 / 750 Loss: 0.4198\n",
      "Epoch: 3 / 3, Step: 718 / 750 Loss: 0.3076\n",
      "Epoch: 3 / 3, Step: 719 / 750 Loss: 0.3254\n",
      "Epoch: 3 / 3, Step: 720 / 750 Loss: 0.1563\n",
      "Epoch: 3 / 3, Step: 721 / 750 Loss: 0.4822\n",
      "Epoch: 3 / 3, Step: 722 / 750 Loss: 0.2289\n",
      "Epoch: 3 / 3, Step: 723 / 750 Loss: 0.4601\n",
      "Epoch: 3 / 3, Step: 724 / 750 Loss: 0.2598\n",
      "Epoch: 3 / 3, Step: 725 / 750 Loss: 0.1595\n",
      "Epoch: 3 / 3, Step: 726 / 750 Loss: 0.1468\n",
      "Epoch: 3 / 3, Step: 727 / 750 Loss: 0.4587\n",
      "Epoch: 3 / 3, Step: 728 / 750 Loss: 0.2987\n",
      "Epoch: 3 / 3, Step: 729 / 750 Loss: 0.1772\n",
      "Epoch: 3 / 3, Step: 730 / 750 Loss: 0.1120\n",
      "Epoch: 3 / 3, Step: 731 / 750 Loss: 0.1954\n",
      "Epoch: 3 / 3, Step: 732 / 750 Loss: 0.5575\n",
      "Epoch: 3 / 3, Step: 733 / 750 Loss: 0.2055\n",
      "Epoch: 3 / 3, Step: 734 / 750 Loss: 0.2423\n",
      "Epoch: 3 / 3, Step: 735 / 750 Loss: 0.1581\n",
      "Epoch: 3 / 3, Step: 736 / 750 Loss: 0.2728\n",
      "Epoch: 3 / 3, Step: 737 / 750 Loss: 0.2812\n",
      "Epoch: 3 / 3, Step: 738 / 750 Loss: 0.2263\n",
      "Epoch: 3 / 3, Step: 739 / 750 Loss: 0.1421\n",
      "Epoch: 3 / 3, Step: 740 / 750 Loss: 0.3581\n",
      "Epoch: 3 / 3, Step: 741 / 750 Loss: 0.2579\n",
      "Epoch: 3 / 3, Step: 742 / 750 Loss: 0.3398\n",
      "Epoch: 3 / 3, Step: 743 / 750 Loss: 0.1890\n",
      "Epoch: 3 / 3, Step: 744 / 750 Loss: 0.2772\n",
      "Epoch: 3 / 3, Step: 745 / 750 Loss: 0.0864\n",
      "Epoch: 3 / 3, Step: 746 / 750 Loss: 0.2084\n",
      "Epoch: 3 / 3, Step: 747 / 750 Loss: 0.1921\n",
      "Epoch: 3 / 3, Step: 748 / 750 Loss: 0.1709\n",
      "Epoch: 3 / 3, Step: 749 / 750 Loss: 0.1250\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "loss_total = 0\n",
    "model.train()\n",
    "for i in range(3):\n",
    "    for j, data in enumerate(train_dataloader):\n",
    "        inputs = {'input_ids': data[0].cuda(), \n",
    "                      'attention_mask': data[1].cuda(), \n",
    "                      'labels': data[2].cuda()}\n",
    "        output = model(**inputs)\n",
    "        loss = output[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Epoch: {} / {}, Step: {} / {} Loss: {:.4f}\".format(i+1, 3, j, len(train_dataloader),\n",
    "                                                                      loss))\n",
    "torch.save(model.state_dict(), 'distilmodelA.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5087483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('distilmodelA.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "loss_total = 0\n",
    "results = defaultdict(dict)\n",
    "predictions, true_vals = [], []\n",
    "for j, data in enumerate(test_dataloader):\n",
    "    inputs = {'input_ids': data[0].cuda(), \n",
    "              'attention_mask': data[1].cuda(), \n",
    "              'labels': data[2].cuda()}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "    loss = output[0]\n",
    "    logits = output[1]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    labels = inputs['labels'].cpu().numpy()\n",
    "    loss_total += loss.item()\n",
    "    predictions.append(logits)\n",
    "    true_vals.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96910c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_vals = np.concatenate(true_vals, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adc215a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.80      0.79      0.79      1017\n",
      "         pos       0.67      0.55      0.60       393\n",
      "         neu       0.92      0.94      0.93      4590\n",
      "\n",
      "    accuracy                           0.89      6000\n",
      "   macro avg       0.80      0.76      0.78      6000\n",
      "weighted avg       0.88      0.89      0.88      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds_flat = np.argmax(predictions, axis = 1).flatten()\n",
    "labels_flat = true_vals.flatten()\n",
    "target_names = ['neg', 'pos', 'neu']\n",
    "print(classification_report(labels_flat, preds_flat, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970f1717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335c0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14a29c11",
   "metadata": {},
   "source": [
    "# Evaluation on Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3bddf4",
   "metadata": {},
   "source": [
    "The Part B of this dataset consists of tweets related to the COVID-19 crises, social distancing, lockdown, and stay at home. All the models above will be used for our evaluation. The pre-processing and processing of the data is same as that for the whole dataset and that for Part A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0452e7d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    22496\n",
       "neg     5471\n",
       "pos     2033\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_senti = pd.read_csv(\"COVIDSenti-main/COVIDSenti-B.csv\")\n",
    "covid_senti[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91c663",
   "metadata": {},
   "source": [
    "As shown above, Part B consists of 22496 neutral samples, 5471 negative while 2033 rows labeled as postive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63ee4db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "import string\n",
    "\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "covid_senti['tokenized_tweet'] = [simple_preprocess(line, deacc=True) for line in covid_senti['tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[word.replace('\\n', '') for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[word.replace('#', '') for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[word.lower() for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[word.translate(table) for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[''.join(filter(lambda x: not word.startswith('https'), word)) for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[''.join(filter(lambda x: not word.startswith('@'), word)) for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "# print(covid_senti['tokenized_tweet'].sample(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c360babe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /s/chopin/a/grad/sanket96/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "covid_senti['lemmatized_tweet'] = [[wordnet_lemmatizer.lemmatize(word) for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "# print(covid_senti['lemmatized_tweet'].sample(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "484b2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(covid_senti)) < 0.8\n",
    "covid_senti_train = covid_senti[mask]\n",
    "covid_senti_test = covid_senti[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c11a72",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "819a6689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.00      0.00      0.00      1166\n",
      "         pos       0.00      0.00      0.00       379\n",
      "         neu       0.75      1.00      0.86      4579\n",
      "\n",
      "    accuracy                           0.75      6124\n",
      "   macro avg       0.25      0.33      0.29      6124\n",
      "weighted avg       0.56      0.75      0.64      6124\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayes():\n",
    "\n",
    "    def __init__(self):\n",
    "        # be sure to use the right class_dict for each data set\n",
    "        self.class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "        # self.class_dict = {'action': 0, 'comedy': 1}\n",
    "        self.feature_dict = {}\n",
    "        self.prior = np.zeros(len(self.class_dict))\n",
    "        self.likelihood = None\n",
    "    '''\n",
    "    Trains a multinomial Naive Bayes classifier on a training set.\n",
    "    Specifically, fills in self.prior and self.likelihood such that:\n",
    "    self.prior[class] = log(P(class))\n",
    "    self.likelihood[class][feature] = log(P(feature|class))\n",
    "    '''\n",
    "    def train(self, train_set):\n",
    "        self.feature_dict = self.select_features(train_set)\n",
    "        # iterate over training documents\n",
    "        self.likelihood = np.zeros((len(self.class_dict), len(self.feature_dict)))\n",
    "        doc_per_class = {}\n",
    "        word_count = {}\n",
    "        total_words_per_class = {}\n",
    "        vocabulary = set()\n",
    "        for index, row in train_set.iterrows():\n",
    "            class_name = row['label']\n",
    "            if (class_name in self.class_dict):\n",
    "                doc_per_class[class_name] = 1 + doc_per_class.get(class_name, 0)\n",
    "                    # collect class counts and feature counts\n",
    "                data = row['lemmatized_tweet']\n",
    "                for word in data:\n",
    "                    vocabulary.add(word)\n",
    "                    word_count[(word, class_name)] = 1 + word_count.get((word, class_name), 0)\n",
    "        # normalize counts to probabilities, and take logs\n",
    "        for class_name in self.class_dict:\n",
    "            counts = [v for k, v in word_count.items() if k[1] == class_name]\n",
    "            total_words_per_class[class_name] = sum(counts)\n",
    "        for word in self.feature_dict:\n",
    "            for class_name in self.class_dict:\n",
    "                self.likelihood[self.class_dict.get(class_name)][self.feature_dict.get(word)] = np.log(((word_count.get((word,\n",
    "                                                class_name), 0) + 1)/(total_words_per_class[class_name] + len(vocabulary))))\n",
    "        for class_name in self.class_dict:\n",
    "            self.prior[self.class_dict[class_name]] = np.log((doc_per_class[class_name] / sum(doc_per_class.values())))\n",
    "    '''\n",
    "    Tests the classifier on a development or test set.\n",
    "    Returns a dictionary of filenames mapped to their correct and predicted\n",
    "    classes such that:\n",
    "    results[filename]['correct'] = correct class\n",
    "    results[filename]['predicted'] = predicted class\n",
    "    '''\n",
    "    def test(self, dev_set):\n",
    "        pred_labels = []\n",
    "        true_labels = []\n",
    "        # iterate over testing documents\n",
    "        for index, row in dev_set.iterrows():\n",
    "            class_name = row['label']\n",
    "            # create feature vectors for each document\n",
    "            word_count = {}\n",
    "            true_labels.append(self.class_dict[class_name])\n",
    "            data = str(row['lemmatized_tweet'])\n",
    "            for word in data:\n",
    "                if word in self.feature_dict:\n",
    "                    word_count[word] = 1 + word_count.get(word, 0)\n",
    "            feature_vector = np.zeros((len(self.feature_dict), 1))\n",
    "            for i, word in enumerate(self.feature_dict):\n",
    "                feature_vector[i] = word_count.get(word, 0)\n",
    "            self.prior = np.reshape(self.prior, (self.prior.shape[0], 1))\n",
    "            probability = self.prior + np.matmul(self.likelihood, feature_vector)\n",
    "            pred_labels.append(np.argmax(probability))\n",
    "                # get most likely class\n",
    "        # print(dict(results))\n",
    "        return pred_labels, true_labels\n",
    "\n",
    "    '''\n",
    "    Given results, calculates the following:\n",
    "    Precision, Recall, F1 for each class\n",
    "    Accuracy overall\n",
    "    Also, prints evaluation metrics in readable format.\n",
    "    '''\n",
    "    def evaluate(self, results):\n",
    "        # you may find this helpful\n",
    "        target_names = ['neg', 'pos', 'neu']\n",
    "        print(classification_report(results[1], results[0], target_names=target_names))\n",
    "    '''\n",
    "    Performs feature selection.\n",
    "    Returns a dictionary of features.\n",
    "    '''\n",
    "    def select_features(self, train_set):\n",
    "        # almost any method of feature selection is fine here\n",
    "        doc_per_class = {}\n",
    "        word_count = {}\n",
    "        total_words_per_class = {}\n",
    "        vocabulary = set()\n",
    "        likelihood_ratio = {}\n",
    "        for index, row in train_set.iterrows():\n",
    "            class_name = row['label']\n",
    "            if (class_name in self.class_dict):\n",
    "                doc_per_class[class_name] = 1 + doc_per_class.get(class_name, 0)\n",
    "                    # collect class counts and feature counts\n",
    "                data = row['lemmatized_tweet']\n",
    "                for word in data:\n",
    "                    vocabulary.add(word)\n",
    "                    word_count[(word, class_name)] = 1 + word_count.get((word, class_name), 0)\n",
    "        # normalize counts to probabilities, and take logs\n",
    "        for class_name in self.class_dict:\n",
    "            counts = [v for k, v in word_count.items() if k[1] == class_name]\n",
    "            total_words_per_class[class_name] = sum(counts)\n",
    "        prob_class = np.zeros((3, 1))\n",
    "        for i, class_name in enumerate(self.class_dict):\n",
    "            prob_class[i] = (doc_per_class[class_name] / sum(doc_per_class.values()))\n",
    "        for word in vocabulary:\n",
    "            class_probs = [1] * len(self.class_dict)\n",
    "            for i, class_name in enumerate(self.class_dict):\n",
    "                class_probs[i] = (word_count.get((word,\n",
    "                                      class_name), 0) + 1) / (total_words_per_class[class_name] + len(vocabulary))\n",
    "                class_probs[i] = class_probs[i] / prob_class[i]\n",
    "            likelihood_ratio[word] = (1 / class_probs[0]) * (1 / class_probs[1]) * (1 / class_probs[2])\n",
    "        #likelihood_ratio_pos = dict(sorted(likelihood_ratio.items(), key=lambda item: item[1], reverse=True))\n",
    "        likelihood_ratio = dict(sorted(likelihood_ratio.items(), key=lambda item: item[1]))\n",
    "        words = []\n",
    "        words.extend(list(likelihood_ratio.keys())[:750])\n",
    "        #words.extend(list(likelihood_ratio_pos.keys())[:750])\n",
    "        # for class_name in self.class_dict:\n",
    "        #     self.prior[self.class_dict[class_name]] = np.log((doc_per_class[class_name] / sum(doc_per_class.values())))\n",
    "        features = {}\n",
    "        for i, word in enumerate(words):\n",
    "            features[word] = i\n",
    "        return features\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nb = NaiveBayes()\n",
    "    # make sure these point to the right directories\n",
    "    nb.train(covid_senti_train)\n",
    "    # nb.train('movie_reviews_small/train')\n",
    "    results = nb.test(covid_senti_test)\n",
    "    # results = nb.test('movie_reviews_small/test')\n",
    "    nb.evaluate(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a46556",
   "metadata": {},
   "source": [
    "## Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ae988ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1006688/623227358.py:111: RuntimeWarning: divide by zero encountered in log\n",
      "  loss += -((y @ np.log(y_hat)) + ((1 - y) @ np.log(1 - y_hat)))\n",
      "/tmp/ipykernel_1006688/623227358.py:111: RuntimeWarning: invalid value encountered in matmul\n",
      "  loss += -((y @ np.log(y_hat)) + ((1 - y) @ np.log(1 - y_hat)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: nan\n",
      "Epoch 2 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 11 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 12 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 13 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 14 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 15 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 16 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 17 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 18 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 19 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 20 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 21 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 22 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 23 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 24 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 25 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 26 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 27 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 28 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 29 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 30 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 31 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 32 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 33 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 34 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 35 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 36 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 37 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 38 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 39 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 40 out of 40\n",
      "Average Train Loss: nan\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.00      0.00      0.00      1166\n",
      "         pos       0.06      1.00      0.12       379\n",
      "         neu       0.00      0.00      0.00      4579\n",
      "\n",
      "    accuracy                           0.06      6124\n",
      "   macro avg       0.02      0.33      0.04      6124\n",
      "weighted avg       0.00      0.06      0.01      6124\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# CS542 Fall 2021 Programming Assignment 2\n",
    "# Logistic Regression Classifier\n",
    "\n",
    "'''\n",
    "Computes the logistic function.\n",
    "'''\n",
    "\n",
    "\n",
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, n_features=400):\n",
    "        # be sure to use the right class_dict for each data set\n",
    "        self.theta = None\n",
    "        self.n_features = n_features\n",
    "        self.feature_dict = None\n",
    "        self.class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "        # self.class_dict = {'action': 0, 'comedy': 1}\n",
    "        # use of self.feature_dict is optional for this assignment\n",
    "        self.feature_dict = self.select_features(covid_senti_train)\n",
    "\n",
    "    '''\n",
    "    Loads a dataset. Specifically, returns a list of filenames, and dictionaries\n",
    "    of classes and documents such that:\n",
    "    classes[filename] = class of the document\n",
    "    documents[filename] = feature vector for the document (use self.featurize)\n",
    "    '''\n",
    "\n",
    "    def select_features(self, data_set):\n",
    "        feature_count = {}\n",
    "        for index, row in data_set.iterrows():\n",
    "            data = str(row['lemmatized_tweet']).split()\n",
    "            for word in data:\n",
    "                feature_count[word] = 1 + feature_count.get(word, 0)\n",
    "\n",
    "        feature_count = list(dict(sorted(feature_count.items(), key=lambda v: v[1], reverse=True)).keys())[:500]\n",
    "        features = {}\n",
    "\n",
    "        for i, word in enumerate(feature_count):\n",
    "            features[word] = i\n",
    "        return features\n",
    "\n",
    "    def load_data(self, data_set):\n",
    "        filenames = []\n",
    "        classes = dict()\n",
    "        documents = dict()\n",
    "        # iterate over documents\n",
    "        for index, row in data_set.iterrows():\n",
    "            # your code here\n",
    "            # BEGIN STUDENT CODE\n",
    "            # if os.path.isfile(os.path.join(root, name)):\n",
    "            class_name = row['label']\n",
    "            classes[index] = self.class_dict[class_name]\n",
    "            documents[index] = self.featurize(row['lemmatized_tweet'])\n",
    "            # END STUDENT CODE\n",
    "        return classes, documents\n",
    "\n",
    "    '''\n",
    "    Given a document (as a list of words), returns a feature vector.\n",
    "    Note that the last element of the vector, corresponding to the bias, is a\n",
    "    \"dummy feature\" with value 1.\n",
    "    '''\n",
    "\n",
    "    def featurize(self, document):\n",
    "        vector = np.zeros(self.n_features + 1)\n",
    "        # BEGIN STUDENT CODE\n",
    "        for word in document:\n",
    "            if word in self.feature_dict:\n",
    "                if word not in w2v_model.wv.key_to_index:\n",
    "                    vector.extend([0] * 500)\n",
    "                else:\n",
    "                    vector.extend(w2v_model.wv[word])\n",
    "        # END STUDENT CODE\n",
    "        vector[-1] = 1\n",
    "        return vector\n",
    "\n",
    "    '''\n",
    "    Trains a logistic regression classifier on a training set.\n",
    "    '''\n",
    "\n",
    "    def train(self, train_set, batch_size=3, n_epochs=1, eta=0.1):\n",
    "        # if train_set == \"movie_reviews_small/train\":\n",
    "        #     self.feature_dict = {'fast': 0, 'couple': 1, 'shoot': 2, 'fly': 3}\n",
    "        # else:\n",
    "        #     self.feature_dict = self.select_features(train_set)\n",
    "        # self.n_features = len(self.feature_dict)\n",
    "        self.theta = np.zeros(self.n_features + 1)  # weights (and bias)\n",
    "        classes, documents = self.load_data(train_set)\n",
    "        n_minibatches = ceil(len(train_set) / batch_size)\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "            loss = 0\n",
    "            for i in range(n_minibatches):\n",
    "                # list of filenames in minibatch\n",
    "                minibatch = train_set[i * batch_size: (i + 1) * batch_size]\n",
    "                # BEGIN STUDENT CODE\n",
    "                # create and fill in matrix x and vector y\n",
    "                x = np.zeros((len(minibatch), self.n_features + 1))\n",
    "                y = np.zeros(len(minibatch))\n",
    "                k = 0\n",
    "                for j, row in minibatch.iterrows():\n",
    "                    x[k][:] = documents[j]\n",
    "                    y[k] = classes[j]\n",
    "                    k += 1\n",
    "                # compute y_hat\n",
    "                y_hat = sigma(np.dot(x, self.theta))\n",
    "                # update loss\n",
    "                loss += -((y @ np.log(y_hat)) + ((1 - y) @ np.log(1 - y_hat)))\n",
    "                # compute gradient\n",
    "                gradient = np.dot(x.T, np.subtract(y_hat, y)) / len(minibatch)\n",
    "                # update weights (and bias)\n",
    "                self.theta = self.theta - (eta * gradient)\n",
    "                # END STUDENT CODE\n",
    "            loss /= len(train_set)\n",
    "            print(\"Average Train Loss: {}\".format(loss))\n",
    "            # randomize order\n",
    "            #Random(epoch).shuffle(train_set)\n",
    "\n",
    "    '''\n",
    "    Tests the classifier on a development or test set.\n",
    "    Returns a dictionary of filenames mapped to their correct and predicted\n",
    "    classes such that:\n",
    "    results[filename]['correct'] = correct class\n",
    "    results[filename]['predicted'] = predicted class\n",
    "    '''\n",
    "\n",
    "    def test(self, dev_set):\n",
    "        pred_labels = []\n",
    "        true_labels = []\n",
    "        classes, documents = self.load_data(dev_set)\n",
    "        for index, row in dev_set.iterrows():\n",
    "            # BEGIN STUDENT CODE\n",
    "            # get most likely class (recall that P(y=1|x) = y_hat)\n",
    "            true_labels.append(classes[index])\n",
    "            prediction = sigma(np.dot(documents[index], self.theta))\n",
    "            pred_label = 1 if prediction > 0.5 else 0\n",
    "            pred_labels.append(pred_label)\n",
    "            # END STUDENT CODE\n",
    "        return pred_labels, true_labels\n",
    "\n",
    "    '''\n",
    "    Given results, calculates the following:\n",
    "    Precision, Recall, F1 for each class\n",
    "    Accuracy overall\n",
    "    Also, prints evaluation metrics in readable format.\n",
    "    '''\n",
    "\n",
    "    def evaluate(self, results):\n",
    "        # you can copy and paste your code from PA1 here\n",
    "        target_names = ['neg', 'pos', 'neu']\n",
    "        print(classification_report(results[1], results[0], target_names=target_names))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lr = LogisticRegression(n_features=750)\n",
    "    # make sure these point to the right directories\n",
    "    batch_size = [1, 2, 3, 8, 16, 32]\n",
    "    n_epochs = [1, 5, 10, 20, 30, 40]\n",
    "    eta = [0.025, 0.05, 0.1, 0.2, 0.4]\n",
    "\n",
    "    # code for grid search\n",
    "#     for b in batch_size:\n",
    "#         for n in n_epochs:\n",
    "#             for ler in eta:\n",
    "#                 lr.train(covid_senti_train, batch_size=b, n_epochs=n, eta=ler)\n",
    "#                 results = lr.test(covid_senti_test)\n",
    "#                 lr.evaluate(results)\n",
    "#                 print(\"Accuracy is for batch size: \", b, \", n_epochs: \", n, \"eta: \", ler)\n",
    "\n",
    "    # best features from grid search\n",
    "    lr.train(covid_senti_train, batch_size=3, n_epochs=40, eta=0.05)\n",
    "    results = lr.test(covid_senti_test)\n",
    "    # lr.train('movie_reviews_small/train', batch_size=3, n_epochs=1, eta=0.1)\n",
    "    # results = lr.test('movie_reviews_small/test')\n",
    "    lr.evaluate(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f64134",
   "metadata": {},
   "source": [
    "## CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "545737be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import gensim\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86ecb5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "tweets = list(covid_senti['lemmatized_tweet'].values)\n",
    "tweets.append(['pad'])\n",
    "w2v_model = Word2Vec(tweets, min_count = 1, vector_size = 500, workers = 3, window = 3, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87aa4aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>lemmatized_tweet</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Coronavirus fears expose a cultural divide ove...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[coronavirus, fears, expose, cultural, divide,...</td>\n",
       "      <td>[coronavirus, fear, expose, cultural, divide, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronavirus Live Updates: Global Outbreak Rais...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[coronavirus, live, updates, global, outbreak,...</td>\n",
       "      <td>[coronavirus, live, update, global, outbreak, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ruling party, government mull W10tr to tackle ...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[ruling, party, government, mull, tr, to, tack...</td>\n",
       "      <td>[ruling, party, government, mull, tr, to, tack...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exclusive: Thousands in Coronavirus Epicenter ...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[exclusive, thousands, in, coronavirus, epicen...</td>\n",
       "      <td>[exclusive, thousand, in, coronavirus, epicent...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Queen_kimo_ Derp derp, what's that have to do...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[queenkimo, derp, derp, what, that, have, to, ...</td>\n",
       "      <td>[queenkimo, derp, derp, what, that, have, to, ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>What Happens if Coronavirus Forces U.S. Movie ...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[what, happens, if, coronavirus, forces, movie...</td>\n",
       "      <td>[what, happens, if, coronavirus, force, movie,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>Real question : how does it seem that people i...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[real, question, how, does, it, seem, that, pe...</td>\n",
       "      <td>[real, question, how, doe, it, seem, that, peo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>#NOOLUYO35: No Time to Die, the latest #JamesB...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[nooluyo, no, time, to, die, the, latest, jame...</td>\n",
       "      <td>[nooluyo, no, time, to, die, the, latest, jame...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>@ShilpiSinghINC @RahulGandhi Ye Italy se laute...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[shilpisinghinc, rahulgandhi, ye, italy, se, l...</td>\n",
       "      <td>[shilpisinghinc, rahulgandhi, ye, italy, se, l...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>This is interesting and well worth a read if y...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[this, is, interesting, and, well, worth, read...</td>\n",
       "      <td>[this, is, interesting, and, well, worth, read...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet label  \\\n",
       "0      Coronavirus fears expose a cultural divide ove...   neu   \n",
       "1      Coronavirus Live Updates: Global Outbreak Rais...   neu   \n",
       "2      Ruling party, government mull W10tr to tackle ...   neu   \n",
       "3      Exclusive: Thousands in Coronavirus Epicenter ...   neg   \n",
       "4      @Queen_kimo_ Derp derp, what's that have to do...   neu   \n",
       "...                                                  ...   ...   \n",
       "29995  What Happens if Coronavirus Forces U.S. Movie ...   neu   \n",
       "29996  Real question : how does it seem that people i...   neu   \n",
       "29997  #NOOLUYO35: No Time to Die, the latest #JamesB...   pos   \n",
       "29998  @ShilpiSinghINC @RahulGandhi Ye Italy se laute...   neu   \n",
       "29999  This is interesting and well worth a read if y...   neu   \n",
       "\n",
       "                                         tokenized_tweet  \\\n",
       "0      [coronavirus, fears, expose, cultural, divide,...   \n",
       "1      [coronavirus, live, updates, global, outbreak,...   \n",
       "2      [ruling, party, government, mull, tr, to, tack...   \n",
       "3      [exclusive, thousands, in, coronavirus, epicen...   \n",
       "4      [queenkimo, derp, derp, what, that, have, to, ...   \n",
       "...                                                  ...   \n",
       "29995  [what, happens, if, coronavirus, forces, movie...   \n",
       "29996  [real, question, how, does, it, seem, that, pe...   \n",
       "29997  [nooluyo, no, time, to, die, the, latest, jame...   \n",
       "29998  [shilpisinghinc, rahulgandhi, ye, italy, se, l...   \n",
       "29999  [this, is, interesting, and, well, worth, read...   \n",
       "\n",
       "                                        lemmatized_tweet  label_num  \n",
       "0      [coronavirus, fear, expose, cultural, divide, ...          2  \n",
       "1      [coronavirus, live, update, global, outbreak, ...          2  \n",
       "2      [ruling, party, government, mull, tr, to, tack...          2  \n",
       "3      [exclusive, thousand, in, coronavirus, epicent...          0  \n",
       "4      [queenkimo, derp, derp, what, that, have, to, ...          2  \n",
       "...                                                  ...        ...  \n",
       "29995  [what, happens, if, coronavirus, force, movie,...          2  \n",
       "29996  [real, question, how, doe, it, seem, that, peo...          2  \n",
       "29997  [nooluyo, no, time, to, die, the, latest, jame...          1  \n",
       "29998  [shilpisinghinc, rahulgandhi, ye, italy, se, l...          2  \n",
       "29999  [this, is, interesting, and, well, worth, read...          2  \n",
       "\n",
       "[30000 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "covid_senti['label_num'] = covid_senti.label.replace(class_dict)\n",
    "covid_senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "195bcc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = covid_senti['lemmatized_tweet'].map(len).max()\n",
    "def make_word_2_vec(sentence):\n",
    "    padding_idx = w2v_model.wv.key_to_index['pad']\n",
    "    padded_X = [padding_idx for i in range(max_len)]\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        if word not in w2v_model.wv.key_to_index:\n",
    "            padded_X[i] = 0\n",
    "            print(word)\n",
    "        else:\n",
    "            padded_X[i] = w2v_model.wv.key_to_index[word]\n",
    "        i += 1\n",
    "    return torch.tensor(padded_X, dtype=torch.long).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72acbcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 500\n",
    "NUM_FILTERS = 10\n",
    "\n",
    "class CnnTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, window_sizes=(1,2,3,5)):\n",
    "        super(CnnTextClassifier, self).__init__()\n",
    "        weights = w2v_model.wv\n",
    "        # With pretrained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors),\n",
    "                                                      padding_idx=w2v_model.wv.key_to_index['pad'])\n",
    "        # Without pretrained embeddings\n",
    "        # self.embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "                                   nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE],\n",
    "                                             padding=(window_size - 1, 0))\n",
    "                                   for window_size in window_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Apply a convolution + max_pool layer for each window size\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        xs = []\n",
    "        for conv in self.convs:\n",
    "            x2 = torch.tanh(conv(x))\n",
    "            x2 = torch.squeeze(x2, -1)\n",
    "            x2 = F.max_pool1d(x2, x2.size(2))\n",
    "            xs.append(x2)\n",
    "        x = torch.cat(xs, 2)\n",
    "\n",
    "        # FC\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d38e7e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch completed in: 73.7431 seconds\n",
      "1,0.8013223460504888\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Epoch completed in: 74.2900 seconds\n",
      "2,0.8010258935897429\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Epoch completed in: 77.4480 seconds\n",
      "3,0.8010258935522965\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 3\n",
    "VOCAB_SIZE = len(w2v_model.wv.key_to_index)\n",
    "\n",
    "cnn_model = CnnTextClassifier(vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\n",
    "# cnn_model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "num_epochs = 3\n",
    "\n",
    "# Open the file for writing loss\n",
    "class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "loss_file_name = 'cnn_class_big_loss_with_padding.csv'\n",
    "losses = []\n",
    "cnn_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch \" + str(epoch + 1))\n",
    "    train_loss = 0\n",
    "    for index, row in covid_senti_train.iterrows():\n",
    "        # Clearing the accumulated gradients\n",
    "        cnn_model.zero_grad()\n",
    "\n",
    "        # Make the bag of words vector for stemmed tokens \n",
    "        bow_vec = make_word_2_vec(row['lemmatized_tweet'])\n",
    "       \n",
    "        # Forward pass to get output\n",
    "        probs = cnn_model(bow_vec)\n",
    "\n",
    "        # Get the target label\n",
    "        target = torch.tensor([class_dict[row['label']]], dtype=torch.long)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = loss_function(probs, target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # if index == 0:\n",
    "    #     continue\n",
    "    print(\"Epoch completed in: %.4f seconds\" % (time.time()-start_time))\n",
    "    print(str((epoch+1)) + \",\" + str(train_loss / len(covid_senti_train)))\n",
    "    print('\\n')\n",
    "    train_loss = 0\n",
    "\n",
    "torch.save(cnn_model, 'cnn_big_model_500_with_paddingB.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "724e98d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.00      0.00      0.00         0\n",
      "         pos       0.00      0.00      0.00         0\n",
      "         neu       1.00      0.75      0.86      6124\n",
      "\n",
      "    accuracy                           0.75      6124\n",
      "   macro avg       0.33      0.25      0.29      6124\n",
      "weighted avg       1.00      0.75      0.86      6124\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions = []\n",
    "correct = []\n",
    "cnn_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = defaultdict(dict)\n",
    "    for index, row in covid_senti_test.iterrows():\n",
    "        bow_vec = make_word_2_vec(row['lemmatized_tweet'])\n",
    "        probs = cnn_model(bow_vec)\n",
    "        correct.append(class_dict[row['label']])\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        predictions.append(predicted.numpy()[0])\n",
    "target_names = ['neg', 'pos', 'neu']\n",
    "print(classification_report(predictions, correct, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeffcaab",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cdd28dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(covid_senti.index.values, \n",
    "                                                    covid_senti.label.values, test_size=0.2,\n",
    "                                                   stratify=covid_senti.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94944698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {'neg': 0, 'pos': 1, 'neu': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9501718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_senti['data_type'] = ['not_set'] * covid_senti.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f4ff01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_senti.loc[X_train, 'data_type'] = 'train'\n",
    "covid_senti.loc[X_test, 'data_type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "300949bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6287104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb39a931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/a/grad/sanket96/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2322: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='train'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')\n",
    "\n",
    "encoded_data_test = tokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='test'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f337e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(covid_senti[covid_senti.data_type == 'train'].label.values)\n",
    "\n",
    "#validation set\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(covid_senti[covid_senti.data_type == 'test'].label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f923156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(class_dict),\n",
    "                                                     output_attentions = False,\n",
    "                                                      output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b07751c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8f6a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train,labels_train)\n",
    "\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d679f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "46667f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dd359a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 3, Step: 0 / 750 Loss: 1.1228\n",
      "Epoch: 1 / 3, Step: 1 / 750 Loss: 1.1532\n",
      "Epoch: 1 / 3, Step: 2 / 750 Loss: 1.0181\n",
      "Epoch: 1 / 3, Step: 3 / 750 Loss: 1.0761\n",
      "Epoch: 1 / 3, Step: 4 / 750 Loss: 0.9837\n",
      "Epoch: 1 / 3, Step: 5 / 750 Loss: 0.9835\n",
      "Epoch: 1 / 3, Step: 6 / 750 Loss: 0.9329\n",
      "Epoch: 1 / 3, Step: 7 / 750 Loss: 0.8398\n",
      "Epoch: 1 / 3, Step: 8 / 750 Loss: 0.9530\n",
      "Epoch: 1 / 3, Step: 9 / 750 Loss: 0.9688\n",
      "Epoch: 1 / 3, Step: 10 / 750 Loss: 0.8938\n",
      "Epoch: 1 / 3, Step: 11 / 750 Loss: 0.9393\n",
      "Epoch: 1 / 3, Step: 12 / 750 Loss: 0.8730\n",
      "Epoch: 1 / 3, Step: 13 / 750 Loss: 0.7420\n",
      "Epoch: 1 / 3, Step: 14 / 750 Loss: 0.9393\n",
      "Epoch: 1 / 3, Step: 15 / 750 Loss: 0.7057\n",
      "Epoch: 1 / 3, Step: 16 / 750 Loss: 0.6884\n",
      "Epoch: 1 / 3, Step: 17 / 750 Loss: 0.7565\n",
      "Epoch: 1 / 3, Step: 18 / 750 Loss: 0.7399\n",
      "Epoch: 1 / 3, Step: 19 / 750 Loss: 0.6450\n",
      "Epoch: 1 / 3, Step: 20 / 750 Loss: 0.6797\n",
      "Epoch: 1 / 3, Step: 21 / 750 Loss: 0.5143\n",
      "Epoch: 1 / 3, Step: 22 / 750 Loss: 0.4315\n",
      "Epoch: 1 / 3, Step: 23 / 750 Loss: 0.8014\n",
      "Epoch: 1 / 3, Step: 24 / 750 Loss: 0.8653\n",
      "Epoch: 1 / 3, Step: 25 / 750 Loss: 0.5517\n",
      "Epoch: 1 / 3, Step: 26 / 750 Loss: 0.6904\n",
      "Epoch: 1 / 3, Step: 27 / 750 Loss: 0.7767\n",
      "Epoch: 1 / 3, Step: 28 / 750 Loss: 0.8106\n",
      "Epoch: 1 / 3, Step: 29 / 750 Loss: 0.6251\n",
      "Epoch: 1 / 3, Step: 30 / 750 Loss: 0.5905\n",
      "Epoch: 1 / 3, Step: 31 / 750 Loss: 0.8294\n",
      "Epoch: 1 / 3, Step: 32 / 750 Loss: 0.6395\n",
      "Epoch: 1 / 3, Step: 33 / 750 Loss: 0.8451\n",
      "Epoch: 1 / 3, Step: 34 / 750 Loss: 0.7612\n",
      "Epoch: 1 / 3, Step: 35 / 750 Loss: 0.4444\n",
      "Epoch: 1 / 3, Step: 36 / 750 Loss: 0.7188\n",
      "Epoch: 1 / 3, Step: 37 / 750 Loss: 0.7192\n",
      "Epoch: 1 / 3, Step: 38 / 750 Loss: 0.7066\n",
      "Epoch: 1 / 3, Step: 39 / 750 Loss: 0.6859\n",
      "Epoch: 1 / 3, Step: 40 / 750 Loss: 0.6136\n",
      "Epoch: 1 / 3, Step: 41 / 750 Loss: 1.0258\n",
      "Epoch: 1 / 3, Step: 42 / 750 Loss: 0.7786\n",
      "Epoch: 1 / 3, Step: 43 / 750 Loss: 0.6355\n",
      "Epoch: 1 / 3, Step: 44 / 750 Loss: 0.6489\n",
      "Epoch: 1 / 3, Step: 45 / 750 Loss: 0.6462\n",
      "Epoch: 1 / 3, Step: 46 / 750 Loss: 0.8048\n",
      "Epoch: 1 / 3, Step: 47 / 750 Loss: 0.6773\n",
      "Epoch: 1 / 3, Step: 48 / 750 Loss: 0.8160\n",
      "Epoch: 1 / 3, Step: 49 / 750 Loss: 0.6443\n",
      "Epoch: 1 / 3, Step: 50 / 750 Loss: 0.6305\n",
      "Epoch: 1 / 3, Step: 51 / 750 Loss: 0.9900\n",
      "Epoch: 1 / 3, Step: 52 / 750 Loss: 0.5210\n",
      "Epoch: 1 / 3, Step: 53 / 750 Loss: 0.7860\n",
      "Epoch: 1 / 3, Step: 54 / 750 Loss: 0.5244\n",
      "Epoch: 1 / 3, Step: 55 / 750 Loss: 0.5604\n",
      "Epoch: 1 / 3, Step: 56 / 750 Loss: 0.6586\n",
      "Epoch: 1 / 3, Step: 57 / 750 Loss: 0.7313\n",
      "Epoch: 1 / 3, Step: 58 / 750 Loss: 0.7232\n",
      "Epoch: 1 / 3, Step: 59 / 750 Loss: 0.6563\n",
      "Epoch: 1 / 3, Step: 60 / 750 Loss: 0.4519\n",
      "Epoch: 1 / 3, Step: 61 / 750 Loss: 0.5983\n",
      "Epoch: 1 / 3, Step: 62 / 750 Loss: 0.5398\n",
      "Epoch: 1 / 3, Step: 63 / 750 Loss: 0.4988\n",
      "Epoch: 1 / 3, Step: 64 / 750 Loss: 0.6206\n",
      "Epoch: 1 / 3, Step: 65 / 750 Loss: 1.0551\n",
      "Epoch: 1 / 3, Step: 66 / 750 Loss: 0.9406\n",
      "Epoch: 1 / 3, Step: 67 / 750 Loss: 0.6805\n",
      "Epoch: 1 / 3, Step: 68 / 750 Loss: 0.6131\n",
      "Epoch: 1 / 3, Step: 69 / 750 Loss: 0.8968\n",
      "Epoch: 1 / 3, Step: 70 / 750 Loss: 0.7656\n",
      "Epoch: 1 / 3, Step: 71 / 750 Loss: 0.5143\n",
      "Epoch: 1 / 3, Step: 72 / 750 Loss: 0.6813\n",
      "Epoch: 1 / 3, Step: 73 / 750 Loss: 0.5056\n",
      "Epoch: 1 / 3, Step: 74 / 750 Loss: 0.7688\n",
      "Epoch: 1 / 3, Step: 75 / 750 Loss: 0.6566\n",
      "Epoch: 1 / 3, Step: 76 / 750 Loss: 0.6633\n",
      "Epoch: 1 / 3, Step: 77 / 750 Loss: 0.4862\n",
      "Epoch: 1 / 3, Step: 78 / 750 Loss: 0.6004\n",
      "Epoch: 1 / 3, Step: 79 / 750 Loss: 0.5057\n",
      "Epoch: 1 / 3, Step: 80 / 750 Loss: 0.7374\n",
      "Epoch: 1 / 3, Step: 81 / 750 Loss: 0.5848\n",
      "Epoch: 1 / 3, Step: 82 / 750 Loss: 0.6411\n",
      "Epoch: 1 / 3, Step: 83 / 750 Loss: 0.7518\n",
      "Epoch: 1 / 3, Step: 84 / 750 Loss: 0.8408\n",
      "Epoch: 1 / 3, Step: 85 / 750 Loss: 0.6277\n",
      "Epoch: 1 / 3, Step: 86 / 750 Loss: 0.6773\n",
      "Epoch: 1 / 3, Step: 87 / 750 Loss: 0.7168\n",
      "Epoch: 1 / 3, Step: 88 / 750 Loss: 0.6182\n",
      "Epoch: 1 / 3, Step: 89 / 750 Loss: 0.4548\n",
      "Epoch: 1 / 3, Step: 90 / 750 Loss: 0.6188\n",
      "Epoch: 1 / 3, Step: 91 / 750 Loss: 0.9401\n",
      "Epoch: 1 / 3, Step: 92 / 750 Loss: 0.8383\n",
      "Epoch: 1 / 3, Step: 93 / 750 Loss: 0.8967\n",
      "Epoch: 1 / 3, Step: 94 / 750 Loss: 0.7813\n",
      "Epoch: 1 / 3, Step: 95 / 750 Loss: 0.9382\n",
      "Epoch: 1 / 3, Step: 96 / 750 Loss: 0.5819\n",
      "Epoch: 1 / 3, Step: 97 / 750 Loss: 0.7324\n",
      "Epoch: 1 / 3, Step: 98 / 750 Loss: 0.6334\n",
      "Epoch: 1 / 3, Step: 99 / 750 Loss: 0.6823\n",
      "Epoch: 1 / 3, Step: 100 / 750 Loss: 0.5277\n",
      "Epoch: 1 / 3, Step: 101 / 750 Loss: 0.5484\n",
      "Epoch: 1 / 3, Step: 102 / 750 Loss: 0.4486\n",
      "Epoch: 1 / 3, Step: 103 / 750 Loss: 0.6848\n",
      "Epoch: 1 / 3, Step: 104 / 750 Loss: 0.6485\n",
      "Epoch: 1 / 3, Step: 105 / 750 Loss: 0.7943\n",
      "Epoch: 1 / 3, Step: 106 / 750 Loss: 0.6982\n",
      "Epoch: 1 / 3, Step: 107 / 750 Loss: 0.7947\n",
      "Epoch: 1 / 3, Step: 108 / 750 Loss: 0.4957\n",
      "Epoch: 1 / 3, Step: 109 / 750 Loss: 0.6321\n",
      "Epoch: 1 / 3, Step: 110 / 750 Loss: 0.6381\n",
      "Epoch: 1 / 3, Step: 111 / 750 Loss: 0.5546\n",
      "Epoch: 1 / 3, Step: 112 / 750 Loss: 0.7208\n",
      "Epoch: 1 / 3, Step: 113 / 750 Loss: 0.4174\n",
      "Epoch: 1 / 3, Step: 114 / 750 Loss: 0.8334\n",
      "Epoch: 1 / 3, Step: 115 / 750 Loss: 0.7024\n",
      "Epoch: 1 / 3, Step: 116 / 750 Loss: 0.7666\n",
      "Epoch: 1 / 3, Step: 117 / 750 Loss: 0.5409\n",
      "Epoch: 1 / 3, Step: 118 / 750 Loss: 0.8149\n",
      "Epoch: 1 / 3, Step: 119 / 750 Loss: 0.7006\n",
      "Epoch: 1 / 3, Step: 120 / 750 Loss: 0.5421\n",
      "Epoch: 1 / 3, Step: 121 / 750 Loss: 0.7983\n",
      "Epoch: 1 / 3, Step: 122 / 750 Loss: 0.5588\n",
      "Epoch: 1 / 3, Step: 123 / 750 Loss: 0.6804\n",
      "Epoch: 1 / 3, Step: 124 / 750 Loss: 0.8251\n",
      "Epoch: 1 / 3, Step: 125 / 750 Loss: 0.6524\n",
      "Epoch: 1 / 3, Step: 126 / 750 Loss: 0.5698\n",
      "Epoch: 1 / 3, Step: 127 / 750 Loss: 0.6099\n",
      "Epoch: 1 / 3, Step: 128 / 750 Loss: 0.3794\n",
      "Epoch: 1 / 3, Step: 129 / 750 Loss: 0.7059\n",
      "Epoch: 1 / 3, Step: 130 / 750 Loss: 0.5577\n",
      "Epoch: 1 / 3, Step: 131 / 750 Loss: 0.5634\n",
      "Epoch: 1 / 3, Step: 132 / 750 Loss: 0.6408\n",
      "Epoch: 1 / 3, Step: 133 / 750 Loss: 0.5672\n",
      "Epoch: 1 / 3, Step: 134 / 750 Loss: 0.2813\n",
      "Epoch: 1 / 3, Step: 135 / 750 Loss: 0.4594\n",
      "Epoch: 1 / 3, Step: 136 / 750 Loss: 0.6434\n",
      "Epoch: 1 / 3, Step: 137 / 750 Loss: 0.8287\n",
      "Epoch: 1 / 3, Step: 138 / 750 Loss: 0.7807\n",
      "Epoch: 1 / 3, Step: 139 / 750 Loss: 0.7938\n",
      "Epoch: 1 / 3, Step: 140 / 750 Loss: 0.7410\n",
      "Epoch: 1 / 3, Step: 141 / 750 Loss: 0.6738\n",
      "Epoch: 1 / 3, Step: 142 / 750 Loss: 0.5593\n",
      "Epoch: 1 / 3, Step: 143 / 750 Loss: 0.7795\n",
      "Epoch: 1 / 3, Step: 144 / 750 Loss: 0.5468\n",
      "Epoch: 1 / 3, Step: 145 / 750 Loss: 0.6881\n",
      "Epoch: 1 / 3, Step: 146 / 750 Loss: 0.6226\n",
      "Epoch: 1 / 3, Step: 147 / 750 Loss: 0.7973\n",
      "Epoch: 1 / 3, Step: 148 / 750 Loss: 0.9471\n",
      "Epoch: 1 / 3, Step: 149 / 750 Loss: 0.6098\n",
      "Epoch: 1 / 3, Step: 150 / 750 Loss: 0.4375\n",
      "Epoch: 1 / 3, Step: 151 / 750 Loss: 0.8700\n",
      "Epoch: 1 / 3, Step: 152 / 750 Loss: 0.7950\n",
      "Epoch: 1 / 3, Step: 153 / 750 Loss: 0.7898\n",
      "Epoch: 1 / 3, Step: 154 / 750 Loss: 0.7471\n",
      "Epoch: 1 / 3, Step: 155 / 750 Loss: 0.6794\n",
      "Epoch: 1 / 3, Step: 156 / 750 Loss: 0.7096\n",
      "Epoch: 1 / 3, Step: 157 / 750 Loss: 0.9892\n",
      "Epoch: 1 / 3, Step: 158 / 750 Loss: 0.5057\n",
      "Epoch: 1 / 3, Step: 159 / 750 Loss: 0.7107\n",
      "Epoch: 1 / 3, Step: 160 / 750 Loss: 0.7475\n",
      "Epoch: 1 / 3, Step: 161 / 750 Loss: 0.6719\n",
      "Epoch: 1 / 3, Step: 162 / 750 Loss: 0.6112\n",
      "Epoch: 1 / 3, Step: 163 / 750 Loss: 0.8963\n",
      "Epoch: 1 / 3, Step: 164 / 750 Loss: 0.7628\n",
      "Epoch: 1 / 3, Step: 165 / 750 Loss: 0.6147\n",
      "Epoch: 1 / 3, Step: 166 / 750 Loss: 1.1326\n",
      "Epoch: 1 / 3, Step: 167 / 750 Loss: 0.7369\n",
      "Epoch: 1 / 3, Step: 168 / 750 Loss: 0.7568\n",
      "Epoch: 1 / 3, Step: 169 / 750 Loss: 0.8165\n",
      "Epoch: 1 / 3, Step: 170 / 750 Loss: 0.5787\n",
      "Epoch: 1 / 3, Step: 171 / 750 Loss: 0.8811\n",
      "Epoch: 1 / 3, Step: 172 / 750 Loss: 0.5998\n",
      "Epoch: 1 / 3, Step: 173 / 750 Loss: 0.6882\n",
      "Epoch: 1 / 3, Step: 174 / 750 Loss: 0.6511\n",
      "Epoch: 1 / 3, Step: 175 / 750 Loss: 0.7959\n",
      "Epoch: 1 / 3, Step: 176 / 750 Loss: 0.7579\n",
      "Epoch: 1 / 3, Step: 177 / 750 Loss: 0.8121\n",
      "Epoch: 1 / 3, Step: 178 / 750 Loss: 0.5266\n",
      "Epoch: 1 / 3, Step: 179 / 750 Loss: 0.6940\n",
      "Epoch: 1 / 3, Step: 180 / 750 Loss: 0.7162\n",
      "Epoch: 1 / 3, Step: 181 / 750 Loss: 0.7571\n",
      "Epoch: 1 / 3, Step: 182 / 750 Loss: 0.5581\n",
      "Epoch: 1 / 3, Step: 183 / 750 Loss: 0.6891\n",
      "Epoch: 1 / 3, Step: 184 / 750 Loss: 0.5371\n",
      "Epoch: 1 / 3, Step: 185 / 750 Loss: 0.8272\n",
      "Epoch: 1 / 3, Step: 186 / 750 Loss: 0.5806\n",
      "Epoch: 1 / 3, Step: 187 / 750 Loss: 0.7842\n",
      "Epoch: 1 / 3, Step: 188 / 750 Loss: 0.7669\n",
      "Epoch: 1 / 3, Step: 189 / 750 Loss: 0.8758\n",
      "Epoch: 1 / 3, Step: 190 / 750 Loss: 0.6633\n",
      "Epoch: 1 / 3, Step: 191 / 750 Loss: 0.9218\n",
      "Epoch: 1 / 3, Step: 192 / 750 Loss: 0.5125\n",
      "Epoch: 1 / 3, Step: 193 / 750 Loss: 0.8855\n",
      "Epoch: 1 / 3, Step: 194 / 750 Loss: 0.8340\n",
      "Epoch: 1 / 3, Step: 195 / 750 Loss: 0.4281\n",
      "Epoch: 1 / 3, Step: 196 / 750 Loss: 0.6730\n",
      "Epoch: 1 / 3, Step: 197 / 750 Loss: 0.6838\n",
      "Epoch: 1 / 3, Step: 198 / 750 Loss: 0.6097\n",
      "Epoch: 1 / 3, Step: 199 / 750 Loss: 0.6893\n",
      "Epoch: 1 / 3, Step: 200 / 750 Loss: 0.6551\n",
      "Epoch: 1 / 3, Step: 201 / 750 Loss: 0.4572\n",
      "Epoch: 1 / 3, Step: 202 / 750 Loss: 0.3846\n",
      "Epoch: 1 / 3, Step: 203 / 750 Loss: 0.4257\n",
      "Epoch: 1 / 3, Step: 204 / 750 Loss: 0.9586\n",
      "Epoch: 1 / 3, Step: 205 / 750 Loss: 0.4991\n",
      "Epoch: 1 / 3, Step: 206 / 750 Loss: 0.8398\n",
      "Epoch: 1 / 3, Step: 207 / 750 Loss: 0.3990\n",
      "Epoch: 1 / 3, Step: 208 / 750 Loss: 0.6961\n",
      "Epoch: 1 / 3, Step: 209 / 750 Loss: 0.6787\n",
      "Epoch: 1 / 3, Step: 210 / 750 Loss: 0.5730\n",
      "Epoch: 1 / 3, Step: 211 / 750 Loss: 0.8189\n",
      "Epoch: 1 / 3, Step: 212 / 750 Loss: 0.6456\n",
      "Epoch: 1 / 3, Step: 213 / 750 Loss: 0.8126\n",
      "Epoch: 1 / 3, Step: 214 / 750 Loss: 0.6426\n",
      "Epoch: 1 / 3, Step: 215 / 750 Loss: 0.4749\n",
      "Epoch: 1 / 3, Step: 216 / 750 Loss: 0.6312\n",
      "Epoch: 1 / 3, Step: 217 / 750 Loss: 0.7003\n",
      "Epoch: 1 / 3, Step: 218 / 750 Loss: 0.6525\n",
      "Epoch: 1 / 3, Step: 219 / 750 Loss: 0.7193\n",
      "Epoch: 1 / 3, Step: 220 / 750 Loss: 0.6172\n",
      "Epoch: 1 / 3, Step: 221 / 750 Loss: 0.5778\n",
      "Epoch: 1 / 3, Step: 222 / 750 Loss: 0.6150\n",
      "Epoch: 1 / 3, Step: 223 / 750 Loss: 0.6892\n",
      "Epoch: 1 / 3, Step: 224 / 750 Loss: 0.7668\n",
      "Epoch: 1 / 3, Step: 225 / 750 Loss: 0.6781\n",
      "Epoch: 1 / 3, Step: 226 / 750 Loss: 0.7762\n",
      "Epoch: 1 / 3, Step: 227 / 750 Loss: 0.6658\n",
      "Epoch: 1 / 3, Step: 228 / 750 Loss: 0.6891\n",
      "Epoch: 1 / 3, Step: 229 / 750 Loss: 0.6778\n",
      "Epoch: 1 / 3, Step: 230 / 750 Loss: 0.5707\n",
      "Epoch: 1 / 3, Step: 231 / 750 Loss: 0.5064\n",
      "Epoch: 1 / 3, Step: 232 / 750 Loss: 0.4991\n",
      "Epoch: 1 / 3, Step: 233 / 750 Loss: 0.5880\n",
      "Epoch: 1 / 3, Step: 234 / 750 Loss: 0.5988\n",
      "Epoch: 1 / 3, Step: 235 / 750 Loss: 0.7252\n",
      "Epoch: 1 / 3, Step: 236 / 750 Loss: 0.8207\n",
      "Epoch: 1 / 3, Step: 237 / 750 Loss: 0.5508\n",
      "Epoch: 1 / 3, Step: 238 / 750 Loss: 0.5472\n",
      "Epoch: 1 / 3, Step: 239 / 750 Loss: 0.5439\n",
      "Epoch: 1 / 3, Step: 240 / 750 Loss: 0.6961\n",
      "Epoch: 1 / 3, Step: 241 / 750 Loss: 0.4895\n",
      "Epoch: 1 / 3, Step: 242 / 750 Loss: 0.7177\n",
      "Epoch: 1 / 3, Step: 243 / 750 Loss: 0.5108\n",
      "Epoch: 1 / 3, Step: 244 / 750 Loss: 0.6730\n",
      "Epoch: 1 / 3, Step: 245 / 750 Loss: 0.8319\n",
      "Epoch: 1 / 3, Step: 246 / 750 Loss: 0.4024\n",
      "Epoch: 1 / 3, Step: 247 / 750 Loss: 0.6055\n",
      "Epoch: 1 / 3, Step: 248 / 750 Loss: 0.5785\n",
      "Epoch: 1 / 3, Step: 249 / 750 Loss: 0.3928\n",
      "Epoch: 1 / 3, Step: 250 / 750 Loss: 0.5341\n",
      "Epoch: 1 / 3, Step: 251 / 750 Loss: 0.4837\n",
      "Epoch: 1 / 3, Step: 252 / 750 Loss: 0.3906\n",
      "Epoch: 1 / 3, Step: 253 / 750 Loss: 0.4120\n",
      "Epoch: 1 / 3, Step: 254 / 750 Loss: 0.4779\n",
      "Epoch: 1 / 3, Step: 255 / 750 Loss: 0.7492\n",
      "Epoch: 1 / 3, Step: 256 / 750 Loss: 0.3723\n",
      "Epoch: 1 / 3, Step: 257 / 750 Loss: 0.4616\n",
      "Epoch: 1 / 3, Step: 258 / 750 Loss: 0.5529\n",
      "Epoch: 1 / 3, Step: 259 / 750 Loss: 0.5762\n",
      "Epoch: 1 / 3, Step: 260 / 750 Loss: 0.6202\n",
      "Epoch: 1 / 3, Step: 261 / 750 Loss: 0.5863\n",
      "Epoch: 1 / 3, Step: 262 / 750 Loss: 0.5040\n",
      "Epoch: 1 / 3, Step: 263 / 750 Loss: 0.4216\n",
      "Epoch: 1 / 3, Step: 264 / 750 Loss: 0.5943\n",
      "Epoch: 1 / 3, Step: 265 / 750 Loss: 0.6380\n",
      "Epoch: 1 / 3, Step: 266 / 750 Loss: 0.5625\n",
      "Epoch: 1 / 3, Step: 267 / 750 Loss: 0.7622\n",
      "Epoch: 1 / 3, Step: 268 / 750 Loss: 0.5286\n",
      "Epoch: 1 / 3, Step: 269 / 750 Loss: 0.6241\n",
      "Epoch: 1 / 3, Step: 270 / 750 Loss: 0.6884\n",
      "Epoch: 1 / 3, Step: 271 / 750 Loss: 0.4549\n",
      "Epoch: 1 / 3, Step: 272 / 750 Loss: 0.5980\n",
      "Epoch: 1 / 3, Step: 273 / 750 Loss: 0.7103\n",
      "Epoch: 1 / 3, Step: 274 / 750 Loss: 0.6065\n",
      "Epoch: 1 / 3, Step: 275 / 750 Loss: 0.7390\n",
      "Epoch: 1 / 3, Step: 276 / 750 Loss: 0.6572\n",
      "Epoch: 1 / 3, Step: 277 / 750 Loss: 0.6348\n",
      "Epoch: 1 / 3, Step: 278 / 750 Loss: 0.4979\n",
      "Epoch: 1 / 3, Step: 279 / 750 Loss: 0.5364\n",
      "Epoch: 1 / 3, Step: 280 / 750 Loss: 0.6730\n",
      "Epoch: 1 / 3, Step: 281 / 750 Loss: 0.5982\n",
      "Epoch: 1 / 3, Step: 282 / 750 Loss: 0.6421\n",
      "Epoch: 1 / 3, Step: 283 / 750 Loss: 0.8729\n",
      "Epoch: 1 / 3, Step: 284 / 750 Loss: 0.3717\n",
      "Epoch: 1 / 3, Step: 285 / 750 Loss: 0.5237\n",
      "Epoch: 1 / 3, Step: 286 / 750 Loss: 0.8240\n",
      "Epoch: 1 / 3, Step: 287 / 750 Loss: 0.5051\n",
      "Epoch: 1 / 3, Step: 288 / 750 Loss: 0.5599\n",
      "Epoch: 1 / 3, Step: 289 / 750 Loss: 0.2959\n",
      "Epoch: 1 / 3, Step: 290 / 750 Loss: 0.4878\n",
      "Epoch: 1 / 3, Step: 291 / 750 Loss: 0.5734\n",
      "Epoch: 1 / 3, Step: 292 / 750 Loss: 0.5200\n",
      "Epoch: 1 / 3, Step: 293 / 750 Loss: 0.5372\n",
      "Epoch: 1 / 3, Step: 294 / 750 Loss: 0.7647\n",
      "Epoch: 1 / 3, Step: 295 / 750 Loss: 0.4499\n",
      "Epoch: 1 / 3, Step: 296 / 750 Loss: 0.9718\n",
      "Epoch: 1 / 3, Step: 297 / 750 Loss: 0.8755\n",
      "Epoch: 1 / 3, Step: 298 / 750 Loss: 0.5405\n",
      "Epoch: 1 / 3, Step: 299 / 750 Loss: 0.6248\n",
      "Epoch: 1 / 3, Step: 300 / 750 Loss: 0.6939\n",
      "Epoch: 1 / 3, Step: 301 / 750 Loss: 0.6322\n",
      "Epoch: 1 / 3, Step: 302 / 750 Loss: 0.6702\n",
      "Epoch: 1 / 3, Step: 303 / 750 Loss: 0.4713\n",
      "Epoch: 1 / 3, Step: 304 / 750 Loss: 0.5841\n",
      "Epoch: 1 / 3, Step: 305 / 750 Loss: 0.5192\n",
      "Epoch: 1 / 3, Step: 306 / 750 Loss: 0.6306\n",
      "Epoch: 1 / 3, Step: 307 / 750 Loss: 0.6764\n",
      "Epoch: 1 / 3, Step: 308 / 750 Loss: 0.7251\n",
      "Epoch: 1 / 3, Step: 309 / 750 Loss: 0.6226\n",
      "Epoch: 1 / 3, Step: 310 / 750 Loss: 0.6557\n",
      "Epoch: 1 / 3, Step: 311 / 750 Loss: 0.4966\n",
      "Epoch: 1 / 3, Step: 312 / 750 Loss: 0.5308\n",
      "Epoch: 1 / 3, Step: 313 / 750 Loss: 0.4280\n",
      "Epoch: 1 / 3, Step: 314 / 750 Loss: 0.6466\n",
      "Epoch: 1 / 3, Step: 315 / 750 Loss: 0.6523\n",
      "Epoch: 1 / 3, Step: 316 / 750 Loss: 0.5323\n",
      "Epoch: 1 / 3, Step: 317 / 750 Loss: 0.3432\n",
      "Epoch: 1 / 3, Step: 318 / 750 Loss: 0.5270\n",
      "Epoch: 1 / 3, Step: 319 / 750 Loss: 0.3992\n",
      "Epoch: 1 / 3, Step: 320 / 750 Loss: 0.5008\n",
      "Epoch: 1 / 3, Step: 321 / 750 Loss: 0.4137\n",
      "Epoch: 1 / 3, Step: 322 / 750 Loss: 0.7822\n",
      "Epoch: 1 / 3, Step: 323 / 750 Loss: 0.5796\n",
      "Epoch: 1 / 3, Step: 324 / 750 Loss: 0.4192\n",
      "Epoch: 1 / 3, Step: 325 / 750 Loss: 0.6377\n",
      "Epoch: 1 / 3, Step: 326 / 750 Loss: 0.7119\n",
      "Epoch: 1 / 3, Step: 327 / 750 Loss: 0.4674\n",
      "Epoch: 1 / 3, Step: 328 / 750 Loss: 0.3859\n",
      "Epoch: 1 / 3, Step: 329 / 750 Loss: 0.5607\n",
      "Epoch: 1 / 3, Step: 330 / 750 Loss: 0.6063\n",
      "Epoch: 1 / 3, Step: 331 / 750 Loss: 0.6408\n",
      "Epoch: 1 / 3, Step: 332 / 750 Loss: 0.6351\n",
      "Epoch: 1 / 3, Step: 333 / 750 Loss: 0.4199\n",
      "Epoch: 1 / 3, Step: 334 / 750 Loss: 0.6439\n",
      "Epoch: 1 / 3, Step: 335 / 750 Loss: 0.5155\n",
      "Epoch: 1 / 3, Step: 336 / 750 Loss: 0.5474\n",
      "Epoch: 1 / 3, Step: 337 / 750 Loss: 0.5881\n",
      "Epoch: 1 / 3, Step: 338 / 750 Loss: 0.5451\n",
      "Epoch: 1 / 3, Step: 339 / 750 Loss: 0.4593\n",
      "Epoch: 1 / 3, Step: 340 / 750 Loss: 0.7246\n",
      "Epoch: 1 / 3, Step: 341 / 750 Loss: 0.3792\n",
      "Epoch: 1 / 3, Step: 342 / 750 Loss: 0.4295\n",
      "Epoch: 1 / 3, Step: 343 / 750 Loss: 0.4398\n",
      "Epoch: 1 / 3, Step: 344 / 750 Loss: 0.6099\n",
      "Epoch: 1 / 3, Step: 345 / 750 Loss: 0.5205\n",
      "Epoch: 1 / 3, Step: 346 / 750 Loss: 0.5499\n",
      "Epoch: 1 / 3, Step: 347 / 750 Loss: 0.7207\n",
      "Epoch: 1 / 3, Step: 348 / 750 Loss: 0.5851\n",
      "Epoch: 1 / 3, Step: 349 / 750 Loss: 0.3042\n",
      "Epoch: 1 / 3, Step: 350 / 750 Loss: 0.4643\n",
      "Epoch: 1 / 3, Step: 351 / 750 Loss: 0.6135\n",
      "Epoch: 1 / 3, Step: 352 / 750 Loss: 0.4190\n",
      "Epoch: 1 / 3, Step: 353 / 750 Loss: 0.3493\n",
      "Epoch: 1 / 3, Step: 354 / 750 Loss: 0.5140\n",
      "Epoch: 1 / 3, Step: 355 / 750 Loss: 0.7276\n",
      "Epoch: 1 / 3, Step: 356 / 750 Loss: 0.4772\n",
      "Epoch: 1 / 3, Step: 357 / 750 Loss: 0.3087\n",
      "Epoch: 1 / 3, Step: 358 / 750 Loss: 0.3246\n",
      "Epoch: 1 / 3, Step: 359 / 750 Loss: 0.4672\n",
      "Epoch: 1 / 3, Step: 360 / 750 Loss: 0.7627\n",
      "Epoch: 1 / 3, Step: 361 / 750 Loss: 0.6411\n",
      "Epoch: 1 / 3, Step: 362 / 750 Loss: 0.6830\n",
      "Epoch: 1 / 3, Step: 363 / 750 Loss: 0.4842\n",
      "Epoch: 1 / 3, Step: 364 / 750 Loss: 0.7608\n",
      "Epoch: 1 / 3, Step: 365 / 750 Loss: 0.3528\n",
      "Epoch: 1 / 3, Step: 366 / 750 Loss: 0.6602\n",
      "Epoch: 1 / 3, Step: 367 / 750 Loss: 0.4595\n",
      "Epoch: 1 / 3, Step: 368 / 750 Loss: 0.5200\n",
      "Epoch: 1 / 3, Step: 369 / 750 Loss: 0.8433\n",
      "Epoch: 1 / 3, Step: 370 / 750 Loss: 0.4048\n",
      "Epoch: 1 / 3, Step: 371 / 750 Loss: 0.3597\n",
      "Epoch: 1 / 3, Step: 372 / 750 Loss: 0.6661\n",
      "Epoch: 1 / 3, Step: 373 / 750 Loss: 0.3937\n",
      "Epoch: 1 / 3, Step: 374 / 750 Loss: 0.5806\n",
      "Epoch: 1 / 3, Step: 375 / 750 Loss: 0.4805\n",
      "Epoch: 1 / 3, Step: 376 / 750 Loss: 0.5075\n",
      "Epoch: 1 / 3, Step: 377 / 750 Loss: 0.5354\n",
      "Epoch: 1 / 3, Step: 378 / 750 Loss: 0.5222\n",
      "Epoch: 1 / 3, Step: 379 / 750 Loss: 0.3874\n",
      "Epoch: 1 / 3, Step: 380 / 750 Loss: 0.3231\n",
      "Epoch: 1 / 3, Step: 381 / 750 Loss: 0.6173\n",
      "Epoch: 1 / 3, Step: 382 / 750 Loss: 0.6012\n",
      "Epoch: 1 / 3, Step: 383 / 750 Loss: 0.4718\n",
      "Epoch: 1 / 3, Step: 384 / 750 Loss: 0.5147\n",
      "Epoch: 1 / 3, Step: 385 / 750 Loss: 0.5540\n",
      "Epoch: 1 / 3, Step: 386 / 750 Loss: 0.4385\n",
      "Epoch: 1 / 3, Step: 387 / 750 Loss: 0.5489\n",
      "Epoch: 1 / 3, Step: 388 / 750 Loss: 0.6226\n",
      "Epoch: 1 / 3, Step: 389 / 750 Loss: 0.3190\n",
      "Epoch: 1 / 3, Step: 390 / 750 Loss: 0.4602\n",
      "Epoch: 1 / 3, Step: 391 / 750 Loss: 0.5498\n",
      "Epoch: 1 / 3, Step: 392 / 750 Loss: 0.4275\n",
      "Epoch: 1 / 3, Step: 393 / 750 Loss: 0.5600\n",
      "Epoch: 1 / 3, Step: 394 / 750 Loss: 0.3136\n",
      "Epoch: 1 / 3, Step: 395 / 750 Loss: 0.5226\n",
      "Epoch: 1 / 3, Step: 396 / 750 Loss: 0.5023\n",
      "Epoch: 1 / 3, Step: 397 / 750 Loss: 0.4136\n",
      "Epoch: 1 / 3, Step: 398 / 750 Loss: 0.5014\n",
      "Epoch: 1 / 3, Step: 399 / 750 Loss: 0.3432\n",
      "Epoch: 1 / 3, Step: 400 / 750 Loss: 0.6282\n",
      "Epoch: 1 / 3, Step: 401 / 750 Loss: 0.6397\n",
      "Epoch: 1 / 3, Step: 402 / 750 Loss: 0.4234\n",
      "Epoch: 1 / 3, Step: 403 / 750 Loss: 0.4695\n",
      "Epoch: 1 / 3, Step: 404 / 750 Loss: 0.5065\n",
      "Epoch: 1 / 3, Step: 405 / 750 Loss: 0.5187\n",
      "Epoch: 1 / 3, Step: 406 / 750 Loss: 0.4938\n",
      "Epoch: 1 / 3, Step: 407 / 750 Loss: 0.4218\n",
      "Epoch: 1 / 3, Step: 408 / 750 Loss: 0.4402\n",
      "Epoch: 1 / 3, Step: 409 / 750 Loss: 0.4464\n",
      "Epoch: 1 / 3, Step: 410 / 750 Loss: 0.4238\n",
      "Epoch: 1 / 3, Step: 411 / 750 Loss: 0.4185\n",
      "Epoch: 1 / 3, Step: 412 / 750 Loss: 0.3322\n",
      "Epoch: 1 / 3, Step: 413 / 750 Loss: 0.5338\n",
      "Epoch: 1 / 3, Step: 414 / 750 Loss: 0.3879\n",
      "Epoch: 1 / 3, Step: 415 / 750 Loss: 0.5708\n",
      "Epoch: 1 / 3, Step: 416 / 750 Loss: 0.2877\n",
      "Epoch: 1 / 3, Step: 417 / 750 Loss: 0.3808\n",
      "Epoch: 1 / 3, Step: 418 / 750 Loss: 0.1871\n",
      "Epoch: 1 / 3, Step: 419 / 750 Loss: 0.6245\n",
      "Epoch: 1 / 3, Step: 420 / 750 Loss: 0.5789\n",
      "Epoch: 1 / 3, Step: 421 / 750 Loss: 0.3958\n",
      "Epoch: 1 / 3, Step: 422 / 750 Loss: 0.4275\n",
      "Epoch: 1 / 3, Step: 423 / 750 Loss: 0.5809\n",
      "Epoch: 1 / 3, Step: 424 / 750 Loss: 0.5995\n",
      "Epoch: 1 / 3, Step: 425 / 750 Loss: 0.3161\n",
      "Epoch: 1 / 3, Step: 426 / 750 Loss: 0.4909\n",
      "Epoch: 1 / 3, Step: 427 / 750 Loss: 0.3955\n",
      "Epoch: 1 / 3, Step: 428 / 750 Loss: 0.3421\n",
      "Epoch: 1 / 3, Step: 429 / 750 Loss: 0.6340\n",
      "Epoch: 1 / 3, Step: 430 / 750 Loss: 0.5927\n",
      "Epoch: 1 / 3, Step: 431 / 750 Loss: 0.7004\n",
      "Epoch: 1 / 3, Step: 432 / 750 Loss: 0.2968\n",
      "Epoch: 1 / 3, Step: 433 / 750 Loss: 0.7823\n",
      "Epoch: 1 / 3, Step: 434 / 750 Loss: 0.3977\n",
      "Epoch: 1 / 3, Step: 435 / 750 Loss: 0.3466\n",
      "Epoch: 1 / 3, Step: 436 / 750 Loss: 0.3863\n",
      "Epoch: 1 / 3, Step: 437 / 750 Loss: 0.3779\n",
      "Epoch: 1 / 3, Step: 438 / 750 Loss: 0.3807\n",
      "Epoch: 1 / 3, Step: 439 / 750 Loss: 0.4775\n",
      "Epoch: 1 / 3, Step: 440 / 750 Loss: 0.4711\n",
      "Epoch: 1 / 3, Step: 441 / 750 Loss: 0.5865\n",
      "Epoch: 1 / 3, Step: 442 / 750 Loss: 0.5970\n",
      "Epoch: 1 / 3, Step: 443 / 750 Loss: 0.3473\n",
      "Epoch: 1 / 3, Step: 444 / 750 Loss: 0.5383\n",
      "Epoch: 1 / 3, Step: 445 / 750 Loss: 0.5142\n",
      "Epoch: 1 / 3, Step: 446 / 750 Loss: 0.4334\n",
      "Epoch: 1 / 3, Step: 447 / 750 Loss: 0.4783\n",
      "Epoch: 1 / 3, Step: 448 / 750 Loss: 0.1649\n",
      "Epoch: 1 / 3, Step: 449 / 750 Loss: 0.6377\n",
      "Epoch: 1 / 3, Step: 450 / 750 Loss: 0.5452\n",
      "Epoch: 1 / 3, Step: 451 / 750 Loss: 0.3272\n",
      "Epoch: 1 / 3, Step: 452 / 750 Loss: 0.6933\n",
      "Epoch: 1 / 3, Step: 453 / 750 Loss: 0.4722\n",
      "Epoch: 1 / 3, Step: 454 / 750 Loss: 0.3822\n",
      "Epoch: 1 / 3, Step: 455 / 750 Loss: 0.5314\n",
      "Epoch: 1 / 3, Step: 456 / 750 Loss: 0.3232\n",
      "Epoch: 1 / 3, Step: 457 / 750 Loss: 0.4118\n",
      "Epoch: 1 / 3, Step: 458 / 750 Loss: 0.3439\n",
      "Epoch: 1 / 3, Step: 459 / 750 Loss: 0.5537\n",
      "Epoch: 1 / 3, Step: 460 / 750 Loss: 0.3851\n",
      "Epoch: 1 / 3, Step: 461 / 750 Loss: 0.3747\n",
      "Epoch: 1 / 3, Step: 462 / 750 Loss: 0.2097\n",
      "Epoch: 1 / 3, Step: 463 / 750 Loss: 0.4713\n",
      "Epoch: 1 / 3, Step: 464 / 750 Loss: 0.3591\n",
      "Epoch: 1 / 3, Step: 465 / 750 Loss: 0.2774\n",
      "Epoch: 1 / 3, Step: 466 / 750 Loss: 0.3868\n",
      "Epoch: 1 / 3, Step: 467 / 750 Loss: 0.5658\n",
      "Epoch: 1 / 3, Step: 468 / 750 Loss: 0.2793\n",
      "Epoch: 1 / 3, Step: 469 / 750 Loss: 0.4514\n",
      "Epoch: 1 / 3, Step: 470 / 750 Loss: 0.4342\n",
      "Epoch: 1 / 3, Step: 471 / 750 Loss: 0.7190\n",
      "Epoch: 1 / 3, Step: 472 / 750 Loss: 0.5118\n",
      "Epoch: 1 / 3, Step: 473 / 750 Loss: 0.3861\n",
      "Epoch: 1 / 3, Step: 474 / 750 Loss: 0.4324\n",
      "Epoch: 1 / 3, Step: 475 / 750 Loss: 0.3414\n",
      "Epoch: 1 / 3, Step: 476 / 750 Loss: 0.8578\n",
      "Epoch: 1 / 3, Step: 477 / 750 Loss: 0.5914\n",
      "Epoch: 1 / 3, Step: 478 / 750 Loss: 0.5185\n",
      "Epoch: 1 / 3, Step: 479 / 750 Loss: 0.3324\n",
      "Epoch: 1 / 3, Step: 480 / 750 Loss: 0.3293\n",
      "Epoch: 1 / 3, Step: 481 / 750 Loss: 0.5214\n",
      "Epoch: 1 / 3, Step: 482 / 750 Loss: 0.4567\n",
      "Epoch: 1 / 3, Step: 483 / 750 Loss: 0.5280\n",
      "Epoch: 1 / 3, Step: 484 / 750 Loss: 0.2819\n",
      "Epoch: 1 / 3, Step: 485 / 750 Loss: 0.3214\n",
      "Epoch: 1 / 3, Step: 486 / 750 Loss: 0.4176\n",
      "Epoch: 1 / 3, Step: 487 / 750 Loss: 0.2581\n",
      "Epoch: 1 / 3, Step: 488 / 750 Loss: 0.4704\n",
      "Epoch: 1 / 3, Step: 489 / 750 Loss: 0.7148\n",
      "Epoch: 1 / 3, Step: 490 / 750 Loss: 0.3749\n",
      "Epoch: 1 / 3, Step: 491 / 750 Loss: 0.3399\n",
      "Epoch: 1 / 3, Step: 492 / 750 Loss: 0.3530\n",
      "Epoch: 1 / 3, Step: 493 / 750 Loss: 0.3513\n",
      "Epoch: 1 / 3, Step: 494 / 750 Loss: 0.5564\n",
      "Epoch: 1 / 3, Step: 495 / 750 Loss: 0.4532\n",
      "Epoch: 1 / 3, Step: 496 / 750 Loss: 0.4202\n",
      "Epoch: 1 / 3, Step: 497 / 750 Loss: 0.4614\n",
      "Epoch: 1 / 3, Step: 498 / 750 Loss: 0.3986\n",
      "Epoch: 1 / 3, Step: 499 / 750 Loss: 0.3272\n",
      "Epoch: 1 / 3, Step: 500 / 750 Loss: 0.3399\n",
      "Epoch: 1 / 3, Step: 501 / 750 Loss: 0.5993\n",
      "Epoch: 1 / 3, Step: 502 / 750 Loss: 0.3024\n",
      "Epoch: 1 / 3, Step: 503 / 750 Loss: 0.7690\n",
      "Epoch: 1 / 3, Step: 504 / 750 Loss: 0.2844\n",
      "Epoch: 1 / 3, Step: 505 / 750 Loss: 0.4884\n",
      "Epoch: 1 / 3, Step: 506 / 750 Loss: 0.4055\n",
      "Epoch: 1 / 3, Step: 507 / 750 Loss: 0.5235\n",
      "Epoch: 1 / 3, Step: 508 / 750 Loss: 0.5205\n",
      "Epoch: 1 / 3, Step: 509 / 750 Loss: 0.2948\n",
      "Epoch: 1 / 3, Step: 510 / 750 Loss: 0.3512\n",
      "Epoch: 1 / 3, Step: 511 / 750 Loss: 0.2464\n",
      "Epoch: 1 / 3, Step: 512 / 750 Loss: 0.4506\n",
      "Epoch: 1 / 3, Step: 513 / 750 Loss: 0.5852\n",
      "Epoch: 1 / 3, Step: 514 / 750 Loss: 0.4503\n",
      "Epoch: 1 / 3, Step: 515 / 750 Loss: 0.5321\n",
      "Epoch: 1 / 3, Step: 516 / 750 Loss: 0.4198\n",
      "Epoch: 1 / 3, Step: 517 / 750 Loss: 0.4777\n",
      "Epoch: 1 / 3, Step: 518 / 750 Loss: 0.3224\n",
      "Epoch: 1 / 3, Step: 519 / 750 Loss: 0.2535\n",
      "Epoch: 1 / 3, Step: 520 / 750 Loss: 0.4357\n",
      "Epoch: 1 / 3, Step: 521 / 750 Loss: 0.3616\n",
      "Epoch: 1 / 3, Step: 522 / 750 Loss: 0.4028\n",
      "Epoch: 1 / 3, Step: 523 / 750 Loss: 0.4044\n",
      "Epoch: 1 / 3, Step: 524 / 750 Loss: 0.3552\n",
      "Epoch: 1 / 3, Step: 525 / 750 Loss: 0.3475\n",
      "Epoch: 1 / 3, Step: 526 / 750 Loss: 0.4238\n",
      "Epoch: 1 / 3, Step: 527 / 750 Loss: 0.4106\n",
      "Epoch: 1 / 3, Step: 528 / 750 Loss: 0.5781\n",
      "Epoch: 1 / 3, Step: 529 / 750 Loss: 0.5506\n",
      "Epoch: 1 / 3, Step: 530 / 750 Loss: 0.3948\n",
      "Epoch: 1 / 3, Step: 531 / 750 Loss: 0.2834\n",
      "Epoch: 1 / 3, Step: 532 / 750 Loss: 0.4141\n",
      "Epoch: 1 / 3, Step: 533 / 750 Loss: 0.3623\n",
      "Epoch: 1 / 3, Step: 534 / 750 Loss: 0.3240\n",
      "Epoch: 1 / 3, Step: 535 / 750 Loss: 0.3651\n",
      "Epoch: 1 / 3, Step: 536 / 750 Loss: 0.5876\n",
      "Epoch: 1 / 3, Step: 537 / 750 Loss: 0.3104\n",
      "Epoch: 1 / 3, Step: 538 / 750 Loss: 0.3912\n",
      "Epoch: 1 / 3, Step: 539 / 750 Loss: 0.4476\n",
      "Epoch: 1 / 3, Step: 540 / 750 Loss: 0.3729\n",
      "Epoch: 1 / 3, Step: 541 / 750 Loss: 0.6403\n",
      "Epoch: 1 / 3, Step: 542 / 750 Loss: 0.4966\n",
      "Epoch: 1 / 3, Step: 543 / 750 Loss: 0.4485\n",
      "Epoch: 1 / 3, Step: 544 / 750 Loss: 0.2531\n",
      "Epoch: 1 / 3, Step: 545 / 750 Loss: 0.3390\n",
      "Epoch: 1 / 3, Step: 546 / 750 Loss: 0.2662\n",
      "Epoch: 1 / 3, Step: 547 / 750 Loss: 0.3389\n",
      "Epoch: 1 / 3, Step: 548 / 750 Loss: 0.3628\n",
      "Epoch: 1 / 3, Step: 549 / 750 Loss: 0.2799\n",
      "Epoch: 1 / 3, Step: 550 / 750 Loss: 0.4358\n",
      "Epoch: 1 / 3, Step: 551 / 750 Loss: 0.5205\n",
      "Epoch: 1 / 3, Step: 552 / 750 Loss: 0.3009\n",
      "Epoch: 1 / 3, Step: 553 / 750 Loss: 0.5272\n",
      "Epoch: 1 / 3, Step: 554 / 750 Loss: 0.2873\n",
      "Epoch: 1 / 3, Step: 555 / 750 Loss: 0.3044\n",
      "Epoch: 1 / 3, Step: 556 / 750 Loss: 0.3064\n",
      "Epoch: 1 / 3, Step: 557 / 750 Loss: 0.3193\n",
      "Epoch: 1 / 3, Step: 558 / 750 Loss: 0.4777\n",
      "Epoch: 1 / 3, Step: 559 / 750 Loss: 0.4384\n",
      "Epoch: 1 / 3, Step: 560 / 750 Loss: 0.2482\n",
      "Epoch: 1 / 3, Step: 561 / 750 Loss: 0.2624\n",
      "Epoch: 1 / 3, Step: 562 / 750 Loss: 0.3273\n",
      "Epoch: 1 / 3, Step: 563 / 750 Loss: 0.6309\n",
      "Epoch: 1 / 3, Step: 564 / 750 Loss: 0.3725\n",
      "Epoch: 1 / 3, Step: 565 / 750 Loss: 0.3709\n",
      "Epoch: 1 / 3, Step: 566 / 750 Loss: 0.4681\n",
      "Epoch: 1 / 3, Step: 567 / 750 Loss: 0.3001\n",
      "Epoch: 1 / 3, Step: 568 / 750 Loss: 0.3568\n",
      "Epoch: 1 / 3, Step: 569 / 750 Loss: 0.3323\n",
      "Epoch: 1 / 3, Step: 570 / 750 Loss: 0.4584\n",
      "Epoch: 1 / 3, Step: 571 / 750 Loss: 0.3012\n",
      "Epoch: 1 / 3, Step: 572 / 750 Loss: 0.3050\n",
      "Epoch: 1 / 3, Step: 573 / 750 Loss: 0.6226\n",
      "Epoch: 1 / 3, Step: 574 / 750 Loss: 0.1835\n",
      "Epoch: 1 / 3, Step: 575 / 750 Loss: 0.4238\n",
      "Epoch: 1 / 3, Step: 576 / 750 Loss: 0.2096\n",
      "Epoch: 1 / 3, Step: 577 / 750 Loss: 0.3413\n",
      "Epoch: 1 / 3, Step: 578 / 750 Loss: 0.2994\n",
      "Epoch: 1 / 3, Step: 579 / 750 Loss: 0.3449\n",
      "Epoch: 1 / 3, Step: 580 / 750 Loss: 0.2264\n",
      "Epoch: 1 / 3, Step: 581 / 750 Loss: 0.5178\n",
      "Epoch: 1 / 3, Step: 582 / 750 Loss: 0.3476\n",
      "Epoch: 1 / 3, Step: 583 / 750 Loss: 0.4705\n",
      "Epoch: 1 / 3, Step: 584 / 750 Loss: 0.4745\n",
      "Epoch: 1 / 3, Step: 585 / 750 Loss: 0.1921\n",
      "Epoch: 1 / 3, Step: 586 / 750 Loss: 0.3989\n",
      "Epoch: 1 / 3, Step: 587 / 750 Loss: 0.4305\n",
      "Epoch: 1 / 3, Step: 588 / 750 Loss: 0.4553\n",
      "Epoch: 1 / 3, Step: 589 / 750 Loss: 0.5862\n",
      "Epoch: 1 / 3, Step: 590 / 750 Loss: 0.2168\n",
      "Epoch: 1 / 3, Step: 591 / 750 Loss: 0.4781\n",
      "Epoch: 1 / 3, Step: 592 / 750 Loss: 0.3237\n",
      "Epoch: 1 / 3, Step: 593 / 750 Loss: 0.3313\n",
      "Epoch: 1 / 3, Step: 594 / 750 Loss: 0.4369\n",
      "Epoch: 1 / 3, Step: 595 / 750 Loss: 0.6683\n",
      "Epoch: 1 / 3, Step: 596 / 750 Loss: 0.4940\n",
      "Epoch: 1 / 3, Step: 597 / 750 Loss: 0.3970\n",
      "Epoch: 1 / 3, Step: 598 / 750 Loss: 0.2950\n",
      "Epoch: 1 / 3, Step: 599 / 750 Loss: 0.4114\n",
      "Epoch: 1 / 3, Step: 600 / 750 Loss: 0.4547\n",
      "Epoch: 1 / 3, Step: 601 / 750 Loss: 0.3293\n",
      "Epoch: 1 / 3, Step: 602 / 750 Loss: 0.1627\n",
      "Epoch: 1 / 3, Step: 603 / 750 Loss: 0.3309\n",
      "Epoch: 1 / 3, Step: 604 / 750 Loss: 0.2400\n",
      "Epoch: 1 / 3, Step: 605 / 750 Loss: 0.2398\n",
      "Epoch: 1 / 3, Step: 606 / 750 Loss: 0.6058\n",
      "Epoch: 1 / 3, Step: 607 / 750 Loss: 0.4220\n",
      "Epoch: 1 / 3, Step: 608 / 750 Loss: 0.3763\n",
      "Epoch: 1 / 3, Step: 609 / 750 Loss: 0.5235\n",
      "Epoch: 1 / 3, Step: 610 / 750 Loss: 0.3916\n",
      "Epoch: 1 / 3, Step: 611 / 750 Loss: 0.5183\n",
      "Epoch: 1 / 3, Step: 612 / 750 Loss: 0.4509\n",
      "Epoch: 1 / 3, Step: 613 / 750 Loss: 0.4410\n",
      "Epoch: 1 / 3, Step: 614 / 750 Loss: 0.4001\n",
      "Epoch: 1 / 3, Step: 615 / 750 Loss: 0.4193\n",
      "Epoch: 1 / 3, Step: 616 / 750 Loss: 0.2997\n",
      "Epoch: 1 / 3, Step: 617 / 750 Loss: 0.3512\n",
      "Epoch: 1 / 3, Step: 618 / 750 Loss: 0.4803\n",
      "Epoch: 1 / 3, Step: 619 / 750 Loss: 0.2838\n",
      "Epoch: 1 / 3, Step: 620 / 750 Loss: 0.6113\n",
      "Epoch: 1 / 3, Step: 621 / 750 Loss: 0.3982\n",
      "Epoch: 1 / 3, Step: 622 / 750 Loss: 0.3438\n",
      "Epoch: 1 / 3, Step: 623 / 750 Loss: 0.1469\n",
      "Epoch: 1 / 3, Step: 624 / 750 Loss: 0.3757\n",
      "Epoch: 1 / 3, Step: 625 / 750 Loss: 0.1664\n",
      "Epoch: 1 / 3, Step: 626 / 750 Loss: 0.2287\n",
      "Epoch: 1 / 3, Step: 627 / 750 Loss: 0.4577\n",
      "Epoch: 1 / 3, Step: 628 / 750 Loss: 0.3927\n",
      "Epoch: 1 / 3, Step: 629 / 750 Loss: 0.3626\n",
      "Epoch: 1 / 3, Step: 630 / 750 Loss: 0.4401\n",
      "Epoch: 1 / 3, Step: 631 / 750 Loss: 0.4845\n",
      "Epoch: 1 / 3, Step: 632 / 750 Loss: 0.2068\n",
      "Epoch: 1 / 3, Step: 633 / 750 Loss: 0.3904\n",
      "Epoch: 1 / 3, Step: 634 / 750 Loss: 0.2620\n",
      "Epoch: 1 / 3, Step: 635 / 750 Loss: 0.6109\n",
      "Epoch: 1 / 3, Step: 636 / 750 Loss: 0.1997\n",
      "Epoch: 1 / 3, Step: 637 / 750 Loss: 0.4322\n",
      "Epoch: 1 / 3, Step: 638 / 750 Loss: 0.1675\n",
      "Epoch: 1 / 3, Step: 639 / 750 Loss: 0.6041\n",
      "Epoch: 1 / 3, Step: 640 / 750 Loss: 0.2535\n",
      "Epoch: 1 / 3, Step: 641 / 750 Loss: 0.2470\n",
      "Epoch: 1 / 3, Step: 642 / 750 Loss: 0.2440\n",
      "Epoch: 1 / 3, Step: 643 / 750 Loss: 0.3097\n",
      "Epoch: 1 / 3, Step: 644 / 750 Loss: 0.2999\n",
      "Epoch: 1 / 3, Step: 645 / 750 Loss: 0.4632\n",
      "Epoch: 1 / 3, Step: 646 / 750 Loss: 0.3176\n",
      "Epoch: 1 / 3, Step: 647 / 750 Loss: 0.3456\n",
      "Epoch: 1 / 3, Step: 648 / 750 Loss: 0.4597\n",
      "Epoch: 1 / 3, Step: 649 / 750 Loss: 0.5907\n",
      "Epoch: 1 / 3, Step: 650 / 750 Loss: 0.2605\n",
      "Epoch: 1 / 3, Step: 651 / 750 Loss: 0.1828\n",
      "Epoch: 1 / 3, Step: 652 / 750 Loss: 0.4122\n",
      "Epoch: 1 / 3, Step: 653 / 750 Loss: 0.2261\n",
      "Epoch: 1 / 3, Step: 654 / 750 Loss: 0.4262\n",
      "Epoch: 1 / 3, Step: 655 / 750 Loss: 0.4233\n",
      "Epoch: 1 / 3, Step: 656 / 750 Loss: 0.3788\n",
      "Epoch: 1 / 3, Step: 657 / 750 Loss: 0.4861\n",
      "Epoch: 1 / 3, Step: 658 / 750 Loss: 0.4834\n",
      "Epoch: 1 / 3, Step: 659 / 750 Loss: 0.2798\n",
      "Epoch: 1 / 3, Step: 660 / 750 Loss: 0.2179\n",
      "Epoch: 1 / 3, Step: 661 / 750 Loss: 0.3538\n",
      "Epoch: 1 / 3, Step: 662 / 750 Loss: 0.3861\n",
      "Epoch: 1 / 3, Step: 663 / 750 Loss: 0.4772\n",
      "Epoch: 1 / 3, Step: 664 / 750 Loss: 0.3625\n",
      "Epoch: 1 / 3, Step: 665 / 750 Loss: 0.3983\n",
      "Epoch: 1 / 3, Step: 666 / 750 Loss: 0.2880\n",
      "Epoch: 1 / 3, Step: 667 / 750 Loss: 0.4791\n",
      "Epoch: 1 / 3, Step: 668 / 750 Loss: 0.4717\n",
      "Epoch: 1 / 3, Step: 669 / 750 Loss: 0.2693\n",
      "Epoch: 1 / 3, Step: 670 / 750 Loss: 0.2768\n",
      "Epoch: 1 / 3, Step: 671 / 750 Loss: 0.2696\n",
      "Epoch: 1 / 3, Step: 672 / 750 Loss: 0.3230\n",
      "Epoch: 1 / 3, Step: 673 / 750 Loss: 0.4063\n",
      "Epoch: 1 / 3, Step: 674 / 750 Loss: 0.3763\n",
      "Epoch: 1 / 3, Step: 675 / 750 Loss: 0.2477\n",
      "Epoch: 1 / 3, Step: 676 / 750 Loss: 0.2268\n",
      "Epoch: 1 / 3, Step: 677 / 750 Loss: 0.3777\n",
      "Epoch: 1 / 3, Step: 678 / 750 Loss: 0.4474\n",
      "Epoch: 1 / 3, Step: 679 / 750 Loss: 0.3302\n",
      "Epoch: 1 / 3, Step: 680 / 750 Loss: 0.2749\n",
      "Epoch: 1 / 3, Step: 681 / 750 Loss: 0.4895\n",
      "Epoch: 1 / 3, Step: 682 / 750 Loss: 0.4354\n",
      "Epoch: 1 / 3, Step: 683 / 750 Loss: 0.3520\n",
      "Epoch: 1 / 3, Step: 684 / 750 Loss: 0.2896\n",
      "Epoch: 1 / 3, Step: 685 / 750 Loss: 0.2023\n",
      "Epoch: 1 / 3, Step: 686 / 750 Loss: 0.4366\n",
      "Epoch: 1 / 3, Step: 687 / 750 Loss: 0.4063\n",
      "Epoch: 1 / 3, Step: 688 / 750 Loss: 0.4688\n",
      "Epoch: 1 / 3, Step: 689 / 750 Loss: 0.3790\n",
      "Epoch: 1 / 3, Step: 690 / 750 Loss: 0.3556\n",
      "Epoch: 1 / 3, Step: 691 / 750 Loss: 0.2489\n",
      "Epoch: 1 / 3, Step: 692 / 750 Loss: 0.2769\n",
      "Epoch: 1 / 3, Step: 693 / 750 Loss: 0.3906\n",
      "Epoch: 1 / 3, Step: 694 / 750 Loss: 0.1841\n",
      "Epoch: 1 / 3, Step: 695 / 750 Loss: 0.4231\n",
      "Epoch: 1 / 3, Step: 696 / 750 Loss: 0.2447\n",
      "Epoch: 1 / 3, Step: 697 / 750 Loss: 0.7041\n",
      "Epoch: 1 / 3, Step: 698 / 750 Loss: 0.6802\n",
      "Epoch: 1 / 3, Step: 699 / 750 Loss: 0.2823\n",
      "Epoch: 1 / 3, Step: 700 / 750 Loss: 0.2826\n",
      "Epoch: 1 / 3, Step: 701 / 750 Loss: 0.2474\n",
      "Epoch: 1 / 3, Step: 702 / 750 Loss: 0.2236\n",
      "Epoch: 1 / 3, Step: 703 / 750 Loss: 0.3019\n",
      "Epoch: 1 / 3, Step: 704 / 750 Loss: 0.4489\n",
      "Epoch: 1 / 3, Step: 705 / 750 Loss: 0.4653\n",
      "Epoch: 1 / 3, Step: 706 / 750 Loss: 0.5428\n",
      "Epoch: 1 / 3, Step: 707 / 750 Loss: 0.6702\n",
      "Epoch: 1 / 3, Step: 708 / 750 Loss: 0.2287\n",
      "Epoch: 1 / 3, Step: 709 / 750 Loss: 0.5765\n",
      "Epoch: 1 / 3, Step: 710 / 750 Loss: 0.2528\n",
      "Epoch: 1 / 3, Step: 711 / 750 Loss: 0.2323\n",
      "Epoch: 1 / 3, Step: 712 / 750 Loss: 0.2617\n",
      "Epoch: 1 / 3, Step: 713 / 750 Loss: 0.2078\n",
      "Epoch: 1 / 3, Step: 714 / 750 Loss: 0.2394\n",
      "Epoch: 1 / 3, Step: 715 / 750 Loss: 0.3412\n",
      "Epoch: 1 / 3, Step: 716 / 750 Loss: 0.5586\n",
      "Epoch: 1 / 3, Step: 717 / 750 Loss: 0.3251\n",
      "Epoch: 1 / 3, Step: 718 / 750 Loss: 0.2314\n",
      "Epoch: 1 / 3, Step: 719 / 750 Loss: 0.1746\n",
      "Epoch: 1 / 3, Step: 720 / 750 Loss: 0.2619\n",
      "Epoch: 1 / 3, Step: 721 / 750 Loss: 0.4077\n",
      "Epoch: 1 / 3, Step: 722 / 750 Loss: 0.2911\n",
      "Epoch: 1 / 3, Step: 723 / 750 Loss: 0.4139\n",
      "Epoch: 1 / 3, Step: 724 / 750 Loss: 0.5244\n",
      "Epoch: 1 / 3, Step: 725 / 750 Loss: 0.2915\n",
      "Epoch: 1 / 3, Step: 726 / 750 Loss: 0.3535\n",
      "Epoch: 1 / 3, Step: 727 / 750 Loss: 0.5155\n",
      "Epoch: 1 / 3, Step: 728 / 750 Loss: 0.6748\n",
      "Epoch: 1 / 3, Step: 729 / 750 Loss: 0.6030\n",
      "Epoch: 1 / 3, Step: 730 / 750 Loss: 0.2078\n",
      "Epoch: 1 / 3, Step: 731 / 750 Loss: 0.3085\n",
      "Epoch: 1 / 3, Step: 732 / 750 Loss: 0.2383\n",
      "Epoch: 1 / 3, Step: 733 / 750 Loss: 0.2272\n",
      "Epoch: 1 / 3, Step: 734 / 750 Loss: 0.3932\n",
      "Epoch: 1 / 3, Step: 735 / 750 Loss: 0.4248\n",
      "Epoch: 1 / 3, Step: 736 / 750 Loss: 0.1337\n",
      "Epoch: 1 / 3, Step: 737 / 750 Loss: 0.2734\n",
      "Epoch: 1 / 3, Step: 738 / 750 Loss: 0.3885\n",
      "Epoch: 1 / 3, Step: 739 / 750 Loss: 0.2699\n",
      "Epoch: 1 / 3, Step: 740 / 750 Loss: 0.2314\n",
      "Epoch: 1 / 3, Step: 741 / 750 Loss: 0.3056\n",
      "Epoch: 1 / 3, Step: 742 / 750 Loss: 0.2160\n",
      "Epoch: 1 / 3, Step: 743 / 750 Loss: 0.3080\n",
      "Epoch: 1 / 3, Step: 744 / 750 Loss: 0.2056\n",
      "Epoch: 1 / 3, Step: 745 / 750 Loss: 0.3119\n",
      "Epoch: 1 / 3, Step: 746 / 750 Loss: 0.4292\n",
      "Epoch: 1 / 3, Step: 747 / 750 Loss: 0.3214\n",
      "Epoch: 1 / 3, Step: 748 / 750 Loss: 0.3013\n",
      "Epoch: 1 / 3, Step: 749 / 750 Loss: 0.4004\n",
      "Epoch: 2 / 3, Step: 0 / 750 Loss: 0.4571\n",
      "Epoch: 2 / 3, Step: 1 / 750 Loss: 0.3983\n",
      "Epoch: 2 / 3, Step: 2 / 750 Loss: 0.2166\n",
      "Epoch: 2 / 3, Step: 3 / 750 Loss: 0.5058\n",
      "Epoch: 2 / 3, Step: 4 / 750 Loss: 0.1569\n",
      "Epoch: 2 / 3, Step: 5 / 750 Loss: 0.2437\n",
      "Epoch: 2 / 3, Step: 6 / 750 Loss: 0.3093\n",
      "Epoch: 2 / 3, Step: 7 / 750 Loss: 0.2509\n",
      "Epoch: 2 / 3, Step: 8 / 750 Loss: 0.2774\n",
      "Epoch: 2 / 3, Step: 9 / 750 Loss: 0.3795\n",
      "Epoch: 2 / 3, Step: 10 / 750 Loss: 0.2026\n",
      "Epoch: 2 / 3, Step: 11 / 750 Loss: 0.2948\n",
      "Epoch: 2 / 3, Step: 12 / 750 Loss: 0.2768\n",
      "Epoch: 2 / 3, Step: 13 / 750 Loss: 0.2808\n",
      "Epoch: 2 / 3, Step: 14 / 750 Loss: 0.3659\n",
      "Epoch: 2 / 3, Step: 15 / 750 Loss: 0.1560\n",
      "Epoch: 2 / 3, Step: 16 / 750 Loss: 0.1191\n",
      "Epoch: 2 / 3, Step: 17 / 750 Loss: 0.3664\n",
      "Epoch: 2 / 3, Step: 18 / 750 Loss: 0.2963\n",
      "Epoch: 2 / 3, Step: 19 / 750 Loss: 0.1178\n",
      "Epoch: 2 / 3, Step: 20 / 750 Loss: 0.1970\n",
      "Epoch: 2 / 3, Step: 21 / 750 Loss: 0.2376\n",
      "Epoch: 2 / 3, Step: 22 / 750 Loss: 0.1341\n",
      "Epoch: 2 / 3, Step: 23 / 750 Loss: 0.3910\n",
      "Epoch: 2 / 3, Step: 24 / 750 Loss: 0.3243\n",
      "Epoch: 2 / 3, Step: 25 / 750 Loss: 0.1944\n",
      "Epoch: 2 / 3, Step: 26 / 750 Loss: 0.1275\n",
      "Epoch: 2 / 3, Step: 27 / 750 Loss: 0.3298\n",
      "Epoch: 2 / 3, Step: 28 / 750 Loss: 0.4244\n",
      "Epoch: 2 / 3, Step: 29 / 750 Loss: 0.5968\n",
      "Epoch: 2 / 3, Step: 30 / 750 Loss: 0.1846\n",
      "Epoch: 2 / 3, Step: 31 / 750 Loss: 0.6219\n",
      "Epoch: 2 / 3, Step: 32 / 750 Loss: 0.3096\n",
      "Epoch: 2 / 3, Step: 33 / 750 Loss: 0.2452\n",
      "Epoch: 2 / 3, Step: 34 / 750 Loss: 0.2956\n",
      "Epoch: 2 / 3, Step: 35 / 750 Loss: 0.3599\n",
      "Epoch: 2 / 3, Step: 36 / 750 Loss: 0.1993\n",
      "Epoch: 2 / 3, Step: 37 / 750 Loss: 0.3650\n",
      "Epoch: 2 / 3, Step: 38 / 750 Loss: 0.2143\n",
      "Epoch: 2 / 3, Step: 39 / 750 Loss: 0.2029\n",
      "Epoch: 2 / 3, Step: 40 / 750 Loss: 0.1781\n",
      "Epoch: 2 / 3, Step: 41 / 750 Loss: 0.2229\n",
      "Epoch: 2 / 3, Step: 42 / 750 Loss: 0.4354\n",
      "Epoch: 2 / 3, Step: 43 / 750 Loss: 0.1398\n",
      "Epoch: 2 / 3, Step: 44 / 750 Loss: 0.2707\n",
      "Epoch: 2 / 3, Step: 45 / 750 Loss: 0.4140\n",
      "Epoch: 2 / 3, Step: 46 / 750 Loss: 0.3768\n",
      "Epoch: 2 / 3, Step: 47 / 750 Loss: 0.3763\n",
      "Epoch: 2 / 3, Step: 48 / 750 Loss: 0.2515\n",
      "Epoch: 2 / 3, Step: 49 / 750 Loss: 0.0886\n",
      "Epoch: 2 / 3, Step: 50 / 750 Loss: 0.5997\n",
      "Epoch: 2 / 3, Step: 51 / 750 Loss: 0.3471\n",
      "Epoch: 2 / 3, Step: 52 / 750 Loss: 0.1703\n",
      "Epoch: 2 / 3, Step: 53 / 750 Loss: 0.2078\n",
      "Epoch: 2 / 3, Step: 54 / 750 Loss: 0.3721\n",
      "Epoch: 2 / 3, Step: 55 / 750 Loss: 0.3965\n",
      "Epoch: 2 / 3, Step: 56 / 750 Loss: 0.4417\n",
      "Epoch: 2 / 3, Step: 57 / 750 Loss: 0.3695\n",
      "Epoch: 2 / 3, Step: 58 / 750 Loss: 0.2212\n",
      "Epoch: 2 / 3, Step: 59 / 750 Loss: 0.2109\n",
      "Epoch: 2 / 3, Step: 60 / 750 Loss: 0.1236\n",
      "Epoch: 2 / 3, Step: 61 / 750 Loss: 0.3380\n",
      "Epoch: 2 / 3, Step: 62 / 750 Loss: 0.4896\n",
      "Epoch: 2 / 3, Step: 63 / 750 Loss: 0.2722\n",
      "Epoch: 2 / 3, Step: 64 / 750 Loss: 0.4155\n",
      "Epoch: 2 / 3, Step: 65 / 750 Loss: 0.1911\n",
      "Epoch: 2 / 3, Step: 66 / 750 Loss: 0.3619\n",
      "Epoch: 2 / 3, Step: 67 / 750 Loss: 0.2807\n",
      "Epoch: 2 / 3, Step: 68 / 750 Loss: 0.2755\n",
      "Epoch: 2 / 3, Step: 69 / 750 Loss: 0.1368\n",
      "Epoch: 2 / 3, Step: 70 / 750 Loss: 0.2490\n",
      "Epoch: 2 / 3, Step: 71 / 750 Loss: 0.1148\n",
      "Epoch: 2 / 3, Step: 72 / 750 Loss: 0.2962\n",
      "Epoch: 2 / 3, Step: 73 / 750 Loss: 0.2885\n",
      "Epoch: 2 / 3, Step: 74 / 750 Loss: 0.3154\n",
      "Epoch: 2 / 3, Step: 75 / 750 Loss: 0.4265\n",
      "Epoch: 2 / 3, Step: 76 / 750 Loss: 0.4234\n",
      "Epoch: 2 / 3, Step: 77 / 750 Loss: 0.3318\n",
      "Epoch: 2 / 3, Step: 78 / 750 Loss: 0.3064\n",
      "Epoch: 2 / 3, Step: 79 / 750 Loss: 0.3940\n",
      "Epoch: 2 / 3, Step: 80 / 750 Loss: 0.2968\n",
      "Epoch: 2 / 3, Step: 81 / 750 Loss: 0.2272\n",
      "Epoch: 2 / 3, Step: 82 / 750 Loss: 0.2931\n",
      "Epoch: 2 / 3, Step: 83 / 750 Loss: 0.2975\n",
      "Epoch: 2 / 3, Step: 84 / 750 Loss: 0.2860\n",
      "Epoch: 2 / 3, Step: 85 / 750 Loss: 0.2652\n",
      "Epoch: 2 / 3, Step: 86 / 750 Loss: 0.1755\n",
      "Epoch: 2 / 3, Step: 87 / 750 Loss: 0.4953\n",
      "Epoch: 2 / 3, Step: 88 / 750 Loss: 0.4286\n",
      "Epoch: 2 / 3, Step: 89 / 750 Loss: 0.1532\n",
      "Epoch: 2 / 3, Step: 90 / 750 Loss: 0.2324\n",
      "Epoch: 2 / 3, Step: 91 / 750 Loss: 0.3188\n",
      "Epoch: 2 / 3, Step: 92 / 750 Loss: 0.2218\n",
      "Epoch: 2 / 3, Step: 93 / 750 Loss: 0.1412\n",
      "Epoch: 2 / 3, Step: 94 / 750 Loss: 0.1486\n",
      "Epoch: 2 / 3, Step: 95 / 750 Loss: 0.3022\n",
      "Epoch: 2 / 3, Step: 96 / 750 Loss: 0.1985\n",
      "Epoch: 2 / 3, Step: 97 / 750 Loss: 0.2373\n",
      "Epoch: 2 / 3, Step: 98 / 750 Loss: 0.1574\n",
      "Epoch: 2 / 3, Step: 99 / 750 Loss: 0.4196\n",
      "Epoch: 2 / 3, Step: 100 / 750 Loss: 0.1327\n",
      "Epoch: 2 / 3, Step: 101 / 750 Loss: 0.3518\n",
      "Epoch: 2 / 3, Step: 102 / 750 Loss: 0.2606\n",
      "Epoch: 2 / 3, Step: 103 / 750 Loss: 0.2608\n",
      "Epoch: 2 / 3, Step: 104 / 750 Loss: 0.1850\n",
      "Epoch: 2 / 3, Step: 105 / 750 Loss: 0.2490\n",
      "Epoch: 2 / 3, Step: 106 / 750 Loss: 0.2531\n",
      "Epoch: 2 / 3, Step: 107 / 750 Loss: 0.1972\n",
      "Epoch: 2 / 3, Step: 108 / 750 Loss: 0.3907\n",
      "Epoch: 2 / 3, Step: 109 / 750 Loss: 0.2072\n",
      "Epoch: 2 / 3, Step: 110 / 750 Loss: 0.5676\n",
      "Epoch: 2 / 3, Step: 111 / 750 Loss: 0.2653\n",
      "Epoch: 2 / 3, Step: 112 / 750 Loss: 0.2820\n",
      "Epoch: 2 / 3, Step: 113 / 750 Loss: 0.3678\n",
      "Epoch: 2 / 3, Step: 114 / 750 Loss: 0.1599\n",
      "Epoch: 2 / 3, Step: 115 / 750 Loss: 0.1294\n",
      "Epoch: 2 / 3, Step: 116 / 750 Loss: 0.3498\n",
      "Epoch: 2 / 3, Step: 117 / 750 Loss: 0.1863\n",
      "Epoch: 2 / 3, Step: 118 / 750 Loss: 0.3283\n",
      "Epoch: 2 / 3, Step: 119 / 750 Loss: 0.4191\n",
      "Epoch: 2 / 3, Step: 120 / 750 Loss: 0.3048\n",
      "Epoch: 2 / 3, Step: 121 / 750 Loss: 0.2604\n",
      "Epoch: 2 / 3, Step: 122 / 750 Loss: 0.2003\n",
      "Epoch: 2 / 3, Step: 123 / 750 Loss: 0.2250\n",
      "Epoch: 2 / 3, Step: 124 / 750 Loss: 0.3646\n",
      "Epoch: 2 / 3, Step: 125 / 750 Loss: 0.0864\n",
      "Epoch: 2 / 3, Step: 126 / 750 Loss: 0.3095\n",
      "Epoch: 2 / 3, Step: 127 / 750 Loss: 0.1850\n",
      "Epoch: 2 / 3, Step: 128 / 750 Loss: 0.1398\n",
      "Epoch: 2 / 3, Step: 129 / 750 Loss: 0.4531\n",
      "Epoch: 2 / 3, Step: 130 / 750 Loss: 0.3578\n",
      "Epoch: 2 / 3, Step: 131 / 750 Loss: 0.3913\n",
      "Epoch: 2 / 3, Step: 132 / 750 Loss: 0.2617\n",
      "Epoch: 2 / 3, Step: 133 / 750 Loss: 0.1215\n",
      "Epoch: 2 / 3, Step: 134 / 750 Loss: 0.2983\n",
      "Epoch: 2 / 3, Step: 135 / 750 Loss: 0.2579\n",
      "Epoch: 2 / 3, Step: 136 / 750 Loss: 0.1234\n",
      "Epoch: 2 / 3, Step: 137 / 750 Loss: 0.1247\n",
      "Epoch: 2 / 3, Step: 138 / 750 Loss: 0.3912\n",
      "Epoch: 2 / 3, Step: 139 / 750 Loss: 0.1986\n",
      "Epoch: 2 / 3, Step: 140 / 750 Loss: 0.2438\n",
      "Epoch: 2 / 3, Step: 141 / 750 Loss: 0.3619\n",
      "Epoch: 2 / 3, Step: 142 / 750 Loss: 0.2372\n",
      "Epoch: 2 / 3, Step: 143 / 750 Loss: 0.2117\n",
      "Epoch: 2 / 3, Step: 144 / 750 Loss: 0.4049\n",
      "Epoch: 2 / 3, Step: 145 / 750 Loss: 0.6850\n",
      "Epoch: 2 / 3, Step: 146 / 750 Loss: 0.3214\n",
      "Epoch: 2 / 3, Step: 147 / 750 Loss: 0.4135\n",
      "Epoch: 2 / 3, Step: 148 / 750 Loss: 0.3205\n",
      "Epoch: 2 / 3, Step: 149 / 750 Loss: 0.2076\n",
      "Epoch: 2 / 3, Step: 150 / 750 Loss: 0.3200\n",
      "Epoch: 2 / 3, Step: 151 / 750 Loss: 0.2867\n",
      "Epoch: 2 / 3, Step: 152 / 750 Loss: 0.5176\n",
      "Epoch: 2 / 3, Step: 153 / 750 Loss: 0.1832\n",
      "Epoch: 2 / 3, Step: 154 / 750 Loss: 0.1898\n",
      "Epoch: 2 / 3, Step: 155 / 750 Loss: 0.5240\n",
      "Epoch: 2 / 3, Step: 156 / 750 Loss: 0.1990\n",
      "Epoch: 2 / 3, Step: 157 / 750 Loss: 0.1575\n",
      "Epoch: 2 / 3, Step: 158 / 750 Loss: 0.2545\n",
      "Epoch: 2 / 3, Step: 159 / 750 Loss: 0.2221\n",
      "Epoch: 2 / 3, Step: 160 / 750 Loss: 0.2105\n",
      "Epoch: 2 / 3, Step: 161 / 750 Loss: 0.3151\n",
      "Epoch: 2 / 3, Step: 162 / 750 Loss: 0.3542\n",
      "Epoch: 2 / 3, Step: 163 / 750 Loss: 0.1951\n",
      "Epoch: 2 / 3, Step: 164 / 750 Loss: 0.3959\n",
      "Epoch: 2 / 3, Step: 165 / 750 Loss: 0.2578\n",
      "Epoch: 2 / 3, Step: 166 / 750 Loss: 0.2992\n",
      "Epoch: 2 / 3, Step: 167 / 750 Loss: 0.3918\n",
      "Epoch: 2 / 3, Step: 168 / 750 Loss: 0.2572\n",
      "Epoch: 2 / 3, Step: 169 / 750 Loss: 0.4784\n",
      "Epoch: 2 / 3, Step: 170 / 750 Loss: 0.5964\n",
      "Epoch: 2 / 3, Step: 171 / 750 Loss: 0.2278\n",
      "Epoch: 2 / 3, Step: 172 / 750 Loss: 0.2370\n",
      "Epoch: 2 / 3, Step: 173 / 750 Loss: 0.3835\n",
      "Epoch: 2 / 3, Step: 174 / 750 Loss: 0.3709\n",
      "Epoch: 2 / 3, Step: 175 / 750 Loss: 0.1847\n",
      "Epoch: 2 / 3, Step: 176 / 750 Loss: 0.4666\n",
      "Epoch: 2 / 3, Step: 177 / 750 Loss: 0.1927\n",
      "Epoch: 2 / 3, Step: 178 / 750 Loss: 0.1466\n",
      "Epoch: 2 / 3, Step: 179 / 750 Loss: 0.4565\n",
      "Epoch: 2 / 3, Step: 180 / 750 Loss: 0.2966\n",
      "Epoch: 2 / 3, Step: 181 / 750 Loss: 0.1110\n",
      "Epoch: 2 / 3, Step: 182 / 750 Loss: 0.2321\n",
      "Epoch: 2 / 3, Step: 183 / 750 Loss: 0.3178\n",
      "Epoch: 2 / 3, Step: 184 / 750 Loss: 0.3110\n",
      "Epoch: 2 / 3, Step: 185 / 750 Loss: 0.2600\n",
      "Epoch: 2 / 3, Step: 186 / 750 Loss: 0.0911\n",
      "Epoch: 2 / 3, Step: 187 / 750 Loss: 0.1609\n",
      "Epoch: 2 / 3, Step: 188 / 750 Loss: 0.1902\n",
      "Epoch: 2 / 3, Step: 189 / 750 Loss: 0.1185\n",
      "Epoch: 2 / 3, Step: 190 / 750 Loss: 0.2031\n",
      "Epoch: 2 / 3, Step: 191 / 750 Loss: 0.2349\n",
      "Epoch: 2 / 3, Step: 192 / 750 Loss: 0.4780\n",
      "Epoch: 2 / 3, Step: 193 / 750 Loss: 0.4951\n",
      "Epoch: 2 / 3, Step: 194 / 750 Loss: 0.1686\n",
      "Epoch: 2 / 3, Step: 195 / 750 Loss: 0.2139\n",
      "Epoch: 2 / 3, Step: 196 / 750 Loss: 0.2241\n",
      "Epoch: 2 / 3, Step: 197 / 750 Loss: 0.2907\n",
      "Epoch: 2 / 3, Step: 198 / 750 Loss: 0.3065\n",
      "Epoch: 2 / 3, Step: 199 / 750 Loss: 0.2107\n",
      "Epoch: 2 / 3, Step: 200 / 750 Loss: 0.2715\n",
      "Epoch: 2 / 3, Step: 201 / 750 Loss: 0.0745\n",
      "Epoch: 2 / 3, Step: 202 / 750 Loss: 0.2416\n",
      "Epoch: 2 / 3, Step: 203 / 750 Loss: 0.1258\n",
      "Epoch: 2 / 3, Step: 204 / 750 Loss: 0.1347\n",
      "Epoch: 2 / 3, Step: 205 / 750 Loss: 0.6377\n",
      "Epoch: 2 / 3, Step: 206 / 750 Loss: 0.3449\n",
      "Epoch: 2 / 3, Step: 207 / 750 Loss: 0.2472\n",
      "Epoch: 2 / 3, Step: 208 / 750 Loss: 0.3915\n",
      "Epoch: 2 / 3, Step: 209 / 750 Loss: 0.2614\n",
      "Epoch: 2 / 3, Step: 210 / 750 Loss: 0.1042\n",
      "Epoch: 2 / 3, Step: 211 / 750 Loss: 0.2728\n",
      "Epoch: 2 / 3, Step: 212 / 750 Loss: 0.2839\n",
      "Epoch: 2 / 3, Step: 213 / 750 Loss: 0.2171\n",
      "Epoch: 2 / 3, Step: 214 / 750 Loss: 0.3269\n",
      "Epoch: 2 / 3, Step: 215 / 750 Loss: 0.1623\n",
      "Epoch: 2 / 3, Step: 216 / 750 Loss: 0.4154\n",
      "Epoch: 2 / 3, Step: 217 / 750 Loss: 0.2429\n",
      "Epoch: 2 / 3, Step: 218 / 750 Loss: 0.3614\n",
      "Epoch: 2 / 3, Step: 219 / 750 Loss: 0.3298\n",
      "Epoch: 2 / 3, Step: 220 / 750 Loss: 0.4184\n",
      "Epoch: 2 / 3, Step: 221 / 750 Loss: 0.2974\n",
      "Epoch: 2 / 3, Step: 222 / 750 Loss: 0.3683\n",
      "Epoch: 2 / 3, Step: 223 / 750 Loss: 0.2969\n",
      "Epoch: 2 / 3, Step: 224 / 750 Loss: 0.2830\n",
      "Epoch: 2 / 3, Step: 225 / 750 Loss: 0.2391\n",
      "Epoch: 2 / 3, Step: 226 / 750 Loss: 0.1992\n",
      "Epoch: 2 / 3, Step: 227 / 750 Loss: 0.4049\n",
      "Epoch: 2 / 3, Step: 228 / 750 Loss: 0.4399\n",
      "Epoch: 2 / 3, Step: 229 / 750 Loss: 0.2215\n",
      "Epoch: 2 / 3, Step: 230 / 750 Loss: 0.3494\n",
      "Epoch: 2 / 3, Step: 231 / 750 Loss: 0.2164\n",
      "Epoch: 2 / 3, Step: 232 / 750 Loss: 0.2890\n",
      "Epoch: 2 / 3, Step: 233 / 750 Loss: 0.3402\n",
      "Epoch: 2 / 3, Step: 234 / 750 Loss: 0.2232\n",
      "Epoch: 2 / 3, Step: 235 / 750 Loss: 0.1097\n",
      "Epoch: 2 / 3, Step: 236 / 750 Loss: 0.1557\n",
      "Epoch: 2 / 3, Step: 237 / 750 Loss: 0.2004\n",
      "Epoch: 2 / 3, Step: 238 / 750 Loss: 0.3443\n",
      "Epoch: 2 / 3, Step: 239 / 750 Loss: 0.3176\n",
      "Epoch: 2 / 3, Step: 240 / 750 Loss: 0.2381\n",
      "Epoch: 2 / 3, Step: 241 / 750 Loss: 0.2128\n",
      "Epoch: 2 / 3, Step: 242 / 750 Loss: 0.3248\n",
      "Epoch: 2 / 3, Step: 243 / 750 Loss: 0.2281\n",
      "Epoch: 2 / 3, Step: 244 / 750 Loss: 0.4640\n",
      "Epoch: 2 / 3, Step: 245 / 750 Loss: 0.2719\n",
      "Epoch: 2 / 3, Step: 246 / 750 Loss: 0.2396\n",
      "Epoch: 2 / 3, Step: 247 / 750 Loss: 0.4109\n",
      "Epoch: 2 / 3, Step: 248 / 750 Loss: 0.3376\n",
      "Epoch: 2 / 3, Step: 249 / 750 Loss: 0.3849\n",
      "Epoch: 2 / 3, Step: 250 / 750 Loss: 0.5833\n",
      "Epoch: 2 / 3, Step: 251 / 750 Loss: 0.4677\n",
      "Epoch: 2 / 3, Step: 252 / 750 Loss: 0.3994\n",
      "Epoch: 2 / 3, Step: 253 / 750 Loss: 0.1684\n",
      "Epoch: 2 / 3, Step: 254 / 750 Loss: 0.2330\n",
      "Epoch: 2 / 3, Step: 255 / 750 Loss: 0.2051\n",
      "Epoch: 2 / 3, Step: 256 / 750 Loss: 0.3103\n",
      "Epoch: 2 / 3, Step: 257 / 750 Loss: 0.1956\n",
      "Epoch: 2 / 3, Step: 258 / 750 Loss: 0.2579\n",
      "Epoch: 2 / 3, Step: 259 / 750 Loss: 0.1788\n",
      "Epoch: 2 / 3, Step: 260 / 750 Loss: 0.2347\n",
      "Epoch: 2 / 3, Step: 261 / 750 Loss: 0.2204\n",
      "Epoch: 2 / 3, Step: 262 / 750 Loss: 0.2735\n",
      "Epoch: 2 / 3, Step: 263 / 750 Loss: 0.1429\n",
      "Epoch: 2 / 3, Step: 264 / 750 Loss: 0.3219\n",
      "Epoch: 2 / 3, Step: 265 / 750 Loss: 0.5720\n",
      "Epoch: 2 / 3, Step: 266 / 750 Loss: 0.1716\n",
      "Epoch: 2 / 3, Step: 267 / 750 Loss: 0.3240\n",
      "Epoch: 2 / 3, Step: 268 / 750 Loss: 0.2269\n",
      "Epoch: 2 / 3, Step: 269 / 750 Loss: 0.3806\n",
      "Epoch: 2 / 3, Step: 270 / 750 Loss: 0.1483\n",
      "Epoch: 2 / 3, Step: 271 / 750 Loss: 0.1693\n",
      "Epoch: 2 / 3, Step: 272 / 750 Loss: 0.2866\n",
      "Epoch: 2 / 3, Step: 273 / 750 Loss: 0.1930\n",
      "Epoch: 2 / 3, Step: 274 / 750 Loss: 0.4194\n",
      "Epoch: 2 / 3, Step: 275 / 750 Loss: 0.2698\n",
      "Epoch: 2 / 3, Step: 276 / 750 Loss: 0.4022\n",
      "Epoch: 2 / 3, Step: 277 / 750 Loss: 0.1467\n",
      "Epoch: 2 / 3, Step: 278 / 750 Loss: 0.3253\n",
      "Epoch: 2 / 3, Step: 279 / 750 Loss: 0.2210\n",
      "Epoch: 2 / 3, Step: 280 / 750 Loss: 0.1028\n",
      "Epoch: 2 / 3, Step: 281 / 750 Loss: 0.1992\n",
      "Epoch: 2 / 3, Step: 282 / 750 Loss: 0.2291\n",
      "Epoch: 2 / 3, Step: 283 / 750 Loss: 0.1431\n",
      "Epoch: 2 / 3, Step: 284 / 750 Loss: 0.3262\n",
      "Epoch: 2 / 3, Step: 285 / 750 Loss: 0.2187\n",
      "Epoch: 2 / 3, Step: 286 / 750 Loss: 0.2071\n",
      "Epoch: 2 / 3, Step: 287 / 750 Loss: 0.4012\n",
      "Epoch: 2 / 3, Step: 288 / 750 Loss: 0.4127\n",
      "Epoch: 2 / 3, Step: 289 / 750 Loss: 0.3733\n",
      "Epoch: 2 / 3, Step: 290 / 750 Loss: 0.2137\n",
      "Epoch: 2 / 3, Step: 291 / 750 Loss: 0.1480\n",
      "Epoch: 2 / 3, Step: 292 / 750 Loss: 0.6380\n",
      "Epoch: 2 / 3, Step: 293 / 750 Loss: 0.4456\n",
      "Epoch: 2 / 3, Step: 294 / 750 Loss: 0.3424\n",
      "Epoch: 2 / 3, Step: 295 / 750 Loss: 0.1236\n",
      "Epoch: 2 / 3, Step: 296 / 750 Loss: 0.1450\n",
      "Epoch: 2 / 3, Step: 297 / 750 Loss: 0.4256\n",
      "Epoch: 2 / 3, Step: 298 / 750 Loss: 0.2587\n",
      "Epoch: 2 / 3, Step: 299 / 750 Loss: 0.2839\n",
      "Epoch: 2 / 3, Step: 300 / 750 Loss: 0.1998\n",
      "Epoch: 2 / 3, Step: 301 / 750 Loss: 0.2261\n",
      "Epoch: 2 / 3, Step: 302 / 750 Loss: 0.2120\n",
      "Epoch: 2 / 3, Step: 303 / 750 Loss: 0.2199\n",
      "Epoch: 2 / 3, Step: 304 / 750 Loss: 0.3731\n",
      "Epoch: 2 / 3, Step: 305 / 750 Loss: 0.3487\n",
      "Epoch: 2 / 3, Step: 306 / 750 Loss: 0.2550\n",
      "Epoch: 2 / 3, Step: 307 / 750 Loss: 0.5149\n",
      "Epoch: 2 / 3, Step: 308 / 750 Loss: 0.3775\n",
      "Epoch: 2 / 3, Step: 309 / 750 Loss: 0.1914\n",
      "Epoch: 2 / 3, Step: 310 / 750 Loss: 0.2827\n",
      "Epoch: 2 / 3, Step: 311 / 750 Loss: 0.3720\n",
      "Epoch: 2 / 3, Step: 312 / 750 Loss: 0.1665\n",
      "Epoch: 2 / 3, Step: 313 / 750 Loss: 0.3068\n",
      "Epoch: 2 / 3, Step: 314 / 750 Loss: 0.5000\n",
      "Epoch: 2 / 3, Step: 315 / 750 Loss: 0.1447\n",
      "Epoch: 2 / 3, Step: 316 / 750 Loss: 0.1109\n",
      "Epoch: 2 / 3, Step: 317 / 750 Loss: 0.1052\n",
      "Epoch: 2 / 3, Step: 318 / 750 Loss: 0.3176\n",
      "Epoch: 2 / 3, Step: 319 / 750 Loss: 0.3986\n",
      "Epoch: 2 / 3, Step: 320 / 750 Loss: 0.2124\n",
      "Epoch: 2 / 3, Step: 321 / 750 Loss: 0.1519\n",
      "Epoch: 2 / 3, Step: 322 / 750 Loss: 0.1208\n",
      "Epoch: 2 / 3, Step: 323 / 750 Loss: 0.3313\n",
      "Epoch: 2 / 3, Step: 324 / 750 Loss: 0.4497\n",
      "Epoch: 2 / 3, Step: 325 / 750 Loss: 0.1350\n",
      "Epoch: 2 / 3, Step: 326 / 750 Loss: 0.1857\n",
      "Epoch: 2 / 3, Step: 327 / 750 Loss: 0.1492\n",
      "Epoch: 2 / 3, Step: 328 / 750 Loss: 0.3178\n",
      "Epoch: 2 / 3, Step: 329 / 750 Loss: 0.1868\n",
      "Epoch: 2 / 3, Step: 330 / 750 Loss: 0.2168\n",
      "Epoch: 2 / 3, Step: 331 / 750 Loss: 0.1880\n",
      "Epoch: 2 / 3, Step: 332 / 750 Loss: 0.2583\n",
      "Epoch: 2 / 3, Step: 333 / 750 Loss: 0.4843\n",
      "Epoch: 2 / 3, Step: 334 / 750 Loss: 0.2785\n",
      "Epoch: 2 / 3, Step: 335 / 750 Loss: 0.3088\n",
      "Epoch: 2 / 3, Step: 336 / 750 Loss: 0.2112\n",
      "Epoch: 2 / 3, Step: 337 / 750 Loss: 0.0781\n",
      "Epoch: 2 / 3, Step: 338 / 750 Loss: 0.1162\n",
      "Epoch: 2 / 3, Step: 339 / 750 Loss: 0.4004\n",
      "Epoch: 2 / 3, Step: 340 / 750 Loss: 0.2733\n",
      "Epoch: 2 / 3, Step: 341 / 750 Loss: 0.2470\n",
      "Epoch: 2 / 3, Step: 342 / 750 Loss: 0.3232\n",
      "Epoch: 2 / 3, Step: 343 / 750 Loss: 0.1903\n",
      "Epoch: 2 / 3, Step: 344 / 750 Loss: 0.4891\n",
      "Epoch: 2 / 3, Step: 345 / 750 Loss: 0.1417\n",
      "Epoch: 2 / 3, Step: 346 / 750 Loss: 0.2667\n",
      "Epoch: 2 / 3, Step: 347 / 750 Loss: 0.3685\n",
      "Epoch: 2 / 3, Step: 348 / 750 Loss: 0.4917\n",
      "Epoch: 2 / 3, Step: 349 / 750 Loss: 0.4228\n",
      "Epoch: 2 / 3, Step: 350 / 750 Loss: 0.2103\n",
      "Epoch: 2 / 3, Step: 351 / 750 Loss: 0.3235\n",
      "Epoch: 2 / 3, Step: 352 / 750 Loss: 0.1846\n",
      "Epoch: 2 / 3, Step: 353 / 750 Loss: 0.3020\n",
      "Epoch: 2 / 3, Step: 354 / 750 Loss: 0.2494\n",
      "Epoch: 2 / 3, Step: 355 / 750 Loss: 0.3879\n",
      "Epoch: 2 / 3, Step: 356 / 750 Loss: 0.3984\n",
      "Epoch: 2 / 3, Step: 357 / 750 Loss: 0.2946\n",
      "Epoch: 2 / 3, Step: 358 / 750 Loss: 0.1893\n",
      "Epoch: 2 / 3, Step: 359 / 750 Loss: 0.1995\n",
      "Epoch: 2 / 3, Step: 360 / 750 Loss: 0.1319\n",
      "Epoch: 2 / 3, Step: 361 / 750 Loss: 0.1464\n",
      "Epoch: 2 / 3, Step: 362 / 750 Loss: 0.2647\n",
      "Epoch: 2 / 3, Step: 363 / 750 Loss: 0.4980\n",
      "Epoch: 2 / 3, Step: 364 / 750 Loss: 0.1008\n",
      "Epoch: 2 / 3, Step: 365 / 750 Loss: 0.1703\n",
      "Epoch: 2 / 3, Step: 366 / 750 Loss: 0.2247\n",
      "Epoch: 2 / 3, Step: 367 / 750 Loss: 0.2665\n",
      "Epoch: 2 / 3, Step: 368 / 750 Loss: 0.2875\n",
      "Epoch: 2 / 3, Step: 369 / 750 Loss: 0.4342\n",
      "Epoch: 2 / 3, Step: 370 / 750 Loss: 0.1188\n",
      "Epoch: 2 / 3, Step: 371 / 750 Loss: 0.2781\n",
      "Epoch: 2 / 3, Step: 372 / 750 Loss: 0.3235\n",
      "Epoch: 2 / 3, Step: 373 / 750 Loss: 0.4673\n",
      "Epoch: 2 / 3, Step: 374 / 750 Loss: 0.2882\n",
      "Epoch: 2 / 3, Step: 375 / 750 Loss: 0.2500\n",
      "Epoch: 2 / 3, Step: 376 / 750 Loss: 0.1302\n",
      "Epoch: 2 / 3, Step: 377 / 750 Loss: 0.2981\n",
      "Epoch: 2 / 3, Step: 378 / 750 Loss: 0.4145\n",
      "Epoch: 2 / 3, Step: 379 / 750 Loss: 0.1369\n",
      "Epoch: 2 / 3, Step: 380 / 750 Loss: 0.1677\n",
      "Epoch: 2 / 3, Step: 381 / 750 Loss: 0.1098\n",
      "Epoch: 2 / 3, Step: 382 / 750 Loss: 0.3807\n",
      "Epoch: 2 / 3, Step: 383 / 750 Loss: 0.5144\n",
      "Epoch: 2 / 3, Step: 384 / 750 Loss: 0.1255\n",
      "Epoch: 2 / 3, Step: 385 / 750 Loss: 0.2656\n",
      "Epoch: 2 / 3, Step: 386 / 750 Loss: 0.1128\n",
      "Epoch: 2 / 3, Step: 387 / 750 Loss: 0.3210\n",
      "Epoch: 2 / 3, Step: 388 / 750 Loss: 0.1566\n",
      "Epoch: 2 / 3, Step: 389 / 750 Loss: 0.2251\n",
      "Epoch: 2 / 3, Step: 390 / 750 Loss: 0.0835\n",
      "Epoch: 2 / 3, Step: 391 / 750 Loss: 0.2101\n",
      "Epoch: 2 / 3, Step: 392 / 750 Loss: 0.1350\n",
      "Epoch: 2 / 3, Step: 393 / 750 Loss: 0.1417\n",
      "Epoch: 2 / 3, Step: 394 / 750 Loss: 0.2413\n",
      "Epoch: 2 / 3, Step: 395 / 750 Loss: 0.0799\n",
      "Epoch: 2 / 3, Step: 396 / 750 Loss: 0.5503\n",
      "Epoch: 2 / 3, Step: 397 / 750 Loss: 0.3867\n",
      "Epoch: 2 / 3, Step: 398 / 750 Loss: 0.6242\n",
      "Epoch: 2 / 3, Step: 399 / 750 Loss: 0.4898\n",
      "Epoch: 2 / 3, Step: 400 / 750 Loss: 0.1189\n",
      "Epoch: 2 / 3, Step: 401 / 750 Loss: 0.1873\n",
      "Epoch: 2 / 3, Step: 402 / 750 Loss: 0.1021\n",
      "Epoch: 2 / 3, Step: 403 / 750 Loss: 0.1965\n",
      "Epoch: 2 / 3, Step: 404 / 750 Loss: 0.0523\n",
      "Epoch: 2 / 3, Step: 405 / 750 Loss: 0.3193\n",
      "Epoch: 2 / 3, Step: 406 / 750 Loss: 0.3282\n",
      "Epoch: 2 / 3, Step: 407 / 750 Loss: 0.2850\n",
      "Epoch: 2 / 3, Step: 408 / 750 Loss: 0.2785\n",
      "Epoch: 2 / 3, Step: 409 / 750 Loss: 0.3054\n",
      "Epoch: 2 / 3, Step: 410 / 750 Loss: 0.3546\n",
      "Epoch: 2 / 3, Step: 411 / 750 Loss: 0.1619\n",
      "Epoch: 2 / 3, Step: 412 / 750 Loss: 0.4045\n",
      "Epoch: 2 / 3, Step: 413 / 750 Loss: 0.1470\n",
      "Epoch: 2 / 3, Step: 414 / 750 Loss: 0.4373\n",
      "Epoch: 2 / 3, Step: 415 / 750 Loss: 0.3406\n",
      "Epoch: 2 / 3, Step: 416 / 750 Loss: 0.1918\n",
      "Epoch: 2 / 3, Step: 417 / 750 Loss: 0.2360\n",
      "Epoch: 2 / 3, Step: 418 / 750 Loss: 0.3552\n",
      "Epoch: 2 / 3, Step: 419 / 750 Loss: 0.3489\n",
      "Epoch: 2 / 3, Step: 420 / 750 Loss: 0.3281\n",
      "Epoch: 2 / 3, Step: 421 / 750 Loss: 0.2602\n",
      "Epoch: 2 / 3, Step: 422 / 750 Loss: 0.1891\n",
      "Epoch: 2 / 3, Step: 423 / 750 Loss: 0.1924\n",
      "Epoch: 2 / 3, Step: 424 / 750 Loss: 0.1871\n",
      "Epoch: 2 / 3, Step: 425 / 750 Loss: 0.1661\n",
      "Epoch: 2 / 3, Step: 426 / 750 Loss: 0.1622\n",
      "Epoch: 2 / 3, Step: 427 / 750 Loss: 0.0762\n",
      "Epoch: 2 / 3, Step: 428 / 750 Loss: 0.1313\n",
      "Epoch: 2 / 3, Step: 429 / 750 Loss: 0.4523\n",
      "Epoch: 2 / 3, Step: 430 / 750 Loss: 0.2973\n",
      "Epoch: 2 / 3, Step: 431 / 750 Loss: 0.6523\n",
      "Epoch: 2 / 3, Step: 432 / 750 Loss: 0.3964\n",
      "Epoch: 2 / 3, Step: 433 / 750 Loss: 0.2316\n",
      "Epoch: 2 / 3, Step: 434 / 750 Loss: 0.1485\n",
      "Epoch: 2 / 3, Step: 435 / 750 Loss: 0.2449\n",
      "Epoch: 2 / 3, Step: 436 / 750 Loss: 0.2385\n",
      "Epoch: 2 / 3, Step: 437 / 750 Loss: 0.2623\n",
      "Epoch: 2 / 3, Step: 438 / 750 Loss: 0.2545\n",
      "Epoch: 2 / 3, Step: 439 / 750 Loss: 0.2097\n",
      "Epoch: 2 / 3, Step: 440 / 750 Loss: 0.1157\n",
      "Epoch: 2 / 3, Step: 441 / 750 Loss: 0.1776\n",
      "Epoch: 2 / 3, Step: 442 / 750 Loss: 0.2684\n",
      "Epoch: 2 / 3, Step: 443 / 750 Loss: 0.4495\n",
      "Epoch: 2 / 3, Step: 444 / 750 Loss: 0.1395\n",
      "Epoch: 2 / 3, Step: 445 / 750 Loss: 0.1732\n",
      "Epoch: 2 / 3, Step: 446 / 750 Loss: 0.2251\n",
      "Epoch: 2 / 3, Step: 447 / 750 Loss: 0.0983\n",
      "Epoch: 2 / 3, Step: 448 / 750 Loss: 0.2408\n",
      "Epoch: 2 / 3, Step: 449 / 750 Loss: 0.2061\n",
      "Epoch: 2 / 3, Step: 450 / 750 Loss: 0.3713\n",
      "Epoch: 2 / 3, Step: 451 / 750 Loss: 0.2664\n",
      "Epoch: 2 / 3, Step: 452 / 750 Loss: 0.1728\n",
      "Epoch: 2 / 3, Step: 453 / 750 Loss: 0.1874\n",
      "Epoch: 2 / 3, Step: 454 / 750 Loss: 0.1899\n",
      "Epoch: 2 / 3, Step: 455 / 750 Loss: 0.0971\n",
      "Epoch: 2 / 3, Step: 456 / 750 Loss: 0.3147\n",
      "Epoch: 2 / 3, Step: 457 / 750 Loss: 0.1505\n",
      "Epoch: 2 / 3, Step: 458 / 750 Loss: 0.5114\n",
      "Epoch: 2 / 3, Step: 459 / 750 Loss: 0.0723\n",
      "Epoch: 2 / 3, Step: 460 / 750 Loss: 0.2296\n",
      "Epoch: 2 / 3, Step: 461 / 750 Loss: 0.3633\n",
      "Epoch: 2 / 3, Step: 462 / 750 Loss: 0.2656\n",
      "Epoch: 2 / 3, Step: 463 / 750 Loss: 0.1716\n",
      "Epoch: 2 / 3, Step: 464 / 750 Loss: 0.1143\n",
      "Epoch: 2 / 3, Step: 465 / 750 Loss: 0.5933\n",
      "Epoch: 2 / 3, Step: 466 / 750 Loss: 0.4531\n",
      "Epoch: 2 / 3, Step: 467 / 750 Loss: 0.1261\n",
      "Epoch: 2 / 3, Step: 468 / 750 Loss: 0.3031\n",
      "Epoch: 2 / 3, Step: 469 / 750 Loss: 0.2068\n",
      "Epoch: 2 / 3, Step: 470 / 750 Loss: 0.4047\n",
      "Epoch: 2 / 3, Step: 471 / 750 Loss: 0.3668\n",
      "Epoch: 2 / 3, Step: 472 / 750 Loss: 0.2545\n",
      "Epoch: 2 / 3, Step: 473 / 750 Loss: 0.3676\n",
      "Epoch: 2 / 3, Step: 474 / 750 Loss: 0.3366\n",
      "Epoch: 2 / 3, Step: 475 / 750 Loss: 0.1614\n",
      "Epoch: 2 / 3, Step: 476 / 750 Loss: 0.2656\n",
      "Epoch: 2 / 3, Step: 477 / 750 Loss: 0.1192\n",
      "Epoch: 2 / 3, Step: 478 / 750 Loss: 0.2648\n",
      "Epoch: 2 / 3, Step: 479 / 750 Loss: 0.6288\n",
      "Epoch: 2 / 3, Step: 480 / 750 Loss: 0.2269\n",
      "Epoch: 2 / 3, Step: 481 / 750 Loss: 0.3218\n",
      "Epoch: 2 / 3, Step: 482 / 750 Loss: 0.1244\n",
      "Epoch: 2 / 3, Step: 483 / 750 Loss: 0.1880\n",
      "Epoch: 2 / 3, Step: 484 / 750 Loss: 0.3096\n",
      "Epoch: 2 / 3, Step: 485 / 750 Loss: 0.1879\n",
      "Epoch: 2 / 3, Step: 486 / 750 Loss: 0.1052\n",
      "Epoch: 2 / 3, Step: 487 / 750 Loss: 0.2730\n",
      "Epoch: 2 / 3, Step: 488 / 750 Loss: 0.1136\n",
      "Epoch: 2 / 3, Step: 489 / 750 Loss: 0.1318\n",
      "Epoch: 2 / 3, Step: 490 / 750 Loss: 0.2581\n",
      "Epoch: 2 / 3, Step: 491 / 750 Loss: 0.2373\n",
      "Epoch: 2 / 3, Step: 492 / 750 Loss: 0.1073\n",
      "Epoch: 2 / 3, Step: 493 / 750 Loss: 0.4126\n",
      "Epoch: 2 / 3, Step: 494 / 750 Loss: 0.2707\n",
      "Epoch: 2 / 3, Step: 495 / 750 Loss: 0.3076\n",
      "Epoch: 2 / 3, Step: 496 / 750 Loss: 0.2384\n",
      "Epoch: 2 / 3, Step: 497 / 750 Loss: 0.1934\n",
      "Epoch: 2 / 3, Step: 498 / 750 Loss: 0.2483\n",
      "Epoch: 2 / 3, Step: 499 / 750 Loss: 0.1086\n",
      "Epoch: 2 / 3, Step: 500 / 750 Loss: 0.2949\n",
      "Epoch: 2 / 3, Step: 501 / 750 Loss: 0.3482\n",
      "Epoch: 2 / 3, Step: 502 / 750 Loss: 0.1558\n",
      "Epoch: 2 / 3, Step: 503 / 750 Loss: 0.1895\n",
      "Epoch: 2 / 3, Step: 504 / 750 Loss: 0.1204\n",
      "Epoch: 2 / 3, Step: 505 / 750 Loss: 0.5594\n",
      "Epoch: 2 / 3, Step: 506 / 750 Loss: 0.2043\n",
      "Epoch: 2 / 3, Step: 507 / 750 Loss: 0.0688\n",
      "Epoch: 2 / 3, Step: 508 / 750 Loss: 0.1156\n",
      "Epoch: 2 / 3, Step: 509 / 750 Loss: 0.2220\n",
      "Epoch: 2 / 3, Step: 510 / 750 Loss: 0.2032\n",
      "Epoch: 2 / 3, Step: 511 / 750 Loss: 0.0807\n",
      "Epoch: 2 / 3, Step: 512 / 750 Loss: 0.2894\n",
      "Epoch: 2 / 3, Step: 513 / 750 Loss: 0.2363\n",
      "Epoch: 2 / 3, Step: 514 / 750 Loss: 0.3449\n",
      "Epoch: 2 / 3, Step: 515 / 750 Loss: 0.5414\n",
      "Epoch: 2 / 3, Step: 516 / 750 Loss: 0.2832\n",
      "Epoch: 2 / 3, Step: 517 / 750 Loss: 0.2984\n",
      "Epoch: 2 / 3, Step: 518 / 750 Loss: 0.4307\n",
      "Epoch: 2 / 3, Step: 519 / 750 Loss: 0.1427\n",
      "Epoch: 2 / 3, Step: 520 / 750 Loss: 0.1530\n",
      "Epoch: 2 / 3, Step: 521 / 750 Loss: 0.2019\n",
      "Epoch: 2 / 3, Step: 522 / 750 Loss: 0.2432\n",
      "Epoch: 2 / 3, Step: 523 / 750 Loss: 0.1627\n",
      "Epoch: 2 / 3, Step: 524 / 750 Loss: 0.4061\n",
      "Epoch: 2 / 3, Step: 525 / 750 Loss: 0.4512\n",
      "Epoch: 2 / 3, Step: 526 / 750 Loss: 0.2378\n",
      "Epoch: 2 / 3, Step: 527 / 750 Loss: 0.1852\n",
      "Epoch: 2 / 3, Step: 528 / 750 Loss: 0.3360\n",
      "Epoch: 2 / 3, Step: 529 / 750 Loss: 0.1300\n",
      "Epoch: 2 / 3, Step: 530 / 750 Loss: 0.2003\n",
      "Epoch: 2 / 3, Step: 531 / 750 Loss: 0.1142\n",
      "Epoch: 2 / 3, Step: 532 / 750 Loss: 0.1125\n",
      "Epoch: 2 / 3, Step: 533 / 750 Loss: 0.2115\n",
      "Epoch: 2 / 3, Step: 534 / 750 Loss: 0.3852\n",
      "Epoch: 2 / 3, Step: 535 / 750 Loss: 0.1657\n",
      "Epoch: 2 / 3, Step: 536 / 750 Loss: 0.2522\n",
      "Epoch: 2 / 3, Step: 537 / 750 Loss: 0.3145\n",
      "Epoch: 2 / 3, Step: 538 / 750 Loss: 0.0523\n",
      "Epoch: 2 / 3, Step: 539 / 750 Loss: 0.2251\n",
      "Epoch: 2 / 3, Step: 540 / 750 Loss: 0.2837\n",
      "Epoch: 2 / 3, Step: 541 / 750 Loss: 0.2570\n",
      "Epoch: 2 / 3, Step: 542 / 750 Loss: 0.1933\n",
      "Epoch: 2 / 3, Step: 543 / 750 Loss: 0.2758\n",
      "Epoch: 2 / 3, Step: 544 / 750 Loss: 0.1099\n",
      "Epoch: 2 / 3, Step: 545 / 750 Loss: 0.1214\n",
      "Epoch: 2 / 3, Step: 546 / 750 Loss: 0.2169\n",
      "Epoch: 2 / 3, Step: 547 / 750 Loss: 0.1653\n",
      "Epoch: 2 / 3, Step: 548 / 750 Loss: 0.2268\n",
      "Epoch: 2 / 3, Step: 549 / 750 Loss: 0.2470\n",
      "Epoch: 2 / 3, Step: 550 / 750 Loss: 0.3024\n",
      "Epoch: 2 / 3, Step: 551 / 750 Loss: 0.1549\n",
      "Epoch: 2 / 3, Step: 552 / 750 Loss: 0.1135\n",
      "Epoch: 2 / 3, Step: 553 / 750 Loss: 0.1632\n",
      "Epoch: 2 / 3, Step: 554 / 750 Loss: 0.5242\n",
      "Epoch: 2 / 3, Step: 555 / 750 Loss: 0.0872\n",
      "Epoch: 2 / 3, Step: 556 / 750 Loss: 0.3874\n",
      "Epoch: 2 / 3, Step: 557 / 750 Loss: 0.0526\n",
      "Epoch: 2 / 3, Step: 558 / 750 Loss: 0.2873\n",
      "Epoch: 2 / 3, Step: 559 / 750 Loss: 0.1902\n",
      "Epoch: 2 / 3, Step: 560 / 750 Loss: 0.2739\n",
      "Epoch: 2 / 3, Step: 561 / 750 Loss: 0.2961\n",
      "Epoch: 2 / 3, Step: 562 / 750 Loss: 0.2360\n",
      "Epoch: 2 / 3, Step: 563 / 750 Loss: 0.1809\n",
      "Epoch: 2 / 3, Step: 564 / 750 Loss: 0.3509\n",
      "Epoch: 2 / 3, Step: 565 / 750 Loss: 0.1879\n",
      "Epoch: 2 / 3, Step: 566 / 750 Loss: 0.4492\n",
      "Epoch: 2 / 3, Step: 567 / 750 Loss: 0.1952\n",
      "Epoch: 2 / 3, Step: 568 / 750 Loss: 0.1070\n",
      "Epoch: 2 / 3, Step: 569 / 750 Loss: 0.4145\n",
      "Epoch: 2 / 3, Step: 570 / 750 Loss: 0.1617\n",
      "Epoch: 2 / 3, Step: 571 / 750 Loss: 0.3015\n",
      "Epoch: 2 / 3, Step: 572 / 750 Loss: 0.2555\n",
      "Epoch: 2 / 3, Step: 573 / 750 Loss: 0.2997\n",
      "Epoch: 2 / 3, Step: 574 / 750 Loss: 0.3583\n",
      "Epoch: 2 / 3, Step: 575 / 750 Loss: 0.1417\n",
      "Epoch: 2 / 3, Step: 576 / 750 Loss: 0.2477\n",
      "Epoch: 2 / 3, Step: 577 / 750 Loss: 0.3473\n",
      "Epoch: 2 / 3, Step: 578 / 750 Loss: 0.3655\n",
      "Epoch: 2 / 3, Step: 579 / 750 Loss: 0.2376\n",
      "Epoch: 2 / 3, Step: 580 / 750 Loss: 0.1274\n",
      "Epoch: 2 / 3, Step: 581 / 750 Loss: 0.2773\n",
      "Epoch: 2 / 3, Step: 582 / 750 Loss: 0.1394\n",
      "Epoch: 2 / 3, Step: 583 / 750 Loss: 0.1422\n",
      "Epoch: 2 / 3, Step: 584 / 750 Loss: 0.2315\n",
      "Epoch: 2 / 3, Step: 585 / 750 Loss: 0.3848\n",
      "Epoch: 2 / 3, Step: 586 / 750 Loss: 0.2716\n",
      "Epoch: 2 / 3, Step: 587 / 750 Loss: 0.1331\n",
      "Epoch: 2 / 3, Step: 588 / 750 Loss: 0.4501\n",
      "Epoch: 2 / 3, Step: 589 / 750 Loss: 0.3638\n",
      "Epoch: 2 / 3, Step: 590 / 750 Loss: 0.2674\n",
      "Epoch: 2 / 3, Step: 591 / 750 Loss: 0.2471\n",
      "Epoch: 2 / 3, Step: 592 / 750 Loss: 0.0808\n",
      "Epoch: 2 / 3, Step: 593 / 750 Loss: 0.4883\n",
      "Epoch: 2 / 3, Step: 594 / 750 Loss: 0.3956\n",
      "Epoch: 2 / 3, Step: 595 / 750 Loss: 0.3619\n",
      "Epoch: 2 / 3, Step: 596 / 750 Loss: 0.1538\n",
      "Epoch: 2 / 3, Step: 597 / 750 Loss: 0.6847\n",
      "Epoch: 2 / 3, Step: 598 / 750 Loss: 0.2098\n",
      "Epoch: 2 / 3, Step: 599 / 750 Loss: 0.2467\n",
      "Epoch: 2 / 3, Step: 600 / 750 Loss: 0.1089\n",
      "Epoch: 2 / 3, Step: 601 / 750 Loss: 0.2985\n",
      "Epoch: 2 / 3, Step: 602 / 750 Loss: 0.2222\n",
      "Epoch: 2 / 3, Step: 603 / 750 Loss: 0.1725\n",
      "Epoch: 2 / 3, Step: 604 / 750 Loss: 0.2058\n",
      "Epoch: 2 / 3, Step: 605 / 750 Loss: 0.1047\n",
      "Epoch: 2 / 3, Step: 606 / 750 Loss: 0.2249\n",
      "Epoch: 2 / 3, Step: 607 / 750 Loss: 0.1409\n",
      "Epoch: 2 / 3, Step: 608 / 750 Loss: 0.1542\n",
      "Epoch: 2 / 3, Step: 609 / 750 Loss: 0.1314\n",
      "Epoch: 2 / 3, Step: 610 / 750 Loss: 0.1459\n",
      "Epoch: 2 / 3, Step: 611 / 750 Loss: 0.3274\n",
      "Epoch: 2 / 3, Step: 612 / 750 Loss: 0.2081\n",
      "Epoch: 2 / 3, Step: 613 / 750 Loss: 0.0848\n",
      "Epoch: 2 / 3, Step: 614 / 750 Loss: 0.0877\n",
      "Epoch: 2 / 3, Step: 615 / 750 Loss: 0.1032\n",
      "Epoch: 2 / 3, Step: 616 / 750 Loss: 0.1504\n",
      "Epoch: 2 / 3, Step: 617 / 750 Loss: 0.1754\n",
      "Epoch: 2 / 3, Step: 618 / 750 Loss: 0.2827\n",
      "Epoch: 2 / 3, Step: 619 / 750 Loss: 0.2167\n",
      "Epoch: 2 / 3, Step: 620 / 750 Loss: 0.1609\n",
      "Epoch: 2 / 3, Step: 621 / 750 Loss: 0.0816\n",
      "Epoch: 2 / 3, Step: 622 / 750 Loss: 0.2271\n",
      "Epoch: 2 / 3, Step: 623 / 750 Loss: 0.1429\n",
      "Epoch: 2 / 3, Step: 624 / 750 Loss: 0.3293\n",
      "Epoch: 2 / 3, Step: 625 / 750 Loss: 0.3135\n",
      "Epoch: 2 / 3, Step: 626 / 750 Loss: 0.2872\n",
      "Epoch: 2 / 3, Step: 627 / 750 Loss: 0.2100\n",
      "Epoch: 2 / 3, Step: 628 / 750 Loss: 0.2652\n",
      "Epoch: 2 / 3, Step: 629 / 750 Loss: 0.2668\n",
      "Epoch: 2 / 3, Step: 630 / 750 Loss: 0.2810\n",
      "Epoch: 2 / 3, Step: 631 / 750 Loss: 0.7699\n",
      "Epoch: 2 / 3, Step: 632 / 750 Loss: 0.2844\n",
      "Epoch: 2 / 3, Step: 633 / 750 Loss: 0.2554\n",
      "Epoch: 2 / 3, Step: 634 / 750 Loss: 0.1113\n",
      "Epoch: 2 / 3, Step: 635 / 750 Loss: 0.4060\n",
      "Epoch: 2 / 3, Step: 636 / 750 Loss: 0.3808\n",
      "Epoch: 2 / 3, Step: 637 / 750 Loss: 0.3885\n",
      "Epoch: 2 / 3, Step: 638 / 750 Loss: 0.1088\n",
      "Epoch: 2 / 3, Step: 639 / 750 Loss: 0.2739\n",
      "Epoch: 2 / 3, Step: 640 / 750 Loss: 0.0874\n",
      "Epoch: 2 / 3, Step: 641 / 750 Loss: 0.3561\n",
      "Epoch: 2 / 3, Step: 642 / 750 Loss: 0.1679\n",
      "Epoch: 2 / 3, Step: 643 / 750 Loss: 0.2698\n",
      "Epoch: 2 / 3, Step: 644 / 750 Loss: 0.0947\n",
      "Epoch: 2 / 3, Step: 645 / 750 Loss: 0.2617\n",
      "Epoch: 2 / 3, Step: 646 / 750 Loss: 0.1087\n",
      "Epoch: 2 / 3, Step: 647 / 750 Loss: 0.3665\n",
      "Epoch: 2 / 3, Step: 648 / 750 Loss: 0.0982\n",
      "Epoch: 2 / 3, Step: 649 / 750 Loss: 0.1846\n",
      "Epoch: 2 / 3, Step: 650 / 750 Loss: 0.3957\n",
      "Epoch: 2 / 3, Step: 651 / 750 Loss: 0.2103\n",
      "Epoch: 2 / 3, Step: 652 / 750 Loss: 0.2518\n",
      "Epoch: 2 / 3, Step: 653 / 750 Loss: 0.3487\n",
      "Epoch: 2 / 3, Step: 654 / 750 Loss: 0.1167\n",
      "Epoch: 2 / 3, Step: 655 / 750 Loss: 0.3209\n",
      "Epoch: 2 / 3, Step: 656 / 750 Loss: 0.1759\n",
      "Epoch: 2 / 3, Step: 657 / 750 Loss: 0.3258\n",
      "Epoch: 2 / 3, Step: 658 / 750 Loss: 0.2631\n",
      "Epoch: 2 / 3, Step: 659 / 750 Loss: 0.3344\n",
      "Epoch: 2 / 3, Step: 660 / 750 Loss: 0.3151\n",
      "Epoch: 2 / 3, Step: 661 / 750 Loss: 0.2576\n",
      "Epoch: 2 / 3, Step: 662 / 750 Loss: 0.1110\n",
      "Epoch: 2 / 3, Step: 663 / 750 Loss: 0.1200\n",
      "Epoch: 2 / 3, Step: 664 / 750 Loss: 0.2424\n",
      "Epoch: 2 / 3, Step: 665 / 750 Loss: 0.2290\n",
      "Epoch: 2 / 3, Step: 666 / 750 Loss: 0.2618\n",
      "Epoch: 2 / 3, Step: 667 / 750 Loss: 0.3478\n",
      "Epoch: 2 / 3, Step: 668 / 750 Loss: 0.1740\n",
      "Epoch: 2 / 3, Step: 669 / 750 Loss: 0.0874\n",
      "Epoch: 2 / 3, Step: 670 / 750 Loss: 0.5400\n",
      "Epoch: 2 / 3, Step: 671 / 750 Loss: 0.3063\n",
      "Epoch: 2 / 3, Step: 672 / 750 Loss: 0.3705\n",
      "Epoch: 2 / 3, Step: 673 / 750 Loss: 0.2390\n",
      "Epoch: 2 / 3, Step: 674 / 750 Loss: 0.0960\n",
      "Epoch: 2 / 3, Step: 675 / 750 Loss: 0.2802\n",
      "Epoch: 2 / 3, Step: 676 / 750 Loss: 0.3012\n",
      "Epoch: 2 / 3, Step: 677 / 750 Loss: 0.0923\n",
      "Epoch: 2 / 3, Step: 678 / 750 Loss: 0.3459\n",
      "Epoch: 2 / 3, Step: 679 / 750 Loss: 0.2870\n",
      "Epoch: 2 / 3, Step: 680 / 750 Loss: 0.5242\n",
      "Epoch: 2 / 3, Step: 681 / 750 Loss: 0.2133\n",
      "Epoch: 2 / 3, Step: 682 / 750 Loss: 0.1446\n",
      "Epoch: 2 / 3, Step: 683 / 750 Loss: 0.2734\n",
      "Epoch: 2 / 3, Step: 684 / 750 Loss: 0.2494\n",
      "Epoch: 2 / 3, Step: 685 / 750 Loss: 0.1839\n",
      "Epoch: 2 / 3, Step: 686 / 750 Loss: 0.2236\n",
      "Epoch: 2 / 3, Step: 687 / 750 Loss: 0.3850\n",
      "Epoch: 2 / 3, Step: 688 / 750 Loss: 0.4188\n",
      "Epoch: 2 / 3, Step: 689 / 750 Loss: 0.1083\n",
      "Epoch: 2 / 3, Step: 690 / 750 Loss: 0.2492\n",
      "Epoch: 2 / 3, Step: 691 / 750 Loss: 0.1104\n",
      "Epoch: 2 / 3, Step: 692 / 750 Loss: 0.2098\n",
      "Epoch: 2 / 3, Step: 693 / 750 Loss: 0.1091\n",
      "Epoch: 2 / 3, Step: 694 / 750 Loss: 0.1174\n",
      "Epoch: 2 / 3, Step: 695 / 750 Loss: 0.1233\n",
      "Epoch: 2 / 3, Step: 696 / 750 Loss: 0.1012\n",
      "Epoch: 2 / 3, Step: 697 / 750 Loss: 0.1899\n",
      "Epoch: 2 / 3, Step: 698 / 750 Loss: 0.3368\n",
      "Epoch: 2 / 3, Step: 699 / 750 Loss: 0.3358\n",
      "Epoch: 2 / 3, Step: 700 / 750 Loss: 0.1044\n",
      "Epoch: 2 / 3, Step: 701 / 750 Loss: 0.1992\n",
      "Epoch: 2 / 3, Step: 702 / 750 Loss: 0.2598\n",
      "Epoch: 2 / 3, Step: 703 / 750 Loss: 0.3279\n",
      "Epoch: 2 / 3, Step: 704 / 750 Loss: 0.3855\n",
      "Epoch: 2 / 3, Step: 705 / 750 Loss: 0.6757\n",
      "Epoch: 2 / 3, Step: 706 / 750 Loss: 0.1489\n",
      "Epoch: 2 / 3, Step: 707 / 750 Loss: 0.2330\n",
      "Epoch: 2 / 3, Step: 708 / 750 Loss: 0.2254\n",
      "Epoch: 2 / 3, Step: 709 / 750 Loss: 0.1777\n",
      "Epoch: 2 / 3, Step: 710 / 750 Loss: 0.2495\n",
      "Epoch: 2 / 3, Step: 711 / 750 Loss: 0.0373\n",
      "Epoch: 2 / 3, Step: 712 / 750 Loss: 0.3272\n",
      "Epoch: 2 / 3, Step: 713 / 750 Loss: 0.2869\n",
      "Epoch: 2 / 3, Step: 714 / 750 Loss: 0.2987\n",
      "Epoch: 2 / 3, Step: 715 / 750 Loss: 0.2160\n",
      "Epoch: 2 / 3, Step: 716 / 750 Loss: 0.0817\n",
      "Epoch: 2 / 3, Step: 717 / 750 Loss: 0.3363\n",
      "Epoch: 2 / 3, Step: 718 / 750 Loss: 0.2874\n",
      "Epoch: 2 / 3, Step: 719 / 750 Loss: 0.7180\n",
      "Epoch: 2 / 3, Step: 720 / 750 Loss: 0.0622\n",
      "Epoch: 2 / 3, Step: 721 / 750 Loss: 0.2319\n",
      "Epoch: 2 / 3, Step: 722 / 750 Loss: 0.5421\n",
      "Epoch: 2 / 3, Step: 723 / 750 Loss: 0.3611\n",
      "Epoch: 2 / 3, Step: 724 / 750 Loss: 0.1348\n",
      "Epoch: 2 / 3, Step: 725 / 750 Loss: 0.0797\n",
      "Epoch: 2 / 3, Step: 726 / 750 Loss: 0.0920\n",
      "Epoch: 2 / 3, Step: 727 / 750 Loss: 0.1363\n",
      "Epoch: 2 / 3, Step: 728 / 750 Loss: 0.1891\n",
      "Epoch: 2 / 3, Step: 729 / 750 Loss: 0.2529\n",
      "Epoch: 2 / 3, Step: 730 / 750 Loss: 0.1873\n",
      "Epoch: 2 / 3, Step: 731 / 750 Loss: 0.1613\n",
      "Epoch: 2 / 3, Step: 732 / 750 Loss: 0.1175\n",
      "Epoch: 2 / 3, Step: 733 / 750 Loss: 0.1545\n",
      "Epoch: 2 / 3, Step: 734 / 750 Loss: 0.2025\n",
      "Epoch: 2 / 3, Step: 735 / 750 Loss: 0.0958\n",
      "Epoch: 2 / 3, Step: 736 / 750 Loss: 0.2321\n",
      "Epoch: 2 / 3, Step: 737 / 750 Loss: 0.2074\n",
      "Epoch: 2 / 3, Step: 738 / 750 Loss: 0.2707\n",
      "Epoch: 2 / 3, Step: 739 / 750 Loss: 0.3654\n",
      "Epoch: 2 / 3, Step: 740 / 750 Loss: 0.5972\n",
      "Epoch: 2 / 3, Step: 741 / 750 Loss: 0.3282\n",
      "Epoch: 2 / 3, Step: 742 / 750 Loss: 0.3912\n",
      "Epoch: 2 / 3, Step: 743 / 750 Loss: 0.1139\n",
      "Epoch: 2 / 3, Step: 744 / 750 Loss: 0.0785\n",
      "Epoch: 2 / 3, Step: 745 / 750 Loss: 0.0830\n",
      "Epoch: 2 / 3, Step: 746 / 750 Loss: 0.2078\n",
      "Epoch: 2 / 3, Step: 747 / 750 Loss: 0.0839\n",
      "Epoch: 2 / 3, Step: 748 / 750 Loss: 0.3069\n",
      "Epoch: 2 / 3, Step: 749 / 750 Loss: 0.0697\n",
      "Epoch: 3 / 3, Step: 0 / 750 Loss: 0.1903\n",
      "Epoch: 3 / 3, Step: 1 / 750 Loss: 0.1214\n",
      "Epoch: 3 / 3, Step: 2 / 750 Loss: 0.2252\n",
      "Epoch: 3 / 3, Step: 3 / 750 Loss: 0.0903\n",
      "Epoch: 3 / 3, Step: 4 / 750 Loss: 0.1959\n",
      "Epoch: 3 / 3, Step: 5 / 750 Loss: 0.0594\n",
      "Epoch: 3 / 3, Step: 6 / 750 Loss: 0.2289\n",
      "Epoch: 3 / 3, Step: 7 / 750 Loss: 0.1878\n",
      "Epoch: 3 / 3, Step: 8 / 750 Loss: 0.1931\n",
      "Epoch: 3 / 3, Step: 9 / 750 Loss: 0.1275\n",
      "Epoch: 3 / 3, Step: 10 / 750 Loss: 0.1446\n",
      "Epoch: 3 / 3, Step: 11 / 750 Loss: 0.0840\n",
      "Epoch: 3 / 3, Step: 12 / 750 Loss: 0.1375\n",
      "Epoch: 3 / 3, Step: 13 / 750 Loss: 0.0672\n",
      "Epoch: 3 / 3, Step: 14 / 750 Loss: 0.0842\n",
      "Epoch: 3 / 3, Step: 15 / 750 Loss: 0.2649\n",
      "Epoch: 3 / 3, Step: 16 / 750 Loss: 0.1170\n",
      "Epoch: 3 / 3, Step: 17 / 750 Loss: 0.1161\n",
      "Epoch: 3 / 3, Step: 18 / 750 Loss: 0.0418\n",
      "Epoch: 3 / 3, Step: 19 / 750 Loss: 0.1164\n",
      "Epoch: 3 / 3, Step: 20 / 750 Loss: 0.0621\n",
      "Epoch: 3 / 3, Step: 21 / 750 Loss: 0.1535\n",
      "Epoch: 3 / 3, Step: 22 / 750 Loss: 0.1060\n",
      "Epoch: 3 / 3, Step: 23 / 750 Loss: 0.2157\n",
      "Epoch: 3 / 3, Step: 24 / 750 Loss: 0.1984\n",
      "Epoch: 3 / 3, Step: 25 / 750 Loss: 0.1201\n",
      "Epoch: 3 / 3, Step: 26 / 750 Loss: 0.2771\n",
      "Epoch: 3 / 3, Step: 27 / 750 Loss: 0.1265\n",
      "Epoch: 3 / 3, Step: 28 / 750 Loss: 0.2418\n",
      "Epoch: 3 / 3, Step: 29 / 750 Loss: 0.0898\n",
      "Epoch: 3 / 3, Step: 30 / 750 Loss: 0.2056\n",
      "Epoch: 3 / 3, Step: 31 / 750 Loss: 0.1611\n",
      "Epoch: 3 / 3, Step: 32 / 750 Loss: 0.2081\n",
      "Epoch: 3 / 3, Step: 33 / 750 Loss: 0.2392\n",
      "Epoch: 3 / 3, Step: 34 / 750 Loss: 0.3523\n",
      "Epoch: 3 / 3, Step: 35 / 750 Loss: 0.1231\n",
      "Epoch: 3 / 3, Step: 36 / 750 Loss: 0.0700\n",
      "Epoch: 3 / 3, Step: 37 / 750 Loss: 0.1966\n",
      "Epoch: 3 / 3, Step: 38 / 750 Loss: 0.1859\n",
      "Epoch: 3 / 3, Step: 39 / 750 Loss: 0.1386\n",
      "Epoch: 3 / 3, Step: 40 / 750 Loss: 0.1891\n",
      "Epoch: 3 / 3, Step: 41 / 750 Loss: 0.1251\n",
      "Epoch: 3 / 3, Step: 42 / 750 Loss: 0.1870\n",
      "Epoch: 3 / 3, Step: 43 / 750 Loss: 0.0942\n",
      "Epoch: 3 / 3, Step: 44 / 750 Loss: 0.1551\n",
      "Epoch: 3 / 3, Step: 45 / 750 Loss: 0.2071\n",
      "Epoch: 3 / 3, Step: 46 / 750 Loss: 0.0909\n",
      "Epoch: 3 / 3, Step: 47 / 750 Loss: 0.3021\n",
      "Epoch: 3 / 3, Step: 48 / 750 Loss: 0.5930\n",
      "Epoch: 3 / 3, Step: 49 / 750 Loss: 0.1052\n",
      "Epoch: 3 / 3, Step: 50 / 750 Loss: 0.2143\n",
      "Epoch: 3 / 3, Step: 51 / 750 Loss: 0.1404\n",
      "Epoch: 3 / 3, Step: 52 / 750 Loss: 0.0706\n",
      "Epoch: 3 / 3, Step: 53 / 750 Loss: 0.0655\n",
      "Epoch: 3 / 3, Step: 54 / 750 Loss: 0.1233\n",
      "Epoch: 3 / 3, Step: 55 / 750 Loss: 0.3156\n",
      "Epoch: 3 / 3, Step: 56 / 750 Loss: 0.1389\n",
      "Epoch: 3 / 3, Step: 57 / 750 Loss: 0.0437\n",
      "Epoch: 3 / 3, Step: 58 / 750 Loss: 0.0665\n",
      "Epoch: 3 / 3, Step: 59 / 750 Loss: 0.3771\n",
      "Epoch: 3 / 3, Step: 60 / 750 Loss: 0.1487\n",
      "Epoch: 3 / 3, Step: 61 / 750 Loss: 0.2037\n",
      "Epoch: 3 / 3, Step: 62 / 750 Loss: 0.2186\n",
      "Epoch: 3 / 3, Step: 63 / 750 Loss: 0.1367\n",
      "Epoch: 3 / 3, Step: 64 / 750 Loss: 0.1514\n",
      "Epoch: 3 / 3, Step: 65 / 750 Loss: 0.1325\n",
      "Epoch: 3 / 3, Step: 66 / 750 Loss: 0.2518\n",
      "Epoch: 3 / 3, Step: 67 / 750 Loss: 0.2269\n",
      "Epoch: 3 / 3, Step: 68 / 750 Loss: 0.2632\n",
      "Epoch: 3 / 3, Step: 69 / 750 Loss: 0.0756\n",
      "Epoch: 3 / 3, Step: 70 / 750 Loss: 0.2216\n",
      "Epoch: 3 / 3, Step: 71 / 750 Loss: 0.0828\n",
      "Epoch: 3 / 3, Step: 72 / 750 Loss: 0.1181\n",
      "Epoch: 3 / 3, Step: 73 / 750 Loss: 0.2879\n",
      "Epoch: 3 / 3, Step: 74 / 750 Loss: 0.1213\n",
      "Epoch: 3 / 3, Step: 75 / 750 Loss: 0.3119\n",
      "Epoch: 3 / 3, Step: 76 / 750 Loss: 0.1637\n",
      "Epoch: 3 / 3, Step: 77 / 750 Loss: 0.1061\n",
      "Epoch: 3 / 3, Step: 78 / 750 Loss: 0.1910\n",
      "Epoch: 3 / 3, Step: 79 / 750 Loss: 0.2341\n",
      "Epoch: 3 / 3, Step: 80 / 750 Loss: 0.2150\n",
      "Epoch: 3 / 3, Step: 81 / 750 Loss: 0.3574\n",
      "Epoch: 3 / 3, Step: 82 / 750 Loss: 0.1741\n",
      "Epoch: 3 / 3, Step: 83 / 750 Loss: 0.3873\n",
      "Epoch: 3 / 3, Step: 84 / 750 Loss: 0.2213\n",
      "Epoch: 3 / 3, Step: 85 / 750 Loss: 0.2623\n",
      "Epoch: 3 / 3, Step: 86 / 750 Loss: 0.0916\n",
      "Epoch: 3 / 3, Step: 87 / 750 Loss: 0.3238\n",
      "Epoch: 3 / 3, Step: 88 / 750 Loss: 0.0396\n",
      "Epoch: 3 / 3, Step: 89 / 750 Loss: 0.1529\n",
      "Epoch: 3 / 3, Step: 90 / 750 Loss: 0.0745\n",
      "Epoch: 3 / 3, Step: 91 / 750 Loss: 0.1704\n",
      "Epoch: 3 / 3, Step: 92 / 750 Loss: 0.2845\n",
      "Epoch: 3 / 3, Step: 93 / 750 Loss: 0.0548\n",
      "Epoch: 3 / 3, Step: 94 / 750 Loss: 0.4197\n",
      "Epoch: 3 / 3, Step: 95 / 750 Loss: 0.0397\n",
      "Epoch: 3 / 3, Step: 96 / 750 Loss: 0.1701\n",
      "Epoch: 3 / 3, Step: 97 / 750 Loss: 0.0698\n",
      "Epoch: 3 / 3, Step: 98 / 750 Loss: 0.1269\n",
      "Epoch: 3 / 3, Step: 99 / 750 Loss: 0.3283\n",
      "Epoch: 3 / 3, Step: 100 / 750 Loss: 0.4219\n",
      "Epoch: 3 / 3, Step: 101 / 750 Loss: 0.0987\n",
      "Epoch: 3 / 3, Step: 102 / 750 Loss: 0.1895\n",
      "Epoch: 3 / 3, Step: 103 / 750 Loss: 0.0982\n",
      "Epoch: 3 / 3, Step: 104 / 750 Loss: 0.1392\n",
      "Epoch: 3 / 3, Step: 105 / 750 Loss: 0.2183\n",
      "Epoch: 3 / 3, Step: 106 / 750 Loss: 0.1263\n",
      "Epoch: 3 / 3, Step: 107 / 750 Loss: 0.0816\n",
      "Epoch: 3 / 3, Step: 108 / 750 Loss: 0.2477\n",
      "Epoch: 3 / 3, Step: 109 / 750 Loss: 0.1305\n",
      "Epoch: 3 / 3, Step: 110 / 750 Loss: 0.2241\n",
      "Epoch: 3 / 3, Step: 111 / 750 Loss: 0.0712\n",
      "Epoch: 3 / 3, Step: 112 / 750 Loss: 0.0457\n",
      "Epoch: 3 / 3, Step: 113 / 750 Loss: 0.2478\n",
      "Epoch: 3 / 3, Step: 114 / 750 Loss: 0.1228\n",
      "Epoch: 3 / 3, Step: 115 / 750 Loss: 0.3854\n",
      "Epoch: 3 / 3, Step: 116 / 750 Loss: 0.1166\n",
      "Epoch: 3 / 3, Step: 117 / 750 Loss: 0.3441\n",
      "Epoch: 3 / 3, Step: 118 / 750 Loss: 0.0534\n",
      "Epoch: 3 / 3, Step: 119 / 750 Loss: 0.1474\n",
      "Epoch: 3 / 3, Step: 120 / 750 Loss: 0.3036\n",
      "Epoch: 3 / 3, Step: 121 / 750 Loss: 0.0890\n",
      "Epoch: 3 / 3, Step: 122 / 750 Loss: 0.1108\n",
      "Epoch: 3 / 3, Step: 123 / 750 Loss: 0.2867\n",
      "Epoch: 3 / 3, Step: 124 / 750 Loss: 0.0637\n",
      "Epoch: 3 / 3, Step: 125 / 750 Loss: 0.0562\n",
      "Epoch: 3 / 3, Step: 126 / 750 Loss: 0.1640\n",
      "Epoch: 3 / 3, Step: 127 / 750 Loss: 0.1302\n",
      "Epoch: 3 / 3, Step: 128 / 750 Loss: 0.0870\n",
      "Epoch: 3 / 3, Step: 129 / 750 Loss: 0.2150\n",
      "Epoch: 3 / 3, Step: 130 / 750 Loss: 0.2333\n",
      "Epoch: 3 / 3, Step: 131 / 750 Loss: 0.0242\n",
      "Epoch: 3 / 3, Step: 132 / 750 Loss: 0.0510\n",
      "Epoch: 3 / 3, Step: 133 / 750 Loss: 0.0580\n",
      "Epoch: 3 / 3, Step: 134 / 750 Loss: 0.1569\n",
      "Epoch: 3 / 3, Step: 135 / 750 Loss: 0.1545\n",
      "Epoch: 3 / 3, Step: 136 / 750 Loss: 0.0635\n",
      "Epoch: 3 / 3, Step: 137 / 750 Loss: 0.3742\n",
      "Epoch: 3 / 3, Step: 138 / 750 Loss: 0.3850\n",
      "Epoch: 3 / 3, Step: 139 / 750 Loss: 0.2214\n",
      "Epoch: 3 / 3, Step: 140 / 750 Loss: 0.0984\n",
      "Epoch: 3 / 3, Step: 141 / 750 Loss: 0.1659\n",
      "Epoch: 3 / 3, Step: 142 / 750 Loss: 0.0632\n",
      "Epoch: 3 / 3, Step: 143 / 750 Loss: 0.1348\n",
      "Epoch: 3 / 3, Step: 144 / 750 Loss: 0.1769\n",
      "Epoch: 3 / 3, Step: 145 / 750 Loss: 0.1575\n",
      "Epoch: 3 / 3, Step: 146 / 750 Loss: 0.2886\n",
      "Epoch: 3 / 3, Step: 147 / 750 Loss: 0.2106\n",
      "Epoch: 3 / 3, Step: 148 / 750 Loss: 0.3686\n",
      "Epoch: 3 / 3, Step: 149 / 750 Loss: 0.0755\n",
      "Epoch: 3 / 3, Step: 150 / 750 Loss: 0.1165\n",
      "Epoch: 3 / 3, Step: 151 / 750 Loss: 0.2451\n",
      "Epoch: 3 / 3, Step: 152 / 750 Loss: 0.1290\n",
      "Epoch: 3 / 3, Step: 153 / 750 Loss: 0.1747\n",
      "Epoch: 3 / 3, Step: 154 / 750 Loss: 0.1208\n",
      "Epoch: 3 / 3, Step: 155 / 750 Loss: 0.2585\n",
      "Epoch: 3 / 3, Step: 156 / 750 Loss: 0.2994\n",
      "Epoch: 3 / 3, Step: 157 / 750 Loss: 0.3377\n",
      "Epoch: 3 / 3, Step: 158 / 750 Loss: 0.0817\n",
      "Epoch: 3 / 3, Step: 159 / 750 Loss: 0.3787\n",
      "Epoch: 3 / 3, Step: 160 / 750 Loss: 0.0619\n",
      "Epoch: 3 / 3, Step: 161 / 750 Loss: 0.1062\n",
      "Epoch: 3 / 3, Step: 162 / 750 Loss: 0.1968\n",
      "Epoch: 3 / 3, Step: 163 / 750 Loss: 0.0507\n",
      "Epoch: 3 / 3, Step: 164 / 750 Loss: 0.2774\n",
      "Epoch: 3 / 3, Step: 165 / 750 Loss: 0.3421\n",
      "Epoch: 3 / 3, Step: 166 / 750 Loss: 0.2902\n",
      "Epoch: 3 / 3, Step: 167 / 750 Loss: 0.2718\n",
      "Epoch: 3 / 3, Step: 168 / 750 Loss: 0.1225\n",
      "Epoch: 3 / 3, Step: 169 / 750 Loss: 0.1120\n",
      "Epoch: 3 / 3, Step: 170 / 750 Loss: 0.0445\n",
      "Epoch: 3 / 3, Step: 171 / 750 Loss: 0.3181\n",
      "Epoch: 3 / 3, Step: 172 / 750 Loss: 0.1003\n",
      "Epoch: 3 / 3, Step: 173 / 750 Loss: 0.2155\n",
      "Epoch: 3 / 3, Step: 174 / 750 Loss: 0.1082\n",
      "Epoch: 3 / 3, Step: 175 / 750 Loss: 0.1126\n",
      "Epoch: 3 / 3, Step: 176 / 750 Loss: 0.2329\n",
      "Epoch: 3 / 3, Step: 177 / 750 Loss: 0.0774\n",
      "Epoch: 3 / 3, Step: 178 / 750 Loss: 0.0844\n",
      "Epoch: 3 / 3, Step: 179 / 750 Loss: 0.0442\n",
      "Epoch: 3 / 3, Step: 180 / 750 Loss: 0.0770\n",
      "Epoch: 3 / 3, Step: 181 / 750 Loss: 0.3647\n",
      "Epoch: 3 / 3, Step: 182 / 750 Loss: 0.4242\n",
      "Epoch: 3 / 3, Step: 183 / 750 Loss: 0.1150\n",
      "Epoch: 3 / 3, Step: 184 / 750 Loss: 0.1354\n",
      "Epoch: 3 / 3, Step: 185 / 750 Loss: 0.3016\n",
      "Epoch: 3 / 3, Step: 186 / 750 Loss: 0.2199\n",
      "Epoch: 3 / 3, Step: 187 / 750 Loss: 0.0620\n",
      "Epoch: 3 / 3, Step: 188 / 750 Loss: 0.2276\n",
      "Epoch: 3 / 3, Step: 189 / 750 Loss: 0.1075\n",
      "Epoch: 3 / 3, Step: 190 / 750 Loss: 0.1781\n",
      "Epoch: 3 / 3, Step: 191 / 750 Loss: 0.3258\n",
      "Epoch: 3 / 3, Step: 192 / 750 Loss: 0.0538\n",
      "Epoch: 3 / 3, Step: 193 / 750 Loss: 0.2468\n",
      "Epoch: 3 / 3, Step: 194 / 750 Loss: 0.1474\n",
      "Epoch: 3 / 3, Step: 195 / 750 Loss: 0.4055\n",
      "Epoch: 3 / 3, Step: 196 / 750 Loss: 0.4308\n",
      "Epoch: 3 / 3, Step: 197 / 750 Loss: 0.0632\n",
      "Epoch: 3 / 3, Step: 198 / 750 Loss: 0.2611\n",
      "Epoch: 3 / 3, Step: 199 / 750 Loss: 0.0834\n",
      "Epoch: 3 / 3, Step: 200 / 750 Loss: 0.0922\n",
      "Epoch: 3 / 3, Step: 201 / 750 Loss: 0.0997\n",
      "Epoch: 3 / 3, Step: 202 / 750 Loss: 0.0659\n",
      "Epoch: 3 / 3, Step: 203 / 750 Loss: 0.1551\n",
      "Epoch: 3 / 3, Step: 204 / 750 Loss: 0.0622\n",
      "Epoch: 3 / 3, Step: 205 / 750 Loss: 0.2787\n",
      "Epoch: 3 / 3, Step: 206 / 750 Loss: 0.1931\n",
      "Epoch: 3 / 3, Step: 207 / 750 Loss: 0.2534\n",
      "Epoch: 3 / 3, Step: 208 / 750 Loss: 0.1476\n",
      "Epoch: 3 / 3, Step: 209 / 750 Loss: 0.0762\n",
      "Epoch: 3 / 3, Step: 210 / 750 Loss: 0.2267\n",
      "Epoch: 3 / 3, Step: 211 / 750 Loss: 0.1387\n",
      "Epoch: 3 / 3, Step: 212 / 750 Loss: 0.0621\n",
      "Epoch: 3 / 3, Step: 213 / 750 Loss: 0.2260\n",
      "Epoch: 3 / 3, Step: 214 / 750 Loss: 0.0368\n",
      "Epoch: 3 / 3, Step: 215 / 750 Loss: 0.1641\n",
      "Epoch: 3 / 3, Step: 216 / 750 Loss: 0.0791\n",
      "Epoch: 3 / 3, Step: 217 / 750 Loss: 0.1594\n",
      "Epoch: 3 / 3, Step: 218 / 750 Loss: 0.2712\n",
      "Epoch: 3 / 3, Step: 219 / 750 Loss: 0.1091\n",
      "Epoch: 3 / 3, Step: 220 / 750 Loss: 0.1039\n",
      "Epoch: 3 / 3, Step: 221 / 750 Loss: 0.0988\n",
      "Epoch: 3 / 3, Step: 222 / 750 Loss: 0.0464\n",
      "Epoch: 3 / 3, Step: 223 / 750 Loss: 0.2016\n",
      "Epoch: 3 / 3, Step: 224 / 750 Loss: 0.1952\n",
      "Epoch: 3 / 3, Step: 225 / 750 Loss: 0.2284\n",
      "Epoch: 3 / 3, Step: 226 / 750 Loss: 0.0404\n",
      "Epoch: 3 / 3, Step: 227 / 750 Loss: 0.0351\n",
      "Epoch: 3 / 3, Step: 228 / 750 Loss: 0.5688\n",
      "Epoch: 3 / 3, Step: 229 / 750 Loss: 0.1073\n",
      "Epoch: 3 / 3, Step: 230 / 750 Loss: 0.2300\n",
      "Epoch: 3 / 3, Step: 231 / 750 Loss: 0.0638\n",
      "Epoch: 3 / 3, Step: 232 / 750 Loss: 0.1109\n",
      "Epoch: 3 / 3, Step: 233 / 750 Loss: 0.1797\n",
      "Epoch: 3 / 3, Step: 234 / 750 Loss: 0.1029\n",
      "Epoch: 3 / 3, Step: 235 / 750 Loss: 0.2476\n",
      "Epoch: 3 / 3, Step: 236 / 750 Loss: 0.0290\n",
      "Epoch: 3 / 3, Step: 237 / 750 Loss: 0.2177\n",
      "Epoch: 3 / 3, Step: 238 / 750 Loss: 0.1758\n",
      "Epoch: 3 / 3, Step: 239 / 750 Loss: 0.0982\n",
      "Epoch: 3 / 3, Step: 240 / 750 Loss: 0.0679\n",
      "Epoch: 3 / 3, Step: 241 / 750 Loss: 0.2067\n",
      "Epoch: 3 / 3, Step: 242 / 750 Loss: 0.0765\n",
      "Epoch: 3 / 3, Step: 243 / 750 Loss: 0.2774\n",
      "Epoch: 3 / 3, Step: 244 / 750 Loss: 0.0920\n",
      "Epoch: 3 / 3, Step: 245 / 750 Loss: 0.1051\n",
      "Epoch: 3 / 3, Step: 246 / 750 Loss: 0.1458\n",
      "Epoch: 3 / 3, Step: 247 / 750 Loss: 0.1949\n",
      "Epoch: 3 / 3, Step: 248 / 750 Loss: 0.1668\n",
      "Epoch: 3 / 3, Step: 249 / 750 Loss: 0.1588\n",
      "Epoch: 3 / 3, Step: 250 / 750 Loss: 0.1007\n",
      "Epoch: 3 / 3, Step: 251 / 750 Loss: 0.1405\n",
      "Epoch: 3 / 3, Step: 252 / 750 Loss: 0.2908\n",
      "Epoch: 3 / 3, Step: 253 / 750 Loss: 0.5183\n",
      "Epoch: 3 / 3, Step: 254 / 750 Loss: 0.1202\n",
      "Epoch: 3 / 3, Step: 255 / 750 Loss: 0.2244\n",
      "Epoch: 3 / 3, Step: 256 / 750 Loss: 0.2068\n",
      "Epoch: 3 / 3, Step: 257 / 750 Loss: 0.2286\n",
      "Epoch: 3 / 3, Step: 258 / 750 Loss: 0.0567\n",
      "Epoch: 3 / 3, Step: 259 / 750 Loss: 0.1972\n",
      "Epoch: 3 / 3, Step: 260 / 750 Loss: 0.1514\n",
      "Epoch: 3 / 3, Step: 261 / 750 Loss: 0.1338\n",
      "Epoch: 3 / 3, Step: 262 / 750 Loss: 0.0707\n",
      "Epoch: 3 / 3, Step: 263 / 750 Loss: 0.3229\n",
      "Epoch: 3 / 3, Step: 264 / 750 Loss: 0.0583\n",
      "Epoch: 3 / 3, Step: 265 / 750 Loss: 0.2361\n",
      "Epoch: 3 / 3, Step: 266 / 750 Loss: 0.2305\n",
      "Epoch: 3 / 3, Step: 267 / 750 Loss: 0.1927\n",
      "Epoch: 3 / 3, Step: 268 / 750 Loss: 0.0877\n",
      "Epoch: 3 / 3, Step: 269 / 750 Loss: 0.1872\n",
      "Epoch: 3 / 3, Step: 270 / 750 Loss: 0.2751\n",
      "Epoch: 3 / 3, Step: 271 / 750 Loss: 0.0927\n",
      "Epoch: 3 / 3, Step: 272 / 750 Loss: 0.2316\n",
      "Epoch: 3 / 3, Step: 273 / 750 Loss: 0.6246\n",
      "Epoch: 3 / 3, Step: 274 / 750 Loss: 0.1545\n",
      "Epoch: 3 / 3, Step: 275 / 750 Loss: 0.5090\n",
      "Epoch: 3 / 3, Step: 276 / 750 Loss: 0.0465\n",
      "Epoch: 3 / 3, Step: 277 / 750 Loss: 0.2997\n",
      "Epoch: 3 / 3, Step: 278 / 750 Loss: 0.1706\n",
      "Epoch: 3 / 3, Step: 279 / 750 Loss: 0.0984\n",
      "Epoch: 3 / 3, Step: 280 / 750 Loss: 0.0398\n",
      "Epoch: 3 / 3, Step: 281 / 750 Loss: 0.2411\n",
      "Epoch: 3 / 3, Step: 282 / 750 Loss: 0.1006\n",
      "Epoch: 3 / 3, Step: 283 / 750 Loss: 0.0538\n",
      "Epoch: 3 / 3, Step: 284 / 750 Loss: 0.1158\n",
      "Epoch: 3 / 3, Step: 285 / 750 Loss: 0.0574\n",
      "Epoch: 3 / 3, Step: 286 / 750 Loss: 0.1450\n",
      "Epoch: 3 / 3, Step: 287 / 750 Loss: 0.1667\n",
      "Epoch: 3 / 3, Step: 288 / 750 Loss: 0.2921\n",
      "Epoch: 3 / 3, Step: 289 / 750 Loss: 0.2060\n",
      "Epoch: 3 / 3, Step: 290 / 750 Loss: 0.1286\n",
      "Epoch: 3 / 3, Step: 291 / 750 Loss: 0.0778\n",
      "Epoch: 3 / 3, Step: 292 / 750 Loss: 0.0465\n",
      "Epoch: 3 / 3, Step: 293 / 750 Loss: 0.0610\n",
      "Epoch: 3 / 3, Step: 294 / 750 Loss: 0.1752\n",
      "Epoch: 3 / 3, Step: 295 / 750 Loss: 0.0919\n",
      "Epoch: 3 / 3, Step: 296 / 750 Loss: 0.5031\n",
      "Epoch: 3 / 3, Step: 297 / 750 Loss: 0.2787\n",
      "Epoch: 3 / 3, Step: 298 / 750 Loss: 0.2173\n",
      "Epoch: 3 / 3, Step: 299 / 750 Loss: 0.2401\n",
      "Epoch: 3 / 3, Step: 300 / 750 Loss: 0.1183\n",
      "Epoch: 3 / 3, Step: 301 / 750 Loss: 0.0916\n",
      "Epoch: 3 / 3, Step: 302 / 750 Loss: 0.0706\n",
      "Epoch: 3 / 3, Step: 303 / 750 Loss: 0.1037\n",
      "Epoch: 3 / 3, Step: 304 / 750 Loss: 0.1865\n",
      "Epoch: 3 / 3, Step: 305 / 750 Loss: 0.4289\n",
      "Epoch: 3 / 3, Step: 306 / 750 Loss: 0.1484\n",
      "Epoch: 3 / 3, Step: 307 / 750 Loss: 0.0323\n",
      "Epoch: 3 / 3, Step: 308 / 750 Loss: 0.3454\n",
      "Epoch: 3 / 3, Step: 309 / 750 Loss: 0.1509\n",
      "Epoch: 3 / 3, Step: 310 / 750 Loss: 0.3387\n",
      "Epoch: 3 / 3, Step: 311 / 750 Loss: 0.1545\n",
      "Epoch: 3 / 3, Step: 312 / 750 Loss: 0.3231\n",
      "Epoch: 3 / 3, Step: 313 / 750 Loss: 0.1269\n",
      "Epoch: 3 / 3, Step: 314 / 750 Loss: 0.1498\n",
      "Epoch: 3 / 3, Step: 315 / 750 Loss: 0.0852\n",
      "Epoch: 3 / 3, Step: 316 / 750 Loss: 0.0307\n",
      "Epoch: 3 / 3, Step: 317 / 750 Loss: 0.0486\n",
      "Epoch: 3 / 3, Step: 318 / 750 Loss: 0.1889\n",
      "Epoch: 3 / 3, Step: 319 / 750 Loss: 0.0630\n",
      "Epoch: 3 / 3, Step: 320 / 750 Loss: 0.0772\n",
      "Epoch: 3 / 3, Step: 321 / 750 Loss: 0.1364\n",
      "Epoch: 3 / 3, Step: 322 / 750 Loss: 0.0482\n",
      "Epoch: 3 / 3, Step: 323 / 750 Loss: 0.2305\n",
      "Epoch: 3 / 3, Step: 324 / 750 Loss: 0.1887\n",
      "Epoch: 3 / 3, Step: 325 / 750 Loss: 0.1689\n",
      "Epoch: 3 / 3, Step: 326 / 750 Loss: 0.0754\n",
      "Epoch: 3 / 3, Step: 327 / 750 Loss: 0.0253\n",
      "Epoch: 3 / 3, Step: 328 / 750 Loss: 0.1224\n",
      "Epoch: 3 / 3, Step: 329 / 750 Loss: 0.0442\n",
      "Epoch: 3 / 3, Step: 330 / 750 Loss: 0.3548\n",
      "Epoch: 3 / 3, Step: 331 / 750 Loss: 0.2301\n",
      "Epoch: 3 / 3, Step: 332 / 750 Loss: 0.3644\n",
      "Epoch: 3 / 3, Step: 333 / 750 Loss: 0.0907\n",
      "Epoch: 3 / 3, Step: 334 / 750 Loss: 0.1840\n",
      "Epoch: 3 / 3, Step: 335 / 750 Loss: 0.0851\n",
      "Epoch: 3 / 3, Step: 336 / 750 Loss: 0.0743\n",
      "Epoch: 3 / 3, Step: 337 / 750 Loss: 0.0611\n",
      "Epoch: 3 / 3, Step: 338 / 750 Loss: 0.2248\n",
      "Epoch: 3 / 3, Step: 339 / 750 Loss: 0.1879\n",
      "Epoch: 3 / 3, Step: 340 / 750 Loss: 0.0946\n",
      "Epoch: 3 / 3, Step: 341 / 750 Loss: 0.0874\n",
      "Epoch: 3 / 3, Step: 342 / 750 Loss: 0.0759\n",
      "Epoch: 3 / 3, Step: 343 / 750 Loss: 0.3892\n",
      "Epoch: 3 / 3, Step: 344 / 750 Loss: 0.1812\n",
      "Epoch: 3 / 3, Step: 345 / 750 Loss: 0.2814\n",
      "Epoch: 3 / 3, Step: 346 / 750 Loss: 0.2747\n",
      "Epoch: 3 / 3, Step: 347 / 750 Loss: 0.1239\n",
      "Epoch: 3 / 3, Step: 348 / 750 Loss: 0.0657\n",
      "Epoch: 3 / 3, Step: 349 / 750 Loss: 0.1678\n",
      "Epoch: 3 / 3, Step: 350 / 750 Loss: 0.2641\n",
      "Epoch: 3 / 3, Step: 351 / 750 Loss: 0.2629\n",
      "Epoch: 3 / 3, Step: 352 / 750 Loss: 0.1233\n",
      "Epoch: 3 / 3, Step: 353 / 750 Loss: 0.0565\n",
      "Epoch: 3 / 3, Step: 354 / 750 Loss: 0.1311\n",
      "Epoch: 3 / 3, Step: 355 / 750 Loss: 0.0539\n",
      "Epoch: 3 / 3, Step: 356 / 750 Loss: 0.3290\n",
      "Epoch: 3 / 3, Step: 357 / 750 Loss: 0.1160\n",
      "Epoch: 3 / 3, Step: 358 / 750 Loss: 0.2363\n",
      "Epoch: 3 / 3, Step: 359 / 750 Loss: 0.1293\n",
      "Epoch: 3 / 3, Step: 360 / 750 Loss: 0.1873\n",
      "Epoch: 3 / 3, Step: 361 / 750 Loss: 0.0464\n",
      "Epoch: 3 / 3, Step: 362 / 750 Loss: 0.3092\n",
      "Epoch: 3 / 3, Step: 363 / 750 Loss: 0.2858\n",
      "Epoch: 3 / 3, Step: 364 / 750 Loss: 0.2446\n",
      "Epoch: 3 / 3, Step: 365 / 750 Loss: 0.1516\n",
      "Epoch: 3 / 3, Step: 366 / 750 Loss: 0.2398\n",
      "Epoch: 3 / 3, Step: 367 / 750 Loss: 0.2468\n",
      "Epoch: 3 / 3, Step: 368 / 750 Loss: 0.0448\n",
      "Epoch: 3 / 3, Step: 369 / 750 Loss: 0.3146\n",
      "Epoch: 3 / 3, Step: 370 / 750 Loss: 0.1190\n",
      "Epoch: 3 / 3, Step: 371 / 750 Loss: 0.3023\n",
      "Epoch: 3 / 3, Step: 372 / 750 Loss: 0.2156\n",
      "Epoch: 3 / 3, Step: 373 / 750 Loss: 0.1591\n",
      "Epoch: 3 / 3, Step: 374 / 750 Loss: 0.1762\n",
      "Epoch: 3 / 3, Step: 375 / 750 Loss: 0.1511\n",
      "Epoch: 3 / 3, Step: 376 / 750 Loss: 0.1895\n",
      "Epoch: 3 / 3, Step: 377 / 750 Loss: 0.1054\n",
      "Epoch: 3 / 3, Step: 378 / 750 Loss: 0.1143\n",
      "Epoch: 3 / 3, Step: 379 / 750 Loss: 0.2373\n",
      "Epoch: 3 / 3, Step: 380 / 750 Loss: 0.2775\n",
      "Epoch: 3 / 3, Step: 381 / 750 Loss: 0.1777\n",
      "Epoch: 3 / 3, Step: 382 / 750 Loss: 0.0339\n",
      "Epoch: 3 / 3, Step: 383 / 750 Loss: 0.2198\n",
      "Epoch: 3 / 3, Step: 384 / 750 Loss: 0.1734\n",
      "Epoch: 3 / 3, Step: 385 / 750 Loss: 0.1184\n",
      "Epoch: 3 / 3, Step: 386 / 750 Loss: 0.0638\n",
      "Epoch: 3 / 3, Step: 387 / 750 Loss: 0.2076\n",
      "Epoch: 3 / 3, Step: 388 / 750 Loss: 0.1068\n",
      "Epoch: 3 / 3, Step: 389 / 750 Loss: 0.2206\n",
      "Epoch: 3 / 3, Step: 390 / 750 Loss: 0.2255\n",
      "Epoch: 3 / 3, Step: 391 / 750 Loss: 0.1842\n",
      "Epoch: 3 / 3, Step: 392 / 750 Loss: 0.2153\n",
      "Epoch: 3 / 3, Step: 393 / 750 Loss: 0.1777\n",
      "Epoch: 3 / 3, Step: 394 / 750 Loss: 0.2219\n",
      "Epoch: 3 / 3, Step: 395 / 750 Loss: 0.1679\n",
      "Epoch: 3 / 3, Step: 396 / 750 Loss: 0.1916\n",
      "Epoch: 3 / 3, Step: 397 / 750 Loss: 0.0553\n",
      "Epoch: 3 / 3, Step: 398 / 750 Loss: 0.1736\n",
      "Epoch: 3 / 3, Step: 399 / 750 Loss: 0.1316\n",
      "Epoch: 3 / 3, Step: 400 / 750 Loss: 0.0813\n",
      "Epoch: 3 / 3, Step: 401 / 750 Loss: 0.1662\n",
      "Epoch: 3 / 3, Step: 402 / 750 Loss: 0.0678\n",
      "Epoch: 3 / 3, Step: 403 / 750 Loss: 0.1828\n",
      "Epoch: 3 / 3, Step: 404 / 750 Loss: 0.1772\n",
      "Epoch: 3 / 3, Step: 405 / 750 Loss: 0.2715\n",
      "Epoch: 3 / 3, Step: 406 / 750 Loss: 0.1156\n",
      "Epoch: 3 / 3, Step: 407 / 750 Loss: 0.1288\n",
      "Epoch: 3 / 3, Step: 408 / 750 Loss: 0.2809\n",
      "Epoch: 3 / 3, Step: 409 / 750 Loss: 0.1741\n",
      "Epoch: 3 / 3, Step: 410 / 750 Loss: 0.3763\n",
      "Epoch: 3 / 3, Step: 411 / 750 Loss: 0.2344\n",
      "Epoch: 3 / 3, Step: 412 / 750 Loss: 0.2012\n",
      "Epoch: 3 / 3, Step: 413 / 750 Loss: 0.1748\n",
      "Epoch: 3 / 3, Step: 414 / 750 Loss: 0.0763\n",
      "Epoch: 3 / 3, Step: 415 / 750 Loss: 0.0527\n",
      "Epoch: 3 / 3, Step: 416 / 750 Loss: 0.1424\n",
      "Epoch: 3 / 3, Step: 417 / 750 Loss: 0.1240\n",
      "Epoch: 3 / 3, Step: 418 / 750 Loss: 0.2166\n",
      "Epoch: 3 / 3, Step: 419 / 750 Loss: 0.1373\n",
      "Epoch: 3 / 3, Step: 420 / 750 Loss: 0.0440\n",
      "Epoch: 3 / 3, Step: 421 / 750 Loss: 0.2717\n",
      "Epoch: 3 / 3, Step: 422 / 750 Loss: 0.0871\n",
      "Epoch: 3 / 3, Step: 423 / 750 Loss: 0.0486\n",
      "Epoch: 3 / 3, Step: 424 / 750 Loss: 0.1935\n",
      "Epoch: 3 / 3, Step: 425 / 750 Loss: 0.1532\n",
      "Epoch: 3 / 3, Step: 426 / 750 Loss: 0.1961\n",
      "Epoch: 3 / 3, Step: 427 / 750 Loss: 0.1766\n",
      "Epoch: 3 / 3, Step: 428 / 750 Loss: 0.0709\n",
      "Epoch: 3 / 3, Step: 429 / 750 Loss: 0.1251\n",
      "Epoch: 3 / 3, Step: 430 / 750 Loss: 0.0239\n",
      "Epoch: 3 / 3, Step: 431 / 750 Loss: 0.0257\n",
      "Epoch: 3 / 3, Step: 432 / 750 Loss: 0.2706\n",
      "Epoch: 3 / 3, Step: 433 / 750 Loss: 0.0940\n",
      "Epoch: 3 / 3, Step: 434 / 750 Loss: 0.1483\n",
      "Epoch: 3 / 3, Step: 435 / 750 Loss: 0.1135\n",
      "Epoch: 3 / 3, Step: 436 / 750 Loss: 0.2985\n",
      "Epoch: 3 / 3, Step: 437 / 750 Loss: 0.1249\n",
      "Epoch: 3 / 3, Step: 438 / 750 Loss: 0.0876\n",
      "Epoch: 3 / 3, Step: 439 / 750 Loss: 0.0712\n",
      "Epoch: 3 / 3, Step: 440 / 750 Loss: 0.2129\n",
      "Epoch: 3 / 3, Step: 441 / 750 Loss: 0.1388\n",
      "Epoch: 3 / 3, Step: 442 / 750 Loss: 0.1353\n",
      "Epoch: 3 / 3, Step: 443 / 750 Loss: 0.1508\n",
      "Epoch: 3 / 3, Step: 444 / 750 Loss: 0.0258\n",
      "Epoch: 3 / 3, Step: 445 / 750 Loss: 0.1264\n",
      "Epoch: 3 / 3, Step: 446 / 750 Loss: 0.1947\n",
      "Epoch: 3 / 3, Step: 447 / 750 Loss: 0.0682\n",
      "Epoch: 3 / 3, Step: 448 / 750 Loss: 0.1514\n",
      "Epoch: 3 / 3, Step: 449 / 750 Loss: 0.1803\n",
      "Epoch: 3 / 3, Step: 450 / 750 Loss: 0.0811\n",
      "Epoch: 3 / 3, Step: 451 / 750 Loss: 0.1751\n",
      "Epoch: 3 / 3, Step: 452 / 750 Loss: 0.0911\n",
      "Epoch: 3 / 3, Step: 453 / 750 Loss: 0.1880\n",
      "Epoch: 3 / 3, Step: 454 / 750 Loss: 0.1253\n",
      "Epoch: 3 / 3, Step: 455 / 750 Loss: 0.0432\n",
      "Epoch: 3 / 3, Step: 456 / 750 Loss: 0.2005\n",
      "Epoch: 3 / 3, Step: 457 / 750 Loss: 0.2151\n",
      "Epoch: 3 / 3, Step: 458 / 750 Loss: 0.0699\n",
      "Epoch: 3 / 3, Step: 459 / 750 Loss: 0.1114\n",
      "Epoch: 3 / 3, Step: 460 / 750 Loss: 0.2306\n",
      "Epoch: 3 / 3, Step: 461 / 750 Loss: 0.3168\n",
      "Epoch: 3 / 3, Step: 462 / 750 Loss: 0.2100\n",
      "Epoch: 3 / 3, Step: 463 / 750 Loss: 0.2135\n",
      "Epoch: 3 / 3, Step: 464 / 750 Loss: 0.4233\n",
      "Epoch: 3 / 3, Step: 465 / 750 Loss: 0.0593\n",
      "Epoch: 3 / 3, Step: 466 / 750 Loss: 0.1871\n",
      "Epoch: 3 / 3, Step: 467 / 750 Loss: 0.0973\n",
      "Epoch: 3 / 3, Step: 468 / 750 Loss: 0.0672\n",
      "Epoch: 3 / 3, Step: 469 / 750 Loss: 0.2141\n",
      "Epoch: 3 / 3, Step: 470 / 750 Loss: 0.3002\n",
      "Epoch: 3 / 3, Step: 471 / 750 Loss: 0.2707\n",
      "Epoch: 3 / 3, Step: 472 / 750 Loss: 0.0377\n",
      "Epoch: 3 / 3, Step: 473 / 750 Loss: 0.0890\n",
      "Epoch: 3 / 3, Step: 474 / 750 Loss: 0.0645\n",
      "Epoch: 3 / 3, Step: 475 / 750 Loss: 0.1149\n",
      "Epoch: 3 / 3, Step: 476 / 750 Loss: 0.1944\n",
      "Epoch: 3 / 3, Step: 477 / 750 Loss: 0.4500\n",
      "Epoch: 3 / 3, Step: 478 / 750 Loss: 0.5146\n",
      "Epoch: 3 / 3, Step: 479 / 750 Loss: 0.2305\n",
      "Epoch: 3 / 3, Step: 480 / 750 Loss: 0.0237\n",
      "Epoch: 3 / 3, Step: 481 / 750 Loss: 0.0682\n",
      "Epoch: 3 / 3, Step: 482 / 750 Loss: 0.2023\n",
      "Epoch: 3 / 3, Step: 483 / 750 Loss: 0.2042\n",
      "Epoch: 3 / 3, Step: 484 / 750 Loss: 0.0966\n",
      "Epoch: 3 / 3, Step: 485 / 750 Loss: 0.0375\n",
      "Epoch: 3 / 3, Step: 486 / 750 Loss: 0.1618\n",
      "Epoch: 3 / 3, Step: 487 / 750 Loss: 0.3149\n",
      "Epoch: 3 / 3, Step: 488 / 750 Loss: 0.5428\n",
      "Epoch: 3 / 3, Step: 489 / 750 Loss: 0.1705\n",
      "Epoch: 3 / 3, Step: 490 / 750 Loss: 0.0706\n",
      "Epoch: 3 / 3, Step: 491 / 750 Loss: 0.0799\n",
      "Epoch: 3 / 3, Step: 492 / 750 Loss: 0.1005\n",
      "Epoch: 3 / 3, Step: 493 / 750 Loss: 0.2675\n",
      "Epoch: 3 / 3, Step: 494 / 750 Loss: 0.0960\n",
      "Epoch: 3 / 3, Step: 495 / 750 Loss: 0.0783\n",
      "Epoch: 3 / 3, Step: 496 / 750 Loss: 0.3121\n",
      "Epoch: 3 / 3, Step: 497 / 750 Loss: 0.0598\n",
      "Epoch: 3 / 3, Step: 498 / 750 Loss: 0.1127\n",
      "Epoch: 3 / 3, Step: 499 / 750 Loss: 0.2490\n",
      "Epoch: 3 / 3, Step: 500 / 750 Loss: 0.0983\n",
      "Epoch: 3 / 3, Step: 501 / 750 Loss: 0.0533\n",
      "Epoch: 3 / 3, Step: 502 / 750 Loss: 0.2362\n",
      "Epoch: 3 / 3, Step: 503 / 750 Loss: 0.0679\n",
      "Epoch: 3 / 3, Step: 504 / 750 Loss: 0.2603\n",
      "Epoch: 3 / 3, Step: 505 / 750 Loss: 0.1511\n",
      "Epoch: 3 / 3, Step: 506 / 750 Loss: 0.0773\n",
      "Epoch: 3 / 3, Step: 507 / 750 Loss: 0.0792\n",
      "Epoch: 3 / 3, Step: 508 / 750 Loss: 0.1427\n",
      "Epoch: 3 / 3, Step: 509 / 750 Loss: 0.1895\n",
      "Epoch: 3 / 3, Step: 510 / 750 Loss: 0.2196\n",
      "Epoch: 3 / 3, Step: 511 / 750 Loss: 0.1689\n",
      "Epoch: 3 / 3, Step: 512 / 750 Loss: 0.2659\n",
      "Epoch: 3 / 3, Step: 513 / 750 Loss: 0.1042\n",
      "Epoch: 3 / 3, Step: 514 / 750 Loss: 0.0678\n",
      "Epoch: 3 / 3, Step: 515 / 750 Loss: 0.2624\n",
      "Epoch: 3 / 3, Step: 516 / 750 Loss: 0.0841\n",
      "Epoch: 3 / 3, Step: 517 / 750 Loss: 0.0866\n",
      "Epoch: 3 / 3, Step: 518 / 750 Loss: 0.1885\n",
      "Epoch: 3 / 3, Step: 519 / 750 Loss: 0.2352\n",
      "Epoch: 3 / 3, Step: 520 / 750 Loss: 0.2224\n",
      "Epoch: 3 / 3, Step: 521 / 750 Loss: 0.0677\n",
      "Epoch: 3 / 3, Step: 522 / 750 Loss: 0.2232\n",
      "Epoch: 3 / 3, Step: 523 / 750 Loss: 0.0678\n",
      "Epoch: 3 / 3, Step: 524 / 750 Loss: 0.0934\n",
      "Epoch: 3 / 3, Step: 525 / 750 Loss: 0.1710\n",
      "Epoch: 3 / 3, Step: 526 / 750 Loss: 0.0527\n",
      "Epoch: 3 / 3, Step: 527 / 750 Loss: 0.0911\n",
      "Epoch: 3 / 3, Step: 528 / 750 Loss: 0.1274\n",
      "Epoch: 3 / 3, Step: 529 / 750 Loss: 0.0547\n",
      "Epoch: 3 / 3, Step: 530 / 750 Loss: 0.1012\n",
      "Epoch: 3 / 3, Step: 531 / 750 Loss: 0.1725\n",
      "Epoch: 3 / 3, Step: 532 / 750 Loss: 0.2016\n",
      "Epoch: 3 / 3, Step: 533 / 750 Loss: 0.0793\n",
      "Epoch: 3 / 3, Step: 534 / 750 Loss: 0.1645\n",
      "Epoch: 3 / 3, Step: 535 / 750 Loss: 0.0290\n",
      "Epoch: 3 / 3, Step: 536 / 750 Loss: 0.3109\n",
      "Epoch: 3 / 3, Step: 537 / 750 Loss: 0.1025\n",
      "Epoch: 3 / 3, Step: 538 / 750 Loss: 0.0675\n",
      "Epoch: 3 / 3, Step: 539 / 750 Loss: 0.0664\n",
      "Epoch: 3 / 3, Step: 540 / 750 Loss: 0.0257\n",
      "Epoch: 3 / 3, Step: 541 / 750 Loss: 0.2275\n",
      "Epoch: 3 / 3, Step: 542 / 750 Loss: 0.2250\n",
      "Epoch: 3 / 3, Step: 543 / 750 Loss: 0.2294\n",
      "Epoch: 3 / 3, Step: 544 / 750 Loss: 0.0735\n",
      "Epoch: 3 / 3, Step: 545 / 750 Loss: 0.2924\n",
      "Epoch: 3 / 3, Step: 546 / 750 Loss: 0.1048\n",
      "Epoch: 3 / 3, Step: 547 / 750 Loss: 0.3126\n",
      "Epoch: 3 / 3, Step: 548 / 750 Loss: 0.2690\n",
      "Epoch: 3 / 3, Step: 549 / 750 Loss: 0.0782\n",
      "Epoch: 3 / 3, Step: 550 / 750 Loss: 0.0891\n",
      "Epoch: 3 / 3, Step: 551 / 750 Loss: 0.2632\n",
      "Epoch: 3 / 3, Step: 552 / 750 Loss: 0.1645\n",
      "Epoch: 3 / 3, Step: 553 / 750 Loss: 0.1452\n",
      "Epoch: 3 / 3, Step: 554 / 750 Loss: 0.1584\n",
      "Epoch: 3 / 3, Step: 555 / 750 Loss: 0.2236\n",
      "Epoch: 3 / 3, Step: 556 / 750 Loss: 0.0818\n",
      "Epoch: 3 / 3, Step: 557 / 750 Loss: 0.0870\n",
      "Epoch: 3 / 3, Step: 558 / 750 Loss: 0.2215\n",
      "Epoch: 3 / 3, Step: 559 / 750 Loss: 0.0323\n",
      "Epoch: 3 / 3, Step: 560 / 750 Loss: 0.1035\n",
      "Epoch: 3 / 3, Step: 561 / 750 Loss: 0.0717\n",
      "Epoch: 3 / 3, Step: 562 / 750 Loss: 0.0370\n",
      "Epoch: 3 / 3, Step: 563 / 750 Loss: 0.2648\n",
      "Epoch: 3 / 3, Step: 564 / 750 Loss: 0.0611\n",
      "Epoch: 3 / 3, Step: 565 / 750 Loss: 0.3451\n",
      "Epoch: 3 / 3, Step: 566 / 750 Loss: 0.0514\n",
      "Epoch: 3 / 3, Step: 567 / 750 Loss: 0.2681\n",
      "Epoch: 3 / 3, Step: 568 / 750 Loss: 0.2790\n",
      "Epoch: 3 / 3, Step: 569 / 750 Loss: 0.0636\n",
      "Epoch: 3 / 3, Step: 570 / 750 Loss: 0.2505\n",
      "Epoch: 3 / 3, Step: 571 / 750 Loss: 0.1469\n",
      "Epoch: 3 / 3, Step: 572 / 750 Loss: 0.5050\n",
      "Epoch: 3 / 3, Step: 573 / 750 Loss: 0.1411\n",
      "Epoch: 3 / 3, Step: 574 / 750 Loss: 0.5406\n",
      "Epoch: 3 / 3, Step: 575 / 750 Loss: 0.2377\n",
      "Epoch: 3 / 3, Step: 576 / 750 Loss: 0.0887\n",
      "Epoch: 3 / 3, Step: 577 / 750 Loss: 0.2828\n",
      "Epoch: 3 / 3, Step: 578 / 750 Loss: 0.0853\n",
      "Epoch: 3 / 3, Step: 579 / 750 Loss: 0.1597\n",
      "Epoch: 3 / 3, Step: 580 / 750 Loss: 0.1015\n",
      "Epoch: 3 / 3, Step: 581 / 750 Loss: 0.1688\n",
      "Epoch: 3 / 3, Step: 582 / 750 Loss: 0.0651\n",
      "Epoch: 3 / 3, Step: 583 / 750 Loss: 0.1360\n",
      "Epoch: 3 / 3, Step: 584 / 750 Loss: 0.2579\n",
      "Epoch: 3 / 3, Step: 585 / 750 Loss: 0.3297\n",
      "Epoch: 3 / 3, Step: 586 / 750 Loss: 0.1202\n",
      "Epoch: 3 / 3, Step: 587 / 750 Loss: 0.0514\n",
      "Epoch: 3 / 3, Step: 588 / 750 Loss: 0.2732\n",
      "Epoch: 3 / 3, Step: 589 / 750 Loss: 0.1361\n",
      "Epoch: 3 / 3, Step: 590 / 750 Loss: 0.1237\n",
      "Epoch: 3 / 3, Step: 591 / 750 Loss: 0.1261\n",
      "Epoch: 3 / 3, Step: 592 / 750 Loss: 0.2009\n",
      "Epoch: 3 / 3, Step: 593 / 750 Loss: 0.1295\n",
      "Epoch: 3 / 3, Step: 594 / 750 Loss: 0.2144\n",
      "Epoch: 3 / 3, Step: 595 / 750 Loss: 0.0577\n",
      "Epoch: 3 / 3, Step: 596 / 750 Loss: 0.2320\n",
      "Epoch: 3 / 3, Step: 597 / 750 Loss: 0.2098\n",
      "Epoch: 3 / 3, Step: 598 / 750 Loss: 0.0984\n",
      "Epoch: 3 / 3, Step: 599 / 750 Loss: 0.3923\n",
      "Epoch: 3 / 3, Step: 600 / 750 Loss: 0.4676\n",
      "Epoch: 3 / 3, Step: 601 / 750 Loss: 0.0746\n",
      "Epoch: 3 / 3, Step: 602 / 750 Loss: 0.2203\n",
      "Epoch: 3 / 3, Step: 603 / 750 Loss: 0.0692\n",
      "Epoch: 3 / 3, Step: 604 / 750 Loss: 0.0894\n",
      "Epoch: 3 / 3, Step: 605 / 750 Loss: 0.1537\n",
      "Epoch: 3 / 3, Step: 606 / 750 Loss: 0.2820\n",
      "Epoch: 3 / 3, Step: 607 / 750 Loss: 0.1162\n",
      "Epoch: 3 / 3, Step: 608 / 750 Loss: 0.2189\n",
      "Epoch: 3 / 3, Step: 609 / 750 Loss: 0.0909\n",
      "Epoch: 3 / 3, Step: 610 / 750 Loss: 0.1301\n",
      "Epoch: 3 / 3, Step: 611 / 750 Loss: 0.2841\n",
      "Epoch: 3 / 3, Step: 612 / 750 Loss: 0.2382\n",
      "Epoch: 3 / 3, Step: 613 / 750 Loss: 0.2925\n",
      "Epoch: 3 / 3, Step: 614 / 750 Loss: 0.2198\n",
      "Epoch: 3 / 3, Step: 615 / 750 Loss: 0.1077\n",
      "Epoch: 3 / 3, Step: 616 / 750 Loss: 0.1175\n",
      "Epoch: 3 / 3, Step: 617 / 750 Loss: 0.1865\n",
      "Epoch: 3 / 3, Step: 618 / 750 Loss: 0.2811\n",
      "Epoch: 3 / 3, Step: 619 / 750 Loss: 0.1310\n",
      "Epoch: 3 / 3, Step: 620 / 750 Loss: 0.2339\n",
      "Epoch: 3 / 3, Step: 621 / 750 Loss: 0.3280\n",
      "Epoch: 3 / 3, Step: 622 / 750 Loss: 0.1098\n",
      "Epoch: 3 / 3, Step: 623 / 750 Loss: 0.3437\n",
      "Epoch: 3 / 3, Step: 624 / 750 Loss: 0.2211\n",
      "Epoch: 3 / 3, Step: 625 / 750 Loss: 0.2063\n",
      "Epoch: 3 / 3, Step: 626 / 750 Loss: 0.0765\n",
      "Epoch: 3 / 3, Step: 627 / 750 Loss: 0.0504\n",
      "Epoch: 3 / 3, Step: 628 / 750 Loss: 0.2160\n",
      "Epoch: 3 / 3, Step: 629 / 750 Loss: 0.3973\n",
      "Epoch: 3 / 3, Step: 630 / 750 Loss: 0.0825\n",
      "Epoch: 3 / 3, Step: 631 / 750 Loss: 0.2176\n",
      "Epoch: 3 / 3, Step: 632 / 750 Loss: 0.0846\n",
      "Epoch: 3 / 3, Step: 633 / 750 Loss: 0.4182\n",
      "Epoch: 3 / 3, Step: 634 / 750 Loss: 0.3124\n",
      "Epoch: 3 / 3, Step: 635 / 750 Loss: 0.0511\n",
      "Epoch: 3 / 3, Step: 636 / 750 Loss: 0.3783\n",
      "Epoch: 3 / 3, Step: 637 / 750 Loss: 0.1162\n",
      "Epoch: 3 / 3, Step: 638 / 750 Loss: 0.2564\n",
      "Epoch: 3 / 3, Step: 639 / 750 Loss: 0.1906\n",
      "Epoch: 3 / 3, Step: 640 / 750 Loss: 0.2695\n",
      "Epoch: 3 / 3, Step: 641 / 750 Loss: 0.0802\n",
      "Epoch: 3 / 3, Step: 642 / 750 Loss: 0.0965\n",
      "Epoch: 3 / 3, Step: 643 / 750 Loss: 0.0541\n",
      "Epoch: 3 / 3, Step: 644 / 750 Loss: 0.0591\n",
      "Epoch: 3 / 3, Step: 645 / 750 Loss: 0.0375\n",
      "Epoch: 3 / 3, Step: 646 / 750 Loss: 0.1979\n",
      "Epoch: 3 / 3, Step: 647 / 750 Loss: 0.0648\n",
      "Epoch: 3 / 3, Step: 648 / 750 Loss: 0.0348\n",
      "Epoch: 3 / 3, Step: 649 / 750 Loss: 0.2733\n",
      "Epoch: 3 / 3, Step: 650 / 750 Loss: 0.1370\n",
      "Epoch: 3 / 3, Step: 651 / 750 Loss: 0.1859\n",
      "Epoch: 3 / 3, Step: 652 / 750 Loss: 0.1263\n",
      "Epoch: 3 / 3, Step: 653 / 750 Loss: 0.3498\n",
      "Epoch: 3 / 3, Step: 654 / 750 Loss: 0.2075\n",
      "Epoch: 3 / 3, Step: 655 / 750 Loss: 0.1451\n",
      "Epoch: 3 / 3, Step: 656 / 750 Loss: 0.0482\n",
      "Epoch: 3 / 3, Step: 657 / 750 Loss: 0.0322\n",
      "Epoch: 3 / 3, Step: 658 / 750 Loss: 0.1061\n",
      "Epoch: 3 / 3, Step: 659 / 750 Loss: 0.0748\n",
      "Epoch: 3 / 3, Step: 660 / 750 Loss: 0.1356\n",
      "Epoch: 3 / 3, Step: 661 / 750 Loss: 0.1307\n",
      "Epoch: 3 / 3, Step: 662 / 750 Loss: 0.0871\n",
      "Epoch: 3 / 3, Step: 663 / 750 Loss: 0.1106\n",
      "Epoch: 3 / 3, Step: 664 / 750 Loss: 0.1035\n",
      "Epoch: 3 / 3, Step: 665 / 750 Loss: 0.0639\n",
      "Epoch: 3 / 3, Step: 666 / 750 Loss: 0.1957\n",
      "Epoch: 3 / 3, Step: 667 / 750 Loss: 0.0623\n",
      "Epoch: 3 / 3, Step: 668 / 750 Loss: 0.3801\n",
      "Epoch: 3 / 3, Step: 669 / 750 Loss: 0.1038\n",
      "Epoch: 3 / 3, Step: 670 / 750 Loss: 0.3357\n",
      "Epoch: 3 / 3, Step: 671 / 750 Loss: 0.2340\n",
      "Epoch: 3 / 3, Step: 672 / 750 Loss: 0.0190\n",
      "Epoch: 3 / 3, Step: 673 / 750 Loss: 0.3864\n",
      "Epoch: 3 / 3, Step: 674 / 750 Loss: 0.0396\n",
      "Epoch: 3 / 3, Step: 675 / 750 Loss: 0.0345\n",
      "Epoch: 3 / 3, Step: 676 / 750 Loss: 0.1427\n",
      "Epoch: 3 / 3, Step: 677 / 750 Loss: 0.0915\n",
      "Epoch: 3 / 3, Step: 678 / 750 Loss: 0.0165\n",
      "Epoch: 3 / 3, Step: 679 / 750 Loss: 0.1113\n",
      "Epoch: 3 / 3, Step: 680 / 750 Loss: 0.0926\n",
      "Epoch: 3 / 3, Step: 681 / 750 Loss: 0.2087\n",
      "Epoch: 3 / 3, Step: 682 / 750 Loss: 0.1497\n",
      "Epoch: 3 / 3, Step: 683 / 750 Loss: 0.0162\n",
      "Epoch: 3 / 3, Step: 684 / 750 Loss: 0.2000\n",
      "Epoch: 3 / 3, Step: 685 / 750 Loss: 0.0323\n",
      "Epoch: 3 / 3, Step: 686 / 750 Loss: 0.1041\n",
      "Epoch: 3 / 3, Step: 687 / 750 Loss: 0.1848\n",
      "Epoch: 3 / 3, Step: 688 / 750 Loss: 0.1154\n",
      "Epoch: 3 / 3, Step: 689 / 750 Loss: 0.0698\n",
      "Epoch: 3 / 3, Step: 690 / 750 Loss: 0.1582\n",
      "Epoch: 3 / 3, Step: 691 / 750 Loss: 0.0126\n",
      "Epoch: 3 / 3, Step: 692 / 750 Loss: 0.1591\n",
      "Epoch: 3 / 3, Step: 693 / 750 Loss: 0.0790\n",
      "Epoch: 3 / 3, Step: 694 / 750 Loss: 0.1316\n",
      "Epoch: 3 / 3, Step: 695 / 750 Loss: 0.2523\n",
      "Epoch: 3 / 3, Step: 696 / 750 Loss: 0.2425\n",
      "Epoch: 3 / 3, Step: 697 / 750 Loss: 0.2143\n",
      "Epoch: 3 / 3, Step: 698 / 750 Loss: 0.1023\n",
      "Epoch: 3 / 3, Step: 699 / 750 Loss: 0.1379\n",
      "Epoch: 3 / 3, Step: 700 / 750 Loss: 0.0519\n",
      "Epoch: 3 / 3, Step: 701 / 750 Loss: 0.0488\n",
      "Epoch: 3 / 3, Step: 702 / 750 Loss: 0.2235\n",
      "Epoch: 3 / 3, Step: 703 / 750 Loss: 0.0523\n",
      "Epoch: 3 / 3, Step: 704 / 750 Loss: 0.1623\n",
      "Epoch: 3 / 3, Step: 705 / 750 Loss: 0.1021\n",
      "Epoch: 3 / 3, Step: 706 / 750 Loss: 0.3472\n",
      "Epoch: 3 / 3, Step: 707 / 750 Loss: 0.0768\n",
      "Epoch: 3 / 3, Step: 708 / 750 Loss: 0.0834\n",
      "Epoch: 3 / 3, Step: 709 / 750 Loss: 0.1340\n",
      "Epoch: 3 / 3, Step: 710 / 750 Loss: 0.0320\n",
      "Epoch: 3 / 3, Step: 711 / 750 Loss: 0.2138\n",
      "Epoch: 3 / 3, Step: 712 / 750 Loss: 0.0882\n",
      "Epoch: 3 / 3, Step: 713 / 750 Loss: 0.1450\n",
      "Epoch: 3 / 3, Step: 714 / 750 Loss: 0.2568\n",
      "Epoch: 3 / 3, Step: 715 / 750 Loss: 0.1957\n",
      "Epoch: 3 / 3, Step: 716 / 750 Loss: 0.1864\n",
      "Epoch: 3 / 3, Step: 717 / 750 Loss: 0.1433\n",
      "Epoch: 3 / 3, Step: 718 / 750 Loss: 0.2192\n",
      "Epoch: 3 / 3, Step: 719 / 750 Loss: 0.2704\n",
      "Epoch: 3 / 3, Step: 720 / 750 Loss: 0.1562\n",
      "Epoch: 3 / 3, Step: 721 / 750 Loss: 0.0772\n",
      "Epoch: 3 / 3, Step: 722 / 750 Loss: 0.1665\n",
      "Epoch: 3 / 3, Step: 723 / 750 Loss: 0.2170\n",
      "Epoch: 3 / 3, Step: 724 / 750 Loss: 0.0892\n",
      "Epoch: 3 / 3, Step: 725 / 750 Loss: 0.1574\n",
      "Epoch: 3 / 3, Step: 726 / 750 Loss: 0.0646\n",
      "Epoch: 3 / 3, Step: 727 / 750 Loss: 0.0263\n",
      "Epoch: 3 / 3, Step: 728 / 750 Loss: 0.2691\n",
      "Epoch: 3 / 3, Step: 729 / 750 Loss: 0.3738\n",
      "Epoch: 3 / 3, Step: 730 / 750 Loss: 0.1277\n",
      "Epoch: 3 / 3, Step: 731 / 750 Loss: 0.2204\n",
      "Epoch: 3 / 3, Step: 732 / 750 Loss: 0.3741\n",
      "Epoch: 3 / 3, Step: 733 / 750 Loss: 0.1662\n",
      "Epoch: 3 / 3, Step: 734 / 750 Loss: 0.1309\n",
      "Epoch: 3 / 3, Step: 735 / 750 Loss: 0.0854\n",
      "Epoch: 3 / 3, Step: 736 / 750 Loss: 0.2282\n",
      "Epoch: 3 / 3, Step: 737 / 750 Loss: 0.1359\n",
      "Epoch: 3 / 3, Step: 738 / 750 Loss: 0.1008\n",
      "Epoch: 3 / 3, Step: 739 / 750 Loss: 0.2025\n",
      "Epoch: 3 / 3, Step: 740 / 750 Loss: 0.1192\n",
      "Epoch: 3 / 3, Step: 741 / 750 Loss: 0.3216\n",
      "Epoch: 3 / 3, Step: 742 / 750 Loss: 0.0328\n",
      "Epoch: 3 / 3, Step: 743 / 750 Loss: 0.2070\n",
      "Epoch: 3 / 3, Step: 744 / 750 Loss: 0.2556\n",
      "Epoch: 3 / 3, Step: 745 / 750 Loss: 0.1046\n",
      "Epoch: 3 / 3, Step: 746 / 750 Loss: 0.1497\n",
      "Epoch: 3 / 3, Step: 747 / 750 Loss: 0.1739\n",
      "Epoch: 3 / 3, Step: 748 / 750 Loss: 0.1553\n",
      "Epoch: 3 / 3, Step: 749 / 750 Loss: 0.0751\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "loss_total = 0\n",
    "model.train()\n",
    "for i in range(3):\n",
    "    for j, data in enumerate(train_dataloader):\n",
    "        inputs = {'input_ids': data[0].cuda(), \n",
    "                      'attention_mask': data[1].cuda(), \n",
    "                      'labels': data[2].cuda()}\n",
    "        output = model(**inputs)\n",
    "        loss = output[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Epoch: {} / {}, Step: {} / {} Loss: {:.4f}\".format(i+1, 3, j, len(train_dataloader),\n",
    "                                                                      loss))\n",
    "torch.save(model.state_dict(), 'modelB.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fb3815d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('modelB.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "loss_total = 0\n",
    "results = defaultdict(dict)\n",
    "predictions, true_vals = [], []\n",
    "for j, data in enumerate(test_dataloader):\n",
    "    inputs = {'input_ids': data[0].cuda(), \n",
    "              'attention_mask': data[1].cuda(), \n",
    "              'labels': data[2].cuda()}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "    loss = output[0]\n",
    "    logits = output[1]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    labels = inputs['labels'].cpu().numpy()\n",
    "    loss_total += loss.item()\n",
    "    predictions.append(logits)\n",
    "    true_vals.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "31278a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_vals = np.concatenate(true_vals, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a3c24f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "preds_flat = np.argmax(predictions, axis = 1).flatten()\n",
    "labels_flat = true_vals.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "87ede91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.92      0.86      0.89      1094\n",
      "         pos       0.81      0.86      0.83       407\n",
      "         neu       0.95      0.96      0.96      4499\n",
      "\n",
      "    accuracy                           0.94      6000\n",
      "   macro avg       0.89      0.90      0.89      6000\n",
      "weighted avg       0.94      0.94      0.94      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['neg', 'pos', 'neu']\n",
    "print(classification_report(labels_flat, preds_flat, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1028ff",
   "metadata": {},
   "source": [
    "# DistilBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da39d3d",
   "metadata": {},
   "source": [
    "DistilBert is a lighter version of BERT. This model is built by knowledge distillation technique wherein a small model is trained to reproduce the behavior of a larger model. Accordingly, it has 40% less parameters than BERT and runs 60% faster while retaining 95% of the BERT-base-uncased performance[3]. Here, we are planning to compare the results of BERT with DistilBERT and try to get hands on both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0e4dbdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertConfig,DistilBertTokenizer,DistilBertModel\n",
    "distil_berttokenizer = DistilBertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "206902ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/a/grad/sanket96/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2322: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = distil_berttokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='train'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')\n",
    "\n",
    "encoded_data_test = distil_berttokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='test'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "854ef639",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(covid_senti[covid_senti.data_type == 'train'].label_num.values)\n",
    "\n",
    "#validation set\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(covid_senti[covid_senti.data_type == 'test'].label_num.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0a70c1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['transformer.layer.7.ffn.lin1.bias', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.9.output_layer_norm.weight', 'pre_classifier.weight', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.11.attention.v_lin.weight', 'classifier.weight', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.0.ffn.lin1.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.1.attention.q_lin.bias', 'pre_classifier.bias', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.0.ffn.lin2.bias', 'embeddings.position_embeddings.weight', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.6.attention.q_lin.weight', 'classifier.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.8.sa_layer_norm.weight', 'embeddings.LayerNorm.weight', 'transformer.layer.5.sa_layer_norm.weight', 'embeddings.word_embeddings.weight', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.4.output_layer_norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(class_dict),\n",
    "                                                     output_attentions = False,\n",
    "                                                      output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2d5d0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch import nn, optim\n",
    "\n",
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train,labels_train)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test,labels_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e0f84a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f34a8c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 3, Step: 0 / 750 Loss: 0.2118\n",
      "Epoch: 1 / 3, Step: 1 / 750 Loss: 0.1976\n",
      "Epoch: 1 / 3, Step: 2 / 750 Loss: 0.1105\n",
      "Epoch: 1 / 3, Step: 3 / 750 Loss: 0.4373\n",
      "Epoch: 1 / 3, Step: 4 / 750 Loss: 0.1667\n",
      "Epoch: 1 / 3, Step: 5 / 750 Loss: 0.0947\n",
      "Epoch: 1 / 3, Step: 6 / 750 Loss: 0.2525\n",
      "Epoch: 1 / 3, Step: 7 / 750 Loss: 0.1375\n",
      "Epoch: 1 / 3, Step: 8 / 750 Loss: 0.1969\n",
      "Epoch: 1 / 3, Step: 9 / 750 Loss: 0.5194\n",
      "Epoch: 1 / 3, Step: 10 / 750 Loss: 0.3098\n",
      "Epoch: 1 / 3, Step: 11 / 750 Loss: 0.1188\n",
      "Epoch: 1 / 3, Step: 12 / 750 Loss: 0.2719\n",
      "Epoch: 1 / 3, Step: 13 / 750 Loss: 0.1816\n",
      "Epoch: 1 / 3, Step: 14 / 750 Loss: 0.3821\n",
      "Epoch: 1 / 3, Step: 15 / 750 Loss: 0.4668\n",
      "Epoch: 1 / 3, Step: 16 / 750 Loss: 0.3268\n",
      "Epoch: 1 / 3, Step: 17 / 750 Loss: 0.2012\n",
      "Epoch: 1 / 3, Step: 18 / 750 Loss: 0.2884\n",
      "Epoch: 1 / 3, Step: 19 / 750 Loss: 0.2631\n",
      "Epoch: 1 / 3, Step: 20 / 750 Loss: 0.1056\n",
      "Epoch: 1 / 3, Step: 21 / 750 Loss: 0.1297\n",
      "Epoch: 1 / 3, Step: 22 / 750 Loss: 0.1447\n",
      "Epoch: 1 / 3, Step: 23 / 750 Loss: 0.2654\n",
      "Epoch: 1 / 3, Step: 24 / 750 Loss: 0.4054\n",
      "Epoch: 1 / 3, Step: 25 / 750 Loss: 0.2705\n",
      "Epoch: 1 / 3, Step: 26 / 750 Loss: 0.2642\n",
      "Epoch: 1 / 3, Step: 27 / 750 Loss: 0.3609\n",
      "Epoch: 1 / 3, Step: 28 / 750 Loss: 0.1583\n",
      "Epoch: 1 / 3, Step: 29 / 750 Loss: 0.3270\n",
      "Epoch: 1 / 3, Step: 30 / 750 Loss: 0.3921\n",
      "Epoch: 1 / 3, Step: 31 / 750 Loss: 0.2561\n",
      "Epoch: 1 / 3, Step: 32 / 750 Loss: 0.3012\n",
      "Epoch: 1 / 3, Step: 33 / 750 Loss: 0.3846\n",
      "Epoch: 1 / 3, Step: 34 / 750 Loss: 0.1109\n",
      "Epoch: 1 / 3, Step: 35 / 750 Loss: 0.4593\n",
      "Epoch: 1 / 3, Step: 36 / 750 Loss: 0.3706\n",
      "Epoch: 1 / 3, Step: 37 / 750 Loss: 0.2580\n",
      "Epoch: 1 / 3, Step: 38 / 750 Loss: 0.3986\n",
      "Epoch: 1 / 3, Step: 39 / 750 Loss: 0.1929\n",
      "Epoch: 1 / 3, Step: 40 / 750 Loss: 0.0923\n",
      "Epoch: 1 / 3, Step: 41 / 750 Loss: 0.1012\n",
      "Epoch: 1 / 3, Step: 42 / 750 Loss: 0.3519\n",
      "Epoch: 1 / 3, Step: 43 / 750 Loss: 0.3115\n",
      "Epoch: 1 / 3, Step: 44 / 750 Loss: 0.2814\n",
      "Epoch: 1 / 3, Step: 45 / 750 Loss: 0.2353\n",
      "Epoch: 1 / 3, Step: 46 / 750 Loss: 0.1914\n",
      "Epoch: 1 / 3, Step: 47 / 750 Loss: 0.2560\n",
      "Epoch: 1 / 3, Step: 48 / 750 Loss: 0.4545\n",
      "Epoch: 1 / 3, Step: 49 / 750 Loss: 0.3845\n",
      "Epoch: 1 / 3, Step: 50 / 750 Loss: 0.2421\n",
      "Epoch: 1 / 3, Step: 51 / 750 Loss: 0.2788\n",
      "Epoch: 1 / 3, Step: 52 / 750 Loss: 0.1661\n",
      "Epoch: 1 / 3, Step: 53 / 750 Loss: 0.3011\n",
      "Epoch: 1 / 3, Step: 54 / 750 Loss: 0.4281\n",
      "Epoch: 1 / 3, Step: 55 / 750 Loss: 0.1381\n",
      "Epoch: 1 / 3, Step: 56 / 750 Loss: 0.1778\n",
      "Epoch: 1 / 3, Step: 57 / 750 Loss: 0.1130\n",
      "Epoch: 1 / 3, Step: 58 / 750 Loss: 0.2227\n",
      "Epoch: 1 / 3, Step: 59 / 750 Loss: 0.4051\n",
      "Epoch: 1 / 3, Step: 60 / 750 Loss: 0.1668\n",
      "Epoch: 1 / 3, Step: 61 / 750 Loss: 0.1493\n",
      "Epoch: 1 / 3, Step: 62 / 750 Loss: 0.1857\n",
      "Epoch: 1 / 3, Step: 63 / 750 Loss: 0.2100\n",
      "Epoch: 1 / 3, Step: 64 / 750 Loss: 0.3354\n",
      "Epoch: 1 / 3, Step: 65 / 750 Loss: 0.1359\n",
      "Epoch: 1 / 3, Step: 66 / 750 Loss: 0.5175\n",
      "Epoch: 1 / 3, Step: 67 / 750 Loss: 0.2538\n",
      "Epoch: 1 / 3, Step: 68 / 750 Loss: 0.2617\n",
      "Epoch: 1 / 3, Step: 69 / 750 Loss: 0.1488\n",
      "Epoch: 1 / 3, Step: 70 / 750 Loss: 0.2995\n",
      "Epoch: 1 / 3, Step: 71 / 750 Loss: 0.2369\n",
      "Epoch: 1 / 3, Step: 72 / 750 Loss: 0.3500\n",
      "Epoch: 1 / 3, Step: 73 / 750 Loss: 0.2868\n",
      "Epoch: 1 / 3, Step: 74 / 750 Loss: 0.2215\n",
      "Epoch: 1 / 3, Step: 75 / 750 Loss: 0.4036\n",
      "Epoch: 1 / 3, Step: 76 / 750 Loss: 0.2018\n",
      "Epoch: 1 / 3, Step: 77 / 750 Loss: 0.3741\n",
      "Epoch: 1 / 3, Step: 78 / 750 Loss: 0.1931\n",
      "Epoch: 1 / 3, Step: 79 / 750 Loss: 0.3859\n",
      "Epoch: 1 / 3, Step: 80 / 750 Loss: 0.4097\n",
      "Epoch: 1 / 3, Step: 81 / 750 Loss: 0.4070\n",
      "Epoch: 1 / 3, Step: 82 / 750 Loss: 0.2893\n",
      "Epoch: 1 / 3, Step: 83 / 750 Loss: 0.1415\n",
      "Epoch: 1 / 3, Step: 84 / 750 Loss: 0.4264\n",
      "Epoch: 1 / 3, Step: 85 / 750 Loss: 0.2230\n",
      "Epoch: 1 / 3, Step: 86 / 750 Loss: 0.3452\n",
      "Epoch: 1 / 3, Step: 87 / 750 Loss: 0.3181\n",
      "Epoch: 1 / 3, Step: 88 / 750 Loss: 0.2953\n",
      "Epoch: 1 / 3, Step: 89 / 750 Loss: 0.1450\n",
      "Epoch: 1 / 3, Step: 90 / 750 Loss: 0.2739\n",
      "Epoch: 1 / 3, Step: 91 / 750 Loss: 0.1991\n",
      "Epoch: 1 / 3, Step: 92 / 750 Loss: 0.1614\n",
      "Epoch: 1 / 3, Step: 93 / 750 Loss: 0.1910\n",
      "Epoch: 1 / 3, Step: 94 / 750 Loss: 0.1285\n",
      "Epoch: 1 / 3, Step: 95 / 750 Loss: 0.2518\n",
      "Epoch: 1 / 3, Step: 96 / 750 Loss: 0.1823\n",
      "Epoch: 1 / 3, Step: 97 / 750 Loss: 0.2817\n",
      "Epoch: 1 / 3, Step: 98 / 750 Loss: 0.1899\n",
      "Epoch: 1 / 3, Step: 99 / 750 Loss: 0.3592\n",
      "Epoch: 1 / 3, Step: 100 / 750 Loss: 0.3525\n",
      "Epoch: 1 / 3, Step: 101 / 750 Loss: 0.3084\n",
      "Epoch: 1 / 3, Step: 102 / 750 Loss: 0.3171\n",
      "Epoch: 1 / 3, Step: 103 / 750 Loss: 0.1283\n",
      "Epoch: 1 / 3, Step: 104 / 750 Loss: 0.2445\n",
      "Epoch: 1 / 3, Step: 105 / 750 Loss: 0.1820\n",
      "Epoch: 1 / 3, Step: 106 / 750 Loss: 0.1383\n",
      "Epoch: 1 / 3, Step: 107 / 750 Loss: 0.2309\n",
      "Epoch: 1 / 3, Step: 108 / 750 Loss: 0.1211\n",
      "Epoch: 1 / 3, Step: 109 / 750 Loss: 0.2004\n",
      "Epoch: 1 / 3, Step: 110 / 750 Loss: 0.4567\n",
      "Epoch: 1 / 3, Step: 111 / 750 Loss: 0.5601\n",
      "Epoch: 1 / 3, Step: 112 / 750 Loss: 0.4555\n",
      "Epoch: 1 / 3, Step: 113 / 750 Loss: 0.2036\n",
      "Epoch: 1 / 3, Step: 114 / 750 Loss: 0.1288\n",
      "Epoch: 1 / 3, Step: 115 / 750 Loss: 0.1361\n",
      "Epoch: 1 / 3, Step: 116 / 750 Loss: 0.2251\n",
      "Epoch: 1 / 3, Step: 117 / 750 Loss: 0.3796\n",
      "Epoch: 1 / 3, Step: 118 / 750 Loss: 0.3592\n",
      "Epoch: 1 / 3, Step: 119 / 750 Loss: 0.2584\n",
      "Epoch: 1 / 3, Step: 120 / 750 Loss: 0.2497\n",
      "Epoch: 1 / 3, Step: 121 / 750 Loss: 0.2207\n",
      "Epoch: 1 / 3, Step: 122 / 750 Loss: 0.1808\n",
      "Epoch: 1 / 3, Step: 123 / 750 Loss: 0.3416\n",
      "Epoch: 1 / 3, Step: 124 / 750 Loss: 0.1984\n",
      "Epoch: 1 / 3, Step: 125 / 750 Loss: 0.0867\n",
      "Epoch: 1 / 3, Step: 126 / 750 Loss: 0.1472\n",
      "Epoch: 1 / 3, Step: 127 / 750 Loss: 0.4748\n",
      "Epoch: 1 / 3, Step: 128 / 750 Loss: 0.2735\n",
      "Epoch: 1 / 3, Step: 129 / 750 Loss: 0.1623\n",
      "Epoch: 1 / 3, Step: 130 / 750 Loss: 0.2058\n",
      "Epoch: 1 / 3, Step: 131 / 750 Loss: 0.1868\n",
      "Epoch: 1 / 3, Step: 132 / 750 Loss: 0.2582\n",
      "Epoch: 1 / 3, Step: 133 / 750 Loss: 0.1668\n",
      "Epoch: 1 / 3, Step: 134 / 750 Loss: 0.1120\n",
      "Epoch: 1 / 3, Step: 135 / 750 Loss: 0.4760\n",
      "Epoch: 1 / 3, Step: 136 / 750 Loss: 0.3662\n",
      "Epoch: 1 / 3, Step: 137 / 750 Loss: 0.1204\n",
      "Epoch: 1 / 3, Step: 138 / 750 Loss: 0.1922\n",
      "Epoch: 1 / 3, Step: 139 / 750 Loss: 0.0717\n",
      "Epoch: 1 / 3, Step: 140 / 750 Loss: 0.1012\n",
      "Epoch: 1 / 3, Step: 141 / 750 Loss: 0.1660\n",
      "Epoch: 1 / 3, Step: 142 / 750 Loss: 0.3424\n",
      "Epoch: 1 / 3, Step: 143 / 750 Loss: 0.1986\n",
      "Epoch: 1 / 3, Step: 144 / 750 Loss: 0.2736\n",
      "Epoch: 1 / 3, Step: 145 / 750 Loss: 0.1931\n",
      "Epoch: 1 / 3, Step: 146 / 750 Loss: 0.3574\n",
      "Epoch: 1 / 3, Step: 147 / 750 Loss: 0.4101\n",
      "Epoch: 1 / 3, Step: 148 / 750 Loss: 0.1116\n",
      "Epoch: 1 / 3, Step: 149 / 750 Loss: 0.1078\n",
      "Epoch: 1 / 3, Step: 150 / 750 Loss: 0.1820\n",
      "Epoch: 1 / 3, Step: 151 / 750 Loss: 0.3740\n",
      "Epoch: 1 / 3, Step: 152 / 750 Loss: 0.2773\n",
      "Epoch: 1 / 3, Step: 153 / 750 Loss: 0.1399\n",
      "Epoch: 1 / 3, Step: 154 / 750 Loss: 0.1595\n",
      "Epoch: 1 / 3, Step: 155 / 750 Loss: 0.2934\n",
      "Epoch: 1 / 3, Step: 156 / 750 Loss: 0.1928\n",
      "Epoch: 1 / 3, Step: 157 / 750 Loss: 0.2081\n",
      "Epoch: 1 / 3, Step: 158 / 750 Loss: 0.1308\n",
      "Epoch: 1 / 3, Step: 159 / 750 Loss: 0.2365\n",
      "Epoch: 1 / 3, Step: 160 / 750 Loss: 0.2391\n",
      "Epoch: 1 / 3, Step: 161 / 750 Loss: 0.3960\n",
      "Epoch: 1 / 3, Step: 162 / 750 Loss: 0.0936\n",
      "Epoch: 1 / 3, Step: 163 / 750 Loss: 0.1498\n",
      "Epoch: 1 / 3, Step: 164 / 750 Loss: 0.1220\n",
      "Epoch: 1 / 3, Step: 165 / 750 Loss: 0.3062\n",
      "Epoch: 1 / 3, Step: 166 / 750 Loss: 0.0456\n",
      "Epoch: 1 / 3, Step: 167 / 750 Loss: 0.4293\n",
      "Epoch: 1 / 3, Step: 168 / 750 Loss: 0.3983\n",
      "Epoch: 1 / 3, Step: 169 / 750 Loss: 0.4956\n",
      "Epoch: 1 / 3, Step: 170 / 750 Loss: 0.3038\n",
      "Epoch: 1 / 3, Step: 171 / 750 Loss: 0.0833\n",
      "Epoch: 1 / 3, Step: 172 / 750 Loss: 0.3995\n",
      "Epoch: 1 / 3, Step: 173 / 750 Loss: 0.3093\n",
      "Epoch: 1 / 3, Step: 174 / 750 Loss: 0.2611\n",
      "Epoch: 1 / 3, Step: 175 / 750 Loss: 0.2288\n",
      "Epoch: 1 / 3, Step: 176 / 750 Loss: 0.1797\n",
      "Epoch: 1 / 3, Step: 177 / 750 Loss: 0.3732\n",
      "Epoch: 1 / 3, Step: 178 / 750 Loss: 0.3575\n",
      "Epoch: 1 / 3, Step: 179 / 750 Loss: 0.1512\n",
      "Epoch: 1 / 3, Step: 180 / 750 Loss: 0.2705\n",
      "Epoch: 1 / 3, Step: 181 / 750 Loss: 0.1494\n",
      "Epoch: 1 / 3, Step: 182 / 750 Loss: 0.2528\n",
      "Epoch: 1 / 3, Step: 183 / 750 Loss: 0.3088\n",
      "Epoch: 1 / 3, Step: 184 / 750 Loss: 0.2555\n",
      "Epoch: 1 / 3, Step: 185 / 750 Loss: 0.2043\n",
      "Epoch: 1 / 3, Step: 186 / 750 Loss: 0.2518\n",
      "Epoch: 1 / 3, Step: 187 / 750 Loss: 0.2127\n",
      "Epoch: 1 / 3, Step: 188 / 750 Loss: 0.1745\n",
      "Epoch: 1 / 3, Step: 189 / 750 Loss: 0.4069\n",
      "Epoch: 1 / 3, Step: 190 / 750 Loss: 0.1123\n",
      "Epoch: 1 / 3, Step: 191 / 750 Loss: 0.2288\n",
      "Epoch: 1 / 3, Step: 192 / 750 Loss: 0.2027\n",
      "Epoch: 1 / 3, Step: 193 / 750 Loss: 0.4323\n",
      "Epoch: 1 / 3, Step: 194 / 750 Loss: 0.2393\n",
      "Epoch: 1 / 3, Step: 195 / 750 Loss: 0.1126\n",
      "Epoch: 1 / 3, Step: 196 / 750 Loss: 0.3291\n",
      "Epoch: 1 / 3, Step: 197 / 750 Loss: 0.3457\n",
      "Epoch: 1 / 3, Step: 198 / 750 Loss: 0.2443\n",
      "Epoch: 1 / 3, Step: 199 / 750 Loss: 0.1820\n",
      "Epoch: 1 / 3, Step: 200 / 750 Loss: 0.0625\n",
      "Epoch: 1 / 3, Step: 201 / 750 Loss: 0.0783\n",
      "Epoch: 1 / 3, Step: 202 / 750 Loss: 0.2602\n",
      "Epoch: 1 / 3, Step: 203 / 750 Loss: 0.3217\n",
      "Epoch: 1 / 3, Step: 204 / 750 Loss: 0.2153\n",
      "Epoch: 1 / 3, Step: 205 / 750 Loss: 0.2100\n",
      "Epoch: 1 / 3, Step: 206 / 750 Loss: 0.1548\n",
      "Epoch: 1 / 3, Step: 207 / 750 Loss: 0.0979\n",
      "Epoch: 1 / 3, Step: 208 / 750 Loss: 0.2971\n",
      "Epoch: 1 / 3, Step: 209 / 750 Loss: 0.1318\n",
      "Epoch: 1 / 3, Step: 210 / 750 Loss: 0.3381\n",
      "Epoch: 1 / 3, Step: 211 / 750 Loss: 0.3384\n",
      "Epoch: 1 / 3, Step: 212 / 750 Loss: 0.3373\n",
      "Epoch: 1 / 3, Step: 213 / 750 Loss: 0.4436\n",
      "Epoch: 1 / 3, Step: 214 / 750 Loss: 0.2475\n",
      "Epoch: 1 / 3, Step: 215 / 750 Loss: 0.2827\n",
      "Epoch: 1 / 3, Step: 216 / 750 Loss: 0.4483\n",
      "Epoch: 1 / 3, Step: 217 / 750 Loss: 0.3810\n",
      "Epoch: 1 / 3, Step: 218 / 750 Loss: 0.1489\n",
      "Epoch: 1 / 3, Step: 219 / 750 Loss: 0.3785\n",
      "Epoch: 1 / 3, Step: 220 / 750 Loss: 0.3670\n",
      "Epoch: 1 / 3, Step: 221 / 750 Loss: 0.3506\n",
      "Epoch: 1 / 3, Step: 222 / 750 Loss: 0.1727\n",
      "Epoch: 1 / 3, Step: 223 / 750 Loss: 0.4416\n",
      "Epoch: 1 / 3, Step: 224 / 750 Loss: 0.4740\n",
      "Epoch: 1 / 3, Step: 225 / 750 Loss: 0.0746\n",
      "Epoch: 1 / 3, Step: 226 / 750 Loss: 0.3302\n",
      "Epoch: 1 / 3, Step: 227 / 750 Loss: 0.1945\n",
      "Epoch: 1 / 3, Step: 228 / 750 Loss: 0.5440\n",
      "Epoch: 1 / 3, Step: 229 / 750 Loss: 0.1406\n",
      "Epoch: 1 / 3, Step: 230 / 750 Loss: 0.2084\n",
      "Epoch: 1 / 3, Step: 231 / 750 Loss: 0.2705\n",
      "Epoch: 1 / 3, Step: 232 / 750 Loss: 0.2704\n",
      "Epoch: 1 / 3, Step: 233 / 750 Loss: 0.2712\n",
      "Epoch: 1 / 3, Step: 234 / 750 Loss: 0.1308\n",
      "Epoch: 1 / 3, Step: 235 / 750 Loss: 0.2440\n",
      "Epoch: 1 / 3, Step: 236 / 750 Loss: 0.4439\n",
      "Epoch: 1 / 3, Step: 237 / 750 Loss: 0.6953\n",
      "Epoch: 1 / 3, Step: 238 / 750 Loss: 0.3449\n",
      "Epoch: 1 / 3, Step: 239 / 750 Loss: 0.4690\n",
      "Epoch: 1 / 3, Step: 240 / 750 Loss: 0.2331\n",
      "Epoch: 1 / 3, Step: 241 / 750 Loss: 0.2656\n",
      "Epoch: 1 / 3, Step: 242 / 750 Loss: 0.2439\n",
      "Epoch: 1 / 3, Step: 243 / 750 Loss: 0.2739\n",
      "Epoch: 1 / 3, Step: 244 / 750 Loss: 0.1724\n",
      "Epoch: 1 / 3, Step: 245 / 750 Loss: 0.1268\n",
      "Epoch: 1 / 3, Step: 246 / 750 Loss: 0.2305\n",
      "Epoch: 1 / 3, Step: 247 / 750 Loss: 0.2791\n",
      "Epoch: 1 / 3, Step: 248 / 750 Loss: 0.2017\n",
      "Epoch: 1 / 3, Step: 249 / 750 Loss: 0.3137\n",
      "Epoch: 1 / 3, Step: 250 / 750 Loss: 0.2343\n",
      "Epoch: 1 / 3, Step: 251 / 750 Loss: 0.2940\n",
      "Epoch: 1 / 3, Step: 252 / 750 Loss: 0.1591\n",
      "Epoch: 1 / 3, Step: 253 / 750 Loss: 0.1841\n",
      "Epoch: 1 / 3, Step: 254 / 750 Loss: 0.1319\n",
      "Epoch: 1 / 3, Step: 255 / 750 Loss: 0.1475\n",
      "Epoch: 1 / 3, Step: 256 / 750 Loss: 0.4631\n",
      "Epoch: 1 / 3, Step: 257 / 750 Loss: 0.5716\n",
      "Epoch: 1 / 3, Step: 258 / 750 Loss: 0.3010\n",
      "Epoch: 1 / 3, Step: 259 / 750 Loss: 0.3205\n",
      "Epoch: 1 / 3, Step: 260 / 750 Loss: 0.2680\n",
      "Epoch: 1 / 3, Step: 261 / 750 Loss: 0.1292\n",
      "Epoch: 1 / 3, Step: 262 / 750 Loss: 0.2844\n",
      "Epoch: 1 / 3, Step: 263 / 750 Loss: 0.2173\n",
      "Epoch: 1 / 3, Step: 264 / 750 Loss: 0.1697\n",
      "Epoch: 1 / 3, Step: 265 / 750 Loss: 0.1300\n",
      "Epoch: 1 / 3, Step: 266 / 750 Loss: 0.2989\n",
      "Epoch: 1 / 3, Step: 267 / 750 Loss: 0.2063\n",
      "Epoch: 1 / 3, Step: 268 / 750 Loss: 0.3449\n",
      "Epoch: 1 / 3, Step: 269 / 750 Loss: 0.1888\n",
      "Epoch: 1 / 3, Step: 270 / 750 Loss: 0.2962\n",
      "Epoch: 1 / 3, Step: 271 / 750 Loss: 0.3581\n",
      "Epoch: 1 / 3, Step: 272 / 750 Loss: 0.4011\n",
      "Epoch: 1 / 3, Step: 273 / 750 Loss: 0.2589\n",
      "Epoch: 1 / 3, Step: 274 / 750 Loss: 0.3148\n",
      "Epoch: 1 / 3, Step: 275 / 750 Loss: 0.2909\n",
      "Epoch: 1 / 3, Step: 276 / 750 Loss: 0.2098\n",
      "Epoch: 1 / 3, Step: 277 / 750 Loss: 0.2360\n",
      "Epoch: 1 / 3, Step: 278 / 750 Loss: 0.1439\n",
      "Epoch: 1 / 3, Step: 279 / 750 Loss: 0.1915\n",
      "Epoch: 1 / 3, Step: 280 / 750 Loss: 0.2871\n",
      "Epoch: 1 / 3, Step: 281 / 750 Loss: 0.3229\n",
      "Epoch: 1 / 3, Step: 282 / 750 Loss: 0.1871\n",
      "Epoch: 1 / 3, Step: 283 / 750 Loss: 0.2850\n",
      "Epoch: 1 / 3, Step: 284 / 750 Loss: 0.3628\n",
      "Epoch: 1 / 3, Step: 285 / 750 Loss: 0.0725\n",
      "Epoch: 1 / 3, Step: 286 / 750 Loss: 0.3624\n",
      "Epoch: 1 / 3, Step: 287 / 750 Loss: 0.2798\n",
      "Epoch: 1 / 3, Step: 288 / 750 Loss: 0.3058\n",
      "Epoch: 1 / 3, Step: 289 / 750 Loss: 0.1263\n",
      "Epoch: 1 / 3, Step: 290 / 750 Loss: 0.2718\n",
      "Epoch: 1 / 3, Step: 291 / 750 Loss: 0.2693\n",
      "Epoch: 1 / 3, Step: 292 / 750 Loss: 0.3772\n",
      "Epoch: 1 / 3, Step: 293 / 750 Loss: 0.2746\n",
      "Epoch: 1 / 3, Step: 294 / 750 Loss: 0.1622\n",
      "Epoch: 1 / 3, Step: 295 / 750 Loss: 0.4093\n",
      "Epoch: 1 / 3, Step: 296 / 750 Loss: 0.2890\n",
      "Epoch: 1 / 3, Step: 297 / 750 Loss: 0.4612\n",
      "Epoch: 1 / 3, Step: 298 / 750 Loss: 0.1552\n",
      "Epoch: 1 / 3, Step: 299 / 750 Loss: 0.3519\n",
      "Epoch: 1 / 3, Step: 300 / 750 Loss: 0.1670\n",
      "Epoch: 1 / 3, Step: 301 / 750 Loss: 0.3552\n",
      "Epoch: 1 / 3, Step: 302 / 750 Loss: 0.1625\n",
      "Epoch: 1 / 3, Step: 303 / 750 Loss: 0.3524\n",
      "Epoch: 1 / 3, Step: 304 / 750 Loss: 0.2246\n",
      "Epoch: 1 / 3, Step: 305 / 750 Loss: 0.1573\n",
      "Epoch: 1 / 3, Step: 306 / 750 Loss: 0.1249\n",
      "Epoch: 1 / 3, Step: 307 / 750 Loss: 0.3016\n",
      "Epoch: 1 / 3, Step: 308 / 750 Loss: 0.0978\n",
      "Epoch: 1 / 3, Step: 309 / 750 Loss: 0.3723\n",
      "Epoch: 1 / 3, Step: 310 / 750 Loss: 0.2641\n",
      "Epoch: 1 / 3, Step: 311 / 750 Loss: 0.1660\n",
      "Epoch: 1 / 3, Step: 312 / 750 Loss: 0.4447\n",
      "Epoch: 1 / 3, Step: 313 / 750 Loss: 0.4789\n",
      "Epoch: 1 / 3, Step: 314 / 750 Loss: 0.3934\n",
      "Epoch: 1 / 3, Step: 315 / 750 Loss: 0.3423\n",
      "Epoch: 1 / 3, Step: 316 / 750 Loss: 0.2574\n",
      "Epoch: 1 / 3, Step: 317 / 750 Loss: 0.2583\n",
      "Epoch: 1 / 3, Step: 318 / 750 Loss: 0.3313\n",
      "Epoch: 1 / 3, Step: 319 / 750 Loss: 0.3441\n",
      "Epoch: 1 / 3, Step: 320 / 750 Loss: 0.3411\n",
      "Epoch: 1 / 3, Step: 321 / 750 Loss: 0.1657\n",
      "Epoch: 1 / 3, Step: 322 / 750 Loss: 0.3180\n",
      "Epoch: 1 / 3, Step: 323 / 750 Loss: 0.2125\n",
      "Epoch: 1 / 3, Step: 324 / 750 Loss: 0.1086\n",
      "Epoch: 1 / 3, Step: 325 / 750 Loss: 0.2291\n",
      "Epoch: 1 / 3, Step: 326 / 750 Loss: 0.1528\n",
      "Epoch: 1 / 3, Step: 327 / 750 Loss: 0.3624\n",
      "Epoch: 1 / 3, Step: 328 / 750 Loss: 0.4163\n",
      "Epoch: 1 / 3, Step: 329 / 750 Loss: 0.2745\n",
      "Epoch: 1 / 3, Step: 330 / 750 Loss: 0.2254\n",
      "Epoch: 1 / 3, Step: 331 / 750 Loss: 0.1588\n",
      "Epoch: 1 / 3, Step: 332 / 750 Loss: 0.2246\n",
      "Epoch: 1 / 3, Step: 333 / 750 Loss: 0.2092\n",
      "Epoch: 1 / 3, Step: 334 / 750 Loss: 0.2812\n",
      "Epoch: 1 / 3, Step: 335 / 750 Loss: 0.2359\n",
      "Epoch: 1 / 3, Step: 336 / 750 Loss: 0.2197\n",
      "Epoch: 1 / 3, Step: 337 / 750 Loss: 0.2189\n",
      "Epoch: 1 / 3, Step: 338 / 750 Loss: 0.2663\n",
      "Epoch: 1 / 3, Step: 339 / 750 Loss: 0.4274\n",
      "Epoch: 1 / 3, Step: 340 / 750 Loss: 0.2457\n",
      "Epoch: 1 / 3, Step: 341 / 750 Loss: 0.1306\n",
      "Epoch: 1 / 3, Step: 342 / 750 Loss: 0.2556\n",
      "Epoch: 1 / 3, Step: 343 / 750 Loss: 0.2503\n",
      "Epoch: 1 / 3, Step: 344 / 750 Loss: 0.2596\n",
      "Epoch: 1 / 3, Step: 345 / 750 Loss: 0.1951\n",
      "Epoch: 1 / 3, Step: 346 / 750 Loss: 0.4575\n",
      "Epoch: 1 / 3, Step: 347 / 750 Loss: 0.3386\n",
      "Epoch: 1 / 3, Step: 348 / 750 Loss: 0.2789\n",
      "Epoch: 1 / 3, Step: 349 / 750 Loss: 0.4412\n",
      "Epoch: 1 / 3, Step: 350 / 750 Loss: 0.2298\n",
      "Epoch: 1 / 3, Step: 351 / 750 Loss: 0.5529\n",
      "Epoch: 1 / 3, Step: 352 / 750 Loss: 0.3204\n",
      "Epoch: 1 / 3, Step: 353 / 750 Loss: 0.2325\n",
      "Epoch: 1 / 3, Step: 354 / 750 Loss: 0.3221\n",
      "Epoch: 1 / 3, Step: 355 / 750 Loss: 0.3167\n",
      "Epoch: 1 / 3, Step: 356 / 750 Loss: 0.3371\n",
      "Epoch: 1 / 3, Step: 357 / 750 Loss: 0.1408\n",
      "Epoch: 1 / 3, Step: 358 / 750 Loss: 0.0943\n",
      "Epoch: 1 / 3, Step: 359 / 750 Loss: 0.2007\n",
      "Epoch: 1 / 3, Step: 360 / 750 Loss: 0.3192\n",
      "Epoch: 1 / 3, Step: 361 / 750 Loss: 0.2027\n",
      "Epoch: 1 / 3, Step: 362 / 750 Loss: 0.2416\n",
      "Epoch: 1 / 3, Step: 363 / 750 Loss: 0.3225\n",
      "Epoch: 1 / 3, Step: 364 / 750 Loss: 0.2572\n",
      "Epoch: 1 / 3, Step: 365 / 750 Loss: 0.3811\n",
      "Epoch: 1 / 3, Step: 366 / 750 Loss: 0.3373\n",
      "Epoch: 1 / 3, Step: 367 / 750 Loss: 0.0771\n",
      "Epoch: 1 / 3, Step: 368 / 750 Loss: 0.2606\n",
      "Epoch: 1 / 3, Step: 369 / 750 Loss: 0.2093\n",
      "Epoch: 1 / 3, Step: 370 / 750 Loss: 0.2438\n",
      "Epoch: 1 / 3, Step: 371 / 750 Loss: 0.1093\n",
      "Epoch: 1 / 3, Step: 372 / 750 Loss: 0.1771\n",
      "Epoch: 1 / 3, Step: 373 / 750 Loss: 0.3454\n",
      "Epoch: 1 / 3, Step: 374 / 750 Loss: 0.3876\n",
      "Epoch: 1 / 3, Step: 375 / 750 Loss: 0.2032\n",
      "Epoch: 1 / 3, Step: 376 / 750 Loss: 0.1141\n",
      "Epoch: 1 / 3, Step: 377 / 750 Loss: 0.2477\n",
      "Epoch: 1 / 3, Step: 378 / 750 Loss: 0.1647\n",
      "Epoch: 1 / 3, Step: 379 / 750 Loss: 0.3155\n",
      "Epoch: 1 / 3, Step: 380 / 750 Loss: 0.1455\n",
      "Epoch: 1 / 3, Step: 381 / 750 Loss: 0.3410\n",
      "Epoch: 1 / 3, Step: 382 / 750 Loss: 0.2422\n",
      "Epoch: 1 / 3, Step: 383 / 750 Loss: 0.2484\n",
      "Epoch: 1 / 3, Step: 384 / 750 Loss: 0.1631\n",
      "Epoch: 1 / 3, Step: 385 / 750 Loss: 0.2449\n",
      "Epoch: 1 / 3, Step: 386 / 750 Loss: 0.0958\n",
      "Epoch: 1 / 3, Step: 387 / 750 Loss: 0.2426\n",
      "Epoch: 1 / 3, Step: 388 / 750 Loss: 0.1144\n",
      "Epoch: 1 / 3, Step: 389 / 750 Loss: 0.4742\n",
      "Epoch: 1 / 3, Step: 390 / 750 Loss: 0.3552\n",
      "Epoch: 1 / 3, Step: 391 / 750 Loss: 0.0638\n",
      "Epoch: 1 / 3, Step: 392 / 750 Loss: 0.2938\n",
      "Epoch: 1 / 3, Step: 393 / 750 Loss: 0.2452\n",
      "Epoch: 1 / 3, Step: 394 / 750 Loss: 0.1813\n",
      "Epoch: 1 / 3, Step: 395 / 750 Loss: 0.3360\n",
      "Epoch: 1 / 3, Step: 396 / 750 Loss: 0.2417\n",
      "Epoch: 1 / 3, Step: 397 / 750 Loss: 0.1811\n",
      "Epoch: 1 / 3, Step: 398 / 750 Loss: 0.5712\n",
      "Epoch: 1 / 3, Step: 399 / 750 Loss: 0.3886\n",
      "Epoch: 1 / 3, Step: 400 / 750 Loss: 0.1750\n",
      "Epoch: 1 / 3, Step: 401 / 750 Loss: 0.2296\n",
      "Epoch: 1 / 3, Step: 402 / 750 Loss: 0.1992\n",
      "Epoch: 1 / 3, Step: 403 / 750 Loss: 0.2221\n",
      "Epoch: 1 / 3, Step: 404 / 750 Loss: 0.3113\n",
      "Epoch: 1 / 3, Step: 405 / 750 Loss: 0.1417\n",
      "Epoch: 1 / 3, Step: 406 / 750 Loss: 0.1116\n",
      "Epoch: 1 / 3, Step: 407 / 750 Loss: 0.5660\n",
      "Epoch: 1 / 3, Step: 408 / 750 Loss: 0.3297\n",
      "Epoch: 1 / 3, Step: 409 / 750 Loss: 0.1999\n",
      "Epoch: 1 / 3, Step: 410 / 750 Loss: 0.4073\n",
      "Epoch: 1 / 3, Step: 411 / 750 Loss: 0.2667\n",
      "Epoch: 1 / 3, Step: 412 / 750 Loss: 0.2955\n",
      "Epoch: 1 / 3, Step: 413 / 750 Loss: 0.4877\n",
      "Epoch: 1 / 3, Step: 414 / 750 Loss: 0.2968\n",
      "Epoch: 1 / 3, Step: 415 / 750 Loss: 0.2091\n",
      "Epoch: 1 / 3, Step: 416 / 750 Loss: 0.2921\n",
      "Epoch: 1 / 3, Step: 417 / 750 Loss: 0.1826\n",
      "Epoch: 1 / 3, Step: 418 / 750 Loss: 0.3121\n",
      "Epoch: 1 / 3, Step: 419 / 750 Loss: 0.1767\n",
      "Epoch: 1 / 3, Step: 420 / 750 Loss: 0.1478\n",
      "Epoch: 1 / 3, Step: 421 / 750 Loss: 0.2713\n",
      "Epoch: 1 / 3, Step: 422 / 750 Loss: 0.3828\n",
      "Epoch: 1 / 3, Step: 423 / 750 Loss: 0.2727\n",
      "Epoch: 1 / 3, Step: 424 / 750 Loss: 0.2308\n",
      "Epoch: 1 / 3, Step: 425 / 750 Loss: 0.1899\n",
      "Epoch: 1 / 3, Step: 426 / 750 Loss: 0.2718\n",
      "Epoch: 1 / 3, Step: 427 / 750 Loss: 0.1316\n",
      "Epoch: 1 / 3, Step: 428 / 750 Loss: 0.4730\n",
      "Epoch: 1 / 3, Step: 429 / 750 Loss: 0.1682\n",
      "Epoch: 1 / 3, Step: 430 / 750 Loss: 0.0884\n",
      "Epoch: 1 / 3, Step: 431 / 750 Loss: 0.2800\n",
      "Epoch: 1 / 3, Step: 432 / 750 Loss: 0.3216\n",
      "Epoch: 1 / 3, Step: 433 / 750 Loss: 0.5106\n",
      "Epoch: 1 / 3, Step: 434 / 750 Loss: 0.5002\n",
      "Epoch: 1 / 3, Step: 435 / 750 Loss: 0.2543\n",
      "Epoch: 1 / 3, Step: 436 / 750 Loss: 0.1672\n",
      "Epoch: 1 / 3, Step: 437 / 750 Loss: 0.1355\n",
      "Epoch: 1 / 3, Step: 438 / 750 Loss: 0.3187\n",
      "Epoch: 1 / 3, Step: 439 / 750 Loss: 0.3090\n",
      "Epoch: 1 / 3, Step: 440 / 750 Loss: 0.1930\n",
      "Epoch: 1 / 3, Step: 441 / 750 Loss: 0.3130\n",
      "Epoch: 1 / 3, Step: 442 / 750 Loss: 0.0870\n",
      "Epoch: 1 / 3, Step: 443 / 750 Loss: 0.2511\n",
      "Epoch: 1 / 3, Step: 444 / 750 Loss: 0.1792\n",
      "Epoch: 1 / 3, Step: 445 / 750 Loss: 0.3666\n",
      "Epoch: 1 / 3, Step: 446 / 750 Loss: 0.3394\n",
      "Epoch: 1 / 3, Step: 447 / 750 Loss: 0.1526\n",
      "Epoch: 1 / 3, Step: 448 / 750 Loss: 0.4151\n",
      "Epoch: 1 / 3, Step: 449 / 750 Loss: 0.1736\n",
      "Epoch: 1 / 3, Step: 450 / 750 Loss: 0.2629\n",
      "Epoch: 1 / 3, Step: 451 / 750 Loss: 0.1089\n",
      "Epoch: 1 / 3, Step: 452 / 750 Loss: 0.1675\n",
      "Epoch: 1 / 3, Step: 453 / 750 Loss: 0.2271\n",
      "Epoch: 1 / 3, Step: 454 / 750 Loss: 0.2017\n",
      "Epoch: 1 / 3, Step: 455 / 750 Loss: 0.3633\n",
      "Epoch: 1 / 3, Step: 456 / 750 Loss: 0.2526\n",
      "Epoch: 1 / 3, Step: 457 / 750 Loss: 0.1440\n",
      "Epoch: 1 / 3, Step: 458 / 750 Loss: 0.2026\n",
      "Epoch: 1 / 3, Step: 459 / 750 Loss: 0.2736\n",
      "Epoch: 1 / 3, Step: 460 / 750 Loss: 0.1156\n",
      "Epoch: 1 / 3, Step: 461 / 750 Loss: 0.2491\n",
      "Epoch: 1 / 3, Step: 462 / 750 Loss: 0.5806\n",
      "Epoch: 1 / 3, Step: 463 / 750 Loss: 0.2828\n",
      "Epoch: 1 / 3, Step: 464 / 750 Loss: 0.1384\n",
      "Epoch: 1 / 3, Step: 465 / 750 Loss: 0.1917\n",
      "Epoch: 1 / 3, Step: 466 / 750 Loss: 0.3355\n",
      "Epoch: 1 / 3, Step: 467 / 750 Loss: 0.3299\n",
      "Epoch: 1 / 3, Step: 468 / 750 Loss: 0.3530\n",
      "Epoch: 1 / 3, Step: 469 / 750 Loss: 0.2513\n",
      "Epoch: 1 / 3, Step: 470 / 750 Loss: 0.3385\n",
      "Epoch: 1 / 3, Step: 471 / 750 Loss: 0.4252\n",
      "Epoch: 1 / 3, Step: 472 / 750 Loss: 0.6554\n",
      "Epoch: 1 / 3, Step: 473 / 750 Loss: 0.3520\n",
      "Epoch: 1 / 3, Step: 474 / 750 Loss: 0.3370\n",
      "Epoch: 1 / 3, Step: 475 / 750 Loss: 0.2041\n",
      "Epoch: 1 / 3, Step: 476 / 750 Loss: 0.1819\n",
      "Epoch: 1 / 3, Step: 477 / 750 Loss: 0.4235\n",
      "Epoch: 1 / 3, Step: 478 / 750 Loss: 0.1079\n",
      "Epoch: 1 / 3, Step: 479 / 750 Loss: 0.2754\n",
      "Epoch: 1 / 3, Step: 480 / 750 Loss: 0.3175\n",
      "Epoch: 1 / 3, Step: 481 / 750 Loss: 0.3269\n",
      "Epoch: 1 / 3, Step: 482 / 750 Loss: 0.4810\n",
      "Epoch: 1 / 3, Step: 483 / 750 Loss: 0.1457\n",
      "Epoch: 1 / 3, Step: 484 / 750 Loss: 0.4723\n",
      "Epoch: 1 / 3, Step: 485 / 750 Loss: 0.2591\n",
      "Epoch: 1 / 3, Step: 486 / 750 Loss: 0.2556\n",
      "Epoch: 1 / 3, Step: 487 / 750 Loss: 0.1936\n",
      "Epoch: 1 / 3, Step: 488 / 750 Loss: 0.1638\n",
      "Epoch: 1 / 3, Step: 489 / 750 Loss: 0.4438\n",
      "Epoch: 1 / 3, Step: 490 / 750 Loss: 0.2456\n",
      "Epoch: 1 / 3, Step: 491 / 750 Loss: 0.3521\n",
      "Epoch: 1 / 3, Step: 492 / 750 Loss: 0.2562\n",
      "Epoch: 1 / 3, Step: 493 / 750 Loss: 0.2319\n",
      "Epoch: 1 / 3, Step: 494 / 750 Loss: 0.2116\n",
      "Epoch: 1 / 3, Step: 495 / 750 Loss: 0.2658\n",
      "Epoch: 1 / 3, Step: 496 / 750 Loss: 0.3110\n",
      "Epoch: 1 / 3, Step: 497 / 750 Loss: 0.2759\n",
      "Epoch: 1 / 3, Step: 498 / 750 Loss: 0.2455\n",
      "Epoch: 1 / 3, Step: 499 / 750 Loss: 0.2444\n",
      "Epoch: 1 / 3, Step: 500 / 750 Loss: 0.3028\n",
      "Epoch: 1 / 3, Step: 501 / 750 Loss: 0.1709\n",
      "Epoch: 1 / 3, Step: 502 / 750 Loss: 0.3654\n",
      "Epoch: 1 / 3, Step: 503 / 750 Loss: 0.1112\n",
      "Epoch: 1 / 3, Step: 504 / 750 Loss: 0.2493\n",
      "Epoch: 1 / 3, Step: 505 / 750 Loss: 0.6233\n",
      "Epoch: 1 / 3, Step: 506 / 750 Loss: 0.2532\n",
      "Epoch: 1 / 3, Step: 507 / 750 Loss: 0.0851\n",
      "Epoch: 1 / 3, Step: 508 / 750 Loss: 0.3295\n",
      "Epoch: 1 / 3, Step: 509 / 750 Loss: 0.0775\n",
      "Epoch: 1 / 3, Step: 510 / 750 Loss: 0.5037\n",
      "Epoch: 1 / 3, Step: 511 / 750 Loss: 0.6274\n",
      "Epoch: 1 / 3, Step: 512 / 750 Loss: 0.2433\n",
      "Epoch: 1 / 3, Step: 513 / 750 Loss: 0.2439\n",
      "Epoch: 1 / 3, Step: 514 / 750 Loss: 0.3817\n",
      "Epoch: 1 / 3, Step: 515 / 750 Loss: 0.1434\n",
      "Epoch: 1 / 3, Step: 516 / 750 Loss: 0.3747\n",
      "Epoch: 1 / 3, Step: 517 / 750 Loss: 0.1788\n",
      "Epoch: 1 / 3, Step: 518 / 750 Loss: 0.3226\n",
      "Epoch: 1 / 3, Step: 519 / 750 Loss: 0.1211\n",
      "Epoch: 1 / 3, Step: 520 / 750 Loss: 0.2267\n",
      "Epoch: 1 / 3, Step: 521 / 750 Loss: 0.2831\n",
      "Epoch: 1 / 3, Step: 522 / 750 Loss: 0.2810\n",
      "Epoch: 1 / 3, Step: 523 / 750 Loss: 0.3660\n",
      "Epoch: 1 / 3, Step: 524 / 750 Loss: 0.1599\n",
      "Epoch: 1 / 3, Step: 525 / 750 Loss: 0.2454\n",
      "Epoch: 1 / 3, Step: 526 / 750 Loss: 0.2487\n",
      "Epoch: 1 / 3, Step: 527 / 750 Loss: 0.2258\n",
      "Epoch: 1 / 3, Step: 528 / 750 Loss: 0.2801\n",
      "Epoch: 1 / 3, Step: 529 / 750 Loss: 0.0968\n",
      "Epoch: 1 / 3, Step: 530 / 750 Loss: 0.1221\n",
      "Epoch: 1 / 3, Step: 531 / 750 Loss: 0.3452\n",
      "Epoch: 1 / 3, Step: 532 / 750 Loss: 0.1629\n",
      "Epoch: 1 / 3, Step: 533 / 750 Loss: 0.0962\n",
      "Epoch: 1 / 3, Step: 534 / 750 Loss: 0.2037\n",
      "Epoch: 1 / 3, Step: 535 / 750 Loss: 0.4606\n",
      "Epoch: 1 / 3, Step: 536 / 750 Loss: 0.1296\n",
      "Epoch: 1 / 3, Step: 537 / 750 Loss: 0.1208\n",
      "Epoch: 1 / 3, Step: 538 / 750 Loss: 0.2941\n",
      "Epoch: 1 / 3, Step: 539 / 750 Loss: 0.4616\n",
      "Epoch: 1 / 3, Step: 540 / 750 Loss: 0.2204\n",
      "Epoch: 1 / 3, Step: 541 / 750 Loss: 0.2661\n",
      "Epoch: 1 / 3, Step: 542 / 750 Loss: 0.3185\n",
      "Epoch: 1 / 3, Step: 543 / 750 Loss: 0.1064\n",
      "Epoch: 1 / 3, Step: 544 / 750 Loss: 0.3154\n",
      "Epoch: 1 / 3, Step: 545 / 750 Loss: 0.1463\n",
      "Epoch: 1 / 3, Step: 546 / 750 Loss: 0.7281\n",
      "Epoch: 1 / 3, Step: 547 / 750 Loss: 0.2709\n",
      "Epoch: 1 / 3, Step: 548 / 750 Loss: 0.2614\n",
      "Epoch: 1 / 3, Step: 549 / 750 Loss: 0.3083\n",
      "Epoch: 1 / 3, Step: 550 / 750 Loss: 0.1907\n",
      "Epoch: 1 / 3, Step: 551 / 750 Loss: 0.0878\n",
      "Epoch: 1 / 3, Step: 552 / 750 Loss: 0.2395\n",
      "Epoch: 1 / 3, Step: 553 / 750 Loss: 0.2035\n",
      "Epoch: 1 / 3, Step: 554 / 750 Loss: 0.1939\n",
      "Epoch: 1 / 3, Step: 555 / 750 Loss: 0.3057\n",
      "Epoch: 1 / 3, Step: 556 / 750 Loss: 0.1153\n",
      "Epoch: 1 / 3, Step: 557 / 750 Loss: 0.1658\n",
      "Epoch: 1 / 3, Step: 558 / 750 Loss: 0.1197\n",
      "Epoch: 1 / 3, Step: 559 / 750 Loss: 0.2179\n",
      "Epoch: 1 / 3, Step: 560 / 750 Loss: 0.0848\n",
      "Epoch: 1 / 3, Step: 561 / 750 Loss: 0.3154\n",
      "Epoch: 1 / 3, Step: 562 / 750 Loss: 0.2161\n",
      "Epoch: 1 / 3, Step: 563 / 750 Loss: 0.3454\n",
      "Epoch: 1 / 3, Step: 564 / 750 Loss: 0.2359\n",
      "Epoch: 1 / 3, Step: 565 / 750 Loss: 0.1618\n",
      "Epoch: 1 / 3, Step: 566 / 750 Loss: 0.1353\n",
      "Epoch: 1 / 3, Step: 567 / 750 Loss: 0.0875\n",
      "Epoch: 1 / 3, Step: 568 / 750 Loss: 0.5944\n",
      "Epoch: 1 / 3, Step: 569 / 750 Loss: 0.2404\n",
      "Epoch: 1 / 3, Step: 570 / 750 Loss: 0.3644\n",
      "Epoch: 1 / 3, Step: 571 / 750 Loss: 0.3491\n",
      "Epoch: 1 / 3, Step: 572 / 750 Loss: 0.4437\n",
      "Epoch: 1 / 3, Step: 573 / 750 Loss: 0.6173\n",
      "Epoch: 1 / 3, Step: 574 / 750 Loss: 0.5749\n",
      "Epoch: 1 / 3, Step: 575 / 750 Loss: 0.2184\n",
      "Epoch: 1 / 3, Step: 576 / 750 Loss: 0.1820\n",
      "Epoch: 1 / 3, Step: 577 / 750 Loss: 0.1138\n",
      "Epoch: 1 / 3, Step: 578 / 750 Loss: 0.3010\n",
      "Epoch: 1 / 3, Step: 579 / 750 Loss: 0.1272\n",
      "Epoch: 1 / 3, Step: 580 / 750 Loss: 0.2663\n",
      "Epoch: 1 / 3, Step: 581 / 750 Loss: 0.1617\n",
      "Epoch: 1 / 3, Step: 582 / 750 Loss: 0.2064\n",
      "Epoch: 1 / 3, Step: 583 / 750 Loss: 0.3078\n",
      "Epoch: 1 / 3, Step: 584 / 750 Loss: 0.2202\n",
      "Epoch: 1 / 3, Step: 585 / 750 Loss: 0.2633\n",
      "Epoch: 1 / 3, Step: 586 / 750 Loss: 0.1179\n",
      "Epoch: 1 / 3, Step: 587 / 750 Loss: 0.2177\n",
      "Epoch: 1 / 3, Step: 588 / 750 Loss: 0.3545\n",
      "Epoch: 1 / 3, Step: 589 / 750 Loss: 0.3500\n",
      "Epoch: 1 / 3, Step: 590 / 750 Loss: 0.1574\n",
      "Epoch: 1 / 3, Step: 591 / 750 Loss: 0.3428\n",
      "Epoch: 1 / 3, Step: 592 / 750 Loss: 0.2576\n",
      "Epoch: 1 / 3, Step: 593 / 750 Loss: 0.0807\n",
      "Epoch: 1 / 3, Step: 594 / 750 Loss: 0.1755\n",
      "Epoch: 1 / 3, Step: 595 / 750 Loss: 0.3121\n",
      "Epoch: 1 / 3, Step: 596 / 750 Loss: 0.2041\n",
      "Epoch: 1 / 3, Step: 597 / 750 Loss: 0.3778\n",
      "Epoch: 1 / 3, Step: 598 / 750 Loss: 0.2483\n",
      "Epoch: 1 / 3, Step: 599 / 750 Loss: 0.2843\n",
      "Epoch: 1 / 3, Step: 600 / 750 Loss: 0.2166\n",
      "Epoch: 1 / 3, Step: 601 / 750 Loss: 0.3323\n",
      "Epoch: 1 / 3, Step: 602 / 750 Loss: 0.4086\n",
      "Epoch: 1 / 3, Step: 603 / 750 Loss: 0.3304\n",
      "Epoch: 1 / 3, Step: 604 / 750 Loss: 0.3309\n",
      "Epoch: 1 / 3, Step: 605 / 750 Loss: 0.2220\n",
      "Epoch: 1 / 3, Step: 606 / 750 Loss: 0.4168\n",
      "Epoch: 1 / 3, Step: 607 / 750 Loss: 0.2928\n",
      "Epoch: 1 / 3, Step: 608 / 750 Loss: 0.3076\n",
      "Epoch: 1 / 3, Step: 609 / 750 Loss: 0.2687\n",
      "Epoch: 1 / 3, Step: 610 / 750 Loss: 0.0883\n",
      "Epoch: 1 / 3, Step: 611 / 750 Loss: 0.1342\n",
      "Epoch: 1 / 3, Step: 612 / 750 Loss: 0.3577\n",
      "Epoch: 1 / 3, Step: 613 / 750 Loss: 0.4452\n",
      "Epoch: 1 / 3, Step: 614 / 750 Loss: 0.1291\n",
      "Epoch: 1 / 3, Step: 615 / 750 Loss: 0.2099\n",
      "Epoch: 1 / 3, Step: 616 / 750 Loss: 0.3153\n",
      "Epoch: 1 / 3, Step: 617 / 750 Loss: 0.2449\n",
      "Epoch: 1 / 3, Step: 618 / 750 Loss: 0.1686\n",
      "Epoch: 1 / 3, Step: 619 / 750 Loss: 0.2628\n",
      "Epoch: 1 / 3, Step: 620 / 750 Loss: 0.1067\n",
      "Epoch: 1 / 3, Step: 621 / 750 Loss: 0.2187\n",
      "Epoch: 1 / 3, Step: 622 / 750 Loss: 0.3274\n",
      "Epoch: 1 / 3, Step: 623 / 750 Loss: 0.4042\n",
      "Epoch: 1 / 3, Step: 624 / 750 Loss: 0.2401\n",
      "Epoch: 1 / 3, Step: 625 / 750 Loss: 0.1693\n",
      "Epoch: 1 / 3, Step: 626 / 750 Loss: 0.3399\n",
      "Epoch: 1 / 3, Step: 627 / 750 Loss: 0.1473\n",
      "Epoch: 1 / 3, Step: 628 / 750 Loss: 0.1807\n",
      "Epoch: 1 / 3, Step: 629 / 750 Loss: 0.3870\n",
      "Epoch: 1 / 3, Step: 630 / 750 Loss: 0.2124\n",
      "Epoch: 1 / 3, Step: 631 / 750 Loss: 0.4945\n",
      "Epoch: 1 / 3, Step: 632 / 750 Loss: 0.1804\n",
      "Epoch: 1 / 3, Step: 633 / 750 Loss: 0.2035\n",
      "Epoch: 1 / 3, Step: 634 / 750 Loss: 0.0596\n",
      "Epoch: 1 / 3, Step: 635 / 750 Loss: 0.4376\n",
      "Epoch: 1 / 3, Step: 636 / 750 Loss: 0.2550\n",
      "Epoch: 1 / 3, Step: 637 / 750 Loss: 0.1432\n",
      "Epoch: 1 / 3, Step: 638 / 750 Loss: 0.2077\n",
      "Epoch: 1 / 3, Step: 639 / 750 Loss: 0.3267\n",
      "Epoch: 1 / 3, Step: 640 / 750 Loss: 0.3190\n",
      "Epoch: 1 / 3, Step: 641 / 750 Loss: 0.0873\n",
      "Epoch: 1 / 3, Step: 642 / 750 Loss: 0.2230\n",
      "Epoch: 1 / 3, Step: 643 / 750 Loss: 0.2713\n",
      "Epoch: 1 / 3, Step: 644 / 750 Loss: 0.1646\n",
      "Epoch: 1 / 3, Step: 645 / 750 Loss: 0.4524\n",
      "Epoch: 1 / 3, Step: 646 / 750 Loss: 0.4369\n",
      "Epoch: 1 / 3, Step: 647 / 750 Loss: 0.1165\n",
      "Epoch: 1 / 3, Step: 648 / 750 Loss: 0.2389\n",
      "Epoch: 1 / 3, Step: 649 / 750 Loss: 0.1033\n",
      "Epoch: 1 / 3, Step: 650 / 750 Loss: 0.4055\n",
      "Epoch: 1 / 3, Step: 651 / 750 Loss: 0.3112\n",
      "Epoch: 1 / 3, Step: 652 / 750 Loss: 0.3261\n",
      "Epoch: 1 / 3, Step: 653 / 750 Loss: 0.0821\n",
      "Epoch: 1 / 3, Step: 654 / 750 Loss: 0.2593\n",
      "Epoch: 1 / 3, Step: 655 / 750 Loss: 0.1755\n",
      "Epoch: 1 / 3, Step: 656 / 750 Loss: 0.4611\n",
      "Epoch: 1 / 3, Step: 657 / 750 Loss: 0.1315\n",
      "Epoch: 1 / 3, Step: 658 / 750 Loss: 0.2107\n",
      "Epoch: 1 / 3, Step: 659 / 750 Loss: 0.2417\n",
      "Epoch: 1 / 3, Step: 660 / 750 Loss: 0.1762\n",
      "Epoch: 1 / 3, Step: 661 / 750 Loss: 0.0943\n",
      "Epoch: 1 / 3, Step: 662 / 750 Loss: 0.2576\n",
      "Epoch: 1 / 3, Step: 663 / 750 Loss: 0.1486\n",
      "Epoch: 1 / 3, Step: 664 / 750 Loss: 0.1350\n",
      "Epoch: 1 / 3, Step: 665 / 750 Loss: 0.2797\n",
      "Epoch: 1 / 3, Step: 666 / 750 Loss: 0.2331\n",
      "Epoch: 1 / 3, Step: 667 / 750 Loss: 0.1084\n",
      "Epoch: 1 / 3, Step: 668 / 750 Loss: 0.2883\n",
      "Epoch: 1 / 3, Step: 669 / 750 Loss: 0.2932\n",
      "Epoch: 1 / 3, Step: 670 / 750 Loss: 0.3918\n",
      "Epoch: 1 / 3, Step: 671 / 750 Loss: 0.3697\n",
      "Epoch: 1 / 3, Step: 672 / 750 Loss: 0.3960\n",
      "Epoch: 1 / 3, Step: 673 / 750 Loss: 0.4521\n",
      "Epoch: 1 / 3, Step: 674 / 750 Loss: 0.3700\n",
      "Epoch: 1 / 3, Step: 675 / 750 Loss: 0.1896\n",
      "Epoch: 1 / 3, Step: 676 / 750 Loss: 0.1307\n",
      "Epoch: 1 / 3, Step: 677 / 750 Loss: 0.2619\n",
      "Epoch: 1 / 3, Step: 678 / 750 Loss: 0.2539\n",
      "Epoch: 1 / 3, Step: 679 / 750 Loss: 0.2190\n",
      "Epoch: 1 / 3, Step: 680 / 750 Loss: 0.1732\n",
      "Epoch: 1 / 3, Step: 681 / 750 Loss: 0.4614\n",
      "Epoch: 1 / 3, Step: 682 / 750 Loss: 0.2563\n",
      "Epoch: 1 / 3, Step: 683 / 750 Loss: 0.3799\n",
      "Epoch: 1 / 3, Step: 684 / 750 Loss: 0.4293\n",
      "Epoch: 1 / 3, Step: 685 / 750 Loss: 0.1588\n",
      "Epoch: 1 / 3, Step: 686 / 750 Loss: 0.4176\n",
      "Epoch: 1 / 3, Step: 687 / 750 Loss: 0.1730\n",
      "Epoch: 1 / 3, Step: 688 / 750 Loss: 0.1714\n",
      "Epoch: 1 / 3, Step: 689 / 750 Loss: 0.4018\n",
      "Epoch: 1 / 3, Step: 690 / 750 Loss: 0.2607\n",
      "Epoch: 1 / 3, Step: 691 / 750 Loss: 0.2578\n",
      "Epoch: 1 / 3, Step: 692 / 750 Loss: 0.2573\n",
      "Epoch: 1 / 3, Step: 693 / 750 Loss: 0.1132\n",
      "Epoch: 1 / 3, Step: 694 / 750 Loss: 0.2215\n",
      "Epoch: 1 / 3, Step: 695 / 750 Loss: 0.4364\n",
      "Epoch: 1 / 3, Step: 696 / 750 Loss: 0.2155\n",
      "Epoch: 1 / 3, Step: 697 / 750 Loss: 0.2287\n",
      "Epoch: 1 / 3, Step: 698 / 750 Loss: 0.0800\n",
      "Epoch: 1 / 3, Step: 699 / 750 Loss: 0.0799\n",
      "Epoch: 1 / 3, Step: 700 / 750 Loss: 0.0887\n",
      "Epoch: 1 / 3, Step: 701 / 750 Loss: 0.2042\n",
      "Epoch: 1 / 3, Step: 702 / 750 Loss: 0.1044\n",
      "Epoch: 1 / 3, Step: 703 / 750 Loss: 0.3002\n",
      "Epoch: 1 / 3, Step: 704 / 750 Loss: 0.3457\n",
      "Epoch: 1 / 3, Step: 705 / 750 Loss: 0.1398\n",
      "Epoch: 1 / 3, Step: 706 / 750 Loss: 0.2783\n",
      "Epoch: 1 / 3, Step: 707 / 750 Loss: 0.2345\n",
      "Epoch: 1 / 3, Step: 708 / 750 Loss: 0.1645\n",
      "Epoch: 1 / 3, Step: 709 / 750 Loss: 0.1774\n",
      "Epoch: 1 / 3, Step: 710 / 750 Loss: 0.6410\n",
      "Epoch: 1 / 3, Step: 711 / 750 Loss: 0.1467\n",
      "Epoch: 1 / 3, Step: 712 / 750 Loss: 0.2253\n",
      "Epoch: 1 / 3, Step: 713 / 750 Loss: 0.3708\n",
      "Epoch: 1 / 3, Step: 714 / 750 Loss: 0.2939\n",
      "Epoch: 1 / 3, Step: 715 / 750 Loss: 0.1918\n",
      "Epoch: 1 / 3, Step: 716 / 750 Loss: 0.2262\n",
      "Epoch: 1 / 3, Step: 717 / 750 Loss: 0.2117\n",
      "Epoch: 1 / 3, Step: 718 / 750 Loss: 0.2018\n",
      "Epoch: 1 / 3, Step: 719 / 750 Loss: 0.1879\n",
      "Epoch: 1 / 3, Step: 720 / 750 Loss: 0.1664\n",
      "Epoch: 1 / 3, Step: 721 / 750 Loss: 0.2027\n",
      "Epoch: 1 / 3, Step: 722 / 750 Loss: 0.3000\n",
      "Epoch: 1 / 3, Step: 723 / 750 Loss: 0.1138\n",
      "Epoch: 1 / 3, Step: 724 / 750 Loss: 0.3265\n",
      "Epoch: 1 / 3, Step: 725 / 750 Loss: 0.0921\n",
      "Epoch: 1 / 3, Step: 726 / 750 Loss: 0.2759\n",
      "Epoch: 1 / 3, Step: 727 / 750 Loss: 0.2563\n",
      "Epoch: 1 / 3, Step: 728 / 750 Loss: 0.2151\n",
      "Epoch: 1 / 3, Step: 729 / 750 Loss: 0.1953\n",
      "Epoch: 1 / 3, Step: 730 / 750 Loss: 0.3614\n",
      "Epoch: 1 / 3, Step: 731 / 750 Loss: 0.0707\n",
      "Epoch: 1 / 3, Step: 732 / 750 Loss: 0.1715\n",
      "Epoch: 1 / 3, Step: 733 / 750 Loss: 0.1293\n",
      "Epoch: 1 / 3, Step: 734 / 750 Loss: 0.3355\n",
      "Epoch: 1 / 3, Step: 735 / 750 Loss: 0.2984\n",
      "Epoch: 1 / 3, Step: 736 / 750 Loss: 0.4843\n",
      "Epoch: 1 / 3, Step: 737 / 750 Loss: 0.3684\n",
      "Epoch: 1 / 3, Step: 738 / 750 Loss: 0.2238\n",
      "Epoch: 1 / 3, Step: 739 / 750 Loss: 0.3105\n",
      "Epoch: 1 / 3, Step: 740 / 750 Loss: 0.2228\n",
      "Epoch: 1 / 3, Step: 741 / 750 Loss: 0.5827\n",
      "Epoch: 1 / 3, Step: 742 / 750 Loss: 0.2071\n",
      "Epoch: 1 / 3, Step: 743 / 750 Loss: 0.3297\n",
      "Epoch: 1 / 3, Step: 744 / 750 Loss: 0.2212\n",
      "Epoch: 1 / 3, Step: 745 / 750 Loss: 0.1015\n",
      "Epoch: 1 / 3, Step: 746 / 750 Loss: 0.4480\n",
      "Epoch: 1 / 3, Step: 747 / 750 Loss: 0.1527\n",
      "Epoch: 1 / 3, Step: 748 / 750 Loss: 0.0717\n",
      "Epoch: 1 / 3, Step: 749 / 750 Loss: 0.4181\n",
      "Epoch: 2 / 3, Step: 0 / 750 Loss: 0.1454\n",
      "Epoch: 2 / 3, Step: 1 / 750 Loss: 0.2339\n",
      "Epoch: 2 / 3, Step: 2 / 750 Loss: 0.2856\n",
      "Epoch: 2 / 3, Step: 3 / 750 Loss: 0.1395\n",
      "Epoch: 2 / 3, Step: 4 / 750 Loss: 0.1428\n",
      "Epoch: 2 / 3, Step: 5 / 750 Loss: 0.2991\n",
      "Epoch: 2 / 3, Step: 6 / 750 Loss: 0.1863\n",
      "Epoch: 2 / 3, Step: 7 / 750 Loss: 0.3687\n",
      "Epoch: 2 / 3, Step: 8 / 750 Loss: 0.1188\n",
      "Epoch: 2 / 3, Step: 9 / 750 Loss: 0.2624\n",
      "Epoch: 2 / 3, Step: 10 / 750 Loss: 0.3367\n",
      "Epoch: 2 / 3, Step: 11 / 750 Loss: 0.3292\n",
      "Epoch: 2 / 3, Step: 12 / 750 Loss: 0.3590\n",
      "Epoch: 2 / 3, Step: 13 / 750 Loss: 0.1919\n",
      "Epoch: 2 / 3, Step: 14 / 750 Loss: 0.0573\n",
      "Epoch: 2 / 3, Step: 15 / 750 Loss: 0.1786\n",
      "Epoch: 2 / 3, Step: 16 / 750 Loss: 0.2489\n",
      "Epoch: 2 / 3, Step: 17 / 750 Loss: 0.1749\n",
      "Epoch: 2 / 3, Step: 18 / 750 Loss: 0.1792\n",
      "Epoch: 2 / 3, Step: 19 / 750 Loss: 0.1613\n",
      "Epoch: 2 / 3, Step: 20 / 750 Loss: 0.2800\n",
      "Epoch: 2 / 3, Step: 21 / 750 Loss: 0.0562\n",
      "Epoch: 2 / 3, Step: 22 / 750 Loss: 0.0885\n",
      "Epoch: 2 / 3, Step: 23 / 750 Loss: 0.3804\n",
      "Epoch: 2 / 3, Step: 24 / 750 Loss: 0.4781\n",
      "Epoch: 2 / 3, Step: 25 / 750 Loss: 0.0931\n",
      "Epoch: 2 / 3, Step: 26 / 750 Loss: 0.4169\n",
      "Epoch: 2 / 3, Step: 27 / 750 Loss: 0.2403\n",
      "Epoch: 2 / 3, Step: 28 / 750 Loss: 0.1731\n",
      "Epoch: 2 / 3, Step: 29 / 750 Loss: 0.1026\n",
      "Epoch: 2 / 3, Step: 30 / 750 Loss: 0.1764\n",
      "Epoch: 2 / 3, Step: 31 / 750 Loss: 0.1454\n",
      "Epoch: 2 / 3, Step: 32 / 750 Loss: 0.3286\n",
      "Epoch: 2 / 3, Step: 33 / 750 Loss: 0.3048\n",
      "Epoch: 2 / 3, Step: 34 / 750 Loss: 0.2438\n",
      "Epoch: 2 / 3, Step: 35 / 750 Loss: 0.1039\n",
      "Epoch: 2 / 3, Step: 36 / 750 Loss: 0.1299\n",
      "Epoch: 2 / 3, Step: 37 / 750 Loss: 0.2376\n",
      "Epoch: 2 / 3, Step: 38 / 750 Loss: 0.1167\n",
      "Epoch: 2 / 3, Step: 39 / 750 Loss: 0.0781\n",
      "Epoch: 2 / 3, Step: 40 / 750 Loss: 0.1222\n",
      "Epoch: 2 / 3, Step: 41 / 750 Loss: 0.2591\n",
      "Epoch: 2 / 3, Step: 42 / 750 Loss: 0.1870\n",
      "Epoch: 2 / 3, Step: 43 / 750 Loss: 0.0577\n",
      "Epoch: 2 / 3, Step: 44 / 750 Loss: 0.1476\n",
      "Epoch: 2 / 3, Step: 45 / 750 Loss: 0.1519\n",
      "Epoch: 2 / 3, Step: 46 / 750 Loss: 0.4270\n",
      "Epoch: 2 / 3, Step: 47 / 750 Loss: 0.2589\n",
      "Epoch: 2 / 3, Step: 48 / 750 Loss: 0.1501\n",
      "Epoch: 2 / 3, Step: 49 / 750 Loss: 0.1470\n",
      "Epoch: 2 / 3, Step: 50 / 750 Loss: 0.2611\n",
      "Epoch: 2 / 3, Step: 51 / 750 Loss: 0.1872\n",
      "Epoch: 2 / 3, Step: 52 / 750 Loss: 0.1032\n",
      "Epoch: 2 / 3, Step: 53 / 750 Loss: 0.2427\n",
      "Epoch: 2 / 3, Step: 54 / 750 Loss: 0.1206\n",
      "Epoch: 2 / 3, Step: 55 / 750 Loss: 0.3069\n",
      "Epoch: 2 / 3, Step: 56 / 750 Loss: 0.1647\n",
      "Epoch: 2 / 3, Step: 57 / 750 Loss: 0.0954\n",
      "Epoch: 2 / 3, Step: 58 / 750 Loss: 0.1353\n",
      "Epoch: 2 / 3, Step: 59 / 750 Loss: 0.3349\n",
      "Epoch: 2 / 3, Step: 60 / 750 Loss: 0.3028\n",
      "Epoch: 2 / 3, Step: 61 / 750 Loss: 0.2356\n",
      "Epoch: 2 / 3, Step: 62 / 750 Loss: 0.4577\n",
      "Epoch: 2 / 3, Step: 63 / 750 Loss: 0.1909\n",
      "Epoch: 2 / 3, Step: 64 / 750 Loss: 0.1799\n",
      "Epoch: 2 / 3, Step: 65 / 750 Loss: 0.2962\n",
      "Epoch: 2 / 3, Step: 66 / 750 Loss: 0.3561\n",
      "Epoch: 2 / 3, Step: 67 / 750 Loss: 0.3538\n",
      "Epoch: 2 / 3, Step: 68 / 750 Loss: 0.2065\n",
      "Epoch: 2 / 3, Step: 69 / 750 Loss: 0.1436\n",
      "Epoch: 2 / 3, Step: 70 / 750 Loss: 0.0608\n",
      "Epoch: 2 / 3, Step: 71 / 750 Loss: 0.4904\n",
      "Epoch: 2 / 3, Step: 72 / 750 Loss: 0.3871\n",
      "Epoch: 2 / 3, Step: 73 / 750 Loss: 0.1631\n",
      "Epoch: 2 / 3, Step: 74 / 750 Loss: 0.4758\n",
      "Epoch: 2 / 3, Step: 75 / 750 Loss: 0.3622\n",
      "Epoch: 2 / 3, Step: 76 / 750 Loss: 0.1484\n",
      "Epoch: 2 / 3, Step: 77 / 750 Loss: 0.2922\n",
      "Epoch: 2 / 3, Step: 78 / 750 Loss: 0.2930\n",
      "Epoch: 2 / 3, Step: 79 / 750 Loss: 0.0938\n",
      "Epoch: 2 / 3, Step: 80 / 750 Loss: 0.0917\n",
      "Epoch: 2 / 3, Step: 81 / 750 Loss: 0.1654\n",
      "Epoch: 2 / 3, Step: 82 / 750 Loss: 0.1596\n",
      "Epoch: 2 / 3, Step: 83 / 750 Loss: 0.3417\n",
      "Epoch: 2 / 3, Step: 84 / 750 Loss: 0.1500\n",
      "Epoch: 2 / 3, Step: 85 / 750 Loss: 0.2463\n",
      "Epoch: 2 / 3, Step: 86 / 750 Loss: 0.1622\n",
      "Epoch: 2 / 3, Step: 87 / 750 Loss: 0.2540\n",
      "Epoch: 2 / 3, Step: 88 / 750 Loss: 0.3556\n",
      "Epoch: 2 / 3, Step: 89 / 750 Loss: 0.1198\n",
      "Epoch: 2 / 3, Step: 90 / 750 Loss: 0.2073\n",
      "Epoch: 2 / 3, Step: 91 / 750 Loss: 0.3215\n",
      "Epoch: 2 / 3, Step: 92 / 750 Loss: 0.1960\n",
      "Epoch: 2 / 3, Step: 93 / 750 Loss: 0.1726\n",
      "Epoch: 2 / 3, Step: 94 / 750 Loss: 0.0943\n",
      "Epoch: 2 / 3, Step: 95 / 750 Loss: 0.2483\n",
      "Epoch: 2 / 3, Step: 96 / 750 Loss: 0.2643\n",
      "Epoch: 2 / 3, Step: 97 / 750 Loss: 0.1498\n",
      "Epoch: 2 / 3, Step: 98 / 750 Loss: 0.1851\n",
      "Epoch: 2 / 3, Step: 99 / 750 Loss: 0.1265\n",
      "Epoch: 2 / 3, Step: 100 / 750 Loss: 0.1520\n",
      "Epoch: 2 / 3, Step: 101 / 750 Loss: 0.2578\n",
      "Epoch: 2 / 3, Step: 102 / 750 Loss: 0.2317\n",
      "Epoch: 2 / 3, Step: 103 / 750 Loss: 0.6007\n",
      "Epoch: 2 / 3, Step: 104 / 750 Loss: 0.0521\n",
      "Epoch: 2 / 3, Step: 105 / 750 Loss: 0.0330\n",
      "Epoch: 2 / 3, Step: 106 / 750 Loss: 0.2283\n",
      "Epoch: 2 / 3, Step: 107 / 750 Loss: 0.2098\n",
      "Epoch: 2 / 3, Step: 108 / 750 Loss: 0.4564\n",
      "Epoch: 2 / 3, Step: 109 / 750 Loss: 0.2620\n",
      "Epoch: 2 / 3, Step: 110 / 750 Loss: 0.3347\n",
      "Epoch: 2 / 3, Step: 111 / 750 Loss: 0.1656\n",
      "Epoch: 2 / 3, Step: 112 / 750 Loss: 0.2107\n",
      "Epoch: 2 / 3, Step: 113 / 750 Loss: 0.2611\n",
      "Epoch: 2 / 3, Step: 114 / 750 Loss: 0.1447\n",
      "Epoch: 2 / 3, Step: 115 / 750 Loss: 0.1794\n",
      "Epoch: 2 / 3, Step: 116 / 750 Loss: 0.2869\n",
      "Epoch: 2 / 3, Step: 117 / 750 Loss: 0.2722\n",
      "Epoch: 2 / 3, Step: 118 / 750 Loss: 0.2491\n",
      "Epoch: 2 / 3, Step: 119 / 750 Loss: 0.1717\n",
      "Epoch: 2 / 3, Step: 120 / 750 Loss: 0.2931\n",
      "Epoch: 2 / 3, Step: 121 / 750 Loss: 0.2098\n",
      "Epoch: 2 / 3, Step: 122 / 750 Loss: 0.3841\n",
      "Epoch: 2 / 3, Step: 123 / 750 Loss: 0.0740\n",
      "Epoch: 2 / 3, Step: 124 / 750 Loss: 0.1100\n",
      "Epoch: 2 / 3, Step: 125 / 750 Loss: 0.1607\n",
      "Epoch: 2 / 3, Step: 126 / 750 Loss: 0.1153\n",
      "Epoch: 2 / 3, Step: 127 / 750 Loss: 0.1214\n",
      "Epoch: 2 / 3, Step: 128 / 750 Loss: 0.2057\n",
      "Epoch: 2 / 3, Step: 129 / 750 Loss: 0.2337\n",
      "Epoch: 2 / 3, Step: 130 / 750 Loss: 0.3211\n",
      "Epoch: 2 / 3, Step: 131 / 750 Loss: 0.2202\n",
      "Epoch: 2 / 3, Step: 132 / 750 Loss: 0.3086\n",
      "Epoch: 2 / 3, Step: 133 / 750 Loss: 0.2167\n",
      "Epoch: 2 / 3, Step: 134 / 750 Loss: 0.0777\n",
      "Epoch: 2 / 3, Step: 135 / 750 Loss: 0.0992\n",
      "Epoch: 2 / 3, Step: 136 / 750 Loss: 0.2532\n",
      "Epoch: 2 / 3, Step: 137 / 750 Loss: 0.2931\n",
      "Epoch: 2 / 3, Step: 138 / 750 Loss: 0.4249\n",
      "Epoch: 2 / 3, Step: 139 / 750 Loss: 0.1996\n",
      "Epoch: 2 / 3, Step: 140 / 750 Loss: 0.1724\n",
      "Epoch: 2 / 3, Step: 141 / 750 Loss: 0.3527\n",
      "Epoch: 2 / 3, Step: 142 / 750 Loss: 0.2259\n",
      "Epoch: 2 / 3, Step: 143 / 750 Loss: 0.1732\n",
      "Epoch: 2 / 3, Step: 144 / 750 Loss: 0.2002\n",
      "Epoch: 2 / 3, Step: 145 / 750 Loss: 0.1184\n",
      "Epoch: 2 / 3, Step: 146 / 750 Loss: 0.2365\n",
      "Epoch: 2 / 3, Step: 147 / 750 Loss: 0.5617\n",
      "Epoch: 2 / 3, Step: 148 / 750 Loss: 0.1104\n",
      "Epoch: 2 / 3, Step: 149 / 750 Loss: 0.3426\n",
      "Epoch: 2 / 3, Step: 150 / 750 Loss: 0.4808\n",
      "Epoch: 2 / 3, Step: 151 / 750 Loss: 0.3658\n",
      "Epoch: 2 / 3, Step: 152 / 750 Loss: 0.1805\n",
      "Epoch: 2 / 3, Step: 153 / 750 Loss: 0.1314\n",
      "Epoch: 2 / 3, Step: 154 / 750 Loss: 0.2482\n",
      "Epoch: 2 / 3, Step: 155 / 750 Loss: 0.1760\n",
      "Epoch: 2 / 3, Step: 156 / 750 Loss: 0.1432\n",
      "Epoch: 2 / 3, Step: 157 / 750 Loss: 0.2005\n",
      "Epoch: 2 / 3, Step: 158 / 750 Loss: 0.3407\n",
      "Epoch: 2 / 3, Step: 159 / 750 Loss: 0.4580\n",
      "Epoch: 2 / 3, Step: 160 / 750 Loss: 0.2364\n",
      "Epoch: 2 / 3, Step: 161 / 750 Loss: 0.2815\n",
      "Epoch: 2 / 3, Step: 162 / 750 Loss: 0.2928\n",
      "Epoch: 2 / 3, Step: 163 / 750 Loss: 0.1470\n",
      "Epoch: 2 / 3, Step: 164 / 750 Loss: 0.4045\n",
      "Epoch: 2 / 3, Step: 165 / 750 Loss: 0.2122\n",
      "Epoch: 2 / 3, Step: 166 / 750 Loss: 0.1842\n",
      "Epoch: 2 / 3, Step: 167 / 750 Loss: 0.2743\n",
      "Epoch: 2 / 3, Step: 168 / 750 Loss: 0.2110\n",
      "Epoch: 2 / 3, Step: 169 / 750 Loss: 0.1968\n",
      "Epoch: 2 / 3, Step: 170 / 750 Loss: 0.1457\n",
      "Epoch: 2 / 3, Step: 171 / 750 Loss: 0.0904\n",
      "Epoch: 2 / 3, Step: 172 / 750 Loss: 0.2770\n",
      "Epoch: 2 / 3, Step: 173 / 750 Loss: 0.2850\n",
      "Epoch: 2 / 3, Step: 174 / 750 Loss: 0.4185\n",
      "Epoch: 2 / 3, Step: 175 / 750 Loss: 0.1735\n",
      "Epoch: 2 / 3, Step: 176 / 750 Loss: 0.1613\n",
      "Epoch: 2 / 3, Step: 177 / 750 Loss: 0.1670\n",
      "Epoch: 2 / 3, Step: 178 / 750 Loss: 0.2349\n",
      "Epoch: 2 / 3, Step: 179 / 750 Loss: 0.1179\n",
      "Epoch: 2 / 3, Step: 180 / 750 Loss: 0.1183\n",
      "Epoch: 2 / 3, Step: 181 / 750 Loss: 0.4338\n",
      "Epoch: 2 / 3, Step: 182 / 750 Loss: 0.1529\n",
      "Epoch: 2 / 3, Step: 183 / 750 Loss: 0.0425\n",
      "Epoch: 2 / 3, Step: 184 / 750 Loss: 0.2169\n",
      "Epoch: 2 / 3, Step: 185 / 750 Loss: 0.4185\n",
      "Epoch: 2 / 3, Step: 186 / 750 Loss: 0.2829\n",
      "Epoch: 2 / 3, Step: 187 / 750 Loss: 0.2885\n",
      "Epoch: 2 / 3, Step: 188 / 750 Loss: 0.2259\n",
      "Epoch: 2 / 3, Step: 189 / 750 Loss: 0.2233\n",
      "Epoch: 2 / 3, Step: 190 / 750 Loss: 0.0907\n",
      "Epoch: 2 / 3, Step: 191 / 750 Loss: 0.1483\n",
      "Epoch: 2 / 3, Step: 192 / 750 Loss: 0.2707\n",
      "Epoch: 2 / 3, Step: 193 / 750 Loss: 0.1886\n",
      "Epoch: 2 / 3, Step: 194 / 750 Loss: 0.2431\n",
      "Epoch: 2 / 3, Step: 195 / 750 Loss: 0.1421\n",
      "Epoch: 2 / 3, Step: 196 / 750 Loss: 0.2898\n",
      "Epoch: 2 / 3, Step: 197 / 750 Loss: 0.6370\n",
      "Epoch: 2 / 3, Step: 198 / 750 Loss: 0.2515\n",
      "Epoch: 2 / 3, Step: 199 / 750 Loss: 0.0884\n",
      "Epoch: 2 / 3, Step: 200 / 750 Loss: 0.1245\n",
      "Epoch: 2 / 3, Step: 201 / 750 Loss: 0.3021\n",
      "Epoch: 2 / 3, Step: 202 / 750 Loss: 0.3485\n",
      "Epoch: 2 / 3, Step: 203 / 750 Loss: 0.2228\n",
      "Epoch: 2 / 3, Step: 204 / 750 Loss: 0.2964\n",
      "Epoch: 2 / 3, Step: 205 / 750 Loss: 0.2074\n",
      "Epoch: 2 / 3, Step: 206 / 750 Loss: 0.1375\n",
      "Epoch: 2 / 3, Step: 207 / 750 Loss: 0.2315\n",
      "Epoch: 2 / 3, Step: 208 / 750 Loss: 0.1170\n",
      "Epoch: 2 / 3, Step: 209 / 750 Loss: 0.0792\n",
      "Epoch: 2 / 3, Step: 210 / 750 Loss: 0.1634\n",
      "Epoch: 2 / 3, Step: 211 / 750 Loss: 0.1884\n",
      "Epoch: 2 / 3, Step: 212 / 750 Loss: 0.2227\n",
      "Epoch: 2 / 3, Step: 213 / 750 Loss: 0.2532\n",
      "Epoch: 2 / 3, Step: 214 / 750 Loss: 0.1211\n",
      "Epoch: 2 / 3, Step: 215 / 750 Loss: 0.1921\n",
      "Epoch: 2 / 3, Step: 216 / 750 Loss: 0.6978\n",
      "Epoch: 2 / 3, Step: 217 / 750 Loss: 0.2411\n",
      "Epoch: 2 / 3, Step: 218 / 750 Loss: 0.2818\n",
      "Epoch: 2 / 3, Step: 219 / 750 Loss: 0.1114\n",
      "Epoch: 2 / 3, Step: 220 / 750 Loss: 0.1363\n",
      "Epoch: 2 / 3, Step: 221 / 750 Loss: 0.3587\n",
      "Epoch: 2 / 3, Step: 222 / 750 Loss: 0.0654\n",
      "Epoch: 2 / 3, Step: 223 / 750 Loss: 0.1498\n",
      "Epoch: 2 / 3, Step: 224 / 750 Loss: 0.1028\n",
      "Epoch: 2 / 3, Step: 225 / 750 Loss: 0.2183\n",
      "Epoch: 2 / 3, Step: 226 / 750 Loss: 0.2215\n",
      "Epoch: 2 / 3, Step: 227 / 750 Loss: 0.1168\n",
      "Epoch: 2 / 3, Step: 228 / 750 Loss: 0.1814\n",
      "Epoch: 2 / 3, Step: 229 / 750 Loss: 0.6089\n",
      "Epoch: 2 / 3, Step: 230 / 750 Loss: 0.1443\n",
      "Epoch: 2 / 3, Step: 231 / 750 Loss: 0.0783\n",
      "Epoch: 2 / 3, Step: 232 / 750 Loss: 0.3298\n",
      "Epoch: 2 / 3, Step: 233 / 750 Loss: 0.1692\n",
      "Epoch: 2 / 3, Step: 234 / 750 Loss: 0.4356\n",
      "Epoch: 2 / 3, Step: 235 / 750 Loss: 0.2528\n",
      "Epoch: 2 / 3, Step: 236 / 750 Loss: 0.3999\n",
      "Epoch: 2 / 3, Step: 237 / 750 Loss: 0.1200\n",
      "Epoch: 2 / 3, Step: 238 / 750 Loss: 0.4160\n",
      "Epoch: 2 / 3, Step: 239 / 750 Loss: 0.2546\n",
      "Epoch: 2 / 3, Step: 240 / 750 Loss: 0.2789\n",
      "Epoch: 2 / 3, Step: 241 / 750 Loss: 0.1934\n",
      "Epoch: 2 / 3, Step: 242 / 750 Loss: 0.2202\n",
      "Epoch: 2 / 3, Step: 243 / 750 Loss: 0.1374\n",
      "Epoch: 2 / 3, Step: 244 / 750 Loss: 0.0865\n",
      "Epoch: 2 / 3, Step: 245 / 750 Loss: 0.2811\n",
      "Epoch: 2 / 3, Step: 246 / 750 Loss: 0.3591\n",
      "Epoch: 2 / 3, Step: 247 / 750 Loss: 0.2888\n",
      "Epoch: 2 / 3, Step: 248 / 750 Loss: 0.1451\n",
      "Epoch: 2 / 3, Step: 249 / 750 Loss: 0.1705\n",
      "Epoch: 2 / 3, Step: 250 / 750 Loss: 0.2260\n",
      "Epoch: 2 / 3, Step: 251 / 750 Loss: 0.0985\n",
      "Epoch: 2 / 3, Step: 252 / 750 Loss: 0.1011\n",
      "Epoch: 2 / 3, Step: 253 / 750 Loss: 0.2056\n",
      "Epoch: 2 / 3, Step: 254 / 750 Loss: 0.1126\n",
      "Epoch: 2 / 3, Step: 255 / 750 Loss: 0.1354\n",
      "Epoch: 2 / 3, Step: 256 / 750 Loss: 0.1173\n",
      "Epoch: 2 / 3, Step: 257 / 750 Loss: 0.1571\n",
      "Epoch: 2 / 3, Step: 258 / 750 Loss: 0.0540\n",
      "Epoch: 2 / 3, Step: 259 / 750 Loss: 0.1818\n",
      "Epoch: 2 / 3, Step: 260 / 750 Loss: 0.1841\n",
      "Epoch: 2 / 3, Step: 261 / 750 Loss: 0.1001\n",
      "Epoch: 2 / 3, Step: 262 / 750 Loss: 0.2409\n",
      "Epoch: 2 / 3, Step: 263 / 750 Loss: 0.2698\n",
      "Epoch: 2 / 3, Step: 264 / 750 Loss: 0.1654\n",
      "Epoch: 2 / 3, Step: 265 / 750 Loss: 0.3945\n",
      "Epoch: 2 / 3, Step: 266 / 750 Loss: 0.5719\n",
      "Epoch: 2 / 3, Step: 267 / 750 Loss: 0.2952\n",
      "Epoch: 2 / 3, Step: 268 / 750 Loss: 0.0522\n",
      "Epoch: 2 / 3, Step: 269 / 750 Loss: 0.2217\n",
      "Epoch: 2 / 3, Step: 270 / 750 Loss: 0.2192\n",
      "Epoch: 2 / 3, Step: 271 / 750 Loss: 0.1481\n",
      "Epoch: 2 / 3, Step: 272 / 750 Loss: 0.1903\n",
      "Epoch: 2 / 3, Step: 273 / 750 Loss: 0.2405\n",
      "Epoch: 2 / 3, Step: 274 / 750 Loss: 0.3226\n",
      "Epoch: 2 / 3, Step: 275 / 750 Loss: 0.4660\n",
      "Epoch: 2 / 3, Step: 276 / 750 Loss: 0.0855\n",
      "Epoch: 2 / 3, Step: 277 / 750 Loss: 0.1382\n",
      "Epoch: 2 / 3, Step: 278 / 750 Loss: 0.2134\n",
      "Epoch: 2 / 3, Step: 279 / 750 Loss: 0.0701\n",
      "Epoch: 2 / 3, Step: 280 / 750 Loss: 0.0919\n",
      "Epoch: 2 / 3, Step: 281 / 750 Loss: 0.2341\n",
      "Epoch: 2 / 3, Step: 282 / 750 Loss: 0.1542\n",
      "Epoch: 2 / 3, Step: 283 / 750 Loss: 0.5995\n",
      "Epoch: 2 / 3, Step: 284 / 750 Loss: 0.1074\n",
      "Epoch: 2 / 3, Step: 285 / 750 Loss: 0.2704\n",
      "Epoch: 2 / 3, Step: 286 / 750 Loss: 0.1395\n",
      "Epoch: 2 / 3, Step: 287 / 750 Loss: 0.1180\n",
      "Epoch: 2 / 3, Step: 288 / 750 Loss: 0.1536\n",
      "Epoch: 2 / 3, Step: 289 / 750 Loss: 0.2564\n",
      "Epoch: 2 / 3, Step: 290 / 750 Loss: 0.4012\n",
      "Epoch: 2 / 3, Step: 291 / 750 Loss: 0.2246\n",
      "Epoch: 2 / 3, Step: 292 / 750 Loss: 0.2248\n",
      "Epoch: 2 / 3, Step: 293 / 750 Loss: 0.1242\n",
      "Epoch: 2 / 3, Step: 294 / 750 Loss: 0.6195\n",
      "Epoch: 2 / 3, Step: 295 / 750 Loss: 0.2594\n",
      "Epoch: 2 / 3, Step: 296 / 750 Loss: 0.2098\n",
      "Epoch: 2 / 3, Step: 297 / 750 Loss: 0.3156\n",
      "Epoch: 2 / 3, Step: 298 / 750 Loss: 0.1476\n",
      "Epoch: 2 / 3, Step: 299 / 750 Loss: 0.1572\n",
      "Epoch: 2 / 3, Step: 300 / 750 Loss: 0.3654\n",
      "Epoch: 2 / 3, Step: 301 / 750 Loss: 0.2864\n",
      "Epoch: 2 / 3, Step: 302 / 750 Loss: 0.3312\n",
      "Epoch: 2 / 3, Step: 303 / 750 Loss: 0.1008\n",
      "Epoch: 2 / 3, Step: 304 / 750 Loss: 0.2214\n",
      "Epoch: 2 / 3, Step: 305 / 750 Loss: 0.1871\n",
      "Epoch: 2 / 3, Step: 306 / 750 Loss: 0.3104\n",
      "Epoch: 2 / 3, Step: 307 / 750 Loss: 0.2339\n",
      "Epoch: 2 / 3, Step: 308 / 750 Loss: 0.1622\n",
      "Epoch: 2 / 3, Step: 309 / 750 Loss: 0.1661\n",
      "Epoch: 2 / 3, Step: 310 / 750 Loss: 0.2875\n",
      "Epoch: 2 / 3, Step: 311 / 750 Loss: 0.2123\n",
      "Epoch: 2 / 3, Step: 312 / 750 Loss: 0.2643\n",
      "Epoch: 2 / 3, Step: 313 / 750 Loss: 0.1103\n",
      "Epoch: 2 / 3, Step: 314 / 750 Loss: 0.1643\n",
      "Epoch: 2 / 3, Step: 315 / 750 Loss: 0.2784\n",
      "Epoch: 2 / 3, Step: 316 / 750 Loss: 0.2251\n",
      "Epoch: 2 / 3, Step: 317 / 750 Loss: 0.1633\n",
      "Epoch: 2 / 3, Step: 318 / 750 Loss: 0.2124\n",
      "Epoch: 2 / 3, Step: 319 / 750 Loss: 0.1817\n",
      "Epoch: 2 / 3, Step: 320 / 750 Loss: 0.1862\n",
      "Epoch: 2 / 3, Step: 321 / 750 Loss: 0.2219\n",
      "Epoch: 2 / 3, Step: 322 / 750 Loss: 0.1914\n",
      "Epoch: 2 / 3, Step: 323 / 750 Loss: 0.2450\n",
      "Epoch: 2 / 3, Step: 324 / 750 Loss: 0.0680\n",
      "Epoch: 2 / 3, Step: 325 / 750 Loss: 0.1408\n",
      "Epoch: 2 / 3, Step: 326 / 750 Loss: 0.2696\n",
      "Epoch: 2 / 3, Step: 327 / 750 Loss: 0.2218\n",
      "Epoch: 2 / 3, Step: 328 / 750 Loss: 0.1139\n",
      "Epoch: 2 / 3, Step: 329 / 750 Loss: 0.1735\n",
      "Epoch: 2 / 3, Step: 330 / 750 Loss: 0.2302\n",
      "Epoch: 2 / 3, Step: 331 / 750 Loss: 0.1322\n",
      "Epoch: 2 / 3, Step: 332 / 750 Loss: 0.1995\n",
      "Epoch: 2 / 3, Step: 333 / 750 Loss: 0.2932\n",
      "Epoch: 2 / 3, Step: 334 / 750 Loss: 0.2148\n",
      "Epoch: 2 / 3, Step: 335 / 750 Loss: 0.2782\n",
      "Epoch: 2 / 3, Step: 336 / 750 Loss: 0.1080\n",
      "Epoch: 2 / 3, Step: 337 / 750 Loss: 0.2121\n",
      "Epoch: 2 / 3, Step: 338 / 750 Loss: 0.4889\n",
      "Epoch: 2 / 3, Step: 339 / 750 Loss: 0.0724\n",
      "Epoch: 2 / 3, Step: 340 / 750 Loss: 0.2508\n",
      "Epoch: 2 / 3, Step: 341 / 750 Loss: 0.2324\n",
      "Epoch: 2 / 3, Step: 342 / 750 Loss: 0.2076\n",
      "Epoch: 2 / 3, Step: 343 / 750 Loss: 0.3419\n",
      "Epoch: 2 / 3, Step: 344 / 750 Loss: 0.2113\n",
      "Epoch: 2 / 3, Step: 345 / 750 Loss: 0.0679\n",
      "Epoch: 2 / 3, Step: 346 / 750 Loss: 0.5088\n",
      "Epoch: 2 / 3, Step: 347 / 750 Loss: 0.1432\n",
      "Epoch: 2 / 3, Step: 348 / 750 Loss: 0.2192\n",
      "Epoch: 2 / 3, Step: 349 / 750 Loss: 0.4468\n",
      "Epoch: 2 / 3, Step: 350 / 750 Loss: 0.1633\n",
      "Epoch: 2 / 3, Step: 351 / 750 Loss: 0.1651\n",
      "Epoch: 2 / 3, Step: 352 / 750 Loss: 0.2626\n",
      "Epoch: 2 / 3, Step: 353 / 750 Loss: 0.3391\n",
      "Epoch: 2 / 3, Step: 354 / 750 Loss: 0.4649\n",
      "Epoch: 2 / 3, Step: 355 / 750 Loss: 0.0938\n",
      "Epoch: 2 / 3, Step: 356 / 750 Loss: 0.2962\n",
      "Epoch: 2 / 3, Step: 357 / 750 Loss: 0.3108\n",
      "Epoch: 2 / 3, Step: 358 / 750 Loss: 0.0671\n",
      "Epoch: 2 / 3, Step: 359 / 750 Loss: 0.1924\n",
      "Epoch: 2 / 3, Step: 360 / 750 Loss: 0.0827\n",
      "Epoch: 2 / 3, Step: 361 / 750 Loss: 0.1724\n",
      "Epoch: 2 / 3, Step: 362 / 750 Loss: 0.4098\n",
      "Epoch: 2 / 3, Step: 363 / 750 Loss: 0.0656\n",
      "Epoch: 2 / 3, Step: 364 / 750 Loss: 0.0853\n",
      "Epoch: 2 / 3, Step: 365 / 750 Loss: 0.0959\n",
      "Epoch: 2 / 3, Step: 366 / 750 Loss: 0.1242\n",
      "Epoch: 2 / 3, Step: 367 / 750 Loss: 0.2557\n",
      "Epoch: 2 / 3, Step: 368 / 750 Loss: 0.0814\n",
      "Epoch: 2 / 3, Step: 369 / 750 Loss: 0.3798\n",
      "Epoch: 2 / 3, Step: 370 / 750 Loss: 0.2639\n",
      "Epoch: 2 / 3, Step: 371 / 750 Loss: 0.1744\n",
      "Epoch: 2 / 3, Step: 372 / 750 Loss: 0.1112\n",
      "Epoch: 2 / 3, Step: 373 / 750 Loss: 0.0797\n",
      "Epoch: 2 / 3, Step: 374 / 750 Loss: 0.2328\n",
      "Epoch: 2 / 3, Step: 375 / 750 Loss: 0.5238\n",
      "Epoch: 2 / 3, Step: 376 / 750 Loss: 0.0709\n",
      "Epoch: 2 / 3, Step: 377 / 750 Loss: 0.2432\n",
      "Epoch: 2 / 3, Step: 378 / 750 Loss: 0.1587\n",
      "Epoch: 2 / 3, Step: 379 / 750 Loss: 0.1527\n",
      "Epoch: 2 / 3, Step: 380 / 750 Loss: 0.1396\n",
      "Epoch: 2 / 3, Step: 381 / 750 Loss: 0.1804\n",
      "Epoch: 2 / 3, Step: 382 / 750 Loss: 0.2588\n",
      "Epoch: 2 / 3, Step: 383 / 750 Loss: 0.1591\n",
      "Epoch: 2 / 3, Step: 384 / 750 Loss: 0.5577\n",
      "Epoch: 2 / 3, Step: 385 / 750 Loss: 0.2946\n",
      "Epoch: 2 / 3, Step: 386 / 750 Loss: 0.0679\n",
      "Epoch: 2 / 3, Step: 387 / 750 Loss: 0.6195\n",
      "Epoch: 2 / 3, Step: 388 / 750 Loss: 0.2910\n",
      "Epoch: 2 / 3, Step: 389 / 750 Loss: 0.1306\n",
      "Epoch: 2 / 3, Step: 390 / 750 Loss: 0.3431\n",
      "Epoch: 2 / 3, Step: 391 / 750 Loss: 0.1463\n",
      "Epoch: 2 / 3, Step: 392 / 750 Loss: 0.2524\n",
      "Epoch: 2 / 3, Step: 393 / 750 Loss: 0.1991\n",
      "Epoch: 2 / 3, Step: 394 / 750 Loss: 0.1036\n",
      "Epoch: 2 / 3, Step: 395 / 750 Loss: 0.4433\n",
      "Epoch: 2 / 3, Step: 396 / 750 Loss: 0.0767\n",
      "Epoch: 2 / 3, Step: 397 / 750 Loss: 0.1952\n",
      "Epoch: 2 / 3, Step: 398 / 750 Loss: 0.5438\n",
      "Epoch: 2 / 3, Step: 399 / 750 Loss: 0.4842\n",
      "Epoch: 2 / 3, Step: 400 / 750 Loss: 0.0503\n",
      "Epoch: 2 / 3, Step: 401 / 750 Loss: 0.2203\n",
      "Epoch: 2 / 3, Step: 402 / 750 Loss: 0.3677\n",
      "Epoch: 2 / 3, Step: 403 / 750 Loss: 0.1271\n",
      "Epoch: 2 / 3, Step: 404 / 750 Loss: 0.2363\n",
      "Epoch: 2 / 3, Step: 405 / 750 Loss: 0.2694\n",
      "Epoch: 2 / 3, Step: 406 / 750 Loss: 0.1551\n",
      "Epoch: 2 / 3, Step: 407 / 750 Loss: 0.1285\n",
      "Epoch: 2 / 3, Step: 408 / 750 Loss: 0.0850\n",
      "Epoch: 2 / 3, Step: 409 / 750 Loss: 0.2952\n",
      "Epoch: 2 / 3, Step: 410 / 750 Loss: 0.2473\n",
      "Epoch: 2 / 3, Step: 411 / 750 Loss: 0.2271\n",
      "Epoch: 2 / 3, Step: 412 / 750 Loss: 0.2334\n",
      "Epoch: 2 / 3, Step: 413 / 750 Loss: 0.0925\n",
      "Epoch: 2 / 3, Step: 414 / 750 Loss: 0.2030\n",
      "Epoch: 2 / 3, Step: 415 / 750 Loss: 0.2099\n",
      "Epoch: 2 / 3, Step: 416 / 750 Loss: 0.3740\n",
      "Epoch: 2 / 3, Step: 417 / 750 Loss: 0.2155\n",
      "Epoch: 2 / 3, Step: 418 / 750 Loss: 0.1639\n",
      "Epoch: 2 / 3, Step: 419 / 750 Loss: 0.1916\n",
      "Epoch: 2 / 3, Step: 420 / 750 Loss: 0.2250\n",
      "Epoch: 2 / 3, Step: 421 / 750 Loss: 0.2404\n",
      "Epoch: 2 / 3, Step: 422 / 750 Loss: 0.1859\n",
      "Epoch: 2 / 3, Step: 423 / 750 Loss: 0.2837\n",
      "Epoch: 2 / 3, Step: 424 / 750 Loss: 0.1472\n",
      "Epoch: 2 / 3, Step: 425 / 750 Loss: 0.1163\n",
      "Epoch: 2 / 3, Step: 426 / 750 Loss: 0.1861\n",
      "Epoch: 2 / 3, Step: 427 / 750 Loss: 0.4407\n",
      "Epoch: 2 / 3, Step: 428 / 750 Loss: 0.2412\n",
      "Epoch: 2 / 3, Step: 429 / 750 Loss: 0.0833\n",
      "Epoch: 2 / 3, Step: 430 / 750 Loss: 0.1265\n",
      "Epoch: 2 / 3, Step: 431 / 750 Loss: 0.1372\n",
      "Epoch: 2 / 3, Step: 432 / 750 Loss: 0.0372\n",
      "Epoch: 2 / 3, Step: 433 / 750 Loss: 0.1158\n",
      "Epoch: 2 / 3, Step: 434 / 750 Loss: 0.2301\n",
      "Epoch: 2 / 3, Step: 435 / 750 Loss: 0.2994\n",
      "Epoch: 2 / 3, Step: 436 / 750 Loss: 0.0962\n",
      "Epoch: 2 / 3, Step: 437 / 750 Loss: 0.1747\n",
      "Epoch: 2 / 3, Step: 438 / 750 Loss: 0.1539\n",
      "Epoch: 2 / 3, Step: 439 / 750 Loss: 0.2163\n",
      "Epoch: 2 / 3, Step: 440 / 750 Loss: 0.2703\n",
      "Epoch: 2 / 3, Step: 441 / 750 Loss: 0.1752\n",
      "Epoch: 2 / 3, Step: 442 / 750 Loss: 0.2346\n",
      "Epoch: 2 / 3, Step: 443 / 750 Loss: 0.2839\n",
      "Epoch: 2 / 3, Step: 444 / 750 Loss: 0.2430\n",
      "Epoch: 2 / 3, Step: 445 / 750 Loss: 0.1442\n",
      "Epoch: 2 / 3, Step: 446 / 750 Loss: 0.2024\n",
      "Epoch: 2 / 3, Step: 447 / 750 Loss: 0.1976\n",
      "Epoch: 2 / 3, Step: 448 / 750 Loss: 0.3009\n",
      "Epoch: 2 / 3, Step: 449 / 750 Loss: 0.1161\n",
      "Epoch: 2 / 3, Step: 450 / 750 Loss: 0.2259\n",
      "Epoch: 2 / 3, Step: 451 / 750 Loss: 0.2790\n",
      "Epoch: 2 / 3, Step: 452 / 750 Loss: 0.3031\n",
      "Epoch: 2 / 3, Step: 453 / 750 Loss: 0.2066\n",
      "Epoch: 2 / 3, Step: 454 / 750 Loss: 0.1974\n",
      "Epoch: 2 / 3, Step: 455 / 750 Loss: 0.2363\n",
      "Epoch: 2 / 3, Step: 456 / 750 Loss: 0.1741\n",
      "Epoch: 2 / 3, Step: 457 / 750 Loss: 0.3752\n",
      "Epoch: 2 / 3, Step: 458 / 750 Loss: 0.2826\n",
      "Epoch: 2 / 3, Step: 459 / 750 Loss: 0.3889\n",
      "Epoch: 2 / 3, Step: 460 / 750 Loss: 0.1742\n",
      "Epoch: 2 / 3, Step: 461 / 750 Loss: 0.0643\n",
      "Epoch: 2 / 3, Step: 462 / 750 Loss: 0.5834\n",
      "Epoch: 2 / 3, Step: 463 / 750 Loss: 0.2486\n",
      "Epoch: 2 / 3, Step: 464 / 750 Loss: 0.2261\n",
      "Epoch: 2 / 3, Step: 465 / 750 Loss: 0.2646\n",
      "Epoch: 2 / 3, Step: 466 / 750 Loss: 0.1052\n",
      "Epoch: 2 / 3, Step: 467 / 750 Loss: 0.2799\n",
      "Epoch: 2 / 3, Step: 468 / 750 Loss: 0.1709\n",
      "Epoch: 2 / 3, Step: 469 / 750 Loss: 0.2052\n",
      "Epoch: 2 / 3, Step: 470 / 750 Loss: 0.1129\n",
      "Epoch: 2 / 3, Step: 471 / 750 Loss: 0.2387\n",
      "Epoch: 2 / 3, Step: 472 / 750 Loss: 0.1039\n",
      "Epoch: 2 / 3, Step: 473 / 750 Loss: 0.2503\n",
      "Epoch: 2 / 3, Step: 474 / 750 Loss: 0.4005\n",
      "Epoch: 2 / 3, Step: 475 / 750 Loss: 0.1581\n",
      "Epoch: 2 / 3, Step: 476 / 750 Loss: 0.0993\n",
      "Epoch: 2 / 3, Step: 477 / 750 Loss: 0.0718\n",
      "Epoch: 2 / 3, Step: 478 / 750 Loss: 0.2596\n",
      "Epoch: 2 / 3, Step: 479 / 750 Loss: 0.2353\n",
      "Epoch: 2 / 3, Step: 480 / 750 Loss: 0.2357\n",
      "Epoch: 2 / 3, Step: 481 / 750 Loss: 0.2101\n",
      "Epoch: 2 / 3, Step: 482 / 750 Loss: 0.1552\n",
      "Epoch: 2 / 3, Step: 483 / 750 Loss: 0.1877\n",
      "Epoch: 2 / 3, Step: 484 / 750 Loss: 0.1342\n",
      "Epoch: 2 / 3, Step: 485 / 750 Loss: 0.3134\n",
      "Epoch: 2 / 3, Step: 486 / 750 Loss: 0.1267\n",
      "Epoch: 2 / 3, Step: 487 / 750 Loss: 0.1308\n",
      "Epoch: 2 / 3, Step: 488 / 750 Loss: 0.3227\n",
      "Epoch: 2 / 3, Step: 489 / 750 Loss: 0.2544\n",
      "Epoch: 2 / 3, Step: 490 / 750 Loss: 0.1680\n",
      "Epoch: 2 / 3, Step: 491 / 750 Loss: 0.2776\n",
      "Epoch: 2 / 3, Step: 492 / 750 Loss: 0.0540\n",
      "Epoch: 2 / 3, Step: 493 / 750 Loss: 0.2637\n",
      "Epoch: 2 / 3, Step: 494 / 750 Loss: 0.2350\n",
      "Epoch: 2 / 3, Step: 495 / 750 Loss: 0.2136\n",
      "Epoch: 2 / 3, Step: 496 / 750 Loss: 0.0646\n",
      "Epoch: 2 / 3, Step: 497 / 750 Loss: 0.1174\n",
      "Epoch: 2 / 3, Step: 498 / 750 Loss: 0.2223\n",
      "Epoch: 2 / 3, Step: 499 / 750 Loss: 0.3219\n",
      "Epoch: 2 / 3, Step: 500 / 750 Loss: 0.1601\n",
      "Epoch: 2 / 3, Step: 501 / 750 Loss: 0.2199\n",
      "Epoch: 2 / 3, Step: 502 / 750 Loss: 0.0786\n",
      "Epoch: 2 / 3, Step: 503 / 750 Loss: 0.1733\n",
      "Epoch: 2 / 3, Step: 504 / 750 Loss: 0.1934\n",
      "Epoch: 2 / 3, Step: 505 / 750 Loss: 0.1201\n",
      "Epoch: 2 / 3, Step: 506 / 750 Loss: 0.4372\n",
      "Epoch: 2 / 3, Step: 507 / 750 Loss: 0.1799\n",
      "Epoch: 2 / 3, Step: 508 / 750 Loss: 0.2195\n",
      "Epoch: 2 / 3, Step: 509 / 750 Loss: 0.2844\n",
      "Epoch: 2 / 3, Step: 510 / 750 Loss: 0.0922\n",
      "Epoch: 2 / 3, Step: 511 / 750 Loss: 0.2011\n",
      "Epoch: 2 / 3, Step: 512 / 750 Loss: 0.0763\n",
      "Epoch: 2 / 3, Step: 513 / 750 Loss: 0.2579\n",
      "Epoch: 2 / 3, Step: 514 / 750 Loss: 0.3809\n",
      "Epoch: 2 / 3, Step: 515 / 750 Loss: 0.2809\n",
      "Epoch: 2 / 3, Step: 516 / 750 Loss: 0.0787\n",
      "Epoch: 2 / 3, Step: 517 / 750 Loss: 0.0565\n",
      "Epoch: 2 / 3, Step: 518 / 750 Loss: 0.1801\n",
      "Epoch: 2 / 3, Step: 519 / 750 Loss: 0.2834\n",
      "Epoch: 2 / 3, Step: 520 / 750 Loss: 0.1372\n",
      "Epoch: 2 / 3, Step: 521 / 750 Loss: 0.2542\n",
      "Epoch: 2 / 3, Step: 522 / 750 Loss: 0.0629\n",
      "Epoch: 2 / 3, Step: 523 / 750 Loss: 0.0670\n",
      "Epoch: 2 / 3, Step: 524 / 750 Loss: 0.2608\n",
      "Epoch: 2 / 3, Step: 525 / 750 Loss: 0.2742\n",
      "Epoch: 2 / 3, Step: 526 / 750 Loss: 0.0964\n",
      "Epoch: 2 / 3, Step: 527 / 750 Loss: 0.1074\n",
      "Epoch: 2 / 3, Step: 528 / 750 Loss: 0.0931\n",
      "Epoch: 2 / 3, Step: 529 / 750 Loss: 0.2917\n",
      "Epoch: 2 / 3, Step: 530 / 750 Loss: 0.2274\n",
      "Epoch: 2 / 3, Step: 531 / 750 Loss: 0.2214\n",
      "Epoch: 2 / 3, Step: 532 / 750 Loss: 0.1219\n",
      "Epoch: 2 / 3, Step: 533 / 750 Loss: 0.1315\n",
      "Epoch: 2 / 3, Step: 534 / 750 Loss: 0.0578\n",
      "Epoch: 2 / 3, Step: 535 / 750 Loss: 0.3573\n",
      "Epoch: 2 / 3, Step: 536 / 750 Loss: 0.2076\n",
      "Epoch: 2 / 3, Step: 537 / 750 Loss: 0.2681\n",
      "Epoch: 2 / 3, Step: 538 / 750 Loss: 0.3250\n",
      "Epoch: 2 / 3, Step: 539 / 750 Loss: 0.1196\n",
      "Epoch: 2 / 3, Step: 540 / 750 Loss: 0.1118\n",
      "Epoch: 2 / 3, Step: 541 / 750 Loss: 0.2083\n",
      "Epoch: 2 / 3, Step: 542 / 750 Loss: 0.1044\n",
      "Epoch: 2 / 3, Step: 543 / 750 Loss: 0.2484\n",
      "Epoch: 2 / 3, Step: 544 / 750 Loss: 0.2306\n",
      "Epoch: 2 / 3, Step: 545 / 750 Loss: 0.2631\n",
      "Epoch: 2 / 3, Step: 546 / 750 Loss: 0.1466\n",
      "Epoch: 2 / 3, Step: 547 / 750 Loss: 0.2901\n",
      "Epoch: 2 / 3, Step: 548 / 750 Loss: 0.1937\n",
      "Epoch: 2 / 3, Step: 549 / 750 Loss: 0.1839\n",
      "Epoch: 2 / 3, Step: 550 / 750 Loss: 0.5522\n",
      "Epoch: 2 / 3, Step: 551 / 750 Loss: 0.1019\n",
      "Epoch: 2 / 3, Step: 552 / 750 Loss: 0.2500\n",
      "Epoch: 2 / 3, Step: 553 / 750 Loss: 0.0566\n",
      "Epoch: 2 / 3, Step: 554 / 750 Loss: 0.1697\n",
      "Epoch: 2 / 3, Step: 555 / 750 Loss: 0.6719\n",
      "Epoch: 2 / 3, Step: 556 / 750 Loss: 0.2944\n",
      "Epoch: 2 / 3, Step: 557 / 750 Loss: 0.1315\n",
      "Epoch: 2 / 3, Step: 558 / 750 Loss: 0.1832\n",
      "Epoch: 2 / 3, Step: 559 / 750 Loss: 0.1935\n",
      "Epoch: 2 / 3, Step: 560 / 750 Loss: 0.5235\n",
      "Epoch: 2 / 3, Step: 561 / 750 Loss: 0.1817\n",
      "Epoch: 2 / 3, Step: 562 / 750 Loss: 0.4926\n",
      "Epoch: 2 / 3, Step: 563 / 750 Loss: 0.2441\n",
      "Epoch: 2 / 3, Step: 564 / 750 Loss: 0.2771\n",
      "Epoch: 2 / 3, Step: 565 / 750 Loss: 0.3495\n",
      "Epoch: 2 / 3, Step: 566 / 750 Loss: 0.2245\n",
      "Epoch: 2 / 3, Step: 567 / 750 Loss: 0.1865\n",
      "Epoch: 2 / 3, Step: 568 / 750 Loss: 0.4683\n",
      "Epoch: 2 / 3, Step: 569 / 750 Loss: 0.3120\n",
      "Epoch: 2 / 3, Step: 570 / 750 Loss: 0.1546\n",
      "Epoch: 2 / 3, Step: 571 / 750 Loss: 0.1612\n",
      "Epoch: 2 / 3, Step: 572 / 750 Loss: 0.3026\n",
      "Epoch: 2 / 3, Step: 573 / 750 Loss: 0.2721\n",
      "Epoch: 2 / 3, Step: 574 / 750 Loss: 0.1561\n",
      "Epoch: 2 / 3, Step: 575 / 750 Loss: 0.4361\n",
      "Epoch: 2 / 3, Step: 576 / 750 Loss: 0.2966\n",
      "Epoch: 2 / 3, Step: 577 / 750 Loss: 0.2814\n",
      "Epoch: 2 / 3, Step: 578 / 750 Loss: 0.2554\n",
      "Epoch: 2 / 3, Step: 579 / 750 Loss: 0.1004\n",
      "Epoch: 2 / 3, Step: 580 / 750 Loss: 0.3943\n",
      "Epoch: 2 / 3, Step: 581 / 750 Loss: 0.2839\n",
      "Epoch: 2 / 3, Step: 582 / 750 Loss: 0.3328\n",
      "Epoch: 2 / 3, Step: 583 / 750 Loss: 0.1662\n",
      "Epoch: 2 / 3, Step: 584 / 750 Loss: 0.1939\n",
      "Epoch: 2 / 3, Step: 585 / 750 Loss: 0.0801\n",
      "Epoch: 2 / 3, Step: 586 / 750 Loss: 0.1784\n",
      "Epoch: 2 / 3, Step: 587 / 750 Loss: 0.4180\n",
      "Epoch: 2 / 3, Step: 588 / 750 Loss: 0.1672\n",
      "Epoch: 2 / 3, Step: 589 / 750 Loss: 0.1816\n",
      "Epoch: 2 / 3, Step: 590 / 750 Loss: 0.2473\n",
      "Epoch: 2 / 3, Step: 591 / 750 Loss: 0.2432\n",
      "Epoch: 2 / 3, Step: 592 / 750 Loss: 0.1539\n",
      "Epoch: 2 / 3, Step: 593 / 750 Loss: 0.4292\n",
      "Epoch: 2 / 3, Step: 594 / 750 Loss: 0.1944\n",
      "Epoch: 2 / 3, Step: 595 / 750 Loss: 0.1198\n",
      "Epoch: 2 / 3, Step: 596 / 750 Loss: 0.4055\n",
      "Epoch: 2 / 3, Step: 597 / 750 Loss: 0.1630\n",
      "Epoch: 2 / 3, Step: 598 / 750 Loss: 0.2288\n",
      "Epoch: 2 / 3, Step: 599 / 750 Loss: 0.1536\n",
      "Epoch: 2 / 3, Step: 600 / 750 Loss: 0.1472\n",
      "Epoch: 2 / 3, Step: 601 / 750 Loss: 0.1026\n",
      "Epoch: 2 / 3, Step: 602 / 750 Loss: 0.2841\n",
      "Epoch: 2 / 3, Step: 603 / 750 Loss: 0.2305\n",
      "Epoch: 2 / 3, Step: 604 / 750 Loss: 0.3437\n",
      "Epoch: 2 / 3, Step: 605 / 750 Loss: 0.2936\n",
      "Epoch: 2 / 3, Step: 606 / 750 Loss: 0.2637\n",
      "Epoch: 2 / 3, Step: 607 / 750 Loss: 0.2558\n",
      "Epoch: 2 / 3, Step: 608 / 750 Loss: 0.2544\n",
      "Epoch: 2 / 3, Step: 609 / 750 Loss: 0.0965\n",
      "Epoch: 2 / 3, Step: 610 / 750 Loss: 0.1903\n",
      "Epoch: 2 / 3, Step: 611 / 750 Loss: 0.2064\n",
      "Epoch: 2 / 3, Step: 612 / 750 Loss: 0.1228\n",
      "Epoch: 2 / 3, Step: 613 / 750 Loss: 0.2139\n",
      "Epoch: 2 / 3, Step: 614 / 750 Loss: 0.2603\n",
      "Epoch: 2 / 3, Step: 615 / 750 Loss: 0.3032\n",
      "Epoch: 2 / 3, Step: 616 / 750 Loss: 0.1572\n",
      "Epoch: 2 / 3, Step: 617 / 750 Loss: 0.1482\n",
      "Epoch: 2 / 3, Step: 618 / 750 Loss: 0.1362\n",
      "Epoch: 2 / 3, Step: 619 / 750 Loss: 0.1298\n",
      "Epoch: 2 / 3, Step: 620 / 750 Loss: 0.1614\n",
      "Epoch: 2 / 3, Step: 621 / 750 Loss: 0.1275\n",
      "Epoch: 2 / 3, Step: 622 / 750 Loss: 0.3423\n",
      "Epoch: 2 / 3, Step: 623 / 750 Loss: 0.2345\n",
      "Epoch: 2 / 3, Step: 624 / 750 Loss: 0.1344\n",
      "Epoch: 2 / 3, Step: 625 / 750 Loss: 0.3050\n",
      "Epoch: 2 / 3, Step: 626 / 750 Loss: 0.1956\n",
      "Epoch: 2 / 3, Step: 627 / 750 Loss: 0.2406\n",
      "Epoch: 2 / 3, Step: 628 / 750 Loss: 0.3930\n",
      "Epoch: 2 / 3, Step: 629 / 750 Loss: 0.1612\n",
      "Epoch: 2 / 3, Step: 630 / 750 Loss: 0.1580\n",
      "Epoch: 2 / 3, Step: 631 / 750 Loss: 0.1622\n",
      "Epoch: 2 / 3, Step: 632 / 750 Loss: 0.1938\n",
      "Epoch: 2 / 3, Step: 633 / 750 Loss: 0.1042\n",
      "Epoch: 2 / 3, Step: 634 / 750 Loss: 0.1808\n",
      "Epoch: 2 / 3, Step: 635 / 750 Loss: 0.1935\n",
      "Epoch: 2 / 3, Step: 636 / 750 Loss: 0.1580\n",
      "Epoch: 2 / 3, Step: 637 / 750 Loss: 0.2402\n",
      "Epoch: 2 / 3, Step: 638 / 750 Loss: 0.1852\n",
      "Epoch: 2 / 3, Step: 639 / 750 Loss: 0.5496\n",
      "Epoch: 2 / 3, Step: 640 / 750 Loss: 0.1886\n",
      "Epoch: 2 / 3, Step: 641 / 750 Loss: 0.1495\n",
      "Epoch: 2 / 3, Step: 642 / 750 Loss: 0.4302\n",
      "Epoch: 2 / 3, Step: 643 / 750 Loss: 0.1851\n",
      "Epoch: 2 / 3, Step: 644 / 750 Loss: 0.1704\n",
      "Epoch: 2 / 3, Step: 645 / 750 Loss: 0.2934\n",
      "Epoch: 2 / 3, Step: 646 / 750 Loss: 0.1382\n",
      "Epoch: 2 / 3, Step: 647 / 750 Loss: 0.0714\n",
      "Epoch: 2 / 3, Step: 648 / 750 Loss: 0.4226\n",
      "Epoch: 2 / 3, Step: 649 / 750 Loss: 0.2955\n",
      "Epoch: 2 / 3, Step: 650 / 750 Loss: 0.2306\n",
      "Epoch: 2 / 3, Step: 651 / 750 Loss: 0.1990\n",
      "Epoch: 2 / 3, Step: 652 / 750 Loss: 0.1400\n",
      "Epoch: 2 / 3, Step: 653 / 750 Loss: 0.2409\n",
      "Epoch: 2 / 3, Step: 654 / 750 Loss: 0.1591\n",
      "Epoch: 2 / 3, Step: 655 / 750 Loss: 0.3941\n",
      "Epoch: 2 / 3, Step: 656 / 750 Loss: 0.1959\n",
      "Epoch: 2 / 3, Step: 657 / 750 Loss: 0.4336\n",
      "Epoch: 2 / 3, Step: 658 / 750 Loss: 0.3827\n",
      "Epoch: 2 / 3, Step: 659 / 750 Loss: 0.1308\n",
      "Epoch: 2 / 3, Step: 660 / 750 Loss: 0.3457\n",
      "Epoch: 2 / 3, Step: 661 / 750 Loss: 0.2240\n",
      "Epoch: 2 / 3, Step: 662 / 750 Loss: 0.1548\n",
      "Epoch: 2 / 3, Step: 663 / 750 Loss: 0.3618\n",
      "Epoch: 2 / 3, Step: 664 / 750 Loss: 0.1236\n",
      "Epoch: 2 / 3, Step: 665 / 750 Loss: 0.2472\n",
      "Epoch: 2 / 3, Step: 666 / 750 Loss: 0.1737\n",
      "Epoch: 2 / 3, Step: 667 / 750 Loss: 0.2905\n",
      "Epoch: 2 / 3, Step: 668 / 750 Loss: 0.2873\n",
      "Epoch: 2 / 3, Step: 669 / 750 Loss: 0.1263\n",
      "Epoch: 2 / 3, Step: 670 / 750 Loss: 0.2409\n",
      "Epoch: 2 / 3, Step: 671 / 750 Loss: 0.1454\n",
      "Epoch: 2 / 3, Step: 672 / 750 Loss: 0.1913\n",
      "Epoch: 2 / 3, Step: 673 / 750 Loss: 0.2642\n",
      "Epoch: 2 / 3, Step: 674 / 750 Loss: 0.1291\n",
      "Epoch: 2 / 3, Step: 675 / 750 Loss: 0.1351\n",
      "Epoch: 2 / 3, Step: 676 / 750 Loss: 0.1542\n",
      "Epoch: 2 / 3, Step: 677 / 750 Loss: 0.1809\n",
      "Epoch: 2 / 3, Step: 678 / 750 Loss: 0.0591\n",
      "Epoch: 2 / 3, Step: 679 / 750 Loss: 0.1758\n",
      "Epoch: 2 / 3, Step: 680 / 750 Loss: 0.1485\n",
      "Epoch: 2 / 3, Step: 681 / 750 Loss: 0.1510\n",
      "Epoch: 2 / 3, Step: 682 / 750 Loss: 0.1387\n",
      "Epoch: 2 / 3, Step: 683 / 750 Loss: 0.2358\n",
      "Epoch: 2 / 3, Step: 684 / 750 Loss: 0.3113\n",
      "Epoch: 2 / 3, Step: 685 / 750 Loss: 0.1760\n",
      "Epoch: 2 / 3, Step: 686 / 750 Loss: 0.2346\n",
      "Epoch: 2 / 3, Step: 687 / 750 Loss: 0.1618\n",
      "Epoch: 2 / 3, Step: 688 / 750 Loss: 0.1926\n",
      "Epoch: 2 / 3, Step: 689 / 750 Loss: 0.2872\n",
      "Epoch: 2 / 3, Step: 690 / 750 Loss: 0.1565\n",
      "Epoch: 2 / 3, Step: 691 / 750 Loss: 0.4329\n",
      "Epoch: 2 / 3, Step: 692 / 750 Loss: 0.1504\n",
      "Epoch: 2 / 3, Step: 693 / 750 Loss: 0.1313\n",
      "Epoch: 2 / 3, Step: 694 / 750 Loss: 0.4915\n",
      "Epoch: 2 / 3, Step: 695 / 750 Loss: 0.1278\n",
      "Epoch: 2 / 3, Step: 696 / 750 Loss: 0.3285\n",
      "Epoch: 2 / 3, Step: 697 / 750 Loss: 0.3090\n",
      "Epoch: 2 / 3, Step: 698 / 750 Loss: 0.4432\n",
      "Epoch: 2 / 3, Step: 699 / 750 Loss: 0.1643\n",
      "Epoch: 2 / 3, Step: 700 / 750 Loss: 0.2026\n",
      "Epoch: 2 / 3, Step: 701 / 750 Loss: 0.2106\n",
      "Epoch: 2 / 3, Step: 702 / 750 Loss: 0.1342\n",
      "Epoch: 2 / 3, Step: 703 / 750 Loss: 0.0940\n",
      "Epoch: 2 / 3, Step: 704 / 750 Loss: 0.2202\n",
      "Epoch: 2 / 3, Step: 705 / 750 Loss: 0.5364\n",
      "Epoch: 2 / 3, Step: 706 / 750 Loss: 0.3381\n",
      "Epoch: 2 / 3, Step: 707 / 750 Loss: 0.0465\n",
      "Epoch: 2 / 3, Step: 708 / 750 Loss: 0.5040\n",
      "Epoch: 2 / 3, Step: 709 / 750 Loss: 0.2075\n",
      "Epoch: 2 / 3, Step: 710 / 750 Loss: 0.1622\n",
      "Epoch: 2 / 3, Step: 711 / 750 Loss: 0.1492\n",
      "Epoch: 2 / 3, Step: 712 / 750 Loss: 0.2990\n",
      "Epoch: 2 / 3, Step: 713 / 750 Loss: 0.2298\n",
      "Epoch: 2 / 3, Step: 714 / 750 Loss: 0.2035\n",
      "Epoch: 2 / 3, Step: 715 / 750 Loss: 0.1570\n",
      "Epoch: 2 / 3, Step: 716 / 750 Loss: 0.4435\n",
      "Epoch: 2 / 3, Step: 717 / 750 Loss: 0.3155\n",
      "Epoch: 2 / 3, Step: 718 / 750 Loss: 0.2553\n",
      "Epoch: 2 / 3, Step: 719 / 750 Loss: 0.1279\n",
      "Epoch: 2 / 3, Step: 720 / 750 Loss: 0.1973\n",
      "Epoch: 2 / 3, Step: 721 / 750 Loss: 0.3227\n",
      "Epoch: 2 / 3, Step: 722 / 750 Loss: 0.2317\n",
      "Epoch: 2 / 3, Step: 723 / 750 Loss: 0.2213\n",
      "Epoch: 2 / 3, Step: 724 / 750 Loss: 0.3407\n",
      "Epoch: 2 / 3, Step: 725 / 750 Loss: 0.2701\n",
      "Epoch: 2 / 3, Step: 726 / 750 Loss: 0.1632\n",
      "Epoch: 2 / 3, Step: 727 / 750 Loss: 0.1168\n",
      "Epoch: 2 / 3, Step: 728 / 750 Loss: 0.3443\n",
      "Epoch: 2 / 3, Step: 729 / 750 Loss: 0.2262\n",
      "Epoch: 2 / 3, Step: 730 / 750 Loss: 0.2060\n",
      "Epoch: 2 / 3, Step: 731 / 750 Loss: 0.2616\n",
      "Epoch: 2 / 3, Step: 732 / 750 Loss: 0.2155\n",
      "Epoch: 2 / 3, Step: 733 / 750 Loss: 0.1832\n",
      "Epoch: 2 / 3, Step: 734 / 750 Loss: 0.4189\n",
      "Epoch: 2 / 3, Step: 735 / 750 Loss: 0.1302\n",
      "Epoch: 2 / 3, Step: 736 / 750 Loss: 0.2864\n",
      "Epoch: 2 / 3, Step: 737 / 750 Loss: 0.1598\n",
      "Epoch: 2 / 3, Step: 738 / 750 Loss: 0.1220\n",
      "Epoch: 2 / 3, Step: 739 / 750 Loss: 0.1589\n",
      "Epoch: 2 / 3, Step: 740 / 750 Loss: 0.2533\n",
      "Epoch: 2 / 3, Step: 741 / 750 Loss: 0.1842\n",
      "Epoch: 2 / 3, Step: 742 / 750 Loss: 0.1854\n",
      "Epoch: 2 / 3, Step: 743 / 750 Loss: 0.1764\n",
      "Epoch: 2 / 3, Step: 744 / 750 Loss: 0.1548\n",
      "Epoch: 2 / 3, Step: 745 / 750 Loss: 0.2665\n",
      "Epoch: 2 / 3, Step: 746 / 750 Loss: 0.1755\n",
      "Epoch: 2 / 3, Step: 747 / 750 Loss: 0.1945\n",
      "Epoch: 2 / 3, Step: 748 / 750 Loss: 0.1782\n",
      "Epoch: 2 / 3, Step: 749 / 750 Loss: 0.1864\n",
      "Epoch: 3 / 3, Step: 0 / 750 Loss: 0.2690\n",
      "Epoch: 3 / 3, Step: 1 / 750 Loss: 0.1840\n",
      "Epoch: 3 / 3, Step: 2 / 750 Loss: 0.0911\n",
      "Epoch: 3 / 3, Step: 3 / 750 Loss: 0.2885\n",
      "Epoch: 3 / 3, Step: 4 / 750 Loss: 0.0540\n",
      "Epoch: 3 / 3, Step: 5 / 750 Loss: 0.2520\n",
      "Epoch: 3 / 3, Step: 6 / 750 Loss: 0.1786\n",
      "Epoch: 3 / 3, Step: 7 / 750 Loss: 0.1291\n",
      "Epoch: 3 / 3, Step: 8 / 750 Loss: 0.1471\n",
      "Epoch: 3 / 3, Step: 9 / 750 Loss: 0.1007\n",
      "Epoch: 3 / 3, Step: 10 / 750 Loss: 0.2088\n",
      "Epoch: 3 / 3, Step: 11 / 750 Loss: 0.2809\n",
      "Epoch: 3 / 3, Step: 12 / 750 Loss: 0.1337\n",
      "Epoch: 3 / 3, Step: 13 / 750 Loss: 0.2099\n",
      "Epoch: 3 / 3, Step: 14 / 750 Loss: 0.0585\n",
      "Epoch: 3 / 3, Step: 15 / 750 Loss: 0.1205\n",
      "Epoch: 3 / 3, Step: 16 / 750 Loss: 0.3577\n",
      "Epoch: 3 / 3, Step: 17 / 750 Loss: 0.2114\n",
      "Epoch: 3 / 3, Step: 18 / 750 Loss: 0.1376\n",
      "Epoch: 3 / 3, Step: 19 / 750 Loss: 0.2182\n",
      "Epoch: 3 / 3, Step: 20 / 750 Loss: 0.0889\n",
      "Epoch: 3 / 3, Step: 21 / 750 Loss: 0.0896\n",
      "Epoch: 3 / 3, Step: 22 / 750 Loss: 0.2987\n",
      "Epoch: 3 / 3, Step: 23 / 750 Loss: 0.1867\n",
      "Epoch: 3 / 3, Step: 24 / 750 Loss: 0.1900\n",
      "Epoch: 3 / 3, Step: 25 / 750 Loss: 0.3346\n",
      "Epoch: 3 / 3, Step: 26 / 750 Loss: 0.2583\n",
      "Epoch: 3 / 3, Step: 27 / 750 Loss: 0.1979\n",
      "Epoch: 3 / 3, Step: 28 / 750 Loss: 0.4042\n",
      "Epoch: 3 / 3, Step: 29 / 750 Loss: 0.0844\n",
      "Epoch: 3 / 3, Step: 30 / 750 Loss: 0.1554\n",
      "Epoch: 3 / 3, Step: 31 / 750 Loss: 0.1170\n",
      "Epoch: 3 / 3, Step: 32 / 750 Loss: 0.0265\n",
      "Epoch: 3 / 3, Step: 33 / 750 Loss: 0.1489\n",
      "Epoch: 3 / 3, Step: 34 / 750 Loss: 0.1058\n",
      "Epoch: 3 / 3, Step: 35 / 750 Loss: 0.1817\n",
      "Epoch: 3 / 3, Step: 36 / 750 Loss: 0.0654\n",
      "Epoch: 3 / 3, Step: 37 / 750 Loss: 0.0804\n",
      "Epoch: 3 / 3, Step: 38 / 750 Loss: 0.1355\n",
      "Epoch: 3 / 3, Step: 39 / 750 Loss: 0.1515\n",
      "Epoch: 3 / 3, Step: 40 / 750 Loss: 0.0921\n",
      "Epoch: 3 / 3, Step: 41 / 750 Loss: 0.2999\n",
      "Epoch: 3 / 3, Step: 42 / 750 Loss: 0.2016\n",
      "Epoch: 3 / 3, Step: 43 / 750 Loss: 0.2386\n",
      "Epoch: 3 / 3, Step: 44 / 750 Loss: 0.2073\n",
      "Epoch: 3 / 3, Step: 45 / 750 Loss: 0.1424\n",
      "Epoch: 3 / 3, Step: 46 / 750 Loss: 0.2374\n",
      "Epoch: 3 / 3, Step: 47 / 750 Loss: 0.1448\n",
      "Epoch: 3 / 3, Step: 48 / 750 Loss: 0.0379\n",
      "Epoch: 3 / 3, Step: 49 / 750 Loss: 0.0381\n",
      "Epoch: 3 / 3, Step: 50 / 750 Loss: 0.0570\n",
      "Epoch: 3 / 3, Step: 51 / 750 Loss: 0.1536\n",
      "Epoch: 3 / 3, Step: 52 / 750 Loss: 0.3070\n",
      "Epoch: 3 / 3, Step: 53 / 750 Loss: 0.1209\n",
      "Epoch: 3 / 3, Step: 54 / 750 Loss: 0.3701\n",
      "Epoch: 3 / 3, Step: 55 / 750 Loss: 0.1594\n",
      "Epoch: 3 / 3, Step: 56 / 750 Loss: 0.1092\n",
      "Epoch: 3 / 3, Step: 57 / 750 Loss: 0.1297\n",
      "Epoch: 3 / 3, Step: 58 / 750 Loss: 0.1265\n",
      "Epoch: 3 / 3, Step: 59 / 750 Loss: 0.0930\n",
      "Epoch: 3 / 3, Step: 60 / 750 Loss: 0.3196\n",
      "Epoch: 3 / 3, Step: 61 / 750 Loss: 0.2009\n",
      "Epoch: 3 / 3, Step: 62 / 750 Loss: 0.2523\n",
      "Epoch: 3 / 3, Step: 63 / 750 Loss: 0.1854\n",
      "Epoch: 3 / 3, Step: 64 / 750 Loss: 0.5028\n",
      "Epoch: 3 / 3, Step: 65 / 750 Loss: 0.2711\n",
      "Epoch: 3 / 3, Step: 66 / 750 Loss: 0.1554\n",
      "Epoch: 3 / 3, Step: 67 / 750 Loss: 0.1415\n",
      "Epoch: 3 / 3, Step: 68 / 750 Loss: 0.2475\n",
      "Epoch: 3 / 3, Step: 69 / 750 Loss: 0.0936\n",
      "Epoch: 3 / 3, Step: 70 / 750 Loss: 0.1716\n",
      "Epoch: 3 / 3, Step: 71 / 750 Loss: 0.0422\n",
      "Epoch: 3 / 3, Step: 72 / 750 Loss: 0.0705\n",
      "Epoch: 3 / 3, Step: 73 / 750 Loss: 0.0895\n",
      "Epoch: 3 / 3, Step: 74 / 750 Loss: 0.0797\n",
      "Epoch: 3 / 3, Step: 75 / 750 Loss: 0.3491\n",
      "Epoch: 3 / 3, Step: 76 / 750 Loss: 0.1231\n",
      "Epoch: 3 / 3, Step: 77 / 750 Loss: 0.0519\n",
      "Epoch: 3 / 3, Step: 78 / 750 Loss: 0.1853\n",
      "Epoch: 3 / 3, Step: 79 / 750 Loss: 0.1222\n",
      "Epoch: 3 / 3, Step: 80 / 750 Loss: 0.1956\n",
      "Epoch: 3 / 3, Step: 81 / 750 Loss: 0.2763\n",
      "Epoch: 3 / 3, Step: 82 / 750 Loss: 0.1722\n",
      "Epoch: 3 / 3, Step: 83 / 750 Loss: 0.1771\n",
      "Epoch: 3 / 3, Step: 84 / 750 Loss: 0.3199\n",
      "Epoch: 3 / 3, Step: 85 / 750 Loss: 0.1244\n",
      "Epoch: 3 / 3, Step: 86 / 750 Loss: 0.1886\n",
      "Epoch: 3 / 3, Step: 87 / 750 Loss: 0.0388\n",
      "Epoch: 3 / 3, Step: 88 / 750 Loss: 0.3656\n",
      "Epoch: 3 / 3, Step: 89 / 750 Loss: 0.1623\n",
      "Epoch: 3 / 3, Step: 90 / 750 Loss: 0.2044\n",
      "Epoch: 3 / 3, Step: 91 / 750 Loss: 0.2181\n",
      "Epoch: 3 / 3, Step: 92 / 750 Loss: 0.3014\n",
      "Epoch: 3 / 3, Step: 93 / 750 Loss: 0.0567\n",
      "Epoch: 3 / 3, Step: 94 / 750 Loss: 0.0843\n",
      "Epoch: 3 / 3, Step: 95 / 750 Loss: 0.3046\n",
      "Epoch: 3 / 3, Step: 96 / 750 Loss: 0.2035\n",
      "Epoch: 3 / 3, Step: 97 / 750 Loss: 0.2472\n",
      "Epoch: 3 / 3, Step: 98 / 750 Loss: 0.1119\n",
      "Epoch: 3 / 3, Step: 99 / 750 Loss: 0.0615\n",
      "Epoch: 3 / 3, Step: 100 / 750 Loss: 0.1420\n",
      "Epoch: 3 / 3, Step: 101 / 750 Loss: 0.2022\n",
      "Epoch: 3 / 3, Step: 102 / 750 Loss: 0.3068\n",
      "Epoch: 3 / 3, Step: 103 / 750 Loss: 0.2843\n",
      "Epoch: 3 / 3, Step: 104 / 750 Loss: 0.2225\n",
      "Epoch: 3 / 3, Step: 105 / 750 Loss: 0.0743\n",
      "Epoch: 3 / 3, Step: 106 / 750 Loss: 0.1290\n",
      "Epoch: 3 / 3, Step: 107 / 750 Loss: 0.1208\n",
      "Epoch: 3 / 3, Step: 108 / 750 Loss: 0.3146\n",
      "Epoch: 3 / 3, Step: 109 / 750 Loss: 0.0874\n",
      "Epoch: 3 / 3, Step: 110 / 750 Loss: 0.0249\n",
      "Epoch: 3 / 3, Step: 111 / 750 Loss: 0.1981\n",
      "Epoch: 3 / 3, Step: 112 / 750 Loss: 0.1288\n",
      "Epoch: 3 / 3, Step: 113 / 750 Loss: 0.1203\n",
      "Epoch: 3 / 3, Step: 114 / 750 Loss: 0.0637\n",
      "Epoch: 3 / 3, Step: 115 / 750 Loss: 0.3242\n",
      "Epoch: 3 / 3, Step: 116 / 750 Loss: 0.0768\n",
      "Epoch: 3 / 3, Step: 117 / 750 Loss: 0.0928\n",
      "Epoch: 3 / 3, Step: 118 / 750 Loss: 0.1991\n",
      "Epoch: 3 / 3, Step: 119 / 750 Loss: 0.1637\n",
      "Epoch: 3 / 3, Step: 120 / 750 Loss: 0.0665\n",
      "Epoch: 3 / 3, Step: 121 / 750 Loss: 0.2037\n",
      "Epoch: 3 / 3, Step: 122 / 750 Loss: 0.2020\n",
      "Epoch: 3 / 3, Step: 123 / 750 Loss: 0.2290\n",
      "Epoch: 3 / 3, Step: 124 / 750 Loss: 0.2068\n",
      "Epoch: 3 / 3, Step: 125 / 750 Loss: 0.1009\n",
      "Epoch: 3 / 3, Step: 126 / 750 Loss: 0.4861\n",
      "Epoch: 3 / 3, Step: 127 / 750 Loss: 0.2959\n",
      "Epoch: 3 / 3, Step: 128 / 750 Loss: 0.1631\n",
      "Epoch: 3 / 3, Step: 129 / 750 Loss: 0.1114\n",
      "Epoch: 3 / 3, Step: 130 / 750 Loss: 0.1459\n",
      "Epoch: 3 / 3, Step: 131 / 750 Loss: 0.2639\n",
      "Epoch: 3 / 3, Step: 132 / 750 Loss: 0.0541\n",
      "Epoch: 3 / 3, Step: 133 / 750 Loss: 0.3140\n",
      "Epoch: 3 / 3, Step: 134 / 750 Loss: 0.0640\n",
      "Epoch: 3 / 3, Step: 135 / 750 Loss: 0.0447\n",
      "Epoch: 3 / 3, Step: 136 / 750 Loss: 0.1810\n",
      "Epoch: 3 / 3, Step: 137 / 750 Loss: 0.2508\n",
      "Epoch: 3 / 3, Step: 138 / 750 Loss: 0.3264\n",
      "Epoch: 3 / 3, Step: 139 / 750 Loss: 0.0827\n",
      "Epoch: 3 / 3, Step: 140 / 750 Loss: 0.1907\n",
      "Epoch: 3 / 3, Step: 141 / 750 Loss: 0.4179\n",
      "Epoch: 3 / 3, Step: 142 / 750 Loss: 0.1326\n",
      "Epoch: 3 / 3, Step: 143 / 750 Loss: 0.3611\n",
      "Epoch: 3 / 3, Step: 144 / 750 Loss: 0.3871\n",
      "Epoch: 3 / 3, Step: 145 / 750 Loss: 0.2984\n",
      "Epoch: 3 / 3, Step: 146 / 750 Loss: 0.2484\n",
      "Epoch: 3 / 3, Step: 147 / 750 Loss: 0.2835\n",
      "Epoch: 3 / 3, Step: 148 / 750 Loss: 0.1213\n",
      "Epoch: 3 / 3, Step: 149 / 750 Loss: 0.3530\n",
      "Epoch: 3 / 3, Step: 150 / 750 Loss: 0.0952\n",
      "Epoch: 3 / 3, Step: 151 / 750 Loss: 0.1184\n",
      "Epoch: 3 / 3, Step: 152 / 750 Loss: 0.3279\n",
      "Epoch: 3 / 3, Step: 153 / 750 Loss: 0.0740\n",
      "Epoch: 3 / 3, Step: 154 / 750 Loss: 0.0869\n",
      "Epoch: 3 / 3, Step: 155 / 750 Loss: 0.2524\n",
      "Epoch: 3 / 3, Step: 156 / 750 Loss: 0.0951\n",
      "Epoch: 3 / 3, Step: 157 / 750 Loss: 0.0581\n",
      "Epoch: 3 / 3, Step: 158 / 750 Loss: 0.1922\n",
      "Epoch: 3 / 3, Step: 159 / 750 Loss: 0.4095\n",
      "Epoch: 3 / 3, Step: 160 / 750 Loss: 0.2888\n",
      "Epoch: 3 / 3, Step: 161 / 750 Loss: 0.2873\n",
      "Epoch: 3 / 3, Step: 162 / 750 Loss: 0.2215\n",
      "Epoch: 3 / 3, Step: 163 / 750 Loss: 0.0633\n",
      "Epoch: 3 / 3, Step: 164 / 750 Loss: 0.1331\n",
      "Epoch: 3 / 3, Step: 165 / 750 Loss: 0.2272\n",
      "Epoch: 3 / 3, Step: 166 / 750 Loss: 0.4727\n",
      "Epoch: 3 / 3, Step: 167 / 750 Loss: 0.1745\n",
      "Epoch: 3 / 3, Step: 168 / 750 Loss: 0.2810\n",
      "Epoch: 3 / 3, Step: 169 / 750 Loss: 0.2242\n",
      "Epoch: 3 / 3, Step: 170 / 750 Loss: 0.2344\n",
      "Epoch: 3 / 3, Step: 171 / 750 Loss: 0.1900\n",
      "Epoch: 3 / 3, Step: 172 / 750 Loss: 0.3239\n",
      "Epoch: 3 / 3, Step: 173 / 750 Loss: 0.2492\n",
      "Epoch: 3 / 3, Step: 174 / 750 Loss: 0.2151\n",
      "Epoch: 3 / 3, Step: 175 / 750 Loss: 0.1597\n",
      "Epoch: 3 / 3, Step: 176 / 750 Loss: 0.2240\n",
      "Epoch: 3 / 3, Step: 177 / 750 Loss: 0.2620\n",
      "Epoch: 3 / 3, Step: 178 / 750 Loss: 0.1383\n",
      "Epoch: 3 / 3, Step: 179 / 750 Loss: 0.1853\n",
      "Epoch: 3 / 3, Step: 180 / 750 Loss: 0.1846\n",
      "Epoch: 3 / 3, Step: 181 / 750 Loss: 0.1188\n",
      "Epoch: 3 / 3, Step: 182 / 750 Loss: 0.1307\n",
      "Epoch: 3 / 3, Step: 183 / 750 Loss: 0.1095\n",
      "Epoch: 3 / 3, Step: 184 / 750 Loss: 0.1969\n",
      "Epoch: 3 / 3, Step: 185 / 750 Loss: 0.2656\n",
      "Epoch: 3 / 3, Step: 186 / 750 Loss: 0.2528\n",
      "Epoch: 3 / 3, Step: 187 / 750 Loss: 0.1362\n",
      "Epoch: 3 / 3, Step: 188 / 750 Loss: 0.2134\n",
      "Epoch: 3 / 3, Step: 189 / 750 Loss: 0.1495\n",
      "Epoch: 3 / 3, Step: 190 / 750 Loss: 0.0951\n",
      "Epoch: 3 / 3, Step: 191 / 750 Loss: 0.1588\n",
      "Epoch: 3 / 3, Step: 192 / 750 Loss: 0.1507\n",
      "Epoch: 3 / 3, Step: 193 / 750 Loss: 0.2124\n",
      "Epoch: 3 / 3, Step: 194 / 750 Loss: 0.1944\n",
      "Epoch: 3 / 3, Step: 195 / 750 Loss: 0.1703\n",
      "Epoch: 3 / 3, Step: 196 / 750 Loss: 0.2888\n",
      "Epoch: 3 / 3, Step: 197 / 750 Loss: 0.2248\n",
      "Epoch: 3 / 3, Step: 198 / 750 Loss: 0.0983\n",
      "Epoch: 3 / 3, Step: 199 / 750 Loss: 0.3039\n",
      "Epoch: 3 / 3, Step: 200 / 750 Loss: 0.1627\n",
      "Epoch: 3 / 3, Step: 201 / 750 Loss: 0.2333\n",
      "Epoch: 3 / 3, Step: 202 / 750 Loss: 0.0838\n",
      "Epoch: 3 / 3, Step: 203 / 750 Loss: 0.1750\n",
      "Epoch: 3 / 3, Step: 204 / 750 Loss: 0.1154\n",
      "Epoch: 3 / 3, Step: 205 / 750 Loss: 0.0840\n",
      "Epoch: 3 / 3, Step: 206 / 750 Loss: 0.2305\n",
      "Epoch: 3 / 3, Step: 207 / 750 Loss: 0.1351\n",
      "Epoch: 3 / 3, Step: 208 / 750 Loss: 0.2239\n",
      "Epoch: 3 / 3, Step: 209 / 750 Loss: 0.2761\n",
      "Epoch: 3 / 3, Step: 210 / 750 Loss: 0.2877\n",
      "Epoch: 3 / 3, Step: 211 / 750 Loss: 0.4038\n",
      "Epoch: 3 / 3, Step: 212 / 750 Loss: 0.3180\n",
      "Epoch: 3 / 3, Step: 213 / 750 Loss: 0.3217\n",
      "Epoch: 3 / 3, Step: 214 / 750 Loss: 0.1844\n",
      "Epoch: 3 / 3, Step: 215 / 750 Loss: 0.1150\n",
      "Epoch: 3 / 3, Step: 216 / 750 Loss: 0.3102\n",
      "Epoch: 3 / 3, Step: 217 / 750 Loss: 0.2432\n",
      "Epoch: 3 / 3, Step: 218 / 750 Loss: 0.2462\n",
      "Epoch: 3 / 3, Step: 219 / 750 Loss: 0.2281\n",
      "Epoch: 3 / 3, Step: 220 / 750 Loss: 0.2078\n",
      "Epoch: 3 / 3, Step: 221 / 750 Loss: 0.0924\n",
      "Epoch: 3 / 3, Step: 222 / 750 Loss: 0.2129\n",
      "Epoch: 3 / 3, Step: 223 / 750 Loss: 0.0934\n",
      "Epoch: 3 / 3, Step: 224 / 750 Loss: 0.2552\n",
      "Epoch: 3 / 3, Step: 225 / 750 Loss: 0.1862\n",
      "Epoch: 3 / 3, Step: 226 / 750 Loss: 0.0439\n",
      "Epoch: 3 / 3, Step: 227 / 750 Loss: 0.3212\n",
      "Epoch: 3 / 3, Step: 228 / 750 Loss: 0.1536\n",
      "Epoch: 3 / 3, Step: 229 / 750 Loss: 0.3318\n",
      "Epoch: 3 / 3, Step: 230 / 750 Loss: 0.1614\n",
      "Epoch: 3 / 3, Step: 231 / 750 Loss: 0.1728\n",
      "Epoch: 3 / 3, Step: 232 / 750 Loss: 0.0767\n",
      "Epoch: 3 / 3, Step: 233 / 750 Loss: 0.6597\n",
      "Epoch: 3 / 3, Step: 234 / 750 Loss: 0.0848\n",
      "Epoch: 3 / 3, Step: 235 / 750 Loss: 0.1174\n",
      "Epoch: 3 / 3, Step: 236 / 750 Loss: 0.0726\n",
      "Epoch: 3 / 3, Step: 237 / 750 Loss: 0.2645\n",
      "Epoch: 3 / 3, Step: 238 / 750 Loss: 0.1477\n",
      "Epoch: 3 / 3, Step: 239 / 750 Loss: 0.1275\n",
      "Epoch: 3 / 3, Step: 240 / 750 Loss: 0.1263\n",
      "Epoch: 3 / 3, Step: 241 / 750 Loss: 0.2407\n",
      "Epoch: 3 / 3, Step: 242 / 750 Loss: 0.3156\n",
      "Epoch: 3 / 3, Step: 243 / 750 Loss: 0.0655\n",
      "Epoch: 3 / 3, Step: 244 / 750 Loss: 0.0879\n",
      "Epoch: 3 / 3, Step: 245 / 750 Loss: 0.0979\n",
      "Epoch: 3 / 3, Step: 246 / 750 Loss: 0.0684\n",
      "Epoch: 3 / 3, Step: 247 / 750 Loss: 0.2021\n",
      "Epoch: 3 / 3, Step: 248 / 750 Loss: 0.2117\n",
      "Epoch: 3 / 3, Step: 249 / 750 Loss: 0.1128\n",
      "Epoch: 3 / 3, Step: 250 / 750 Loss: 0.1854\n",
      "Epoch: 3 / 3, Step: 251 / 750 Loss: 0.1390\n",
      "Epoch: 3 / 3, Step: 252 / 750 Loss: 0.3650\n",
      "Epoch: 3 / 3, Step: 253 / 750 Loss: 0.1150\n",
      "Epoch: 3 / 3, Step: 254 / 750 Loss: 0.0779\n",
      "Epoch: 3 / 3, Step: 255 / 750 Loss: 0.1232\n",
      "Epoch: 3 / 3, Step: 256 / 750 Loss: 0.1446\n",
      "Epoch: 3 / 3, Step: 257 / 750 Loss: 0.3594\n",
      "Epoch: 3 / 3, Step: 258 / 750 Loss: 0.0696\n",
      "Epoch: 3 / 3, Step: 259 / 750 Loss: 0.1228\n",
      "Epoch: 3 / 3, Step: 260 / 750 Loss: 0.2218\n",
      "Epoch: 3 / 3, Step: 261 / 750 Loss: 0.0833\n",
      "Epoch: 3 / 3, Step: 262 / 750 Loss: 0.1373\n",
      "Epoch: 3 / 3, Step: 263 / 750 Loss: 0.2573\n",
      "Epoch: 3 / 3, Step: 264 / 750 Loss: 0.2148\n",
      "Epoch: 3 / 3, Step: 265 / 750 Loss: 0.1161\n",
      "Epoch: 3 / 3, Step: 266 / 750 Loss: 0.1630\n",
      "Epoch: 3 / 3, Step: 267 / 750 Loss: 0.0637\n",
      "Epoch: 3 / 3, Step: 268 / 750 Loss: 0.0537\n",
      "Epoch: 3 / 3, Step: 269 / 750 Loss: 0.1296\n",
      "Epoch: 3 / 3, Step: 270 / 750 Loss: 0.1952\n",
      "Epoch: 3 / 3, Step: 271 / 750 Loss: 0.1830\n",
      "Epoch: 3 / 3, Step: 272 / 750 Loss: 0.1262\n",
      "Epoch: 3 / 3, Step: 273 / 750 Loss: 0.0690\n",
      "Epoch: 3 / 3, Step: 274 / 750 Loss: 0.2092\n",
      "Epoch: 3 / 3, Step: 275 / 750 Loss: 0.3701\n",
      "Epoch: 3 / 3, Step: 276 / 750 Loss: 0.1183\n",
      "Epoch: 3 / 3, Step: 277 / 750 Loss: 0.1422\n",
      "Epoch: 3 / 3, Step: 278 / 750 Loss: 0.1742\n",
      "Epoch: 3 / 3, Step: 279 / 750 Loss: 0.2709\n",
      "Epoch: 3 / 3, Step: 280 / 750 Loss: 0.1323\n",
      "Epoch: 3 / 3, Step: 281 / 750 Loss: 0.2155\n",
      "Epoch: 3 / 3, Step: 282 / 750 Loss: 0.2094\n",
      "Epoch: 3 / 3, Step: 283 / 750 Loss: 0.2445\n",
      "Epoch: 3 / 3, Step: 284 / 750 Loss: 0.1704\n",
      "Epoch: 3 / 3, Step: 285 / 750 Loss: 0.1839\n",
      "Epoch: 3 / 3, Step: 286 / 750 Loss: 0.1242\n",
      "Epoch: 3 / 3, Step: 287 / 750 Loss: 0.1293\n",
      "Epoch: 3 / 3, Step: 288 / 750 Loss: 0.0918\n",
      "Epoch: 3 / 3, Step: 289 / 750 Loss: 0.2168\n",
      "Epoch: 3 / 3, Step: 290 / 750 Loss: 0.4592\n",
      "Epoch: 3 / 3, Step: 291 / 750 Loss: 0.3496\n",
      "Epoch: 3 / 3, Step: 292 / 750 Loss: 0.0681\n",
      "Epoch: 3 / 3, Step: 293 / 750 Loss: 0.3119\n",
      "Epoch: 3 / 3, Step: 294 / 750 Loss: 0.2712\n",
      "Epoch: 3 / 3, Step: 295 / 750 Loss: 0.2198\n",
      "Epoch: 3 / 3, Step: 296 / 750 Loss: 0.1195\n",
      "Epoch: 3 / 3, Step: 297 / 750 Loss: 0.1548\n",
      "Epoch: 3 / 3, Step: 298 / 750 Loss: 0.2024\n",
      "Epoch: 3 / 3, Step: 299 / 750 Loss: 0.0690\n",
      "Epoch: 3 / 3, Step: 300 / 750 Loss: 0.1316\n",
      "Epoch: 3 / 3, Step: 301 / 750 Loss: 0.0816\n",
      "Epoch: 3 / 3, Step: 302 / 750 Loss: 0.2734\n",
      "Epoch: 3 / 3, Step: 303 / 750 Loss: 0.1416\n",
      "Epoch: 3 / 3, Step: 304 / 750 Loss: 0.1092\n",
      "Epoch: 3 / 3, Step: 305 / 750 Loss: 0.2464\n",
      "Epoch: 3 / 3, Step: 306 / 750 Loss: 0.1520\n",
      "Epoch: 3 / 3, Step: 307 / 750 Loss: 0.1207\n",
      "Epoch: 3 / 3, Step: 308 / 750 Loss: 0.2510\n",
      "Epoch: 3 / 3, Step: 309 / 750 Loss: 0.0165\n",
      "Epoch: 3 / 3, Step: 310 / 750 Loss: 0.2472\n",
      "Epoch: 3 / 3, Step: 311 / 750 Loss: 0.2965\n",
      "Epoch: 3 / 3, Step: 312 / 750 Loss: 0.1381\n",
      "Epoch: 3 / 3, Step: 313 / 750 Loss: 0.2034\n",
      "Epoch: 3 / 3, Step: 314 / 750 Loss: 0.1306\n",
      "Epoch: 3 / 3, Step: 315 / 750 Loss: 0.1334\n",
      "Epoch: 3 / 3, Step: 316 / 750 Loss: 0.1023\n",
      "Epoch: 3 / 3, Step: 317 / 750 Loss: 0.1590\n",
      "Epoch: 3 / 3, Step: 318 / 750 Loss: 0.2049\n",
      "Epoch: 3 / 3, Step: 319 / 750 Loss: 0.0515\n",
      "Epoch: 3 / 3, Step: 320 / 750 Loss: 0.1347\n",
      "Epoch: 3 / 3, Step: 321 / 750 Loss: 0.4508\n",
      "Epoch: 3 / 3, Step: 322 / 750 Loss: 0.1633\n",
      "Epoch: 3 / 3, Step: 323 / 750 Loss: 0.2385\n",
      "Epoch: 3 / 3, Step: 324 / 750 Loss: 0.3987\n",
      "Epoch: 3 / 3, Step: 325 / 750 Loss: 0.1049\n",
      "Epoch: 3 / 3, Step: 326 / 750 Loss: 0.2469\n",
      "Epoch: 3 / 3, Step: 327 / 750 Loss: 0.1800\n",
      "Epoch: 3 / 3, Step: 328 / 750 Loss: 0.2320\n",
      "Epoch: 3 / 3, Step: 329 / 750 Loss: 0.1133\n",
      "Epoch: 3 / 3, Step: 330 / 750 Loss: 0.1936\n",
      "Epoch: 3 / 3, Step: 331 / 750 Loss: 0.1835\n",
      "Epoch: 3 / 3, Step: 332 / 750 Loss: 0.0552\n",
      "Epoch: 3 / 3, Step: 333 / 750 Loss: 0.1052\n",
      "Epoch: 3 / 3, Step: 334 / 750 Loss: 0.2286\n",
      "Epoch: 3 / 3, Step: 335 / 750 Loss: 0.2170\n",
      "Epoch: 3 / 3, Step: 336 / 750 Loss: 0.2139\n",
      "Epoch: 3 / 3, Step: 337 / 750 Loss: 0.2806\n",
      "Epoch: 3 / 3, Step: 338 / 750 Loss: 0.0562\n",
      "Epoch: 3 / 3, Step: 339 / 750 Loss: 0.2535\n",
      "Epoch: 3 / 3, Step: 340 / 750 Loss: 0.0728\n",
      "Epoch: 3 / 3, Step: 341 / 750 Loss: 0.2039\n",
      "Epoch: 3 / 3, Step: 342 / 750 Loss: 0.0498\n",
      "Epoch: 3 / 3, Step: 343 / 750 Loss: 0.2117\n",
      "Epoch: 3 / 3, Step: 344 / 750 Loss: 0.0663\n",
      "Epoch: 3 / 3, Step: 345 / 750 Loss: 0.0468\n",
      "Epoch: 3 / 3, Step: 346 / 750 Loss: 0.2166\n",
      "Epoch: 3 / 3, Step: 347 / 750 Loss: 0.2819\n",
      "Epoch: 3 / 3, Step: 348 / 750 Loss: 0.1461\n",
      "Epoch: 3 / 3, Step: 349 / 750 Loss: 0.2759\n",
      "Epoch: 3 / 3, Step: 350 / 750 Loss: 0.2414\n",
      "Epoch: 3 / 3, Step: 351 / 750 Loss: 0.1883\n",
      "Epoch: 3 / 3, Step: 352 / 750 Loss: 0.0474\n",
      "Epoch: 3 / 3, Step: 353 / 750 Loss: 0.3844\n",
      "Epoch: 3 / 3, Step: 354 / 750 Loss: 0.4306\n",
      "Epoch: 3 / 3, Step: 355 / 750 Loss: 0.2029\n",
      "Epoch: 3 / 3, Step: 356 / 750 Loss: 0.1734\n",
      "Epoch: 3 / 3, Step: 357 / 750 Loss: 0.1849\n",
      "Epoch: 3 / 3, Step: 358 / 750 Loss: 0.1950\n",
      "Epoch: 3 / 3, Step: 359 / 750 Loss: 0.3897\n",
      "Epoch: 3 / 3, Step: 360 / 750 Loss: 0.0727\n",
      "Epoch: 3 / 3, Step: 361 / 750 Loss: 0.1919\n",
      "Epoch: 3 / 3, Step: 362 / 750 Loss: 0.2238\n",
      "Epoch: 3 / 3, Step: 363 / 750 Loss: 0.2817\n",
      "Epoch: 3 / 3, Step: 364 / 750 Loss: 0.3752\n",
      "Epoch: 3 / 3, Step: 365 / 750 Loss: 0.2616\n",
      "Epoch: 3 / 3, Step: 366 / 750 Loss: 0.0609\n",
      "Epoch: 3 / 3, Step: 367 / 750 Loss: 0.3102\n",
      "Epoch: 3 / 3, Step: 368 / 750 Loss: 0.1473\n",
      "Epoch: 3 / 3, Step: 369 / 750 Loss: 0.0902\n",
      "Epoch: 3 / 3, Step: 370 / 750 Loss: 0.1210\n",
      "Epoch: 3 / 3, Step: 371 / 750 Loss: 0.1194\n",
      "Epoch: 3 / 3, Step: 372 / 750 Loss: 0.2225\n",
      "Epoch: 3 / 3, Step: 373 / 750 Loss: 0.2351\n",
      "Epoch: 3 / 3, Step: 374 / 750 Loss: 0.0559\n",
      "Epoch: 3 / 3, Step: 375 / 750 Loss: 0.1560\n",
      "Epoch: 3 / 3, Step: 376 / 750 Loss: 0.1226\n",
      "Epoch: 3 / 3, Step: 377 / 750 Loss: 0.1455\n",
      "Epoch: 3 / 3, Step: 378 / 750 Loss: 0.0509\n",
      "Epoch: 3 / 3, Step: 379 / 750 Loss: 0.1094\n",
      "Epoch: 3 / 3, Step: 380 / 750 Loss: 0.1308\n",
      "Epoch: 3 / 3, Step: 381 / 750 Loss: 0.1599\n",
      "Epoch: 3 / 3, Step: 382 / 750 Loss: 0.3413\n",
      "Epoch: 3 / 3, Step: 383 / 750 Loss: 0.1459\n",
      "Epoch: 3 / 3, Step: 384 / 750 Loss: 0.1584\n",
      "Epoch: 3 / 3, Step: 385 / 750 Loss: 0.0921\n",
      "Epoch: 3 / 3, Step: 386 / 750 Loss: 0.2454\n",
      "Epoch: 3 / 3, Step: 387 / 750 Loss: 0.2100\n",
      "Epoch: 3 / 3, Step: 388 / 750 Loss: 0.0655\n",
      "Epoch: 3 / 3, Step: 389 / 750 Loss: 0.2112\n",
      "Epoch: 3 / 3, Step: 390 / 750 Loss: 0.2290\n",
      "Epoch: 3 / 3, Step: 391 / 750 Loss: 0.1741\n",
      "Epoch: 3 / 3, Step: 392 / 750 Loss: 0.2670\n",
      "Epoch: 3 / 3, Step: 393 / 750 Loss: 0.2185\n",
      "Epoch: 3 / 3, Step: 394 / 750 Loss: 0.1436\n",
      "Epoch: 3 / 3, Step: 395 / 750 Loss: 0.1026\n",
      "Epoch: 3 / 3, Step: 396 / 750 Loss: 0.0913\n",
      "Epoch: 3 / 3, Step: 397 / 750 Loss: 0.1332\n",
      "Epoch: 3 / 3, Step: 398 / 750 Loss: 0.1465\n",
      "Epoch: 3 / 3, Step: 399 / 750 Loss: 0.1791\n",
      "Epoch: 3 / 3, Step: 400 / 750 Loss: 0.0751\n",
      "Epoch: 3 / 3, Step: 401 / 750 Loss: 0.0919\n",
      "Epoch: 3 / 3, Step: 402 / 750 Loss: 0.3292\n",
      "Epoch: 3 / 3, Step: 403 / 750 Loss: 0.3212\n",
      "Epoch: 3 / 3, Step: 404 / 750 Loss: 0.0797\n",
      "Epoch: 3 / 3, Step: 405 / 750 Loss: 0.1181\n",
      "Epoch: 3 / 3, Step: 406 / 750 Loss: 0.1595\n",
      "Epoch: 3 / 3, Step: 407 / 750 Loss: 0.1097\n",
      "Epoch: 3 / 3, Step: 408 / 750 Loss: 0.2209\n",
      "Epoch: 3 / 3, Step: 409 / 750 Loss: 0.2239\n",
      "Epoch: 3 / 3, Step: 410 / 750 Loss: 0.1987\n",
      "Epoch: 3 / 3, Step: 411 / 750 Loss: 0.2417\n",
      "Epoch: 3 / 3, Step: 412 / 750 Loss: 0.2469\n",
      "Epoch: 3 / 3, Step: 413 / 750 Loss: 0.0536\n",
      "Epoch: 3 / 3, Step: 414 / 750 Loss: 0.1361\n",
      "Epoch: 3 / 3, Step: 415 / 750 Loss: 0.1273\n",
      "Epoch: 3 / 3, Step: 416 / 750 Loss: 0.1767\n",
      "Epoch: 3 / 3, Step: 417 / 750 Loss: 0.3074\n",
      "Epoch: 3 / 3, Step: 418 / 750 Loss: 0.1010\n",
      "Epoch: 3 / 3, Step: 419 / 750 Loss: 0.2358\n",
      "Epoch: 3 / 3, Step: 420 / 750 Loss: 0.1049\n",
      "Epoch: 3 / 3, Step: 421 / 750 Loss: 0.0929\n",
      "Epoch: 3 / 3, Step: 422 / 750 Loss: 0.2388\n",
      "Epoch: 3 / 3, Step: 423 / 750 Loss: 0.1175\n",
      "Epoch: 3 / 3, Step: 424 / 750 Loss: 0.1743\n",
      "Epoch: 3 / 3, Step: 425 / 750 Loss: 0.2273\n",
      "Epoch: 3 / 3, Step: 426 / 750 Loss: 0.1200\n",
      "Epoch: 3 / 3, Step: 427 / 750 Loss: 0.3633\n",
      "Epoch: 3 / 3, Step: 428 / 750 Loss: 0.0310\n",
      "Epoch: 3 / 3, Step: 429 / 750 Loss: 0.2330\n",
      "Epoch: 3 / 3, Step: 430 / 750 Loss: 0.2173\n",
      "Epoch: 3 / 3, Step: 431 / 750 Loss: 0.3262\n",
      "Epoch: 3 / 3, Step: 432 / 750 Loss: 0.0520\n",
      "Epoch: 3 / 3, Step: 433 / 750 Loss: 0.2764\n",
      "Epoch: 3 / 3, Step: 434 / 750 Loss: 0.2016\n",
      "Epoch: 3 / 3, Step: 435 / 750 Loss: 0.1001\n",
      "Epoch: 3 / 3, Step: 436 / 750 Loss: 0.1841\n",
      "Epoch: 3 / 3, Step: 437 / 750 Loss: 0.1459\n",
      "Epoch: 3 / 3, Step: 438 / 750 Loss: 0.2154\n",
      "Epoch: 3 / 3, Step: 439 / 750 Loss: 0.3384\n",
      "Epoch: 3 / 3, Step: 440 / 750 Loss: 0.1705\n",
      "Epoch: 3 / 3, Step: 441 / 750 Loss: 0.1819\n",
      "Epoch: 3 / 3, Step: 442 / 750 Loss: 0.1470\n",
      "Epoch: 3 / 3, Step: 443 / 750 Loss: 0.3753\n",
      "Epoch: 3 / 3, Step: 444 / 750 Loss: 0.0894\n",
      "Epoch: 3 / 3, Step: 445 / 750 Loss: 0.0487\n",
      "Epoch: 3 / 3, Step: 446 / 750 Loss: 0.0723\n",
      "Epoch: 3 / 3, Step: 447 / 750 Loss: 0.0625\n",
      "Epoch: 3 / 3, Step: 448 / 750 Loss: 0.3184\n",
      "Epoch: 3 / 3, Step: 449 / 750 Loss: 0.1762\n",
      "Epoch: 3 / 3, Step: 450 / 750 Loss: 0.1923\n",
      "Epoch: 3 / 3, Step: 451 / 750 Loss: 0.0539\n",
      "Epoch: 3 / 3, Step: 452 / 750 Loss: 0.3132\n",
      "Epoch: 3 / 3, Step: 453 / 750 Loss: 0.2122\n",
      "Epoch: 3 / 3, Step: 454 / 750 Loss: 0.3571\n",
      "Epoch: 3 / 3, Step: 455 / 750 Loss: 0.0883\n",
      "Epoch: 3 / 3, Step: 456 / 750 Loss: 0.0620\n",
      "Epoch: 3 / 3, Step: 457 / 750 Loss: 0.0977\n",
      "Epoch: 3 / 3, Step: 458 / 750 Loss: 0.4343\n",
      "Epoch: 3 / 3, Step: 459 / 750 Loss: 0.0980\n",
      "Epoch: 3 / 3, Step: 460 / 750 Loss: 0.2637\n",
      "Epoch: 3 / 3, Step: 461 / 750 Loss: 0.1732\n",
      "Epoch: 3 / 3, Step: 462 / 750 Loss: 0.0811\n",
      "Epoch: 3 / 3, Step: 463 / 750 Loss: 0.1445\n",
      "Epoch: 3 / 3, Step: 464 / 750 Loss: 0.2164\n",
      "Epoch: 3 / 3, Step: 465 / 750 Loss: 0.2448\n",
      "Epoch: 3 / 3, Step: 466 / 750 Loss: 0.3740\n",
      "Epoch: 3 / 3, Step: 467 / 750 Loss: 0.1376\n",
      "Epoch: 3 / 3, Step: 468 / 750 Loss: 0.3297\n",
      "Epoch: 3 / 3, Step: 469 / 750 Loss: 0.0609\n",
      "Epoch: 3 / 3, Step: 470 / 750 Loss: 0.1231\n",
      "Epoch: 3 / 3, Step: 471 / 750 Loss: 0.1140\n",
      "Epoch: 3 / 3, Step: 472 / 750 Loss: 0.1706\n",
      "Epoch: 3 / 3, Step: 473 / 750 Loss: 0.2130\n",
      "Epoch: 3 / 3, Step: 474 / 750 Loss: 0.1837\n",
      "Epoch: 3 / 3, Step: 475 / 750 Loss: 0.3339\n",
      "Epoch: 3 / 3, Step: 476 / 750 Loss: 0.1538\n",
      "Epoch: 3 / 3, Step: 477 / 750 Loss: 0.1441\n",
      "Epoch: 3 / 3, Step: 478 / 750 Loss: 0.2059\n",
      "Epoch: 3 / 3, Step: 479 / 750 Loss: 0.1521\n",
      "Epoch: 3 / 3, Step: 480 / 750 Loss: 0.0683\n",
      "Epoch: 3 / 3, Step: 481 / 750 Loss: 0.0558\n",
      "Epoch: 3 / 3, Step: 482 / 750 Loss: 0.2813\n",
      "Epoch: 3 / 3, Step: 483 / 750 Loss: 0.0818\n",
      "Epoch: 3 / 3, Step: 484 / 750 Loss: 0.1861\n",
      "Epoch: 3 / 3, Step: 485 / 750 Loss: 0.4405\n",
      "Epoch: 3 / 3, Step: 486 / 750 Loss: 0.3504\n",
      "Epoch: 3 / 3, Step: 487 / 750 Loss: 0.0856\n",
      "Epoch: 3 / 3, Step: 488 / 750 Loss: 0.0488\n",
      "Epoch: 3 / 3, Step: 489 / 750 Loss: 0.2174\n",
      "Epoch: 3 / 3, Step: 490 / 750 Loss: 0.1538\n",
      "Epoch: 3 / 3, Step: 491 / 750 Loss: 0.2197\n",
      "Epoch: 3 / 3, Step: 492 / 750 Loss: 0.1851\n",
      "Epoch: 3 / 3, Step: 493 / 750 Loss: 0.5603\n",
      "Epoch: 3 / 3, Step: 494 / 750 Loss: 0.2895\n",
      "Epoch: 3 / 3, Step: 495 / 750 Loss: 0.1450\n",
      "Epoch: 3 / 3, Step: 496 / 750 Loss: 0.3117\n",
      "Epoch: 3 / 3, Step: 497 / 750 Loss: 0.1945\n",
      "Epoch: 3 / 3, Step: 498 / 750 Loss: 0.3682\n",
      "Epoch: 3 / 3, Step: 499 / 750 Loss: 0.0862\n",
      "Epoch: 3 / 3, Step: 500 / 750 Loss: 0.0505\n",
      "Epoch: 3 / 3, Step: 501 / 750 Loss: 0.2853\n",
      "Epoch: 3 / 3, Step: 502 / 750 Loss: 0.2464\n",
      "Epoch: 3 / 3, Step: 503 / 750 Loss: 0.2322\n",
      "Epoch: 3 / 3, Step: 504 / 750 Loss: 0.1011\n",
      "Epoch: 3 / 3, Step: 505 / 750 Loss: 0.1990\n",
      "Epoch: 3 / 3, Step: 506 / 750 Loss: 0.1148\n",
      "Epoch: 3 / 3, Step: 507 / 750 Loss: 0.1884\n",
      "Epoch: 3 / 3, Step: 508 / 750 Loss: 0.2826\n",
      "Epoch: 3 / 3, Step: 509 / 750 Loss: 0.1231\n",
      "Epoch: 3 / 3, Step: 510 / 750 Loss: 0.3511\n",
      "Epoch: 3 / 3, Step: 511 / 750 Loss: 0.1110\n",
      "Epoch: 3 / 3, Step: 512 / 750 Loss: 0.1328\n",
      "Epoch: 3 / 3, Step: 513 / 750 Loss: 0.2007\n",
      "Epoch: 3 / 3, Step: 514 / 750 Loss: 0.1103\n",
      "Epoch: 3 / 3, Step: 515 / 750 Loss: 0.1288\n",
      "Epoch: 3 / 3, Step: 516 / 750 Loss: 0.2189\n",
      "Epoch: 3 / 3, Step: 517 / 750 Loss: 0.1831\n",
      "Epoch: 3 / 3, Step: 518 / 750 Loss: 0.0699\n",
      "Epoch: 3 / 3, Step: 519 / 750 Loss: 0.2966\n",
      "Epoch: 3 / 3, Step: 520 / 750 Loss: 0.1645\n",
      "Epoch: 3 / 3, Step: 521 / 750 Loss: 0.2992\n",
      "Epoch: 3 / 3, Step: 522 / 750 Loss: 0.0971\n",
      "Epoch: 3 / 3, Step: 523 / 750 Loss: 0.1081\n",
      "Epoch: 3 / 3, Step: 524 / 750 Loss: 0.3193\n",
      "Epoch: 3 / 3, Step: 525 / 750 Loss: 0.2764\n",
      "Epoch: 3 / 3, Step: 526 / 750 Loss: 0.3522\n",
      "Epoch: 3 / 3, Step: 527 / 750 Loss: 0.3553\n",
      "Epoch: 3 / 3, Step: 528 / 750 Loss: 0.1843\n",
      "Epoch: 3 / 3, Step: 529 / 750 Loss: 0.1998\n",
      "Epoch: 3 / 3, Step: 530 / 750 Loss: 0.1407\n",
      "Epoch: 3 / 3, Step: 531 / 750 Loss: 0.1346\n",
      "Epoch: 3 / 3, Step: 532 / 750 Loss: 0.1768\n",
      "Epoch: 3 / 3, Step: 533 / 750 Loss: 0.2427\n",
      "Epoch: 3 / 3, Step: 534 / 750 Loss: 0.0905\n",
      "Epoch: 3 / 3, Step: 535 / 750 Loss: 0.2078\n",
      "Epoch: 3 / 3, Step: 536 / 750 Loss: 0.1266\n",
      "Epoch: 3 / 3, Step: 537 / 750 Loss: 0.1648\n",
      "Epoch: 3 / 3, Step: 538 / 750 Loss: 0.1954\n",
      "Epoch: 3 / 3, Step: 539 / 750 Loss: 0.0868\n",
      "Epoch: 3 / 3, Step: 540 / 750 Loss: 0.2947\n",
      "Epoch: 3 / 3, Step: 541 / 750 Loss: 0.3122\n",
      "Epoch: 3 / 3, Step: 542 / 750 Loss: 0.0679\n",
      "Epoch: 3 / 3, Step: 543 / 750 Loss: 0.1881\n",
      "Epoch: 3 / 3, Step: 544 / 750 Loss: 0.2378\n",
      "Epoch: 3 / 3, Step: 545 / 750 Loss: 0.4054\n",
      "Epoch: 3 / 3, Step: 546 / 750 Loss: 0.2100\n",
      "Epoch: 3 / 3, Step: 547 / 750 Loss: 0.3850\n",
      "Epoch: 3 / 3, Step: 548 / 750 Loss: 0.2141\n",
      "Epoch: 3 / 3, Step: 549 / 750 Loss: 0.1024\n",
      "Epoch: 3 / 3, Step: 550 / 750 Loss: 0.0451\n",
      "Epoch: 3 / 3, Step: 551 / 750 Loss: 0.0708\n",
      "Epoch: 3 / 3, Step: 552 / 750 Loss: 0.0456\n",
      "Epoch: 3 / 3, Step: 553 / 750 Loss: 0.0702\n",
      "Epoch: 3 / 3, Step: 554 / 750 Loss: 0.2632\n",
      "Epoch: 3 / 3, Step: 555 / 750 Loss: 0.1134\n",
      "Epoch: 3 / 3, Step: 556 / 750 Loss: 0.1556\n",
      "Epoch: 3 / 3, Step: 557 / 750 Loss: 0.0457\n",
      "Epoch: 3 / 3, Step: 558 / 750 Loss: 0.1259\n",
      "Epoch: 3 / 3, Step: 559 / 750 Loss: 0.1656\n",
      "Epoch: 3 / 3, Step: 560 / 750 Loss: 0.1349\n",
      "Epoch: 3 / 3, Step: 561 / 750 Loss: 0.1293\n",
      "Epoch: 3 / 3, Step: 562 / 750 Loss: 0.2244\n",
      "Epoch: 3 / 3, Step: 563 / 750 Loss: 0.1415\n",
      "Epoch: 3 / 3, Step: 564 / 750 Loss: 0.2186\n",
      "Epoch: 3 / 3, Step: 565 / 750 Loss: 0.1834\n",
      "Epoch: 3 / 3, Step: 566 / 750 Loss: 0.1662\n",
      "Epoch: 3 / 3, Step: 567 / 750 Loss: 0.1273\n",
      "Epoch: 3 / 3, Step: 568 / 750 Loss: 0.2656\n",
      "Epoch: 3 / 3, Step: 569 / 750 Loss: 0.1200\n",
      "Epoch: 3 / 3, Step: 570 / 750 Loss: 0.0538\n",
      "Epoch: 3 / 3, Step: 571 / 750 Loss: 0.3222\n",
      "Epoch: 3 / 3, Step: 572 / 750 Loss: 0.2063\n",
      "Epoch: 3 / 3, Step: 573 / 750 Loss: 0.0757\n",
      "Epoch: 3 / 3, Step: 574 / 750 Loss: 0.1839\n",
      "Epoch: 3 / 3, Step: 575 / 750 Loss: 0.0842\n",
      "Epoch: 3 / 3, Step: 576 / 750 Loss: 0.1848\n",
      "Epoch: 3 / 3, Step: 577 / 750 Loss: 0.0836\n",
      "Epoch: 3 / 3, Step: 578 / 750 Loss: 0.1246\n",
      "Epoch: 3 / 3, Step: 579 / 750 Loss: 0.2060\n",
      "Epoch: 3 / 3, Step: 580 / 750 Loss: 0.3171\n",
      "Epoch: 3 / 3, Step: 581 / 750 Loss: 0.0815\n",
      "Epoch: 3 / 3, Step: 582 / 750 Loss: 0.0744\n",
      "Epoch: 3 / 3, Step: 583 / 750 Loss: 0.1076\n",
      "Epoch: 3 / 3, Step: 584 / 750 Loss: 0.2481\n",
      "Epoch: 3 / 3, Step: 585 / 750 Loss: 0.0682\n",
      "Epoch: 3 / 3, Step: 586 / 750 Loss: 0.3291\n",
      "Epoch: 3 / 3, Step: 587 / 750 Loss: 0.1028\n",
      "Epoch: 3 / 3, Step: 588 / 750 Loss: 0.0911\n",
      "Epoch: 3 / 3, Step: 589 / 750 Loss: 0.3688\n",
      "Epoch: 3 / 3, Step: 590 / 750 Loss: 0.4034\n",
      "Epoch: 3 / 3, Step: 591 / 750 Loss: 0.1403\n",
      "Epoch: 3 / 3, Step: 592 / 750 Loss: 0.1508\n",
      "Epoch: 3 / 3, Step: 593 / 750 Loss: 0.3011\n",
      "Epoch: 3 / 3, Step: 594 / 750 Loss: 0.1010\n",
      "Epoch: 3 / 3, Step: 595 / 750 Loss: 0.2880\n",
      "Epoch: 3 / 3, Step: 596 / 750 Loss: 0.2296\n",
      "Epoch: 3 / 3, Step: 597 / 750 Loss: 0.0412\n",
      "Epoch: 3 / 3, Step: 598 / 750 Loss: 0.2056\n",
      "Epoch: 3 / 3, Step: 599 / 750 Loss: 0.2518\n",
      "Epoch: 3 / 3, Step: 600 / 750 Loss: 0.2111\n",
      "Epoch: 3 / 3, Step: 601 / 750 Loss: 0.1204\n",
      "Epoch: 3 / 3, Step: 602 / 750 Loss: 0.1409\n",
      "Epoch: 3 / 3, Step: 603 / 750 Loss: 0.0711\n",
      "Epoch: 3 / 3, Step: 604 / 750 Loss: 0.0140\n",
      "Epoch: 3 / 3, Step: 605 / 750 Loss: 0.2502\n",
      "Epoch: 3 / 3, Step: 606 / 750 Loss: 0.1235\n",
      "Epoch: 3 / 3, Step: 607 / 750 Loss: 0.1141\n",
      "Epoch: 3 / 3, Step: 608 / 750 Loss: 0.4688\n",
      "Epoch: 3 / 3, Step: 609 / 750 Loss: 0.2433\n",
      "Epoch: 3 / 3, Step: 610 / 750 Loss: 0.1388\n",
      "Epoch: 3 / 3, Step: 611 / 750 Loss: 0.3051\n",
      "Epoch: 3 / 3, Step: 612 / 750 Loss: 0.0968\n",
      "Epoch: 3 / 3, Step: 613 / 750 Loss: 0.2706\n",
      "Epoch: 3 / 3, Step: 614 / 750 Loss: 0.1575\n",
      "Epoch: 3 / 3, Step: 615 / 750 Loss: 0.0612\n",
      "Epoch: 3 / 3, Step: 616 / 750 Loss: 0.0759\n",
      "Epoch: 3 / 3, Step: 617 / 750 Loss: 0.1380\n",
      "Epoch: 3 / 3, Step: 618 / 750 Loss: 0.1126\n",
      "Epoch: 3 / 3, Step: 619 / 750 Loss: 0.2532\n",
      "Epoch: 3 / 3, Step: 620 / 750 Loss: 0.3379\n",
      "Epoch: 3 / 3, Step: 621 / 750 Loss: 0.0718\n",
      "Epoch: 3 / 3, Step: 622 / 750 Loss: 0.2111\n",
      "Epoch: 3 / 3, Step: 623 / 750 Loss: 0.3489\n",
      "Epoch: 3 / 3, Step: 624 / 750 Loss: 0.2506\n",
      "Epoch: 3 / 3, Step: 625 / 750 Loss: 0.1929\n",
      "Epoch: 3 / 3, Step: 626 / 750 Loss: 0.1721\n",
      "Epoch: 3 / 3, Step: 627 / 750 Loss: 0.2185\n",
      "Epoch: 3 / 3, Step: 628 / 750 Loss: 0.0636\n",
      "Epoch: 3 / 3, Step: 629 / 750 Loss: 0.1081\n",
      "Epoch: 3 / 3, Step: 630 / 750 Loss: 0.1218\n",
      "Epoch: 3 / 3, Step: 631 / 750 Loss: 0.1821\n",
      "Epoch: 3 / 3, Step: 632 / 750 Loss: 0.1055\n",
      "Epoch: 3 / 3, Step: 633 / 750 Loss: 0.1460\n",
      "Epoch: 3 / 3, Step: 634 / 750 Loss: 0.1751\n",
      "Epoch: 3 / 3, Step: 635 / 750 Loss: 0.1151\n",
      "Epoch: 3 / 3, Step: 636 / 750 Loss: 0.1139\n",
      "Epoch: 3 / 3, Step: 637 / 750 Loss: 0.0980\n",
      "Epoch: 3 / 3, Step: 638 / 750 Loss: 0.1031\n",
      "Epoch: 3 / 3, Step: 639 / 750 Loss: 0.0446\n",
      "Epoch: 3 / 3, Step: 640 / 750 Loss: 0.2059\n",
      "Epoch: 3 / 3, Step: 641 / 750 Loss: 0.2589\n",
      "Epoch: 3 / 3, Step: 642 / 750 Loss: 0.1886\n",
      "Epoch: 3 / 3, Step: 643 / 750 Loss: 0.1363\n",
      "Epoch: 3 / 3, Step: 644 / 750 Loss: 0.1349\n",
      "Epoch: 3 / 3, Step: 645 / 750 Loss: 0.1213\n",
      "Epoch: 3 / 3, Step: 646 / 750 Loss: 0.2458\n",
      "Epoch: 3 / 3, Step: 647 / 750 Loss: 0.0320\n",
      "Epoch: 3 / 3, Step: 648 / 750 Loss: 0.1044\n",
      "Epoch: 3 / 3, Step: 649 / 750 Loss: 0.1028\n",
      "Epoch: 3 / 3, Step: 650 / 750 Loss: 0.1601\n",
      "Epoch: 3 / 3, Step: 651 / 750 Loss: 0.2639\n",
      "Epoch: 3 / 3, Step: 652 / 750 Loss: 0.1402\n",
      "Epoch: 3 / 3, Step: 653 / 750 Loss: 0.2676\n",
      "Epoch: 3 / 3, Step: 654 / 750 Loss: 0.0829\n",
      "Epoch: 3 / 3, Step: 655 / 750 Loss: 0.2691\n",
      "Epoch: 3 / 3, Step: 656 / 750 Loss: 0.0359\n",
      "Epoch: 3 / 3, Step: 657 / 750 Loss: 0.0654\n",
      "Epoch: 3 / 3, Step: 658 / 750 Loss: 0.2784\n",
      "Epoch: 3 / 3, Step: 659 / 750 Loss: 0.0449\n",
      "Epoch: 3 / 3, Step: 660 / 750 Loss: 0.2390\n",
      "Epoch: 3 / 3, Step: 661 / 750 Loss: 0.2049\n",
      "Epoch: 3 / 3, Step: 662 / 750 Loss: 0.1503\n",
      "Epoch: 3 / 3, Step: 663 / 750 Loss: 0.4390\n",
      "Epoch: 3 / 3, Step: 664 / 750 Loss: 0.0779\n",
      "Epoch: 3 / 3, Step: 665 / 750 Loss: 0.3305\n",
      "Epoch: 3 / 3, Step: 666 / 750 Loss: 0.1079\n",
      "Epoch: 3 / 3, Step: 667 / 750 Loss: 0.3765\n",
      "Epoch: 3 / 3, Step: 668 / 750 Loss: 0.1202\n",
      "Epoch: 3 / 3, Step: 669 / 750 Loss: 0.1072\n",
      "Epoch: 3 / 3, Step: 670 / 750 Loss: 0.1887\n",
      "Epoch: 3 / 3, Step: 671 / 750 Loss: 0.1321\n",
      "Epoch: 3 / 3, Step: 672 / 750 Loss: 0.1543\n",
      "Epoch: 3 / 3, Step: 673 / 750 Loss: 0.2067\n",
      "Epoch: 3 / 3, Step: 674 / 750 Loss: 0.2625\n",
      "Epoch: 3 / 3, Step: 675 / 750 Loss: 0.1615\n",
      "Epoch: 3 / 3, Step: 676 / 750 Loss: 0.2535\n",
      "Epoch: 3 / 3, Step: 677 / 750 Loss: 0.0411\n",
      "Epoch: 3 / 3, Step: 678 / 750 Loss: 0.3235\n",
      "Epoch: 3 / 3, Step: 679 / 750 Loss: 0.1052\n",
      "Epoch: 3 / 3, Step: 680 / 750 Loss: 0.2766\n",
      "Epoch: 3 / 3, Step: 681 / 750 Loss: 0.0853\n",
      "Epoch: 3 / 3, Step: 682 / 750 Loss: 0.0632\n",
      "Epoch: 3 / 3, Step: 683 / 750 Loss: 0.1076\n",
      "Epoch: 3 / 3, Step: 684 / 750 Loss: 0.0766\n",
      "Epoch: 3 / 3, Step: 685 / 750 Loss: 0.1381\n",
      "Epoch: 3 / 3, Step: 686 / 750 Loss: 0.3395\n",
      "Epoch: 3 / 3, Step: 687 / 750 Loss: 0.1731\n",
      "Epoch: 3 / 3, Step: 688 / 750 Loss: 0.0901\n",
      "Epoch: 3 / 3, Step: 689 / 750 Loss: 0.2081\n",
      "Epoch: 3 / 3, Step: 690 / 750 Loss: 0.2212\n",
      "Epoch: 3 / 3, Step: 691 / 750 Loss: 0.4967\n",
      "Epoch: 3 / 3, Step: 692 / 750 Loss: 0.1558\n",
      "Epoch: 3 / 3, Step: 693 / 750 Loss: 0.0833\n",
      "Epoch: 3 / 3, Step: 694 / 750 Loss: 0.2374\n",
      "Epoch: 3 / 3, Step: 695 / 750 Loss: 0.1860\n",
      "Epoch: 3 / 3, Step: 696 / 750 Loss: 0.1822\n",
      "Epoch: 3 / 3, Step: 697 / 750 Loss: 0.1493\n",
      "Epoch: 3 / 3, Step: 698 / 750 Loss: 0.1471\n",
      "Epoch: 3 / 3, Step: 699 / 750 Loss: 0.1496\n",
      "Epoch: 3 / 3, Step: 700 / 750 Loss: 0.0914\n",
      "Epoch: 3 / 3, Step: 701 / 750 Loss: 0.1286\n",
      "Epoch: 3 / 3, Step: 702 / 750 Loss: 0.1991\n",
      "Epoch: 3 / 3, Step: 703 / 750 Loss: 0.0955\n",
      "Epoch: 3 / 3, Step: 704 / 750 Loss: 0.0706\n",
      "Epoch: 3 / 3, Step: 705 / 750 Loss: 0.4035\n",
      "Epoch: 3 / 3, Step: 706 / 750 Loss: 0.1208\n",
      "Epoch: 3 / 3, Step: 707 / 750 Loss: 0.0978\n",
      "Epoch: 3 / 3, Step: 708 / 750 Loss: 0.1725\n",
      "Epoch: 3 / 3, Step: 709 / 750 Loss: 0.2349\n",
      "Epoch: 3 / 3, Step: 710 / 750 Loss: 0.2218\n",
      "Epoch: 3 / 3, Step: 711 / 750 Loss: 0.1304\n",
      "Epoch: 3 / 3, Step: 712 / 750 Loss: 0.1380\n",
      "Epoch: 3 / 3, Step: 713 / 750 Loss: 0.0220\n",
      "Epoch: 3 / 3, Step: 714 / 750 Loss: 0.8124\n",
      "Epoch: 3 / 3, Step: 715 / 750 Loss: 0.2815\n",
      "Epoch: 3 / 3, Step: 716 / 750 Loss: 0.1388\n",
      "Epoch: 3 / 3, Step: 717 / 750 Loss: 0.0374\n",
      "Epoch: 3 / 3, Step: 718 / 750 Loss: 0.3281\n",
      "Epoch: 3 / 3, Step: 719 / 750 Loss: 0.1109\n",
      "Epoch: 3 / 3, Step: 720 / 750 Loss: 0.1399\n",
      "Epoch: 3 / 3, Step: 721 / 750 Loss: 0.2571\n",
      "Epoch: 3 / 3, Step: 722 / 750 Loss: 0.0704\n",
      "Epoch: 3 / 3, Step: 723 / 750 Loss: 0.3623\n",
      "Epoch: 3 / 3, Step: 724 / 750 Loss: 0.1453\n",
      "Epoch: 3 / 3, Step: 725 / 750 Loss: 0.2170\n",
      "Epoch: 3 / 3, Step: 726 / 750 Loss: 0.1900\n",
      "Epoch: 3 / 3, Step: 727 / 750 Loss: 0.3139\n",
      "Epoch: 3 / 3, Step: 728 / 750 Loss: 0.1104\n",
      "Epoch: 3 / 3, Step: 729 / 750 Loss: 0.1793\n",
      "Epoch: 3 / 3, Step: 730 / 750 Loss: 0.2641\n",
      "Epoch: 3 / 3, Step: 731 / 750 Loss: 0.2148\n",
      "Epoch: 3 / 3, Step: 732 / 750 Loss: 0.0959\n",
      "Epoch: 3 / 3, Step: 733 / 750 Loss: 0.0214\n",
      "Epoch: 3 / 3, Step: 734 / 750 Loss: 0.1505\n",
      "Epoch: 3 / 3, Step: 735 / 750 Loss: 0.0830\n",
      "Epoch: 3 / 3, Step: 736 / 750 Loss: 0.0919\n",
      "Epoch: 3 / 3, Step: 737 / 750 Loss: 0.2052\n",
      "Epoch: 3 / 3, Step: 738 / 750 Loss: 0.2451\n",
      "Epoch: 3 / 3, Step: 739 / 750 Loss: 0.1538\n",
      "Epoch: 3 / 3, Step: 740 / 750 Loss: 0.1062\n",
      "Epoch: 3 / 3, Step: 741 / 750 Loss: 0.0634\n",
      "Epoch: 3 / 3, Step: 742 / 750 Loss: 0.2079\n",
      "Epoch: 3 / 3, Step: 743 / 750 Loss: 0.0833\n",
      "Epoch: 3 / 3, Step: 744 / 750 Loss: 0.3216\n",
      "Epoch: 3 / 3, Step: 745 / 750 Loss: 0.1179\n",
      "Epoch: 3 / 3, Step: 746 / 750 Loss: 0.2642\n",
      "Epoch: 3 / 3, Step: 747 / 750 Loss: 0.1711\n",
      "Epoch: 3 / 3, Step: 748 / 750 Loss: 0.3786\n",
      "Epoch: 3 / 3, Step: 749 / 750 Loss: 0.1754\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "loss_total = 0\n",
    "model.train()\n",
    "for i in range(3):\n",
    "    for j, data in enumerate(train_dataloader):\n",
    "        inputs = {'input_ids': data[0].cuda(), \n",
    "                      'attention_mask': data[1].cuda(), \n",
    "                      'labels': data[2].cuda()}\n",
    "        output = model(**inputs)\n",
    "        loss = output[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Epoch: {} / {}, Step: {} / {} Loss: {:.4f}\".format(i+1, 3, j, len(train_dataloader),\n",
    "                                                                      loss))\n",
    "torch.save(model.state_dict(), 'distilmodelB.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "574780a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('distilmodelB.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "loss_total = 0\n",
    "results = defaultdict(dict)\n",
    "predictions, true_vals = [], []\n",
    "for j, data in enumerate(test_dataloader):\n",
    "    inputs = {'input_ids': data[0].cuda(), \n",
    "              'attention_mask': data[1].cuda(), \n",
    "              'labels': data[2].cuda()}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "    loss = output[0]\n",
    "    logits = output[1]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    labels = inputs['labels'].cpu().numpy()\n",
    "    loss_total += loss.item()\n",
    "    predictions.append(logits)\n",
    "    true_vals.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "46cbe848",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_vals = np.concatenate(true_vals, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "33cfed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.79      0.78      0.79      1094\n",
      "         pos       0.76      0.55      0.64       407\n",
      "         neu       0.91      0.94      0.92      4499\n",
      "\n",
      "    accuracy                           0.88      6000\n",
      "   macro avg       0.82      0.76      0.78      6000\n",
      "weighted avg       0.88      0.88      0.88      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds_flat = np.argmax(predictions, axis = 1).flatten()\n",
    "labels_flat = true_vals.flatten()\n",
    "target_names = ['neg', 'pos', 'neu']\n",
    "print(classification_report(labels_flat, preds_flat, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e76d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f5a31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fa90155",
   "metadata": {},
   "source": [
    "# Evaluation on Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e2da2",
   "metadata": {},
   "source": [
    "Now let's start on Part C. Part C of this dataset contains tweets related to COVID-19 cases, outbreak, and stay at home. The evaluation of this part (preprocessing and processing) is same as that of the entire dataset as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "199041f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neu    21940\n",
       "neg     5781\n",
       "pos     2279\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_senti = pd.read_csv(\"COVIDSenti-main/COVIDSenti-C.csv\")\n",
    "covid_senti[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce6446",
   "metadata": {},
   "source": [
    "It contains 21940 samples marked as neutral, 2279 as positive and 5781 as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f7b7c",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b0a0f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "import string\n",
    "\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "covid_senti['tokenized_tweet'] = [simple_preprocess(line, deacc=True) for line in covid_senti['tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[word.replace('\\n', '') for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[word.replace('#', '') for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[word.lower() for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[word.translate(table) for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[''.join(filter(lambda x: not word.startswith('https'), word)) for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "covid_senti['tokenized_tweet'] = [[''.join(filter(lambda x: not word.startswith('@'), word)) for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "# print(covid_senti['tokenized_tweet'].sample(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ef1b513b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /s/chopin/a/grad/sanket96/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "covid_senti['lemmatized_tweet'] = [[wordnet_lemmatizer.lemmatize(word) for word in line] for line in covid_senti['tokenized_tweet']]\n",
    "# print(covid_senti['lemmatized_tweet'].sample(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "39f6566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(covid_senti)) < 0.8\n",
    "covid_senti_train = covid_senti[mask]\n",
    "covid_senti_test = covid_senti[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b879d3",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cdf86070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.00      0.00      0.00      1132\n",
      "         pos       0.00      0.00      0.00       448\n",
      "         neu       0.74      1.00      0.85      4403\n",
      "\n",
      "    accuracy                           0.74      5983\n",
      "   macro avg       0.25      0.33      0.28      5983\n",
      "weighted avg       0.54      0.74      0.62      5983\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayes():\n",
    "\n",
    "    def __init__(self):\n",
    "        # be sure to use the right class_dict for each data set\n",
    "        self.class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "        # self.class_dict = {'action': 0, 'comedy': 1}\n",
    "        self.feature_dict = {}\n",
    "        self.prior = np.zeros(len(self.class_dict))\n",
    "        self.likelihood = None\n",
    "    '''\n",
    "    Trains a multinomial Naive Bayes classifier on a training set.\n",
    "    Specifically, fills in self.prior and self.likelihood such that:\n",
    "    self.prior[class] = log(P(class))\n",
    "    self.likelihood[class][feature] = log(P(feature|class))\n",
    "    '''\n",
    "    def train(self, train_set):\n",
    "        self.feature_dict = self.select_features(train_set)\n",
    "        # iterate over training documents\n",
    "        self.likelihood = np.zeros((len(self.class_dict), len(self.feature_dict)))\n",
    "        doc_per_class = {}\n",
    "        word_count = {}\n",
    "        total_words_per_class = {}\n",
    "        vocabulary = set()\n",
    "        for index, row in train_set.iterrows():\n",
    "            class_name = row['label']\n",
    "            if (class_name in self.class_dict):\n",
    "                doc_per_class[class_name] = 1 + doc_per_class.get(class_name, 0)\n",
    "                    # collect class counts and feature counts\n",
    "                data = row['lemmatized_tweet']\n",
    "                for word in data:\n",
    "                    vocabulary.add(word)\n",
    "                    word_count[(word, class_name)] = 1 + word_count.get((word, class_name), 0)\n",
    "        # normalize counts to probabilities, and take logs\n",
    "        for class_name in self.class_dict:\n",
    "            counts = [v for k, v in word_count.items() if k[1] == class_name]\n",
    "            total_words_per_class[class_name] = sum(counts)\n",
    "        for word in self.feature_dict:\n",
    "            for class_name in self.class_dict:\n",
    "                self.likelihood[self.class_dict.get(class_name)][self.feature_dict.get(word)] = np.log(((word_count.get((word,\n",
    "                                                class_name), 0) + 1)/(total_words_per_class[class_name] + len(vocabulary))))\n",
    "        for class_name in self.class_dict:\n",
    "            self.prior[self.class_dict[class_name]] = np.log((doc_per_class[class_name] / sum(doc_per_class.values())))\n",
    "    '''\n",
    "    Tests the classifier on a development or test set.\n",
    "    Returns a dictionary of filenames mapped to their correct and predicted\n",
    "    classes such that:\n",
    "    results[filename]['correct'] = correct class\n",
    "    results[filename]['predicted'] = predicted class\n",
    "    '''\n",
    "    def test(self, dev_set):\n",
    "        pred_labels = []\n",
    "        true_labels = []\n",
    "        # iterate over testing documents\n",
    "        for index, row in dev_set.iterrows():\n",
    "            class_name = row['label']\n",
    "            # create feature vectors for each document\n",
    "            word_count = {}\n",
    "            true_labels.append(self.class_dict[class_name])\n",
    "            data = str(row['lemmatized_tweet'])\n",
    "            for word in data:\n",
    "                if word in self.feature_dict:\n",
    "                    word_count[word] = 1 + word_count.get(word, 0)\n",
    "            feature_vector = np.zeros((len(self.feature_dict), 1))\n",
    "            for i, word in enumerate(self.feature_dict):\n",
    "                feature_vector[i] = word_count.get(word, 0)\n",
    "            self.prior = np.reshape(self.prior, (self.prior.shape[0], 1))\n",
    "            probability = self.prior + np.matmul(self.likelihood, feature_vector)\n",
    "            pred_labels.append(np.argmax(probability))\n",
    "                # get most likely class\n",
    "        # print(dict(results))\n",
    "        return pred_labels, true_labels\n",
    "\n",
    "    '''\n",
    "    Given results, calculates the following:\n",
    "    Precision, Recall, F1 for each class\n",
    "    Accuracy overall\n",
    "    Also, prints evaluation metrics in readable format.\n",
    "    '''\n",
    "    def evaluate(self, results):\n",
    "        # you may find this helpful\n",
    "        target_names = ['neg', 'pos', 'neu']\n",
    "        print(classification_report(results[1], results[0], target_names=target_names))\n",
    "    '''\n",
    "    Performs feature selection.\n",
    "    Returns a dictionary of features.\n",
    "    '''\n",
    "    def select_features(self, train_set):\n",
    "        # almost any method of feature selection is fine here\n",
    "        doc_per_class = {}\n",
    "        word_count = {}\n",
    "        total_words_per_class = {}\n",
    "        vocabulary = set()\n",
    "        likelihood_ratio = {}\n",
    "        for index, row in train_set.iterrows():\n",
    "            class_name = row['label']\n",
    "            if (class_name in self.class_dict):\n",
    "                doc_per_class[class_name] = 1 + doc_per_class.get(class_name, 0)\n",
    "                    # collect class counts and feature counts\n",
    "                data = row['lemmatized_tweet']\n",
    "                for word in data:\n",
    "                    vocabulary.add(word)\n",
    "                    word_count[(word, class_name)] = 1 + word_count.get((word, class_name), 0)\n",
    "        # normalize counts to probabilities, and take logs\n",
    "        for class_name in self.class_dict:\n",
    "            counts = [v for k, v in word_count.items() if k[1] == class_name]\n",
    "            total_words_per_class[class_name] = sum(counts)\n",
    "        prob_class = np.zeros((3, 1))\n",
    "        for i, class_name in enumerate(self.class_dict):\n",
    "            prob_class[i] = (doc_per_class[class_name] / sum(doc_per_class.values()))\n",
    "        for word in vocabulary:\n",
    "            class_probs = [1] * len(self.class_dict)\n",
    "            for i, class_name in enumerate(self.class_dict):\n",
    "                class_probs[i] = (word_count.get((word,\n",
    "                                      class_name), 0) + 1) / (total_words_per_class[class_name] + len(vocabulary))\n",
    "                class_probs[i] = class_probs[i] / prob_class[i]\n",
    "            likelihood_ratio[word] = (1 / class_probs[0]) * (1 / class_probs[1]) * (1 / class_probs[2])\n",
    "        #likelihood_ratio_pos = dict(sorted(likelihood_ratio.items(), key=lambda item: item[1], reverse=True))\n",
    "        likelihood_ratio = dict(sorted(likelihood_ratio.items(), key=lambda item: item[1]))\n",
    "        words = []\n",
    "        words.extend(list(likelihood_ratio.keys())[:750])\n",
    "        #words.extend(list(likelihood_ratio_pos.keys())[:750])\n",
    "        # for class_name in self.class_dict:\n",
    "        #     self.prior[self.class_dict[class_name]] = np.log((doc_per_class[class_name] / sum(doc_per_class.values())))\n",
    "        features = {}\n",
    "        for i, word in enumerate(words):\n",
    "            features[word] = i\n",
    "        return features\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    nb = NaiveBayes()\n",
    "    # make sure these point to the right directories\n",
    "    nb.train(covid_senti_train)\n",
    "    # nb.train('movie_reviews_small/train')\n",
    "    results = nb.test(covid_senti_test)\n",
    "    # results = nb.test('movie_reviews_small/test')\n",
    "    nb.evaluate(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d61d05",
   "metadata": {},
   "source": [
    "## Logistic Regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a3576985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 out of 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1006688/623227358.py:111: RuntimeWarning: divide by zero encountered in log\n",
      "  loss += -((y @ np.log(y_hat)) + ((1 - y) @ np.log(1 - y_hat)))\n",
      "/tmp/ipykernel_1006688/623227358.py:111: RuntimeWarning: invalid value encountered in matmul\n",
      "  loss += -((y @ np.log(y_hat)) + ((1 - y) @ np.log(1 - y_hat)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Train Loss: nan\n",
      "Epoch 2 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 3 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 4 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 5 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 6 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 7 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 8 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 9 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 10 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 11 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 12 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 13 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 14 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 15 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 16 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 17 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 18 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 19 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 20 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 21 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 22 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 23 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 24 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 25 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 26 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 27 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 28 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 29 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 30 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 31 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 32 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 33 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 34 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 35 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 36 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 37 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 38 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 39 out of 40\n",
      "Average Train Loss: nan\n",
      "Epoch 40 out of 40\n",
      "Average Train Loss: nan\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.00      0.00      0.00      1132\n",
      "         pos       0.07      1.00      0.14       448\n",
      "         neu       0.00      0.00      0.00      4403\n",
      "\n",
      "    accuracy                           0.07      5983\n",
      "   macro avg       0.02      0.33      0.05      5983\n",
      "weighted avg       0.01      0.07      0.01      5983\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# CS542 Fall 2021 Programming Assignment 2\n",
    "# Logistic Regression Classifier\n",
    "\n",
    "'''\n",
    "Computes the logistic function.\n",
    "'''\n",
    "\n",
    "\n",
    "def sigma(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "class LogisticRegression():\n",
    "\n",
    "    def __init__(self, n_features=400):\n",
    "        # be sure to use the right class_dict for each data set\n",
    "        self.theta = None\n",
    "        self.n_features = n_features\n",
    "        self.feature_dict = None\n",
    "        self.class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "        # self.class_dict = {'action': 0, 'comedy': 1}\n",
    "        # use of self.feature_dict is optional for this assignment\n",
    "        self.feature_dict = self.select_features(covid_senti_train)\n",
    "\n",
    "    '''\n",
    "    Loads a dataset. Specifically, returns a list of filenames, and dictionaries\n",
    "    of classes and documents such that:\n",
    "    classes[filename] = class of the document\n",
    "    documents[filename] = feature vector for the document (use self.featurize)\n",
    "    '''\n",
    "\n",
    "    def select_features(self, data_set):\n",
    "        feature_count = {}\n",
    "        for index, row in data_set.iterrows():\n",
    "            data = str(row['lemmatized_tweet']).split()\n",
    "            for word in data:\n",
    "                feature_count[word] = 1 + feature_count.get(word, 0)\n",
    "\n",
    "        feature_count = list(dict(sorted(feature_count.items(), key=lambda v: v[1], reverse=True)).keys())[:500]\n",
    "        features = {}\n",
    "\n",
    "        for i, word in enumerate(feature_count):\n",
    "            features[word] = i\n",
    "        return features\n",
    "\n",
    "    def load_data(self, data_set):\n",
    "        filenames = []\n",
    "        classes = dict()\n",
    "        documents = dict()\n",
    "        # iterate over documents\n",
    "        for index, row in data_set.iterrows():\n",
    "            # your code here\n",
    "            # BEGIN STUDENT CODE\n",
    "            # if os.path.isfile(os.path.join(root, name)):\n",
    "            class_name = row['label']\n",
    "            classes[index] = self.class_dict[class_name]\n",
    "            documents[index] = self.featurize(row['lemmatized_tweet'])\n",
    "            # END STUDENT CODE\n",
    "        return classes, documents\n",
    "\n",
    "    '''\n",
    "    Given a document (as a list of words), returns a feature vector.\n",
    "    Note that the last element of the vector, corresponding to the bias, is a\n",
    "    \"dummy feature\" with value 1.\n",
    "    '''\n",
    "\n",
    "    def featurize(self, document):\n",
    "        vector = np.zeros(self.n_features + 1)\n",
    "        # BEGIN STUDENT CODE\n",
    "        for word in document:\n",
    "            if word in self.feature_dict:\n",
    "                if word not in w2v_model.wv.key_to_index:\n",
    "                    vector.extend([0] * 500)\n",
    "                else:\n",
    "                    vector.extend(w2v_model.wv[word])\n",
    "        # END STUDENT CODE\n",
    "        vector[-1] = 1\n",
    "        return vector\n",
    "\n",
    "    '''\n",
    "    Trains a logistic regression classifier on a training set.\n",
    "    '''\n",
    "\n",
    "    def train(self, train_set, batch_size=3, n_epochs=1, eta=0.1):\n",
    "        # if train_set == \"movie_reviews_small/train\":\n",
    "        #     self.feature_dict = {'fast': 0, 'couple': 1, 'shoot': 2, 'fly': 3}\n",
    "        # else:\n",
    "        #     self.feature_dict = self.select_features(train_set)\n",
    "        # self.n_features = len(self.feature_dict)\n",
    "        self.theta = np.zeros(self.n_features + 1)  # weights (and bias)\n",
    "        classes, documents = self.load_data(train_set)\n",
    "        n_minibatches = ceil(len(train_set) / batch_size)\n",
    "        for epoch in range(n_epochs):\n",
    "            print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
    "            loss = 0\n",
    "            for i in range(n_minibatches):\n",
    "                # list of filenames in minibatch\n",
    "                minibatch = train_set[i * batch_size: (i + 1) * batch_size]\n",
    "                # BEGIN STUDENT CODE\n",
    "                # create and fill in matrix x and vector y\n",
    "                x = np.zeros((len(minibatch), self.n_features + 1))\n",
    "                y = np.zeros(len(minibatch))\n",
    "                k = 0\n",
    "                for j, row in minibatch.iterrows():\n",
    "                    x[k][:] = documents[j]\n",
    "                    y[k] = classes[j]\n",
    "                    k += 1\n",
    "                # compute y_hat\n",
    "                y_hat = sigma(np.dot(x, self.theta))\n",
    "                # update loss\n",
    "                loss += -((y @ np.log(y_hat)) + ((1 - y) @ np.log(1 - y_hat)))\n",
    "                # compute gradient\n",
    "                gradient = np.dot(x.T, np.subtract(y_hat, y)) / len(minibatch)\n",
    "                # update weights (and bias)\n",
    "                self.theta = self.theta - (eta * gradient)\n",
    "                # END STUDENT CODE\n",
    "            loss /= len(train_set)\n",
    "            print(\"Average Train Loss: {}\".format(loss))\n",
    "            # randomize order\n",
    "            #Random(epoch).shuffle(train_set)\n",
    "\n",
    "    '''\n",
    "    Tests the classifier on a development or test set.\n",
    "    Returns a dictionary of filenames mapped to their correct and predicted\n",
    "    classes such that:\n",
    "    results[filename]['correct'] = correct class\n",
    "    results[filename]['predicted'] = predicted class\n",
    "    '''\n",
    "\n",
    "    def test(self, dev_set):\n",
    "        pred_labels = []\n",
    "        true_labels = []\n",
    "        classes, documents = self.load_data(dev_set)\n",
    "        for index, row in dev_set.iterrows():\n",
    "            # BEGIN STUDENT CODE\n",
    "            # get most likely class (recall that P(y=1|x) = y_hat)\n",
    "            true_labels.append(classes[index])\n",
    "            prediction = sigma(np.dot(documents[index], self.theta))\n",
    "            pred_label = 1 if prediction > 0.5 else 0\n",
    "            pred_labels.append(pred_label)\n",
    "            # END STUDENT CODE\n",
    "        return pred_labels, true_labels\n",
    "\n",
    "    '''\n",
    "    Given results, calculates the following:\n",
    "    Precision, Recall, F1 for each class\n",
    "    Accuracy overall\n",
    "    Also, prints evaluation metrics in readable format.\n",
    "    '''\n",
    "\n",
    "    def evaluate(self, results):\n",
    "        # you can copy and paste your code from PA1 here\n",
    "        target_names = ['neg', 'pos', 'neu']\n",
    "        print(classification_report(results[1], results[0], target_names=target_names))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lr = LogisticRegression(n_features=750)\n",
    "    # make sure these point to the right directories\n",
    "    batch_size = [1, 2, 3, 8, 16, 32]\n",
    "    n_epochs = [1, 5, 10, 20, 30, 40]\n",
    "    eta = [0.025, 0.05, 0.1, 0.2, 0.4]\n",
    "\n",
    "    # code for grid search\n",
    "#     for b in batch_size:\n",
    "#         for n in n_epochs:\n",
    "#             for ler in eta:\n",
    "#                 lr.train(covid_senti_train, batch_size=b, n_epochs=n, eta=ler)\n",
    "#                 results = lr.test(covid_senti_test)\n",
    "#                 lr.evaluate(results)\n",
    "#                 print(\"Accuracy is for batch size: \", b, \", n_epochs: \", n, \"eta: \", ler)\n",
    "\n",
    "    # best features from grid search\n",
    "    lr.train(covid_senti_train, batch_size=3, n_epochs=40, eta=0.05)\n",
    "    results = lr.test(covid_senti_test)\n",
    "    # lr.train('movie_reviews_small/train', batch_size=3, n_epochs=1, eta=0.1)\n",
    "    # results = lr.test('movie_reviews_small/test')\n",
    "    lr.evaluate(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54685394",
   "metadata": {},
   "source": [
    "## CNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "75e6fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import gensim\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "bad8a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "tweets = list(covid_senti['lemmatized_tweet'].values)\n",
    "tweets.append(['pad'])\n",
    "w2v_model = Word2Vec(tweets, min_count = 1, vector_size = 500, workers = 3, window = 3, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "13be5593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>lemmatized_tweet</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BREAKING: Kim Jong-un sends condolence letter ...</td>\n",
       "      <td>pos</td>\n",
       "      <td>[breaking, kim, jong, un, sends, condolence, l...</td>\n",
       "      <td>[breaking, kim, jong, un, sends, condolence, l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronavirus: Cases rise in South Korea as Aust...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[coronavirus, cases, rise, in, south, korea, a...</td>\n",
       "      <td>[coronavirus, case, rise, in, south, korea, a,...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How dangerous is coronavirus really, when are ...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[how, dangerous, is, coronavirus, really, when...</td>\n",
       "      <td>[how, dangerous, is, coronavirus, really, when...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Dr_psychiatry Make a mark and also coronaviru...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[drpsychiatry, make, mark, and, also, coronavi...</td>\n",
       "      <td>[drpsychiatry, make, mark, and, also, coronavi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As #Coronavirus positive cases continues to ri...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[as, coronavirus, positive, cases, continues, ...</td>\n",
       "      <td>[a, coronavirus, positive, case, continues, to...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>@C_Racing48 The flu has a 2% death rate.. the ...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[cracing, the, flu, has, death, rate, the, cor...</td>\n",
       "      <td>[cracing, the, flu, ha, death, rate, the, coro...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>@realDonaldTrump We already know that but you‚...</td>\n",
       "      <td>neg</td>\n",
       "      <td>[realdonaldtrump, we, already, know, that, but...</td>\n",
       "      <td>[realdonaldtrump, we, already, know, that, but...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>First coronavirus case reported in St. Joseph ...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[first, coronavirus, case, reported, in, st, j...</td>\n",
       "      <td>[first, coronavirus, case, reported, in, st, j...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>If you ate ants when you were a child, you‚Äôr...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[if, you, ate, ants, when, you, were, child, y...</td>\n",
       "      <td>[if, you, ate, ant, when, you, were, child, yo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>All this Coronavirus talk about to make me bea...</td>\n",
       "      <td>neu</td>\n",
       "      <td>[all, this, coronavirus, talk, about, to, make...</td>\n",
       "      <td>[all, this, coronavirus, talk, about, to, make...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet label  \\\n",
       "0      BREAKING: Kim Jong-un sends condolence letter ...   pos   \n",
       "1      Coronavirus: Cases rise in South Korea as Aust...   neu   \n",
       "2      How dangerous is coronavirus really, when are ...   neu   \n",
       "3      @Dr_psychiatry Make a mark and also coronaviru...   neu   \n",
       "4      As #Coronavirus positive cases continues to ri...   neu   \n",
       "...                                                  ...   ...   \n",
       "29995  @C_Racing48 The flu has a 2% death rate.. the ...   neu   \n",
       "29996  @realDonaldTrump We already know that but you‚...   neg   \n",
       "29997  First coronavirus case reported in St. Joseph ...   neu   \n",
       "29998  If you ate ants when you were a child, you‚Äôr...   neu   \n",
       "29999  All this Coronavirus talk about to make me bea...   neu   \n",
       "\n",
       "                                         tokenized_tweet  \\\n",
       "0      [breaking, kim, jong, un, sends, condolence, l...   \n",
       "1      [coronavirus, cases, rise, in, south, korea, a...   \n",
       "2      [how, dangerous, is, coronavirus, really, when...   \n",
       "3      [drpsychiatry, make, mark, and, also, coronavi...   \n",
       "4      [as, coronavirus, positive, cases, continues, ...   \n",
       "...                                                  ...   \n",
       "29995  [cracing, the, flu, has, death, rate, the, cor...   \n",
       "29996  [realdonaldtrump, we, already, know, that, but...   \n",
       "29997  [first, coronavirus, case, reported, in, st, j...   \n",
       "29998  [if, you, ate, ants, when, you, were, child, y...   \n",
       "29999  [all, this, coronavirus, talk, about, to, make...   \n",
       "\n",
       "                                        lemmatized_tweet  label_num  \n",
       "0      [breaking, kim, jong, un, sends, condolence, l...          1  \n",
       "1      [coronavirus, case, rise, in, south, korea, a,...          2  \n",
       "2      [how, dangerous, is, coronavirus, really, when...          2  \n",
       "3      [drpsychiatry, make, mark, and, also, coronavi...          2  \n",
       "4      [a, coronavirus, positive, case, continues, to...          2  \n",
       "...                                                  ...        ...  \n",
       "29995  [cracing, the, flu, ha, death, rate, the, coro...          2  \n",
       "29996  [realdonaldtrump, we, already, know, that, but...          0  \n",
       "29997  [first, coronavirus, case, reported, in, st, j...          2  \n",
       "29998  [if, you, ate, ant, when, you, were, child, yo...          2  \n",
       "29999  [all, this, coronavirus, talk, about, to, make...          2  \n",
       "\n",
       "[30000 rows x 5 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "covid_senti['label_num'] = covid_senti.label.replace(class_dict)\n",
    "covid_senti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fe7526e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = covid_senti['lemmatized_tweet'].map(len).max()\n",
    "def make_word_2_vec(sentence):\n",
    "    padding_idx = w2v_model.wv.key_to_index['pad']\n",
    "    padded_X = [padding_idx for i in range(max_len)]\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        if word not in w2v_model.wv.key_to_index:\n",
    "            padded_X[i] = 0\n",
    "            print(word)\n",
    "        else:\n",
    "            padded_X[i] = w2v_model.wv.key_to_index[word]\n",
    "        i += 1\n",
    "    return torch.tensor(padded_X, dtype=torch.long).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d61450dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 500\n",
    "NUM_FILTERS = 10\n",
    "\n",
    "class CnnTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, num_classes, window_sizes=(1,2,3,5)):\n",
    "        super(CnnTextClassifier, self).__init__()\n",
    "        weights = w2v_model.wv\n",
    "        # With pretrained embeddings\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights.vectors),\n",
    "                                                      padding_idx=w2v_model.wv.key_to_index['pad'])\n",
    "        # Without pretrained embeddings\n",
    "        # self.embedding = nn.Embedding(vocab_size, EMBEDDING_SIZE)\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "                                   nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE],\n",
    "                                             padding=(window_size - 1, 0))\n",
    "                                   for window_size in window_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Apply a convolution + max_pool layer for each window size\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        xs = []\n",
    "        for conv in self.convs:\n",
    "            x2 = torch.tanh(conv(x))\n",
    "            x2 = torch.squeeze(x2, -1)\n",
    "            x2 = F.max_pool1d(x2, x2.size(2))\n",
    "            xs.append(x2)\n",
    "        x = torch.cat(xs, 2)\n",
    "\n",
    "        # FC\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3ee42a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch completed in: 69.4599 seconds\n",
      "1,0.8216448113270761\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Epoch completed in: 69.1323 seconds\n",
      "2,0.8212536107557186\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Epoch completed in: 69.1177 seconds\n",
      "3,0.8212536107557186\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 3\n",
    "VOCAB_SIZE = len(w2v_model.wv.key_to_index)\n",
    "\n",
    "cnn_model = CnnTextClassifier(vocab_size=VOCAB_SIZE, num_classes=NUM_CLASSES)\n",
    "# cnn_model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "num_epochs = 3\n",
    "\n",
    "# Open the file for writing loss\n",
    "class_dict = {'neg': 0, 'pos': 1, 'neu': 2}\n",
    "loss_file_name = 'cnn_class_big_loss_with_padding.csv'\n",
    "losses = []\n",
    "cnn_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    print(\"Epoch \" + str(epoch + 1))\n",
    "    train_loss = 0\n",
    "    for index, row in covid_senti_train.iterrows():\n",
    "        # Clearing the accumulated gradients\n",
    "        cnn_model.zero_grad()\n",
    "\n",
    "        # Make the bag of words vector for stemmed tokens \n",
    "        bow_vec = make_word_2_vec(row['lemmatized_tweet'])\n",
    "       \n",
    "        # Forward pass to get output\n",
    "        probs = cnn_model(bow_vec)\n",
    "\n",
    "        # Get the target label\n",
    "        target = torch.tensor([class_dict[row['label']]], dtype=torch.long)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = loss_function(probs, target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    # if index == 0:\n",
    "    #     continue\n",
    "    print(\"Epoch completed in: %.4f seconds\" % (time.time()-start_time))\n",
    "    print(str((epoch+1)) + \",\" + str(train_loss / len(covid_senti_train)))\n",
    "    print('\\n')\n",
    "    train_loss = 0\n",
    "\n",
    "torch.save(cnn_model, 'cnn_big_model_500_with_paddingA.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8c663177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.00      0.00      0.00         0\n",
      "         pos       0.00      0.00      0.00         0\n",
      "         neu       1.00      0.74      0.85      5983\n",
      "\n",
      "    accuracy                           0.74      5983\n",
      "   macro avg       0.33      0.25      0.28      5983\n",
      "weighted avg       1.00      0.74      0.85      5983\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/python-env/py39/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions = []\n",
    "correct = []\n",
    "cnn_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    results = defaultdict(dict)\n",
    "    for index, row in covid_senti_test.iterrows():\n",
    "        bow_vec = make_word_2_vec(row['lemmatized_tweet'])\n",
    "        probs = cnn_model(bow_vec)\n",
    "        correct.append(class_dict[row['label']])\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        predictions.append(predicted.numpy()[0])\n",
    "target_names = ['neg', 'pos', 'neu']\n",
    "print(classification_report(predictions, correct, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2bce0c",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7bcaa417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(covid_senti.index.values, \n",
    "                                                    covid_senti.label.values, test_size=0.2,\n",
    "                                                   stratify=covid_senti.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "37f50107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {'neg': 0, 'pos': 1, 'neu': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9df2efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_senti['data_type'] = ['not_set'] * covid_senti.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8c4a1259",
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_senti.loc[X_train, 'data_type'] = 'train'\n",
    "covid_senti.loc[X_test, 'data_type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d56b940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "01cd6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c50facce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/a/grad/sanket96/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2322: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='train'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')\n",
    "\n",
    "encoded_data_test = tokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='test'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9ee1b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(covid_senti[covid_senti.data_type == 'train'].label_num.values)\n",
    "\n",
    "#validation set\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(covid_senti[covid_senti.data_type == 'test'].label_num.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3a42f3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(class_dict),\n",
    "                                                     output_attentions = False,\n",
    "                                                      output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2e1d5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c23ad78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train,labels_train)\n",
    "\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4513b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "083aee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "20a6d070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 3, Step: 0 / 750 Loss: 0.6443\n",
      "Epoch: 1 / 3, Step: 1 / 750 Loss: 0.6138\n",
      "Epoch: 1 / 3, Step: 2 / 750 Loss: 0.6902\n",
      "Epoch: 1 / 3, Step: 3 / 750 Loss: 0.7266\n",
      "Epoch: 1 / 3, Step: 4 / 750 Loss: 0.7678\n",
      "Epoch: 1 / 3, Step: 5 / 750 Loss: 0.8211\n",
      "Epoch: 1 / 3, Step: 6 / 750 Loss: 0.7691\n",
      "Epoch: 1 / 3, Step: 7 / 750 Loss: 0.7659\n",
      "Epoch: 1 / 3, Step: 8 / 750 Loss: 0.9845\n",
      "Epoch: 1 / 3, Step: 9 / 750 Loss: 0.9286\n",
      "Epoch: 1 / 3, Step: 10 / 750 Loss: 1.0161\n",
      "Epoch: 1 / 3, Step: 11 / 750 Loss: 0.6764\n",
      "Epoch: 1 / 3, Step: 12 / 750 Loss: 0.6803\n",
      "Epoch: 1 / 3, Step: 13 / 750 Loss: 0.5819\n",
      "Epoch: 1 / 3, Step: 14 / 750 Loss: 0.5408\n",
      "Epoch: 1 / 3, Step: 15 / 750 Loss: 0.7032\n",
      "Epoch: 1 / 3, Step: 16 / 750 Loss: 0.7146\n",
      "Epoch: 1 / 3, Step: 17 / 750 Loss: 0.7520\n",
      "Epoch: 1 / 3, Step: 18 / 750 Loss: 0.6804\n",
      "Epoch: 1 / 3, Step: 19 / 750 Loss: 0.7841\n",
      "Epoch: 1 / 3, Step: 20 / 750 Loss: 1.0411\n",
      "Epoch: 1 / 3, Step: 21 / 750 Loss: 0.8513\n",
      "Epoch: 1 / 3, Step: 22 / 750 Loss: 0.9568\n",
      "Epoch: 1 / 3, Step: 23 / 750 Loss: 0.8434\n",
      "Epoch: 1 / 3, Step: 24 / 750 Loss: 0.4663\n",
      "Epoch: 1 / 3, Step: 25 / 750 Loss: 0.8724\n",
      "Epoch: 1 / 3, Step: 26 / 750 Loss: 0.5009\n",
      "Epoch: 1 / 3, Step: 27 / 750 Loss: 0.6806\n",
      "Epoch: 1 / 3, Step: 28 / 750 Loss: 0.7656\n",
      "Epoch: 1 / 3, Step: 29 / 750 Loss: 0.8856\n",
      "Epoch: 1 / 3, Step: 30 / 750 Loss: 0.9687\n",
      "Epoch: 1 / 3, Step: 31 / 750 Loss: 0.6674\n",
      "Epoch: 1 / 3, Step: 32 / 750 Loss: 0.7172\n",
      "Epoch: 1 / 3, Step: 33 / 750 Loss: 0.6831\n",
      "Epoch: 1 / 3, Step: 34 / 750 Loss: 0.7988\n",
      "Epoch: 1 / 3, Step: 35 / 750 Loss: 0.7935\n",
      "Epoch: 1 / 3, Step: 36 / 750 Loss: 0.5751\n",
      "Epoch: 1 / 3, Step: 37 / 750 Loss: 0.6133\n",
      "Epoch: 1 / 3, Step: 38 / 750 Loss: 0.7003\n",
      "Epoch: 1 / 3, Step: 39 / 750 Loss: 0.7547\n",
      "Epoch: 1 / 3, Step: 40 / 750 Loss: 0.6107\n",
      "Epoch: 1 / 3, Step: 41 / 750 Loss: 0.8750\n",
      "Epoch: 1 / 3, Step: 42 / 750 Loss: 0.9537\n",
      "Epoch: 1 / 3, Step: 43 / 750 Loss: 0.7206\n",
      "Epoch: 1 / 3, Step: 44 / 750 Loss: 0.8302\n",
      "Epoch: 1 / 3, Step: 45 / 750 Loss: 0.8920\n",
      "Epoch: 1 / 3, Step: 46 / 750 Loss: 0.9056\n",
      "Epoch: 1 / 3, Step: 47 / 750 Loss: 0.6877\n",
      "Epoch: 1 / 3, Step: 48 / 750 Loss: 0.8553\n",
      "Epoch: 1 / 3, Step: 49 / 750 Loss: 0.7923\n",
      "Epoch: 1 / 3, Step: 50 / 750 Loss: 0.6208\n",
      "Epoch: 1 / 3, Step: 51 / 750 Loss: 0.8551\n",
      "Epoch: 1 / 3, Step: 52 / 750 Loss: 0.8234\n",
      "Epoch: 1 / 3, Step: 53 / 750 Loss: 0.9121\n",
      "Epoch: 1 / 3, Step: 54 / 750 Loss: 0.6256\n",
      "Epoch: 1 / 3, Step: 55 / 750 Loss: 0.8587\n",
      "Epoch: 1 / 3, Step: 56 / 750 Loss: 0.8160\n",
      "Epoch: 1 / 3, Step: 57 / 750 Loss: 0.8901\n",
      "Epoch: 1 / 3, Step: 58 / 750 Loss: 0.8325\n",
      "Epoch: 1 / 3, Step: 59 / 750 Loss: 0.6906\n",
      "Epoch: 1 / 3, Step: 60 / 750 Loss: 0.7382\n",
      "Epoch: 1 / 3, Step: 61 / 750 Loss: 0.7743\n",
      "Epoch: 1 / 3, Step: 62 / 750 Loss: 0.7142\n",
      "Epoch: 1 / 3, Step: 63 / 750 Loss: 0.7194\n",
      "Epoch: 1 / 3, Step: 64 / 750 Loss: 0.7543\n",
      "Epoch: 1 / 3, Step: 65 / 750 Loss: 0.9056\n",
      "Epoch: 1 / 3, Step: 66 / 750 Loss: 0.6529\n",
      "Epoch: 1 / 3, Step: 67 / 750 Loss: 0.7878\n",
      "Epoch: 1 / 3, Step: 68 / 750 Loss: 0.8076\n",
      "Epoch: 1 / 3, Step: 69 / 750 Loss: 0.7174\n",
      "Epoch: 1 / 3, Step: 70 / 750 Loss: 0.7486\n",
      "Epoch: 1 / 3, Step: 71 / 750 Loss: 0.8725\n",
      "Epoch: 1 / 3, Step: 72 / 750 Loss: 0.6046\n",
      "Epoch: 1 / 3, Step: 73 / 750 Loss: 0.7384\n",
      "Epoch: 1 / 3, Step: 74 / 750 Loss: 0.6392\n",
      "Epoch: 1 / 3, Step: 75 / 750 Loss: 0.8375\n",
      "Epoch: 1 / 3, Step: 76 / 750 Loss: 0.7523\n",
      "Epoch: 1 / 3, Step: 77 / 750 Loss: 0.7971\n",
      "Epoch: 1 / 3, Step: 78 / 750 Loss: 0.8700\n",
      "Epoch: 1 / 3, Step: 79 / 750 Loss: 0.7265\n",
      "Epoch: 1 / 3, Step: 80 / 750 Loss: 0.7401\n",
      "Epoch: 1 / 3, Step: 81 / 750 Loss: 0.6863\n",
      "Epoch: 1 / 3, Step: 82 / 750 Loss: 0.6685\n",
      "Epoch: 1 / 3, Step: 83 / 750 Loss: 0.6615\n",
      "Epoch: 1 / 3, Step: 84 / 750 Loss: 0.5911\n",
      "Epoch: 1 / 3, Step: 85 / 750 Loss: 0.7302\n",
      "Epoch: 1 / 3, Step: 86 / 750 Loss: 0.7143\n",
      "Epoch: 1 / 3, Step: 87 / 750 Loss: 0.8520\n",
      "Epoch: 1 / 3, Step: 88 / 750 Loss: 0.6283\n",
      "Epoch: 1 / 3, Step: 89 / 750 Loss: 0.7337\n",
      "Epoch: 1 / 3, Step: 90 / 750 Loss: 0.6029\n",
      "Epoch: 1 / 3, Step: 91 / 750 Loss: 0.6837\n",
      "Epoch: 1 / 3, Step: 92 / 750 Loss: 0.5252\n",
      "Epoch: 1 / 3, Step: 93 / 750 Loss: 0.5274\n",
      "Epoch: 1 / 3, Step: 94 / 750 Loss: 0.7216\n",
      "Epoch: 1 / 3, Step: 95 / 750 Loss: 0.7847\n",
      "Epoch: 1 / 3, Step: 96 / 750 Loss: 0.7687\n",
      "Epoch: 1 / 3, Step: 97 / 750 Loss: 0.6715\n",
      "Epoch: 1 / 3, Step: 98 / 750 Loss: 0.5730\n",
      "Epoch: 1 / 3, Step: 99 / 750 Loss: 0.7575\n",
      "Epoch: 1 / 3, Step: 100 / 750 Loss: 0.5152\n",
      "Epoch: 1 / 3, Step: 101 / 750 Loss: 0.7837\n",
      "Epoch: 1 / 3, Step: 102 / 750 Loss: 0.7348\n",
      "Epoch: 1 / 3, Step: 103 / 750 Loss: 0.6475\n",
      "Epoch: 1 / 3, Step: 104 / 750 Loss: 0.6279\n",
      "Epoch: 1 / 3, Step: 105 / 750 Loss: 0.6300\n",
      "Epoch: 1 / 3, Step: 106 / 750 Loss: 0.9981\n",
      "Epoch: 1 / 3, Step: 107 / 750 Loss: 0.8451\n",
      "Epoch: 1 / 3, Step: 108 / 750 Loss: 0.7386\n",
      "Epoch: 1 / 3, Step: 109 / 750 Loss: 0.8148\n",
      "Epoch: 1 / 3, Step: 110 / 750 Loss: 0.7242\n",
      "Epoch: 1 / 3, Step: 111 / 750 Loss: 0.4877\n",
      "Epoch: 1 / 3, Step: 112 / 750 Loss: 0.7375\n",
      "Epoch: 1 / 3, Step: 113 / 750 Loss: 0.7013\n",
      "Epoch: 1 / 3, Step: 114 / 750 Loss: 0.6117\n",
      "Epoch: 1 / 3, Step: 115 / 750 Loss: 0.7132\n",
      "Epoch: 1 / 3, Step: 116 / 750 Loss: 0.7439\n",
      "Epoch: 1 / 3, Step: 117 / 750 Loss: 0.8389\n",
      "Epoch: 1 / 3, Step: 118 / 750 Loss: 0.8547\n",
      "Epoch: 1 / 3, Step: 119 / 750 Loss: 0.7759\n",
      "Epoch: 1 / 3, Step: 120 / 750 Loss: 0.7582\n",
      "Epoch: 1 / 3, Step: 121 / 750 Loss: 0.7421\n",
      "Epoch: 1 / 3, Step: 122 / 750 Loss: 0.9310\n",
      "Epoch: 1 / 3, Step: 123 / 750 Loss: 0.7591\n",
      "Epoch: 1 / 3, Step: 124 / 750 Loss: 0.5877\n",
      "Epoch: 1 / 3, Step: 125 / 750 Loss: 0.7473\n",
      "Epoch: 1 / 3, Step: 126 / 750 Loss: 0.6267\n",
      "Epoch: 1 / 3, Step: 127 / 750 Loss: 0.7102\n",
      "Epoch: 1 / 3, Step: 128 / 750 Loss: 0.5898\n",
      "Epoch: 1 / 3, Step: 129 / 750 Loss: 0.5187\n",
      "Epoch: 1 / 3, Step: 130 / 750 Loss: 0.8705\n",
      "Epoch: 1 / 3, Step: 131 / 750 Loss: 0.6462\n",
      "Epoch: 1 / 3, Step: 132 / 750 Loss: 0.9278\n",
      "Epoch: 1 / 3, Step: 133 / 750 Loss: 0.9165\n",
      "Epoch: 1 / 3, Step: 134 / 750 Loss: 0.7372\n",
      "Epoch: 1 / 3, Step: 135 / 750 Loss: 0.6043\n",
      "Epoch: 1 / 3, Step: 136 / 750 Loss: 0.7816\n",
      "Epoch: 1 / 3, Step: 137 / 750 Loss: 0.6907\n",
      "Epoch: 1 / 3, Step: 138 / 750 Loss: 0.5259\n",
      "Epoch: 1 / 3, Step: 139 / 750 Loss: 0.5966\n",
      "Epoch: 1 / 3, Step: 140 / 750 Loss: 0.5535\n",
      "Epoch: 1 / 3, Step: 141 / 750 Loss: 0.4187\n",
      "Epoch: 1 / 3, Step: 142 / 750 Loss: 0.6587\n",
      "Epoch: 1 / 3, Step: 143 / 750 Loss: 0.5856\n",
      "Epoch: 1 / 3, Step: 144 / 750 Loss: 0.8385\n",
      "Epoch: 1 / 3, Step: 145 / 750 Loss: 0.6965\n",
      "Epoch: 1 / 3, Step: 146 / 750 Loss: 0.7592\n",
      "Epoch: 1 / 3, Step: 147 / 750 Loss: 0.6136\n",
      "Epoch: 1 / 3, Step: 148 / 750 Loss: 0.6738\n",
      "Epoch: 1 / 3, Step: 149 / 750 Loss: 0.8010\n",
      "Epoch: 1 / 3, Step: 150 / 750 Loss: 0.8514\n",
      "Epoch: 1 / 3, Step: 151 / 750 Loss: 0.6939\n",
      "Epoch: 1 / 3, Step: 152 / 750 Loss: 0.8088\n",
      "Epoch: 1 / 3, Step: 153 / 750 Loss: 0.8311\n",
      "Epoch: 1 / 3, Step: 154 / 750 Loss: 0.8055\n",
      "Epoch: 1 / 3, Step: 155 / 750 Loss: 0.6185\n",
      "Epoch: 1 / 3, Step: 156 / 750 Loss: 0.5875\n",
      "Epoch: 1 / 3, Step: 157 / 750 Loss: 0.6812\n",
      "Epoch: 1 / 3, Step: 158 / 750 Loss: 0.5114\n",
      "Epoch: 1 / 3, Step: 159 / 750 Loss: 0.5819\n",
      "Epoch: 1 / 3, Step: 160 / 750 Loss: 0.9983\n",
      "Epoch: 1 / 3, Step: 161 / 750 Loss: 0.6818\n",
      "Epoch: 1 / 3, Step: 162 / 750 Loss: 0.5189\n",
      "Epoch: 1 / 3, Step: 163 / 750 Loss: 0.9074\n",
      "Epoch: 1 / 3, Step: 164 / 750 Loss: 0.7871\n",
      "Epoch: 1 / 3, Step: 165 / 750 Loss: 0.5769\n",
      "Epoch: 1 / 3, Step: 166 / 750 Loss: 0.4507\n",
      "Epoch: 1 / 3, Step: 167 / 750 Loss: 0.5747\n",
      "Epoch: 1 / 3, Step: 168 / 750 Loss: 0.5577\n",
      "Epoch: 1 / 3, Step: 169 / 750 Loss: 0.7394\n",
      "Epoch: 1 / 3, Step: 170 / 750 Loss: 0.7092\n",
      "Epoch: 1 / 3, Step: 171 / 750 Loss: 0.6030\n",
      "Epoch: 1 / 3, Step: 172 / 750 Loss: 0.5247\n",
      "Epoch: 1 / 3, Step: 173 / 750 Loss: 0.4812\n",
      "Epoch: 1 / 3, Step: 174 / 750 Loss: 0.7358\n",
      "Epoch: 1 / 3, Step: 175 / 750 Loss: 0.6691\n",
      "Epoch: 1 / 3, Step: 176 / 750 Loss: 0.7980\n",
      "Epoch: 1 / 3, Step: 177 / 750 Loss: 0.8300\n",
      "Epoch: 1 / 3, Step: 178 / 750 Loss: 0.8342\n",
      "Epoch: 1 / 3, Step: 179 / 750 Loss: 0.6157\n",
      "Epoch: 1 / 3, Step: 180 / 750 Loss: 0.7864\n",
      "Epoch: 1 / 3, Step: 181 / 750 Loss: 0.9830\n",
      "Epoch: 1 / 3, Step: 182 / 750 Loss: 0.4151\n",
      "Epoch: 1 / 3, Step: 183 / 750 Loss: 0.5689\n",
      "Epoch: 1 / 3, Step: 184 / 750 Loss: 0.5520\n",
      "Epoch: 1 / 3, Step: 185 / 750 Loss: 0.4973\n",
      "Epoch: 1 / 3, Step: 186 / 750 Loss: 0.6584\n",
      "Epoch: 1 / 3, Step: 187 / 750 Loss: 0.5350\n",
      "Epoch: 1 / 3, Step: 188 / 750 Loss: 0.6172\n",
      "Epoch: 1 / 3, Step: 189 / 750 Loss: 0.7808\n",
      "Epoch: 1 / 3, Step: 190 / 750 Loss: 0.5305\n",
      "Epoch: 1 / 3, Step: 191 / 750 Loss: 0.6825\n",
      "Epoch: 1 / 3, Step: 192 / 750 Loss: 0.7441\n",
      "Epoch: 1 / 3, Step: 193 / 750 Loss: 0.3521\n",
      "Epoch: 1 / 3, Step: 194 / 750 Loss: 0.5691\n",
      "Epoch: 1 / 3, Step: 195 / 750 Loss: 0.6429\n",
      "Epoch: 1 / 3, Step: 196 / 750 Loss: 0.6163\n",
      "Epoch: 1 / 3, Step: 197 / 750 Loss: 0.6673\n",
      "Epoch: 1 / 3, Step: 198 / 750 Loss: 0.6706\n",
      "Epoch: 1 / 3, Step: 199 / 750 Loss: 0.4603\n",
      "Epoch: 1 / 3, Step: 200 / 750 Loss: 0.6999\n",
      "Epoch: 1 / 3, Step: 201 / 750 Loss: 0.4940\n",
      "Epoch: 1 / 3, Step: 202 / 750 Loss: 0.7718\n",
      "Epoch: 1 / 3, Step: 203 / 750 Loss: 0.6689\n",
      "Epoch: 1 / 3, Step: 204 / 750 Loss: 0.5243\n",
      "Epoch: 1 / 3, Step: 205 / 750 Loss: 0.7960\n",
      "Epoch: 1 / 3, Step: 206 / 750 Loss: 0.5608\n",
      "Epoch: 1 / 3, Step: 207 / 750 Loss: 0.6796\n",
      "Epoch: 1 / 3, Step: 208 / 750 Loss: 0.4632\n",
      "Epoch: 1 / 3, Step: 209 / 750 Loss: 0.4749\n",
      "Epoch: 1 / 3, Step: 210 / 750 Loss: 0.7910\n",
      "Epoch: 1 / 3, Step: 211 / 750 Loss: 0.5237\n",
      "Epoch: 1 / 3, Step: 212 / 750 Loss: 0.7301\n",
      "Epoch: 1 / 3, Step: 213 / 750 Loss: 0.5339\n",
      "Epoch: 1 / 3, Step: 214 / 750 Loss: 0.6597\n",
      "Epoch: 1 / 3, Step: 215 / 750 Loss: 0.5721\n",
      "Epoch: 1 / 3, Step: 216 / 750 Loss: 0.5827\n",
      "Epoch: 1 / 3, Step: 217 / 750 Loss: 0.5676\n",
      "Epoch: 1 / 3, Step: 218 / 750 Loss: 0.7644\n",
      "Epoch: 1 / 3, Step: 219 / 750 Loss: 0.5281\n",
      "Epoch: 1 / 3, Step: 220 / 750 Loss: 0.6480\n",
      "Epoch: 1 / 3, Step: 221 / 750 Loss: 0.5275\n",
      "Epoch: 1 / 3, Step: 222 / 750 Loss: 0.5174\n",
      "Epoch: 1 / 3, Step: 223 / 750 Loss: 0.6938\n",
      "Epoch: 1 / 3, Step: 224 / 750 Loss: 0.5710\n",
      "Epoch: 1 / 3, Step: 225 / 750 Loss: 0.3628\n",
      "Epoch: 1 / 3, Step: 226 / 750 Loss: 0.6819\n",
      "Epoch: 1 / 3, Step: 227 / 750 Loss: 0.6964\n",
      "Epoch: 1 / 3, Step: 228 / 750 Loss: 0.5985\n",
      "Epoch: 1 / 3, Step: 229 / 750 Loss: 0.6647\n",
      "Epoch: 1 / 3, Step: 230 / 750 Loss: 0.5340\n",
      "Epoch: 1 / 3, Step: 231 / 750 Loss: 0.5222\n",
      "Epoch: 1 / 3, Step: 232 / 750 Loss: 0.4063\n",
      "Epoch: 1 / 3, Step: 233 / 750 Loss: 0.6358\n",
      "Epoch: 1 / 3, Step: 234 / 750 Loss: 0.5402\n",
      "Epoch: 1 / 3, Step: 235 / 750 Loss: 0.6116\n",
      "Epoch: 1 / 3, Step: 236 / 750 Loss: 0.6492\n",
      "Epoch: 1 / 3, Step: 237 / 750 Loss: 0.9581\n",
      "Epoch: 1 / 3, Step: 238 / 750 Loss: 0.5636\n",
      "Epoch: 1 / 3, Step: 239 / 750 Loss: 0.7172\n",
      "Epoch: 1 / 3, Step: 240 / 750 Loss: 0.6327\n",
      "Epoch: 1 / 3, Step: 241 / 750 Loss: 0.5725\n",
      "Epoch: 1 / 3, Step: 242 / 750 Loss: 0.6420\n",
      "Epoch: 1 / 3, Step: 243 / 750 Loss: 0.6882\n",
      "Epoch: 1 / 3, Step: 244 / 750 Loss: 0.5797\n",
      "Epoch: 1 / 3, Step: 245 / 750 Loss: 0.5153\n",
      "Epoch: 1 / 3, Step: 246 / 750 Loss: 0.7530\n",
      "Epoch: 1 / 3, Step: 247 / 750 Loss: 0.6518\n",
      "Epoch: 1 / 3, Step: 248 / 750 Loss: 0.6392\n",
      "Epoch: 1 / 3, Step: 249 / 750 Loss: 0.6982\n",
      "Epoch: 1 / 3, Step: 250 / 750 Loss: 0.5390\n",
      "Epoch: 1 / 3, Step: 251 / 750 Loss: 0.7974\n",
      "Epoch: 1 / 3, Step: 252 / 750 Loss: 0.3794\n",
      "Epoch: 1 / 3, Step: 253 / 750 Loss: 0.8648\n",
      "Epoch: 1 / 3, Step: 254 / 750 Loss: 0.3935\n",
      "Epoch: 1 / 3, Step: 255 / 750 Loss: 0.5106\n",
      "Epoch: 1 / 3, Step: 256 / 750 Loss: 0.7107\n",
      "Epoch: 1 / 3, Step: 257 / 750 Loss: 0.7770\n",
      "Epoch: 1 / 3, Step: 258 / 750 Loss: 0.4149\n",
      "Epoch: 1 / 3, Step: 259 / 750 Loss: 0.5297\n",
      "Epoch: 1 / 3, Step: 260 / 750 Loss: 0.5058\n",
      "Epoch: 1 / 3, Step: 261 / 750 Loss: 0.3513\n",
      "Epoch: 1 / 3, Step: 262 / 750 Loss: 0.4348\n",
      "Epoch: 1 / 3, Step: 263 / 750 Loss: 0.3396\n",
      "Epoch: 1 / 3, Step: 264 / 750 Loss: 0.5684\n",
      "Epoch: 1 / 3, Step: 265 / 750 Loss: 0.6695\n",
      "Epoch: 1 / 3, Step: 266 / 750 Loss: 0.5880\n",
      "Epoch: 1 / 3, Step: 267 / 750 Loss: 0.9181\n",
      "Epoch: 1 / 3, Step: 268 / 750 Loss: 0.5829\n",
      "Epoch: 1 / 3, Step: 269 / 750 Loss: 0.5691\n",
      "Epoch: 1 / 3, Step: 270 / 750 Loss: 0.6320\n",
      "Epoch: 1 / 3, Step: 271 / 750 Loss: 0.7630\n",
      "Epoch: 1 / 3, Step: 272 / 750 Loss: 0.3019\n",
      "Epoch: 1 / 3, Step: 273 / 750 Loss: 0.5999\n",
      "Epoch: 1 / 3, Step: 274 / 750 Loss: 0.5967\n",
      "Epoch: 1 / 3, Step: 275 / 750 Loss: 0.6108\n",
      "Epoch: 1 / 3, Step: 276 / 750 Loss: 0.5687\n",
      "Epoch: 1 / 3, Step: 277 / 750 Loss: 0.5437\n",
      "Epoch: 1 / 3, Step: 278 / 750 Loss: 0.6290\n",
      "Epoch: 1 / 3, Step: 279 / 750 Loss: 0.4207\n",
      "Epoch: 1 / 3, Step: 280 / 750 Loss: 0.5569\n",
      "Epoch: 1 / 3, Step: 281 / 750 Loss: 0.4439\n",
      "Epoch: 1 / 3, Step: 282 / 750 Loss: 0.6782\n",
      "Epoch: 1 / 3, Step: 283 / 750 Loss: 0.5371\n",
      "Epoch: 1 / 3, Step: 284 / 750 Loss: 0.6655\n",
      "Epoch: 1 / 3, Step: 285 / 750 Loss: 0.6331\n",
      "Epoch: 1 / 3, Step: 286 / 750 Loss: 0.6736\n",
      "Epoch: 1 / 3, Step: 287 / 750 Loss: 0.4841\n",
      "Epoch: 1 / 3, Step: 288 / 750 Loss: 0.5183\n",
      "Epoch: 1 / 3, Step: 289 / 750 Loss: 0.4401\n",
      "Epoch: 1 / 3, Step: 290 / 750 Loss: 0.4683\n",
      "Epoch: 1 / 3, Step: 291 / 750 Loss: 0.4955\n",
      "Epoch: 1 / 3, Step: 292 / 750 Loss: 0.4023\n",
      "Epoch: 1 / 3, Step: 293 / 750 Loss: 0.6553\n",
      "Epoch: 1 / 3, Step: 294 / 750 Loss: 0.5144\n",
      "Epoch: 1 / 3, Step: 295 / 750 Loss: 0.6084\n",
      "Epoch: 1 / 3, Step: 296 / 750 Loss: 0.4138\n",
      "Epoch: 1 / 3, Step: 297 / 750 Loss: 0.6356\n",
      "Epoch: 1 / 3, Step: 298 / 750 Loss: 0.4251\n",
      "Epoch: 1 / 3, Step: 299 / 750 Loss: 0.6600\n",
      "Epoch: 1 / 3, Step: 300 / 750 Loss: 0.5104\n",
      "Epoch: 1 / 3, Step: 301 / 750 Loss: 0.4774\n",
      "Epoch: 1 / 3, Step: 302 / 750 Loss: 0.6292\n",
      "Epoch: 1 / 3, Step: 303 / 750 Loss: 0.2623\n",
      "Epoch: 1 / 3, Step: 304 / 750 Loss: 0.8210\n",
      "Epoch: 1 / 3, Step: 305 / 750 Loss: 0.6699\n",
      "Epoch: 1 / 3, Step: 306 / 750 Loss: 0.4541\n",
      "Epoch: 1 / 3, Step: 307 / 750 Loss: 0.4990\n",
      "Epoch: 1 / 3, Step: 308 / 750 Loss: 0.7914\n",
      "Epoch: 1 / 3, Step: 309 / 750 Loss: 0.6475\n",
      "Epoch: 1 / 3, Step: 310 / 750 Loss: 0.3795\n",
      "Epoch: 1 / 3, Step: 311 / 750 Loss: 0.6213\n",
      "Epoch: 1 / 3, Step: 312 / 750 Loss: 0.6843\n",
      "Epoch: 1 / 3, Step: 313 / 750 Loss: 0.5220\n",
      "Epoch: 1 / 3, Step: 314 / 750 Loss: 0.4011\n",
      "Epoch: 1 / 3, Step: 315 / 750 Loss: 0.4938\n",
      "Epoch: 1 / 3, Step: 316 / 750 Loss: 0.6408\n",
      "Epoch: 1 / 3, Step: 317 / 750 Loss: 0.9120\n",
      "Epoch: 1 / 3, Step: 318 / 750 Loss: 0.4636\n",
      "Epoch: 1 / 3, Step: 319 / 750 Loss: 0.6148\n",
      "Epoch: 1 / 3, Step: 320 / 750 Loss: 0.5378\n",
      "Epoch: 1 / 3, Step: 321 / 750 Loss: 0.6154\n",
      "Epoch: 1 / 3, Step: 322 / 750 Loss: 0.7210\n",
      "Epoch: 1 / 3, Step: 323 / 750 Loss: 0.5530\n",
      "Epoch: 1 / 3, Step: 324 / 750 Loss: 0.5228\n",
      "Epoch: 1 / 3, Step: 325 / 750 Loss: 0.5676\n",
      "Epoch: 1 / 3, Step: 326 / 750 Loss: 0.4521\n",
      "Epoch: 1 / 3, Step: 327 / 750 Loss: 0.4041\n",
      "Epoch: 1 / 3, Step: 328 / 750 Loss: 0.7577\n",
      "Epoch: 1 / 3, Step: 329 / 750 Loss: 0.4747\n",
      "Epoch: 1 / 3, Step: 330 / 750 Loss: 0.6546\n",
      "Epoch: 1 / 3, Step: 331 / 750 Loss: 0.4670\n",
      "Epoch: 1 / 3, Step: 332 / 750 Loss: 0.4961\n",
      "Epoch: 1 / 3, Step: 333 / 750 Loss: 0.5746\n",
      "Epoch: 1 / 3, Step: 334 / 750 Loss: 0.6026\n",
      "Epoch: 1 / 3, Step: 335 / 750 Loss: 0.6376\n",
      "Epoch: 1 / 3, Step: 336 / 750 Loss: 0.6169\n",
      "Epoch: 1 / 3, Step: 337 / 750 Loss: 0.4481\n",
      "Epoch: 1 / 3, Step: 338 / 750 Loss: 0.3345\n",
      "Epoch: 1 / 3, Step: 339 / 750 Loss: 0.6762\n",
      "Epoch: 1 / 3, Step: 340 / 750 Loss: 0.7105\n",
      "Epoch: 1 / 3, Step: 341 / 750 Loss: 0.5279\n",
      "Epoch: 1 / 3, Step: 342 / 750 Loss: 0.2971\n",
      "Epoch: 1 / 3, Step: 343 / 750 Loss: 0.4416\n",
      "Epoch: 1 / 3, Step: 344 / 750 Loss: 0.5326\n",
      "Epoch: 1 / 3, Step: 345 / 750 Loss: 0.3860\n",
      "Epoch: 1 / 3, Step: 346 / 750 Loss: 0.6840\n",
      "Epoch: 1 / 3, Step: 347 / 750 Loss: 0.2895\n",
      "Epoch: 1 / 3, Step: 348 / 750 Loss: 0.4808\n",
      "Epoch: 1 / 3, Step: 349 / 750 Loss: 0.3658\n",
      "Epoch: 1 / 3, Step: 350 / 750 Loss: 0.6631\n",
      "Epoch: 1 / 3, Step: 351 / 750 Loss: 0.2995\n",
      "Epoch: 1 / 3, Step: 352 / 750 Loss: 0.6860\n",
      "Epoch: 1 / 3, Step: 353 / 750 Loss: 0.5301\n",
      "Epoch: 1 / 3, Step: 354 / 750 Loss: 0.5809\n",
      "Epoch: 1 / 3, Step: 355 / 750 Loss: 0.3053\n",
      "Epoch: 1 / 3, Step: 356 / 750 Loss: 0.3924\n",
      "Epoch: 1 / 3, Step: 357 / 750 Loss: 0.5604\n",
      "Epoch: 1 / 3, Step: 358 / 750 Loss: 0.6043\n",
      "Epoch: 1 / 3, Step: 359 / 750 Loss: 0.3937\n",
      "Epoch: 1 / 3, Step: 360 / 750 Loss: 0.3017\n",
      "Epoch: 1 / 3, Step: 361 / 750 Loss: 0.3899\n",
      "Epoch: 1 / 3, Step: 362 / 750 Loss: 0.4440\n",
      "Epoch: 1 / 3, Step: 363 / 750 Loss: 0.5916\n",
      "Epoch: 1 / 3, Step: 364 / 750 Loss: 0.4379\n",
      "Epoch: 1 / 3, Step: 365 / 750 Loss: 0.5418\n",
      "Epoch: 1 / 3, Step: 366 / 750 Loss: 0.4359\n",
      "Epoch: 1 / 3, Step: 367 / 750 Loss: 0.4897\n",
      "Epoch: 1 / 3, Step: 368 / 750 Loss: 0.4925\n",
      "Epoch: 1 / 3, Step: 369 / 750 Loss: 0.5800\n",
      "Epoch: 1 / 3, Step: 370 / 750 Loss: 0.2484\n",
      "Epoch: 1 / 3, Step: 371 / 750 Loss: 0.4912\n",
      "Epoch: 1 / 3, Step: 372 / 750 Loss: 0.6661\n",
      "Epoch: 1 / 3, Step: 373 / 750 Loss: 0.2292\n",
      "Epoch: 1 / 3, Step: 374 / 750 Loss: 0.4835\n",
      "Epoch: 1 / 3, Step: 375 / 750 Loss: 0.3833\n",
      "Epoch: 1 / 3, Step: 376 / 750 Loss: 0.4303\n",
      "Epoch: 1 / 3, Step: 377 / 750 Loss: 0.5261\n",
      "Epoch: 1 / 3, Step: 378 / 750 Loss: 0.3142\n",
      "Epoch: 1 / 3, Step: 379 / 750 Loss: 0.3849\n",
      "Epoch: 1 / 3, Step: 380 / 750 Loss: 0.4149\n",
      "Epoch: 1 / 3, Step: 381 / 750 Loss: 0.4673\n",
      "Epoch: 1 / 3, Step: 382 / 750 Loss: 0.4479\n",
      "Epoch: 1 / 3, Step: 383 / 750 Loss: 0.6314\n",
      "Epoch: 1 / 3, Step: 384 / 750 Loss: 0.3834\n",
      "Epoch: 1 / 3, Step: 385 / 750 Loss: 0.5887\n",
      "Epoch: 1 / 3, Step: 386 / 750 Loss: 0.2797\n",
      "Epoch: 1 / 3, Step: 387 / 750 Loss: 0.4175\n",
      "Epoch: 1 / 3, Step: 388 / 750 Loss: 0.3566\n",
      "Epoch: 1 / 3, Step: 389 / 750 Loss: 0.5001\n",
      "Epoch: 1 / 3, Step: 390 / 750 Loss: 0.4988\n",
      "Epoch: 1 / 3, Step: 391 / 750 Loss: 0.4274\n",
      "Epoch: 1 / 3, Step: 392 / 750 Loss: 0.6171\n",
      "Epoch: 1 / 3, Step: 393 / 750 Loss: 0.4131\n",
      "Epoch: 1 / 3, Step: 394 / 750 Loss: 0.3750\n",
      "Epoch: 1 / 3, Step: 395 / 750 Loss: 0.5469\n",
      "Epoch: 1 / 3, Step: 396 / 750 Loss: 0.6215\n",
      "Epoch: 1 / 3, Step: 397 / 750 Loss: 0.4298\n",
      "Epoch: 1 / 3, Step: 398 / 750 Loss: 0.4414\n",
      "Epoch: 1 / 3, Step: 399 / 750 Loss: 0.3763\n",
      "Epoch: 1 / 3, Step: 400 / 750 Loss: 0.6246\n",
      "Epoch: 1 / 3, Step: 401 / 750 Loss: 0.2932\n",
      "Epoch: 1 / 3, Step: 402 / 750 Loss: 0.4098\n",
      "Epoch: 1 / 3, Step: 403 / 750 Loss: 0.3218\n",
      "Epoch: 1 / 3, Step: 404 / 750 Loss: 0.6036\n",
      "Epoch: 1 / 3, Step: 405 / 750 Loss: 0.3578\n",
      "Epoch: 1 / 3, Step: 406 / 750 Loss: 0.6009\n",
      "Epoch: 1 / 3, Step: 407 / 750 Loss: 0.2672\n",
      "Epoch: 1 / 3, Step: 408 / 750 Loss: 0.7691\n",
      "Epoch: 1 / 3, Step: 409 / 750 Loss: 0.4547\n",
      "Epoch: 1 / 3, Step: 410 / 750 Loss: 0.5213\n",
      "Epoch: 1 / 3, Step: 411 / 750 Loss: 0.4001\n",
      "Epoch: 1 / 3, Step: 412 / 750 Loss: 0.5620\n",
      "Epoch: 1 / 3, Step: 413 / 750 Loss: 0.5004\n",
      "Epoch: 1 / 3, Step: 414 / 750 Loss: 0.3684\n",
      "Epoch: 1 / 3, Step: 415 / 750 Loss: 0.5392\n",
      "Epoch: 1 / 3, Step: 416 / 750 Loss: 0.3435\n",
      "Epoch: 1 / 3, Step: 417 / 750 Loss: 0.3446\n",
      "Epoch: 1 / 3, Step: 418 / 750 Loss: 0.5111\n",
      "Epoch: 1 / 3, Step: 419 / 750 Loss: 0.4002\n",
      "Epoch: 1 / 3, Step: 420 / 750 Loss: 0.8001\n",
      "Epoch: 1 / 3, Step: 421 / 750 Loss: 0.4610\n",
      "Epoch: 1 / 3, Step: 422 / 750 Loss: 0.4182\n",
      "Epoch: 1 / 3, Step: 423 / 750 Loss: 0.2178\n",
      "Epoch: 1 / 3, Step: 424 / 750 Loss: 0.4913\n",
      "Epoch: 1 / 3, Step: 425 / 750 Loss: 0.4343\n",
      "Epoch: 1 / 3, Step: 426 / 750 Loss: 0.5544\n",
      "Epoch: 1 / 3, Step: 427 / 750 Loss: 0.3646\n",
      "Epoch: 1 / 3, Step: 428 / 750 Loss: 0.4648\n",
      "Epoch: 1 / 3, Step: 429 / 750 Loss: 0.5117\n",
      "Epoch: 1 / 3, Step: 430 / 750 Loss: 0.4703\n",
      "Epoch: 1 / 3, Step: 431 / 750 Loss: 0.7013\n",
      "Epoch: 1 / 3, Step: 432 / 750 Loss: 0.3230\n",
      "Epoch: 1 / 3, Step: 433 / 750 Loss: 0.6468\n",
      "Epoch: 1 / 3, Step: 434 / 750 Loss: 0.4551\n",
      "Epoch: 1 / 3, Step: 435 / 750 Loss: 0.3120\n",
      "Epoch: 1 / 3, Step: 436 / 750 Loss: 0.4765\n",
      "Epoch: 1 / 3, Step: 437 / 750 Loss: 0.3993\n",
      "Epoch: 1 / 3, Step: 438 / 750 Loss: 0.3112\n",
      "Epoch: 1 / 3, Step: 439 / 750 Loss: 0.3249\n",
      "Epoch: 1 / 3, Step: 440 / 750 Loss: 0.6483\n",
      "Epoch: 1 / 3, Step: 441 / 750 Loss: 0.5415\n",
      "Epoch: 1 / 3, Step: 442 / 750 Loss: 0.5453\n",
      "Epoch: 1 / 3, Step: 443 / 750 Loss: 0.6327\n",
      "Epoch: 1 / 3, Step: 444 / 750 Loss: 0.6088\n",
      "Epoch: 1 / 3, Step: 445 / 750 Loss: 0.4932\n",
      "Epoch: 1 / 3, Step: 446 / 750 Loss: 0.3914\n",
      "Epoch: 1 / 3, Step: 447 / 750 Loss: 0.4418\n",
      "Epoch: 1 / 3, Step: 448 / 750 Loss: 0.5840\n",
      "Epoch: 1 / 3, Step: 449 / 750 Loss: 0.2648\n",
      "Epoch: 1 / 3, Step: 450 / 750 Loss: 0.2315\n",
      "Epoch: 1 / 3, Step: 451 / 750 Loss: 0.5916\n",
      "Epoch: 1 / 3, Step: 452 / 750 Loss: 0.4327\n",
      "Epoch: 1 / 3, Step: 453 / 750 Loss: 0.3119\n",
      "Epoch: 1 / 3, Step: 454 / 750 Loss: 0.6004\n",
      "Epoch: 1 / 3, Step: 455 / 750 Loss: 0.4035\n",
      "Epoch: 1 / 3, Step: 456 / 750 Loss: 0.4020\n",
      "Epoch: 1 / 3, Step: 457 / 750 Loss: 0.3990\n",
      "Epoch: 1 / 3, Step: 458 / 750 Loss: 0.3451\n",
      "Epoch: 1 / 3, Step: 459 / 750 Loss: 0.3261\n",
      "Epoch: 1 / 3, Step: 460 / 750 Loss: 0.7848\n",
      "Epoch: 1 / 3, Step: 461 / 750 Loss: 0.2902\n",
      "Epoch: 1 / 3, Step: 462 / 750 Loss: 0.6110\n",
      "Epoch: 1 / 3, Step: 463 / 750 Loss: 0.3968\n",
      "Epoch: 1 / 3, Step: 464 / 750 Loss: 0.5519\n",
      "Epoch: 1 / 3, Step: 465 / 750 Loss: 0.5143\n",
      "Epoch: 1 / 3, Step: 466 / 750 Loss: 0.4323\n",
      "Epoch: 1 / 3, Step: 467 / 750 Loss: 0.3347\n",
      "Epoch: 1 / 3, Step: 468 / 750 Loss: 0.6383\n",
      "Epoch: 1 / 3, Step: 469 / 750 Loss: 0.3158\n",
      "Epoch: 1 / 3, Step: 470 / 750 Loss: 0.5282\n",
      "Epoch: 1 / 3, Step: 471 / 750 Loss: 0.3256\n",
      "Epoch: 1 / 3, Step: 472 / 750 Loss: 0.2148\n",
      "Epoch: 1 / 3, Step: 473 / 750 Loss: 0.3577\n",
      "Epoch: 1 / 3, Step: 474 / 750 Loss: 0.5300\n",
      "Epoch: 1 / 3, Step: 475 / 750 Loss: 0.4784\n",
      "Epoch: 1 / 3, Step: 476 / 750 Loss: 0.4362\n",
      "Epoch: 1 / 3, Step: 477 / 750 Loss: 0.3592\n",
      "Epoch: 1 / 3, Step: 478 / 750 Loss: 0.4158\n",
      "Epoch: 1 / 3, Step: 479 / 750 Loss: 0.2606\n",
      "Epoch: 1 / 3, Step: 480 / 750 Loss: 0.5081\n",
      "Epoch: 1 / 3, Step: 481 / 750 Loss: 0.5350\n",
      "Epoch: 1 / 3, Step: 482 / 750 Loss: 0.4912\n",
      "Epoch: 1 / 3, Step: 483 / 750 Loss: 0.5533\n",
      "Epoch: 1 / 3, Step: 484 / 750 Loss: 0.4611\n",
      "Epoch: 1 / 3, Step: 485 / 750 Loss: 0.4593\n",
      "Epoch: 1 / 3, Step: 486 / 750 Loss: 0.5424\n",
      "Epoch: 1 / 3, Step: 487 / 750 Loss: 0.6474\n",
      "Epoch: 1 / 3, Step: 488 / 750 Loss: 0.4560\n",
      "Epoch: 1 / 3, Step: 489 / 750 Loss: 0.4530\n",
      "Epoch: 1 / 3, Step: 490 / 750 Loss: 0.1713\n",
      "Epoch: 1 / 3, Step: 491 / 750 Loss: 0.7712\n",
      "Epoch: 1 / 3, Step: 492 / 750 Loss: 0.4493\n",
      "Epoch: 1 / 3, Step: 493 / 750 Loss: 0.4725\n",
      "Epoch: 1 / 3, Step: 494 / 750 Loss: 0.4740\n",
      "Epoch: 1 / 3, Step: 495 / 750 Loss: 0.2472\n",
      "Epoch: 1 / 3, Step: 496 / 750 Loss: 0.2171\n",
      "Epoch: 1 / 3, Step: 497 / 750 Loss: 0.2716\n",
      "Epoch: 1 / 3, Step: 498 / 750 Loss: 0.5004\n",
      "Epoch: 1 / 3, Step: 499 / 750 Loss: 0.4005\n",
      "Epoch: 1 / 3, Step: 500 / 750 Loss: 0.3578\n",
      "Epoch: 1 / 3, Step: 501 / 750 Loss: 0.5001\n",
      "Epoch: 1 / 3, Step: 502 / 750 Loss: 0.4175\n",
      "Epoch: 1 / 3, Step: 503 / 750 Loss: 0.3002\n",
      "Epoch: 1 / 3, Step: 504 / 750 Loss: 0.2839\n",
      "Epoch: 1 / 3, Step: 505 / 750 Loss: 0.3221\n",
      "Epoch: 1 / 3, Step: 506 / 750 Loss: 0.2698\n",
      "Epoch: 1 / 3, Step: 507 / 750 Loss: 0.1486\n",
      "Epoch: 1 / 3, Step: 508 / 750 Loss: 0.3653\n",
      "Epoch: 1 / 3, Step: 509 / 750 Loss: 0.4496\n",
      "Epoch: 1 / 3, Step: 510 / 750 Loss: 0.3072\n",
      "Epoch: 1 / 3, Step: 511 / 750 Loss: 0.2957\n",
      "Epoch: 1 / 3, Step: 512 / 750 Loss: 0.1641\n",
      "Epoch: 1 / 3, Step: 513 / 750 Loss: 0.3290\n",
      "Epoch: 1 / 3, Step: 514 / 750 Loss: 0.5548\n",
      "Epoch: 1 / 3, Step: 515 / 750 Loss: 0.4459\n",
      "Epoch: 1 / 3, Step: 516 / 750 Loss: 0.4801\n",
      "Epoch: 1 / 3, Step: 517 / 750 Loss: 0.4763\n",
      "Epoch: 1 / 3, Step: 518 / 750 Loss: 0.3547\n",
      "Epoch: 1 / 3, Step: 519 / 750 Loss: 0.4205\n",
      "Epoch: 1 / 3, Step: 520 / 750 Loss: 0.5066\n",
      "Epoch: 1 / 3, Step: 521 / 750 Loss: 0.5276\n",
      "Epoch: 1 / 3, Step: 522 / 750 Loss: 0.3030\n",
      "Epoch: 1 / 3, Step: 523 / 750 Loss: 0.7081\n",
      "Epoch: 1 / 3, Step: 524 / 750 Loss: 0.4746\n",
      "Epoch: 1 / 3, Step: 525 / 750 Loss: 0.4465\n",
      "Epoch: 1 / 3, Step: 526 / 750 Loss: 0.4262\n",
      "Epoch: 1 / 3, Step: 527 / 750 Loss: 0.5517\n",
      "Epoch: 1 / 3, Step: 528 / 750 Loss: 0.3609\n",
      "Epoch: 1 / 3, Step: 529 / 750 Loss: 0.2738\n",
      "Epoch: 1 / 3, Step: 530 / 750 Loss: 0.4636\n",
      "Epoch: 1 / 3, Step: 531 / 750 Loss: 0.2474\n",
      "Epoch: 1 / 3, Step: 532 / 750 Loss: 0.5505\n",
      "Epoch: 1 / 3, Step: 533 / 750 Loss: 0.4493\n",
      "Epoch: 1 / 3, Step: 534 / 750 Loss: 0.3030\n",
      "Epoch: 1 / 3, Step: 535 / 750 Loss: 0.6054\n",
      "Epoch: 1 / 3, Step: 536 / 750 Loss: 0.6796\n",
      "Epoch: 1 / 3, Step: 537 / 750 Loss: 0.5062\n",
      "Epoch: 1 / 3, Step: 538 / 750 Loss: 0.4641\n",
      "Epoch: 1 / 3, Step: 539 / 750 Loss: 0.4201\n",
      "Epoch: 1 / 3, Step: 540 / 750 Loss: 0.4858\n",
      "Epoch: 1 / 3, Step: 541 / 750 Loss: 0.2875\n",
      "Epoch: 1 / 3, Step: 542 / 750 Loss: 0.6502\n",
      "Epoch: 1 / 3, Step: 543 / 750 Loss: 0.3236\n",
      "Epoch: 1 / 3, Step: 544 / 750 Loss: 0.2197\n",
      "Epoch: 1 / 3, Step: 545 / 750 Loss: 0.3720\n",
      "Epoch: 1 / 3, Step: 546 / 750 Loss: 0.4695\n",
      "Epoch: 1 / 3, Step: 547 / 750 Loss: 0.4492\n",
      "Epoch: 1 / 3, Step: 548 / 750 Loss: 0.3109\n",
      "Epoch: 1 / 3, Step: 549 / 750 Loss: 0.3851\n",
      "Epoch: 1 / 3, Step: 550 / 750 Loss: 0.3163\n",
      "Epoch: 1 / 3, Step: 551 / 750 Loss: 0.6429\n",
      "Epoch: 1 / 3, Step: 552 / 750 Loss: 0.2911\n",
      "Epoch: 1 / 3, Step: 553 / 750 Loss: 0.5841\n",
      "Epoch: 1 / 3, Step: 554 / 750 Loss: 0.2929\n",
      "Epoch: 1 / 3, Step: 555 / 750 Loss: 0.3054\n",
      "Epoch: 1 / 3, Step: 556 / 750 Loss: 0.4069\n",
      "Epoch: 1 / 3, Step: 557 / 750 Loss: 0.4190\n",
      "Epoch: 1 / 3, Step: 558 / 750 Loss: 0.5477\n",
      "Epoch: 1 / 3, Step: 559 / 750 Loss: 0.3557\n",
      "Epoch: 1 / 3, Step: 560 / 750 Loss: 0.4019\n",
      "Epoch: 1 / 3, Step: 561 / 750 Loss: 0.3871\n",
      "Epoch: 1 / 3, Step: 562 / 750 Loss: 0.2646\n",
      "Epoch: 1 / 3, Step: 563 / 750 Loss: 0.2126\n",
      "Epoch: 1 / 3, Step: 564 / 750 Loss: 0.6075\n",
      "Epoch: 1 / 3, Step: 565 / 750 Loss: 0.4263\n",
      "Epoch: 1 / 3, Step: 566 / 750 Loss: 0.3393\n",
      "Epoch: 1 / 3, Step: 567 / 750 Loss: 0.2574\n",
      "Epoch: 1 / 3, Step: 568 / 750 Loss: 0.2351\n",
      "Epoch: 1 / 3, Step: 569 / 750 Loss: 0.4417\n",
      "Epoch: 1 / 3, Step: 570 / 750 Loss: 0.4157\n",
      "Epoch: 1 / 3, Step: 571 / 750 Loss: 0.2610\n",
      "Epoch: 1 / 3, Step: 572 / 750 Loss: 0.3791\n",
      "Epoch: 1 / 3, Step: 573 / 750 Loss: 0.3176\n",
      "Epoch: 1 / 3, Step: 574 / 750 Loss: 0.1609\n",
      "Epoch: 1 / 3, Step: 575 / 750 Loss: 0.2537\n",
      "Epoch: 1 / 3, Step: 576 / 750 Loss: 0.3989\n",
      "Epoch: 1 / 3, Step: 577 / 750 Loss: 0.4736\n",
      "Epoch: 1 / 3, Step: 578 / 750 Loss: 0.5035\n",
      "Epoch: 1 / 3, Step: 579 / 750 Loss: 0.3170\n",
      "Epoch: 1 / 3, Step: 580 / 750 Loss: 0.3330\n",
      "Epoch: 1 / 3, Step: 581 / 750 Loss: 0.7623\n",
      "Epoch: 1 / 3, Step: 582 / 750 Loss: 0.3878\n",
      "Epoch: 1 / 3, Step: 583 / 750 Loss: 0.3632\n",
      "Epoch: 1 / 3, Step: 584 / 750 Loss: 0.2093\n",
      "Epoch: 1 / 3, Step: 585 / 750 Loss: 0.3771\n",
      "Epoch: 1 / 3, Step: 586 / 750 Loss: 0.2834\n",
      "Epoch: 1 / 3, Step: 587 / 750 Loss: 0.1943\n",
      "Epoch: 1 / 3, Step: 588 / 750 Loss: 0.1760\n",
      "Epoch: 1 / 3, Step: 589 / 750 Loss: 0.3202\n",
      "Epoch: 1 / 3, Step: 590 / 750 Loss: 0.6214\n",
      "Epoch: 1 / 3, Step: 591 / 750 Loss: 0.2444\n",
      "Epoch: 1 / 3, Step: 592 / 750 Loss: 0.4832\n",
      "Epoch: 1 / 3, Step: 593 / 750 Loss: 0.5194\n",
      "Epoch: 1 / 3, Step: 594 / 750 Loss: 0.4754\n",
      "Epoch: 1 / 3, Step: 595 / 750 Loss: 0.3909\n",
      "Epoch: 1 / 3, Step: 596 / 750 Loss: 0.1891\n",
      "Epoch: 1 / 3, Step: 597 / 750 Loss: 0.2154\n",
      "Epoch: 1 / 3, Step: 598 / 750 Loss: 0.3516\n",
      "Epoch: 1 / 3, Step: 599 / 750 Loss: 0.5094\n",
      "Epoch: 1 / 3, Step: 600 / 750 Loss: 0.5306\n",
      "Epoch: 1 / 3, Step: 601 / 750 Loss: 0.2718\n",
      "Epoch: 1 / 3, Step: 602 / 750 Loss: 0.3843\n",
      "Epoch: 1 / 3, Step: 603 / 750 Loss: 0.4208\n",
      "Epoch: 1 / 3, Step: 604 / 750 Loss: 0.4146\n",
      "Epoch: 1 / 3, Step: 605 / 750 Loss: 0.4516\n",
      "Epoch: 1 / 3, Step: 606 / 750 Loss: 0.1350\n",
      "Epoch: 1 / 3, Step: 607 / 750 Loss: 0.3137\n",
      "Epoch: 1 / 3, Step: 608 / 750 Loss: 0.3192\n",
      "Epoch: 1 / 3, Step: 609 / 750 Loss: 0.2004\n",
      "Epoch: 1 / 3, Step: 610 / 750 Loss: 0.6024\n",
      "Epoch: 1 / 3, Step: 611 / 750 Loss: 0.2454\n",
      "Epoch: 1 / 3, Step: 612 / 750 Loss: 0.1939\n",
      "Epoch: 1 / 3, Step: 613 / 750 Loss: 0.6002\n",
      "Epoch: 1 / 3, Step: 614 / 750 Loss: 0.4061\n",
      "Epoch: 1 / 3, Step: 615 / 750 Loss: 0.1894\n",
      "Epoch: 1 / 3, Step: 616 / 750 Loss: 0.5199\n",
      "Epoch: 1 / 3, Step: 617 / 750 Loss: 0.1615\n",
      "Epoch: 1 / 3, Step: 618 / 750 Loss: 0.2323\n",
      "Epoch: 1 / 3, Step: 619 / 750 Loss: 0.3647\n",
      "Epoch: 1 / 3, Step: 620 / 750 Loss: 0.3212\n",
      "Epoch: 1 / 3, Step: 621 / 750 Loss: 0.2886\n",
      "Epoch: 1 / 3, Step: 622 / 750 Loss: 0.4588\n",
      "Epoch: 1 / 3, Step: 623 / 750 Loss: 0.5544\n",
      "Epoch: 1 / 3, Step: 624 / 750 Loss: 0.5242\n",
      "Epoch: 1 / 3, Step: 625 / 750 Loss: 0.3276\n",
      "Epoch: 1 / 3, Step: 626 / 750 Loss: 0.2259\n",
      "Epoch: 1 / 3, Step: 627 / 750 Loss: 0.3284\n",
      "Epoch: 1 / 3, Step: 628 / 750 Loss: 0.2555\n",
      "Epoch: 1 / 3, Step: 629 / 750 Loss: 0.2870\n",
      "Epoch: 1 / 3, Step: 630 / 750 Loss: 0.2920\n",
      "Epoch: 1 / 3, Step: 631 / 750 Loss: 0.2739\n",
      "Epoch: 1 / 3, Step: 632 / 750 Loss: 0.2918\n",
      "Epoch: 1 / 3, Step: 633 / 750 Loss: 0.5137\n",
      "Epoch: 1 / 3, Step: 634 / 750 Loss: 0.3461\n",
      "Epoch: 1 / 3, Step: 635 / 750 Loss: 0.3006\n",
      "Epoch: 1 / 3, Step: 636 / 750 Loss: 0.2406\n",
      "Epoch: 1 / 3, Step: 637 / 750 Loss: 0.3165\n",
      "Epoch: 1 / 3, Step: 638 / 750 Loss: 0.6177\n",
      "Epoch: 1 / 3, Step: 639 / 750 Loss: 0.4071\n",
      "Epoch: 1 / 3, Step: 640 / 750 Loss: 0.2731\n",
      "Epoch: 1 / 3, Step: 641 / 750 Loss: 0.1324\n",
      "Epoch: 1 / 3, Step: 642 / 750 Loss: 0.4009\n",
      "Epoch: 1 / 3, Step: 643 / 750 Loss: 0.4223\n",
      "Epoch: 1 / 3, Step: 644 / 750 Loss: 0.3098\n",
      "Epoch: 1 / 3, Step: 645 / 750 Loss: 0.3328\n",
      "Epoch: 1 / 3, Step: 646 / 750 Loss: 0.4271\n",
      "Epoch: 1 / 3, Step: 647 / 750 Loss: 0.5297\n",
      "Epoch: 1 / 3, Step: 648 / 750 Loss: 0.2960\n",
      "Epoch: 1 / 3, Step: 649 / 750 Loss: 0.4918\n",
      "Epoch: 1 / 3, Step: 650 / 750 Loss: 0.2391\n",
      "Epoch: 1 / 3, Step: 651 / 750 Loss: 0.1549\n",
      "Epoch: 1 / 3, Step: 652 / 750 Loss: 0.5334\n",
      "Epoch: 1 / 3, Step: 653 / 750 Loss: 0.2243\n",
      "Epoch: 1 / 3, Step: 654 / 750 Loss: 0.5723\n",
      "Epoch: 1 / 3, Step: 655 / 750 Loss: 0.3277\n",
      "Epoch: 1 / 3, Step: 656 / 750 Loss: 0.2932\n",
      "Epoch: 1 / 3, Step: 657 / 750 Loss: 0.3459\n",
      "Epoch: 1 / 3, Step: 658 / 750 Loss: 0.3574\n",
      "Epoch: 1 / 3, Step: 659 / 750 Loss: 0.2387\n",
      "Epoch: 1 / 3, Step: 660 / 750 Loss: 0.5832\n",
      "Epoch: 1 / 3, Step: 661 / 750 Loss: 0.2927\n",
      "Epoch: 1 / 3, Step: 662 / 750 Loss: 0.2903\n",
      "Epoch: 1 / 3, Step: 663 / 750 Loss: 0.3204\n",
      "Epoch: 1 / 3, Step: 664 / 750 Loss: 0.2605\n",
      "Epoch: 1 / 3, Step: 665 / 750 Loss: 0.3439\n",
      "Epoch: 1 / 3, Step: 666 / 750 Loss: 0.5488\n",
      "Epoch: 1 / 3, Step: 667 / 750 Loss: 0.2044\n",
      "Epoch: 1 / 3, Step: 668 / 750 Loss: 0.2779\n",
      "Epoch: 1 / 3, Step: 669 / 750 Loss: 0.6133\n",
      "Epoch: 1 / 3, Step: 670 / 750 Loss: 0.4903\n",
      "Epoch: 1 / 3, Step: 671 / 750 Loss: 0.3763\n",
      "Epoch: 1 / 3, Step: 672 / 750 Loss: 0.3323\n",
      "Epoch: 1 / 3, Step: 673 / 750 Loss: 0.2868\n",
      "Epoch: 1 / 3, Step: 674 / 750 Loss: 0.6132\n",
      "Epoch: 1 / 3, Step: 675 / 750 Loss: 0.3585\n",
      "Epoch: 1 / 3, Step: 676 / 750 Loss: 0.2521\n",
      "Epoch: 1 / 3, Step: 677 / 750 Loss: 0.4929\n",
      "Epoch: 1 / 3, Step: 678 / 750 Loss: 0.3697\n",
      "Epoch: 1 / 3, Step: 679 / 750 Loss: 0.3708\n",
      "Epoch: 1 / 3, Step: 680 / 750 Loss: 0.2229\n",
      "Epoch: 1 / 3, Step: 681 / 750 Loss: 0.3336\n",
      "Epoch: 1 / 3, Step: 682 / 750 Loss: 0.2957\n",
      "Epoch: 1 / 3, Step: 683 / 750 Loss: 0.3035\n",
      "Epoch: 1 / 3, Step: 684 / 750 Loss: 0.3248\n",
      "Epoch: 1 / 3, Step: 685 / 750 Loss: 0.5178\n",
      "Epoch: 1 / 3, Step: 686 / 750 Loss: 0.3655\n",
      "Epoch: 1 / 3, Step: 687 / 750 Loss: 0.2205\n",
      "Epoch: 1 / 3, Step: 688 / 750 Loss: 0.3286\n",
      "Epoch: 1 / 3, Step: 689 / 750 Loss: 0.2343\n",
      "Epoch: 1 / 3, Step: 690 / 750 Loss: 0.5611\n",
      "Epoch: 1 / 3, Step: 691 / 750 Loss: 0.2625\n",
      "Epoch: 1 / 3, Step: 692 / 750 Loss: 0.4346\n",
      "Epoch: 1 / 3, Step: 693 / 750 Loss: 0.1506\n",
      "Epoch: 1 / 3, Step: 694 / 750 Loss: 0.3331\n",
      "Epoch: 1 / 3, Step: 695 / 750 Loss: 0.2829\n",
      "Epoch: 1 / 3, Step: 696 / 750 Loss: 0.2736\n",
      "Epoch: 1 / 3, Step: 697 / 750 Loss: 0.3543\n",
      "Epoch: 1 / 3, Step: 698 / 750 Loss: 0.4079\n",
      "Epoch: 1 / 3, Step: 699 / 750 Loss: 0.3074\n",
      "Epoch: 1 / 3, Step: 700 / 750 Loss: 0.3784\n",
      "Epoch: 1 / 3, Step: 701 / 750 Loss: 0.2310\n",
      "Epoch: 1 / 3, Step: 702 / 750 Loss: 0.5392\n",
      "Epoch: 1 / 3, Step: 703 / 750 Loss: 0.1397\n",
      "Epoch: 1 / 3, Step: 704 / 750 Loss: 0.4979\n",
      "Epoch: 1 / 3, Step: 705 / 750 Loss: 0.2975\n",
      "Epoch: 1 / 3, Step: 706 / 750 Loss: 0.4104\n",
      "Epoch: 1 / 3, Step: 707 / 750 Loss: 0.1530\n",
      "Epoch: 1 / 3, Step: 708 / 750 Loss: 0.2944\n",
      "Epoch: 1 / 3, Step: 709 / 750 Loss: 0.3775\n",
      "Epoch: 1 / 3, Step: 710 / 750 Loss: 0.3048\n",
      "Epoch: 1 / 3, Step: 711 / 750 Loss: 0.3477\n",
      "Epoch: 1 / 3, Step: 712 / 750 Loss: 0.4511\n",
      "Epoch: 1 / 3, Step: 713 / 750 Loss: 0.2222\n",
      "Epoch: 1 / 3, Step: 714 / 750 Loss: 0.7323\n",
      "Epoch: 1 / 3, Step: 715 / 750 Loss: 0.5430\n",
      "Epoch: 1 / 3, Step: 716 / 750 Loss: 0.2939\n",
      "Epoch: 1 / 3, Step: 717 / 750 Loss: 0.4612\n",
      "Epoch: 1 / 3, Step: 718 / 750 Loss: 0.2770\n",
      "Epoch: 1 / 3, Step: 719 / 750 Loss: 0.3170\n",
      "Epoch: 1 / 3, Step: 720 / 750 Loss: 0.2084\n",
      "Epoch: 1 / 3, Step: 721 / 750 Loss: 0.3982\n",
      "Epoch: 1 / 3, Step: 722 / 750 Loss: 0.2743\n",
      "Epoch: 1 / 3, Step: 723 / 750 Loss: 0.3108\n",
      "Epoch: 1 / 3, Step: 724 / 750 Loss: 0.1753\n",
      "Epoch: 1 / 3, Step: 725 / 750 Loss: 0.1178\n",
      "Epoch: 1 / 3, Step: 726 / 750 Loss: 0.2565\n",
      "Epoch: 1 / 3, Step: 727 / 750 Loss: 0.4868\n",
      "Epoch: 1 / 3, Step: 728 / 750 Loss: 0.2808\n",
      "Epoch: 1 / 3, Step: 729 / 750 Loss: 0.2294\n",
      "Epoch: 1 / 3, Step: 730 / 750 Loss: 0.5358\n",
      "Epoch: 1 / 3, Step: 731 / 750 Loss: 0.1964\n",
      "Epoch: 1 / 3, Step: 732 / 750 Loss: 0.4531\n",
      "Epoch: 1 / 3, Step: 733 / 750 Loss: 0.3626\n",
      "Epoch: 1 / 3, Step: 734 / 750 Loss: 0.3252\n",
      "Epoch: 1 / 3, Step: 735 / 750 Loss: 0.1916\n",
      "Epoch: 1 / 3, Step: 736 / 750 Loss: 0.2360\n",
      "Epoch: 1 / 3, Step: 737 / 750 Loss: 0.3505\n",
      "Epoch: 1 / 3, Step: 738 / 750 Loss: 0.1335\n",
      "Epoch: 1 / 3, Step: 739 / 750 Loss: 0.5269\n",
      "Epoch: 1 / 3, Step: 740 / 750 Loss: 0.2883\n",
      "Epoch: 1 / 3, Step: 741 / 750 Loss: 0.2309\n",
      "Epoch: 1 / 3, Step: 742 / 750 Loss: 0.3373\n",
      "Epoch: 1 / 3, Step: 743 / 750 Loss: 0.2540\n",
      "Epoch: 1 / 3, Step: 744 / 750 Loss: 0.2111\n",
      "Epoch: 1 / 3, Step: 745 / 750 Loss: 0.3766\n",
      "Epoch: 1 / 3, Step: 746 / 750 Loss: 0.3358\n",
      "Epoch: 1 / 3, Step: 747 / 750 Loss: 0.2568\n",
      "Epoch: 1 / 3, Step: 748 / 750 Loss: 0.2350\n",
      "Epoch: 1 / 3, Step: 749 / 750 Loss: 0.1356\n",
      "Epoch: 2 / 3, Step: 0 / 750 Loss: 0.2479\n",
      "Epoch: 2 / 3, Step: 1 / 750 Loss: 0.2667\n",
      "Epoch: 2 / 3, Step: 2 / 750 Loss: 0.4058\n",
      "Epoch: 2 / 3, Step: 3 / 750 Loss: 0.3829\n",
      "Epoch: 2 / 3, Step: 4 / 750 Loss: 0.4996\n",
      "Epoch: 2 / 3, Step: 5 / 750 Loss: 0.1842\n",
      "Epoch: 2 / 3, Step: 6 / 750 Loss: 0.4915\n",
      "Epoch: 2 / 3, Step: 7 / 750 Loss: 0.3138\n",
      "Epoch: 2 / 3, Step: 8 / 750 Loss: 0.2366\n",
      "Epoch: 2 / 3, Step: 9 / 750 Loss: 0.3104\n",
      "Epoch: 2 / 3, Step: 10 / 750 Loss: 0.1110\n",
      "Epoch: 2 / 3, Step: 11 / 750 Loss: 0.7176\n",
      "Epoch: 2 / 3, Step: 12 / 750 Loss: 0.2111\n",
      "Epoch: 2 / 3, Step: 13 / 750 Loss: 0.4178\n",
      "Epoch: 2 / 3, Step: 14 / 750 Loss: 0.4199\n",
      "Epoch: 2 / 3, Step: 15 / 750 Loss: 0.3700\n",
      "Epoch: 2 / 3, Step: 16 / 750 Loss: 0.2285\n",
      "Epoch: 2 / 3, Step: 17 / 750 Loss: 0.4460\n",
      "Epoch: 2 / 3, Step: 18 / 750 Loss: 0.2781\n",
      "Epoch: 2 / 3, Step: 19 / 750 Loss: 0.1163\n",
      "Epoch: 2 / 3, Step: 20 / 750 Loss: 0.4283\n",
      "Epoch: 2 / 3, Step: 21 / 750 Loss: 0.1805\n",
      "Epoch: 2 / 3, Step: 22 / 750 Loss: 0.2063\n",
      "Epoch: 2 / 3, Step: 23 / 750 Loss: 0.1693\n",
      "Epoch: 2 / 3, Step: 24 / 750 Loss: 0.2093\n",
      "Epoch: 2 / 3, Step: 25 / 750 Loss: 0.1915\n",
      "Epoch: 2 / 3, Step: 26 / 750 Loss: 0.1848\n",
      "Epoch: 2 / 3, Step: 27 / 750 Loss: 0.2980\n",
      "Epoch: 2 / 3, Step: 28 / 750 Loss: 0.2366\n",
      "Epoch: 2 / 3, Step: 29 / 750 Loss: 0.3687\n",
      "Epoch: 2 / 3, Step: 30 / 750 Loss: 0.2839\n",
      "Epoch: 2 / 3, Step: 31 / 750 Loss: 0.3754\n",
      "Epoch: 2 / 3, Step: 32 / 750 Loss: 0.2337\n",
      "Epoch: 2 / 3, Step: 33 / 750 Loss: 0.2266\n",
      "Epoch: 2 / 3, Step: 34 / 750 Loss: 0.2986\n",
      "Epoch: 2 / 3, Step: 35 / 750 Loss: 0.1600\n",
      "Epoch: 2 / 3, Step: 36 / 750 Loss: 0.1843\n",
      "Epoch: 2 / 3, Step: 37 / 750 Loss: 0.1675\n",
      "Epoch: 2 / 3, Step: 38 / 750 Loss: 0.4521\n",
      "Epoch: 2 / 3, Step: 39 / 750 Loss: 0.1774\n",
      "Epoch: 2 / 3, Step: 40 / 750 Loss: 0.1285\n",
      "Epoch: 2 / 3, Step: 41 / 750 Loss: 0.1859\n",
      "Epoch: 2 / 3, Step: 42 / 750 Loss: 0.3950\n",
      "Epoch: 2 / 3, Step: 43 / 750 Loss: 0.2687\n",
      "Epoch: 2 / 3, Step: 44 / 750 Loss: 0.2539\n",
      "Epoch: 2 / 3, Step: 45 / 750 Loss: 0.3356\n",
      "Epoch: 2 / 3, Step: 46 / 750 Loss: 0.1312\n",
      "Epoch: 2 / 3, Step: 47 / 750 Loss: 0.4320\n",
      "Epoch: 2 / 3, Step: 48 / 750 Loss: 0.1223\n",
      "Epoch: 2 / 3, Step: 49 / 750 Loss: 0.1740\n",
      "Epoch: 2 / 3, Step: 50 / 750 Loss: 0.3744\n",
      "Epoch: 2 / 3, Step: 51 / 750 Loss: 0.1958\n",
      "Epoch: 2 / 3, Step: 52 / 750 Loss: 0.2563\n",
      "Epoch: 2 / 3, Step: 53 / 750 Loss: 0.2024\n",
      "Epoch: 2 / 3, Step: 54 / 750 Loss: 0.2008\n",
      "Epoch: 2 / 3, Step: 55 / 750 Loss: 0.2622\n",
      "Epoch: 2 / 3, Step: 56 / 750 Loss: 0.1865\n",
      "Epoch: 2 / 3, Step: 57 / 750 Loss: 0.3105\n",
      "Epoch: 2 / 3, Step: 58 / 750 Loss: 0.6355\n",
      "Epoch: 2 / 3, Step: 59 / 750 Loss: 0.4219\n",
      "Epoch: 2 / 3, Step: 60 / 750 Loss: 0.2056\n",
      "Epoch: 2 / 3, Step: 61 / 750 Loss: 0.5240\n",
      "Epoch: 2 / 3, Step: 62 / 750 Loss: 0.4897\n",
      "Epoch: 2 / 3, Step: 63 / 750 Loss: 0.4123\n",
      "Epoch: 2 / 3, Step: 64 / 750 Loss: 0.3368\n",
      "Epoch: 2 / 3, Step: 65 / 750 Loss: 0.2381\n",
      "Epoch: 2 / 3, Step: 66 / 750 Loss: 0.1179\n",
      "Epoch: 2 / 3, Step: 67 / 750 Loss: 0.1118\n",
      "Epoch: 2 / 3, Step: 68 / 750 Loss: 0.1703\n",
      "Epoch: 2 / 3, Step: 69 / 750 Loss: 0.1991\n",
      "Epoch: 2 / 3, Step: 70 / 750 Loss: 0.3308\n",
      "Epoch: 2 / 3, Step: 71 / 750 Loss: 0.2151\n",
      "Epoch: 2 / 3, Step: 72 / 750 Loss: 0.2987\n",
      "Epoch: 2 / 3, Step: 73 / 750 Loss: 0.1693\n",
      "Epoch: 2 / 3, Step: 74 / 750 Loss: 0.4958\n",
      "Epoch: 2 / 3, Step: 75 / 750 Loss: 0.4191\n",
      "Epoch: 2 / 3, Step: 76 / 750 Loss: 0.2453\n",
      "Epoch: 2 / 3, Step: 77 / 750 Loss: 0.4490\n",
      "Epoch: 2 / 3, Step: 78 / 750 Loss: 0.2369\n",
      "Epoch: 2 / 3, Step: 79 / 750 Loss: 0.3552\n",
      "Epoch: 2 / 3, Step: 80 / 750 Loss: 0.1612\n",
      "Epoch: 2 / 3, Step: 81 / 750 Loss: 0.1288\n",
      "Epoch: 2 / 3, Step: 82 / 750 Loss: 0.2489\n",
      "Epoch: 2 / 3, Step: 83 / 750 Loss: 0.1993\n",
      "Epoch: 2 / 3, Step: 84 / 750 Loss: 0.2978\n",
      "Epoch: 2 / 3, Step: 85 / 750 Loss: 0.2780\n",
      "Epoch: 2 / 3, Step: 86 / 750 Loss: 0.3592\n",
      "Epoch: 2 / 3, Step: 87 / 750 Loss: 0.2971\n",
      "Epoch: 2 / 3, Step: 88 / 750 Loss: 0.2106\n",
      "Epoch: 2 / 3, Step: 89 / 750 Loss: 0.3670\n",
      "Epoch: 2 / 3, Step: 90 / 750 Loss: 0.4192\n",
      "Epoch: 2 / 3, Step: 91 / 750 Loss: 0.1565\n",
      "Epoch: 2 / 3, Step: 92 / 750 Loss: 0.1128\n",
      "Epoch: 2 / 3, Step: 93 / 750 Loss: 0.2466\n",
      "Epoch: 2 / 3, Step: 94 / 750 Loss: 0.2093\n",
      "Epoch: 2 / 3, Step: 95 / 750 Loss: 0.3893\n",
      "Epoch: 2 / 3, Step: 96 / 750 Loss: 0.1916\n",
      "Epoch: 2 / 3, Step: 97 / 750 Loss: 0.1684\n",
      "Epoch: 2 / 3, Step: 98 / 750 Loss: 0.1841\n",
      "Epoch: 2 / 3, Step: 99 / 750 Loss: 0.1666\n",
      "Epoch: 2 / 3, Step: 100 / 750 Loss: 0.1680\n",
      "Epoch: 2 / 3, Step: 101 / 750 Loss: 0.2529\n",
      "Epoch: 2 / 3, Step: 102 / 750 Loss: 0.5380\n",
      "Epoch: 2 / 3, Step: 103 / 750 Loss: 0.3583\n",
      "Epoch: 2 / 3, Step: 104 / 750 Loss: 0.4298\n",
      "Epoch: 2 / 3, Step: 105 / 750 Loss: 0.4020\n",
      "Epoch: 2 / 3, Step: 106 / 750 Loss: 0.4045\n",
      "Epoch: 2 / 3, Step: 107 / 750 Loss: 0.4194\n",
      "Epoch: 2 / 3, Step: 108 / 750 Loss: 0.1036\n",
      "Epoch: 2 / 3, Step: 109 / 750 Loss: 0.5117\n",
      "Epoch: 2 / 3, Step: 110 / 750 Loss: 0.3505\n",
      "Epoch: 2 / 3, Step: 111 / 750 Loss: 0.1582\n",
      "Epoch: 2 / 3, Step: 112 / 750 Loss: 0.1256\n",
      "Epoch: 2 / 3, Step: 113 / 750 Loss: 0.1911\n",
      "Epoch: 2 / 3, Step: 114 / 750 Loss: 0.3564\n",
      "Epoch: 2 / 3, Step: 115 / 750 Loss: 0.3334\n",
      "Epoch: 2 / 3, Step: 116 / 750 Loss: 0.2018\n",
      "Epoch: 2 / 3, Step: 117 / 750 Loss: 0.5511\n",
      "Epoch: 2 / 3, Step: 118 / 750 Loss: 0.2470\n",
      "Epoch: 2 / 3, Step: 119 / 750 Loss: 0.1174\n",
      "Epoch: 2 / 3, Step: 120 / 750 Loss: 0.2947\n",
      "Epoch: 2 / 3, Step: 121 / 750 Loss: 0.2293\n",
      "Epoch: 2 / 3, Step: 122 / 750 Loss: 0.1350\n",
      "Epoch: 2 / 3, Step: 123 / 750 Loss: 0.1747\n",
      "Epoch: 2 / 3, Step: 124 / 750 Loss: 0.1060\n",
      "Epoch: 2 / 3, Step: 125 / 750 Loss: 0.2405\n",
      "Epoch: 2 / 3, Step: 126 / 750 Loss: 0.1314\n",
      "Epoch: 2 / 3, Step: 127 / 750 Loss: 0.1836\n",
      "Epoch: 2 / 3, Step: 128 / 750 Loss: 0.2029\n",
      "Epoch: 2 / 3, Step: 129 / 750 Loss: 0.1671\n",
      "Epoch: 2 / 3, Step: 130 / 750 Loss: 0.4669\n",
      "Epoch: 2 / 3, Step: 131 / 750 Loss: 0.1947\n",
      "Epoch: 2 / 3, Step: 132 / 750 Loss: 0.2810\n",
      "Epoch: 2 / 3, Step: 133 / 750 Loss: 0.4292\n",
      "Epoch: 2 / 3, Step: 134 / 750 Loss: 0.3730\n",
      "Epoch: 2 / 3, Step: 135 / 750 Loss: 0.2460\n",
      "Epoch: 2 / 3, Step: 136 / 750 Loss: 0.3560\n",
      "Epoch: 2 / 3, Step: 137 / 750 Loss: 0.2895\n",
      "Epoch: 2 / 3, Step: 138 / 750 Loss: 0.1924\n",
      "Epoch: 2 / 3, Step: 139 / 750 Loss: 0.1602\n",
      "Epoch: 2 / 3, Step: 140 / 750 Loss: 0.2107\n",
      "Epoch: 2 / 3, Step: 141 / 750 Loss: 0.3330\n",
      "Epoch: 2 / 3, Step: 142 / 750 Loss: 0.4805\n",
      "Epoch: 2 / 3, Step: 143 / 750 Loss: 0.1830\n",
      "Epoch: 2 / 3, Step: 144 / 750 Loss: 0.1658\n",
      "Epoch: 2 / 3, Step: 145 / 750 Loss: 0.1639\n",
      "Epoch: 2 / 3, Step: 146 / 750 Loss: 0.2620\n",
      "Epoch: 2 / 3, Step: 147 / 750 Loss: 0.1715\n",
      "Epoch: 2 / 3, Step: 148 / 750 Loss: 0.4756\n",
      "Epoch: 2 / 3, Step: 149 / 750 Loss: 0.4580\n",
      "Epoch: 2 / 3, Step: 150 / 750 Loss: 0.0833\n",
      "Epoch: 2 / 3, Step: 151 / 750 Loss: 0.3742\n",
      "Epoch: 2 / 3, Step: 152 / 750 Loss: 0.3066\n",
      "Epoch: 2 / 3, Step: 153 / 750 Loss: 0.3143\n",
      "Epoch: 2 / 3, Step: 154 / 750 Loss: 0.2741\n",
      "Epoch: 2 / 3, Step: 155 / 750 Loss: 0.1539\n",
      "Epoch: 2 / 3, Step: 156 / 750 Loss: 0.3004\n",
      "Epoch: 2 / 3, Step: 157 / 750 Loss: 0.3587\n",
      "Epoch: 2 / 3, Step: 158 / 750 Loss: 0.4701\n",
      "Epoch: 2 / 3, Step: 159 / 750 Loss: 0.3395\n",
      "Epoch: 2 / 3, Step: 160 / 750 Loss: 0.2896\n",
      "Epoch: 2 / 3, Step: 161 / 750 Loss: 0.5420\n",
      "Epoch: 2 / 3, Step: 162 / 750 Loss: 0.4543\n",
      "Epoch: 2 / 3, Step: 163 / 750 Loss: 0.2778\n",
      "Epoch: 2 / 3, Step: 164 / 750 Loss: 0.3527\n",
      "Epoch: 2 / 3, Step: 165 / 750 Loss: 0.3255\n",
      "Epoch: 2 / 3, Step: 166 / 750 Loss: 0.3322\n",
      "Epoch: 2 / 3, Step: 167 / 750 Loss: 0.3540\n",
      "Epoch: 2 / 3, Step: 168 / 750 Loss: 0.4370\n",
      "Epoch: 2 / 3, Step: 169 / 750 Loss: 0.2209\n",
      "Epoch: 2 / 3, Step: 170 / 750 Loss: 0.3167\n",
      "Epoch: 2 / 3, Step: 171 / 750 Loss: 0.3626\n",
      "Epoch: 2 / 3, Step: 172 / 750 Loss: 0.3496\n",
      "Epoch: 2 / 3, Step: 173 / 750 Loss: 0.2973\n",
      "Epoch: 2 / 3, Step: 174 / 750 Loss: 0.2888\n",
      "Epoch: 2 / 3, Step: 175 / 750 Loss: 0.3604\n",
      "Epoch: 2 / 3, Step: 176 / 750 Loss: 0.2597\n",
      "Epoch: 2 / 3, Step: 177 / 750 Loss: 0.2487\n",
      "Epoch: 2 / 3, Step: 178 / 750 Loss: 0.1576\n",
      "Epoch: 2 / 3, Step: 179 / 750 Loss: 0.2538\n",
      "Epoch: 2 / 3, Step: 180 / 750 Loss: 0.2397\n",
      "Epoch: 2 / 3, Step: 181 / 750 Loss: 0.2586\n",
      "Epoch: 2 / 3, Step: 182 / 750 Loss: 0.1028\n",
      "Epoch: 2 / 3, Step: 183 / 750 Loss: 0.1342\n",
      "Epoch: 2 / 3, Step: 184 / 750 Loss: 0.2429\n",
      "Epoch: 2 / 3, Step: 185 / 750 Loss: 0.1416\n",
      "Epoch: 2 / 3, Step: 186 / 750 Loss: 0.2139\n",
      "Epoch: 2 / 3, Step: 187 / 750 Loss: 0.2795\n",
      "Epoch: 2 / 3, Step: 188 / 750 Loss: 0.2911\n",
      "Epoch: 2 / 3, Step: 189 / 750 Loss: 0.2312\n",
      "Epoch: 2 / 3, Step: 190 / 750 Loss: 0.2157\n",
      "Epoch: 2 / 3, Step: 191 / 750 Loss: 0.4204\n",
      "Epoch: 2 / 3, Step: 192 / 750 Loss: 0.3408\n",
      "Epoch: 2 / 3, Step: 193 / 750 Loss: 0.2030\n",
      "Epoch: 2 / 3, Step: 194 / 750 Loss: 0.2188\n",
      "Epoch: 2 / 3, Step: 195 / 750 Loss: 0.1880\n",
      "Epoch: 2 / 3, Step: 196 / 750 Loss: 0.1252\n",
      "Epoch: 2 / 3, Step: 197 / 750 Loss: 0.0737\n",
      "Epoch: 2 / 3, Step: 198 / 750 Loss: 0.4843\n",
      "Epoch: 2 / 3, Step: 199 / 750 Loss: 0.3125\n",
      "Epoch: 2 / 3, Step: 200 / 750 Loss: 0.2896\n",
      "Epoch: 2 / 3, Step: 201 / 750 Loss: 0.3835\n",
      "Epoch: 2 / 3, Step: 202 / 750 Loss: 0.3471\n",
      "Epoch: 2 / 3, Step: 203 / 750 Loss: 0.3217\n",
      "Epoch: 2 / 3, Step: 204 / 750 Loss: 0.1903\n",
      "Epoch: 2 / 3, Step: 205 / 750 Loss: 0.1318\n",
      "Epoch: 2 / 3, Step: 206 / 750 Loss: 0.4321\n",
      "Epoch: 2 / 3, Step: 207 / 750 Loss: 0.2154\n",
      "Epoch: 2 / 3, Step: 208 / 750 Loss: 0.4637\n",
      "Epoch: 2 / 3, Step: 209 / 750 Loss: 0.2392\n",
      "Epoch: 2 / 3, Step: 210 / 750 Loss: 0.3338\n",
      "Epoch: 2 / 3, Step: 211 / 750 Loss: 0.2649\n",
      "Epoch: 2 / 3, Step: 212 / 750 Loss: 0.3855\n",
      "Epoch: 2 / 3, Step: 213 / 750 Loss: 0.3985\n",
      "Epoch: 2 / 3, Step: 214 / 750 Loss: 0.1073\n",
      "Epoch: 2 / 3, Step: 215 / 750 Loss: 0.3689\n",
      "Epoch: 2 / 3, Step: 216 / 750 Loss: 0.1975\n",
      "Epoch: 2 / 3, Step: 217 / 750 Loss: 0.3583\n",
      "Epoch: 2 / 3, Step: 218 / 750 Loss: 0.2366\n",
      "Epoch: 2 / 3, Step: 219 / 750 Loss: 0.6182\n",
      "Epoch: 2 / 3, Step: 220 / 750 Loss: 0.1956\n",
      "Epoch: 2 / 3, Step: 221 / 750 Loss: 0.1629\n",
      "Epoch: 2 / 3, Step: 222 / 750 Loss: 0.1581\n",
      "Epoch: 2 / 3, Step: 223 / 750 Loss: 0.4109\n",
      "Epoch: 2 / 3, Step: 224 / 750 Loss: 0.2278\n",
      "Epoch: 2 / 3, Step: 225 / 750 Loss: 0.1424\n",
      "Epoch: 2 / 3, Step: 226 / 750 Loss: 0.2920\n",
      "Epoch: 2 / 3, Step: 227 / 750 Loss: 0.1693\n",
      "Epoch: 2 / 3, Step: 228 / 750 Loss: 0.2964\n",
      "Epoch: 2 / 3, Step: 229 / 750 Loss: 0.2136\n",
      "Epoch: 2 / 3, Step: 230 / 750 Loss: 0.1470\n",
      "Epoch: 2 / 3, Step: 231 / 750 Loss: 0.2289\n",
      "Epoch: 2 / 3, Step: 232 / 750 Loss: 0.2820\n",
      "Epoch: 2 / 3, Step: 233 / 750 Loss: 0.3961\n",
      "Epoch: 2 / 3, Step: 234 / 750 Loss: 0.2939\n",
      "Epoch: 2 / 3, Step: 235 / 750 Loss: 0.1646\n",
      "Epoch: 2 / 3, Step: 236 / 750 Loss: 0.2786\n",
      "Epoch: 2 / 3, Step: 237 / 750 Loss: 0.1790\n",
      "Epoch: 2 / 3, Step: 238 / 750 Loss: 0.1095\n",
      "Epoch: 2 / 3, Step: 239 / 750 Loss: 0.2951\n",
      "Epoch: 2 / 3, Step: 240 / 750 Loss: 0.1968\n",
      "Epoch: 2 / 3, Step: 241 / 750 Loss: 0.3097\n",
      "Epoch: 2 / 3, Step: 242 / 750 Loss: 0.1049\n",
      "Epoch: 2 / 3, Step: 243 / 750 Loss: 0.2186\n",
      "Epoch: 2 / 3, Step: 244 / 750 Loss: 0.5862\n",
      "Epoch: 2 / 3, Step: 245 / 750 Loss: 0.1295\n",
      "Epoch: 2 / 3, Step: 246 / 750 Loss: 0.3633\n",
      "Epoch: 2 / 3, Step: 247 / 750 Loss: 0.3103\n",
      "Epoch: 2 / 3, Step: 248 / 750 Loss: 0.2578\n",
      "Epoch: 2 / 3, Step: 249 / 750 Loss: 0.1696\n",
      "Epoch: 2 / 3, Step: 250 / 750 Loss: 0.1323\n",
      "Epoch: 2 / 3, Step: 251 / 750 Loss: 0.1943\n",
      "Epoch: 2 / 3, Step: 252 / 750 Loss: 0.4135\n",
      "Epoch: 2 / 3, Step: 253 / 750 Loss: 0.3293\n",
      "Epoch: 2 / 3, Step: 254 / 750 Loss: 0.2035\n",
      "Epoch: 2 / 3, Step: 255 / 750 Loss: 0.3676\n",
      "Epoch: 2 / 3, Step: 256 / 750 Loss: 0.3773\n",
      "Epoch: 2 / 3, Step: 257 / 750 Loss: 0.4317\n",
      "Epoch: 2 / 3, Step: 258 / 750 Loss: 0.6734\n",
      "Epoch: 2 / 3, Step: 259 / 750 Loss: 0.1208\n",
      "Epoch: 2 / 3, Step: 260 / 750 Loss: 0.4534\n",
      "Epoch: 2 / 3, Step: 261 / 750 Loss: 0.2231\n",
      "Epoch: 2 / 3, Step: 262 / 750 Loss: 0.1903\n",
      "Epoch: 2 / 3, Step: 263 / 750 Loss: 0.3455\n",
      "Epoch: 2 / 3, Step: 264 / 750 Loss: 0.2894\n",
      "Epoch: 2 / 3, Step: 265 / 750 Loss: 0.1983\n",
      "Epoch: 2 / 3, Step: 266 / 750 Loss: 0.3177\n",
      "Epoch: 2 / 3, Step: 267 / 750 Loss: 0.3245\n",
      "Epoch: 2 / 3, Step: 268 / 750 Loss: 0.3957\n",
      "Epoch: 2 / 3, Step: 269 / 750 Loss: 0.2267\n",
      "Epoch: 2 / 3, Step: 270 / 750 Loss: 0.3954\n",
      "Epoch: 2 / 3, Step: 271 / 750 Loss: 0.4353\n",
      "Epoch: 2 / 3, Step: 272 / 750 Loss: 0.2635\n",
      "Epoch: 2 / 3, Step: 273 / 750 Loss: 0.2559\n",
      "Epoch: 2 / 3, Step: 274 / 750 Loss: 0.1783\n",
      "Epoch: 2 / 3, Step: 275 / 750 Loss: 0.2382\n",
      "Epoch: 2 / 3, Step: 276 / 750 Loss: 0.1843\n",
      "Epoch: 2 / 3, Step: 277 / 750 Loss: 0.4024\n",
      "Epoch: 2 / 3, Step: 278 / 750 Loss: 0.3039\n",
      "Epoch: 2 / 3, Step: 279 / 750 Loss: 0.2787\n",
      "Epoch: 2 / 3, Step: 280 / 750 Loss: 0.3377\n",
      "Epoch: 2 / 3, Step: 281 / 750 Loss: 0.3208\n",
      "Epoch: 2 / 3, Step: 282 / 750 Loss: 0.2643\n",
      "Epoch: 2 / 3, Step: 283 / 750 Loss: 0.5414\n",
      "Epoch: 2 / 3, Step: 284 / 750 Loss: 0.1246\n",
      "Epoch: 2 / 3, Step: 285 / 750 Loss: 0.3247\n",
      "Epoch: 2 / 3, Step: 286 / 750 Loss: 0.1382\n",
      "Epoch: 2 / 3, Step: 287 / 750 Loss: 0.1872\n",
      "Epoch: 2 / 3, Step: 288 / 750 Loss: 0.1556\n",
      "Epoch: 2 / 3, Step: 289 / 750 Loss: 0.2528\n",
      "Epoch: 2 / 3, Step: 290 / 750 Loss: 0.3685\n",
      "Epoch: 2 / 3, Step: 291 / 750 Loss: 0.3884\n",
      "Epoch: 2 / 3, Step: 292 / 750 Loss: 0.2675\n",
      "Epoch: 2 / 3, Step: 293 / 750 Loss: 0.1629\n",
      "Epoch: 2 / 3, Step: 294 / 750 Loss: 0.2922\n",
      "Epoch: 2 / 3, Step: 295 / 750 Loss: 0.1052\n",
      "Epoch: 2 / 3, Step: 296 / 750 Loss: 0.3077\n",
      "Epoch: 2 / 3, Step: 297 / 750 Loss: 0.3770\n",
      "Epoch: 2 / 3, Step: 298 / 750 Loss: 0.1390\n",
      "Epoch: 2 / 3, Step: 299 / 750 Loss: 0.1588\n",
      "Epoch: 2 / 3, Step: 300 / 750 Loss: 0.2676\n",
      "Epoch: 2 / 3, Step: 301 / 750 Loss: 0.0843\n",
      "Epoch: 2 / 3, Step: 302 / 750 Loss: 0.1212\n",
      "Epoch: 2 / 3, Step: 303 / 750 Loss: 0.1926\n",
      "Epoch: 2 / 3, Step: 304 / 750 Loss: 0.2229\n",
      "Epoch: 2 / 3, Step: 305 / 750 Loss: 0.1892\n",
      "Epoch: 2 / 3, Step: 306 / 750 Loss: 0.3600\n",
      "Epoch: 2 / 3, Step: 307 / 750 Loss: 0.2776\n",
      "Epoch: 2 / 3, Step: 308 / 750 Loss: 0.0971\n",
      "Epoch: 2 / 3, Step: 309 / 750 Loss: 0.1592\n",
      "Epoch: 2 / 3, Step: 310 / 750 Loss: 0.3016\n",
      "Epoch: 2 / 3, Step: 311 / 750 Loss: 0.3196\n",
      "Epoch: 2 / 3, Step: 312 / 750 Loss: 0.3604\n",
      "Epoch: 2 / 3, Step: 313 / 750 Loss: 0.5045\n",
      "Epoch: 2 / 3, Step: 314 / 750 Loss: 0.1463\n",
      "Epoch: 2 / 3, Step: 315 / 750 Loss: 0.3350\n",
      "Epoch: 2 / 3, Step: 316 / 750 Loss: 0.3086\n",
      "Epoch: 2 / 3, Step: 317 / 750 Loss: 0.2887\n",
      "Epoch: 2 / 3, Step: 318 / 750 Loss: 0.2132\n",
      "Epoch: 2 / 3, Step: 319 / 750 Loss: 0.4664\n",
      "Epoch: 2 / 3, Step: 320 / 750 Loss: 0.2977\n",
      "Epoch: 2 / 3, Step: 321 / 750 Loss: 0.2458\n",
      "Epoch: 2 / 3, Step: 322 / 750 Loss: 0.1316\n",
      "Epoch: 2 / 3, Step: 323 / 750 Loss: 0.1750\n",
      "Epoch: 2 / 3, Step: 324 / 750 Loss: 0.4967\n",
      "Epoch: 2 / 3, Step: 325 / 750 Loss: 0.3956\n",
      "Epoch: 2 / 3, Step: 326 / 750 Loss: 0.1986\n",
      "Epoch: 2 / 3, Step: 327 / 750 Loss: 0.3809\n",
      "Epoch: 2 / 3, Step: 328 / 750 Loss: 0.1816\n",
      "Epoch: 2 / 3, Step: 329 / 750 Loss: 0.1930\n",
      "Epoch: 2 / 3, Step: 330 / 750 Loss: 0.2521\n",
      "Epoch: 2 / 3, Step: 331 / 750 Loss: 0.0633\n",
      "Epoch: 2 / 3, Step: 332 / 750 Loss: 0.3209\n",
      "Epoch: 2 / 3, Step: 333 / 750 Loss: 0.1626\n",
      "Epoch: 2 / 3, Step: 334 / 750 Loss: 0.2687\n",
      "Epoch: 2 / 3, Step: 335 / 750 Loss: 0.1431\n",
      "Epoch: 2 / 3, Step: 336 / 750 Loss: 0.1597\n",
      "Epoch: 2 / 3, Step: 337 / 750 Loss: 0.1926\n",
      "Epoch: 2 / 3, Step: 338 / 750 Loss: 0.1481\n",
      "Epoch: 2 / 3, Step: 339 / 750 Loss: 0.2003\n",
      "Epoch: 2 / 3, Step: 340 / 750 Loss: 0.1883\n",
      "Epoch: 2 / 3, Step: 341 / 750 Loss: 0.2927\n",
      "Epoch: 2 / 3, Step: 342 / 750 Loss: 0.2202\n",
      "Epoch: 2 / 3, Step: 343 / 750 Loss: 0.2027\n",
      "Epoch: 2 / 3, Step: 344 / 750 Loss: 0.3833\n",
      "Epoch: 2 / 3, Step: 345 / 750 Loss: 0.3355\n",
      "Epoch: 2 / 3, Step: 346 / 750 Loss: 0.1460\n",
      "Epoch: 2 / 3, Step: 347 / 750 Loss: 0.2359\n",
      "Epoch: 2 / 3, Step: 348 / 750 Loss: 0.1779\n",
      "Epoch: 2 / 3, Step: 349 / 750 Loss: 0.1271\n",
      "Epoch: 2 / 3, Step: 350 / 750 Loss: 0.4031\n",
      "Epoch: 2 / 3, Step: 351 / 750 Loss: 0.5660\n",
      "Epoch: 2 / 3, Step: 352 / 750 Loss: 0.2534\n",
      "Epoch: 2 / 3, Step: 353 / 750 Loss: 0.1185\n",
      "Epoch: 2 / 3, Step: 354 / 750 Loss: 0.2939\n",
      "Epoch: 2 / 3, Step: 355 / 750 Loss: 0.3491\n",
      "Epoch: 2 / 3, Step: 356 / 750 Loss: 0.1600\n",
      "Epoch: 2 / 3, Step: 357 / 750 Loss: 0.2325\n",
      "Epoch: 2 / 3, Step: 358 / 750 Loss: 0.3031\n",
      "Epoch: 2 / 3, Step: 359 / 750 Loss: 0.2701\n",
      "Epoch: 2 / 3, Step: 360 / 750 Loss: 0.2469\n",
      "Epoch: 2 / 3, Step: 361 / 750 Loss: 0.1559\n",
      "Epoch: 2 / 3, Step: 362 / 750 Loss: 0.1564\n",
      "Epoch: 2 / 3, Step: 363 / 750 Loss: 0.2128\n",
      "Epoch: 2 / 3, Step: 364 / 750 Loss: 0.1765\n",
      "Epoch: 2 / 3, Step: 365 / 750 Loss: 0.5145\n",
      "Epoch: 2 / 3, Step: 366 / 750 Loss: 0.3525\n",
      "Epoch: 2 / 3, Step: 367 / 750 Loss: 0.2057\n",
      "Epoch: 2 / 3, Step: 368 / 750 Loss: 0.1736\n",
      "Epoch: 2 / 3, Step: 369 / 750 Loss: 0.0945\n",
      "Epoch: 2 / 3, Step: 370 / 750 Loss: 0.3444\n",
      "Epoch: 2 / 3, Step: 371 / 750 Loss: 0.3106\n",
      "Epoch: 2 / 3, Step: 372 / 750 Loss: 0.0596\n",
      "Epoch: 2 / 3, Step: 373 / 750 Loss: 0.5224\n",
      "Epoch: 2 / 3, Step: 374 / 750 Loss: 0.2067\n",
      "Epoch: 2 / 3, Step: 375 / 750 Loss: 0.2608\n",
      "Epoch: 2 / 3, Step: 376 / 750 Loss: 0.4989\n",
      "Epoch: 2 / 3, Step: 377 / 750 Loss: 0.1355\n",
      "Epoch: 2 / 3, Step: 378 / 750 Loss: 0.0636\n",
      "Epoch: 2 / 3, Step: 379 / 750 Loss: 0.2532\n",
      "Epoch: 2 / 3, Step: 380 / 750 Loss: 0.0868\n",
      "Epoch: 2 / 3, Step: 381 / 750 Loss: 0.2230\n",
      "Epoch: 2 / 3, Step: 382 / 750 Loss: 0.3329\n",
      "Epoch: 2 / 3, Step: 383 / 750 Loss: 0.1048\n",
      "Epoch: 2 / 3, Step: 384 / 750 Loss: 0.0756\n",
      "Epoch: 2 / 3, Step: 385 / 750 Loss: 0.3749\n",
      "Epoch: 2 / 3, Step: 386 / 750 Loss: 0.1259\n",
      "Epoch: 2 / 3, Step: 387 / 750 Loss: 0.1671\n",
      "Epoch: 2 / 3, Step: 388 / 750 Loss: 0.4470\n",
      "Epoch: 2 / 3, Step: 389 / 750 Loss: 0.2314\n",
      "Epoch: 2 / 3, Step: 390 / 750 Loss: 0.1324\n",
      "Epoch: 2 / 3, Step: 391 / 750 Loss: 0.2263\n",
      "Epoch: 2 / 3, Step: 392 / 750 Loss: 0.2069\n",
      "Epoch: 2 / 3, Step: 393 / 750 Loss: 0.2748\n",
      "Epoch: 2 / 3, Step: 394 / 750 Loss: 0.3179\n",
      "Epoch: 2 / 3, Step: 395 / 750 Loss: 0.1644\n",
      "Epoch: 2 / 3, Step: 396 / 750 Loss: 0.2162\n",
      "Epoch: 2 / 3, Step: 397 / 750 Loss: 0.1094\n",
      "Epoch: 2 / 3, Step: 398 / 750 Loss: 0.1149\n",
      "Epoch: 2 / 3, Step: 399 / 750 Loss: 0.1285\n",
      "Epoch: 2 / 3, Step: 400 / 750 Loss: 0.4290\n",
      "Epoch: 2 / 3, Step: 401 / 750 Loss: 0.1674\n",
      "Epoch: 2 / 3, Step: 402 / 750 Loss: 0.3152\n",
      "Epoch: 2 / 3, Step: 403 / 750 Loss: 0.5887\n",
      "Epoch: 2 / 3, Step: 404 / 750 Loss: 0.3380\n",
      "Epoch: 2 / 3, Step: 405 / 750 Loss: 0.1808\n",
      "Epoch: 2 / 3, Step: 406 / 750 Loss: 0.2349\n",
      "Epoch: 2 / 3, Step: 407 / 750 Loss: 0.2023\n",
      "Epoch: 2 / 3, Step: 408 / 750 Loss: 0.2083\n",
      "Epoch: 2 / 3, Step: 409 / 750 Loss: 0.0628\n",
      "Epoch: 2 / 3, Step: 410 / 750 Loss: 0.3091\n",
      "Epoch: 2 / 3, Step: 411 / 750 Loss: 0.2973\n",
      "Epoch: 2 / 3, Step: 412 / 750 Loss: 0.2047\n",
      "Epoch: 2 / 3, Step: 413 / 750 Loss: 0.1542\n",
      "Epoch: 2 / 3, Step: 414 / 750 Loss: 0.5218\n",
      "Epoch: 2 / 3, Step: 415 / 750 Loss: 0.2482\n",
      "Epoch: 2 / 3, Step: 416 / 750 Loss: 0.1804\n",
      "Epoch: 2 / 3, Step: 417 / 750 Loss: 0.1200\n",
      "Epoch: 2 / 3, Step: 418 / 750 Loss: 0.2123\n",
      "Epoch: 2 / 3, Step: 419 / 750 Loss: 0.2567\n",
      "Epoch: 2 / 3, Step: 420 / 750 Loss: 0.2949\n",
      "Epoch: 2 / 3, Step: 421 / 750 Loss: 0.1191\n",
      "Epoch: 2 / 3, Step: 422 / 750 Loss: 0.0770\n",
      "Epoch: 2 / 3, Step: 423 / 750 Loss: 0.2202\n",
      "Epoch: 2 / 3, Step: 424 / 750 Loss: 0.3692\n",
      "Epoch: 2 / 3, Step: 425 / 750 Loss: 0.2765\n",
      "Epoch: 2 / 3, Step: 426 / 750 Loss: 0.1007\n",
      "Epoch: 2 / 3, Step: 427 / 750 Loss: 0.1009\n",
      "Epoch: 2 / 3, Step: 428 / 750 Loss: 0.4421\n",
      "Epoch: 2 / 3, Step: 429 / 750 Loss: 0.2450\n",
      "Epoch: 2 / 3, Step: 430 / 750 Loss: 0.4343\n",
      "Epoch: 2 / 3, Step: 431 / 750 Loss: 0.1125\n",
      "Epoch: 2 / 3, Step: 432 / 750 Loss: 0.1360\n",
      "Epoch: 2 / 3, Step: 433 / 750 Loss: 0.3533\n",
      "Epoch: 2 / 3, Step: 434 / 750 Loss: 0.1390\n",
      "Epoch: 2 / 3, Step: 435 / 750 Loss: 0.1703\n",
      "Epoch: 2 / 3, Step: 436 / 750 Loss: 0.1939\n",
      "Epoch: 2 / 3, Step: 437 / 750 Loss: 0.2661\n",
      "Epoch: 2 / 3, Step: 438 / 750 Loss: 0.1902\n",
      "Epoch: 2 / 3, Step: 439 / 750 Loss: 0.3102\n",
      "Epoch: 2 / 3, Step: 440 / 750 Loss: 0.2754\n",
      "Epoch: 2 / 3, Step: 441 / 750 Loss: 0.1304\n",
      "Epoch: 2 / 3, Step: 442 / 750 Loss: 0.1249\n",
      "Epoch: 2 / 3, Step: 443 / 750 Loss: 0.2923\n",
      "Epoch: 2 / 3, Step: 444 / 750 Loss: 0.2026\n",
      "Epoch: 2 / 3, Step: 445 / 750 Loss: 0.2474\n",
      "Epoch: 2 / 3, Step: 446 / 750 Loss: 0.2927\n",
      "Epoch: 2 / 3, Step: 447 / 750 Loss: 0.1931\n",
      "Epoch: 2 / 3, Step: 448 / 750 Loss: 0.2073\n",
      "Epoch: 2 / 3, Step: 449 / 750 Loss: 0.3238\n",
      "Epoch: 2 / 3, Step: 450 / 750 Loss: 0.1537\n",
      "Epoch: 2 / 3, Step: 451 / 750 Loss: 0.0945\n",
      "Epoch: 2 / 3, Step: 452 / 750 Loss: 0.2046\n",
      "Epoch: 2 / 3, Step: 453 / 750 Loss: 0.1536\n",
      "Epoch: 2 / 3, Step: 454 / 750 Loss: 0.2909\n",
      "Epoch: 2 / 3, Step: 455 / 750 Loss: 0.1401\n",
      "Epoch: 2 / 3, Step: 456 / 750 Loss: 0.3827\n",
      "Epoch: 2 / 3, Step: 457 / 750 Loss: 0.1486\n",
      "Epoch: 2 / 3, Step: 458 / 750 Loss: 0.1953\n",
      "Epoch: 2 / 3, Step: 459 / 750 Loss: 0.1219\n",
      "Epoch: 2 / 3, Step: 460 / 750 Loss: 0.1457\n",
      "Epoch: 2 / 3, Step: 461 / 750 Loss: 0.1362\n",
      "Epoch: 2 / 3, Step: 462 / 750 Loss: 0.1476\n",
      "Epoch: 2 / 3, Step: 463 / 750 Loss: 0.1245\n",
      "Epoch: 2 / 3, Step: 464 / 750 Loss: 0.1535\n",
      "Epoch: 2 / 3, Step: 465 / 750 Loss: 0.3585\n",
      "Epoch: 2 / 3, Step: 466 / 750 Loss: 0.1278\n",
      "Epoch: 2 / 3, Step: 467 / 750 Loss: 0.1086\n",
      "Epoch: 2 / 3, Step: 468 / 750 Loss: 0.2764\n",
      "Epoch: 2 / 3, Step: 469 / 750 Loss: 0.1037\n",
      "Epoch: 2 / 3, Step: 470 / 750 Loss: 0.1183\n",
      "Epoch: 2 / 3, Step: 471 / 750 Loss: 0.1990\n",
      "Epoch: 2 / 3, Step: 472 / 750 Loss: 0.2515\n",
      "Epoch: 2 / 3, Step: 473 / 750 Loss: 0.0626\n",
      "Epoch: 2 / 3, Step: 474 / 750 Loss: 0.4461\n",
      "Epoch: 2 / 3, Step: 475 / 750 Loss: 0.0861\n",
      "Epoch: 2 / 3, Step: 476 / 750 Loss: 0.2664\n",
      "Epoch: 2 / 3, Step: 477 / 750 Loss: 0.2819\n",
      "Epoch: 2 / 3, Step: 478 / 750 Loss: 0.1823\n",
      "Epoch: 2 / 3, Step: 479 / 750 Loss: 0.1169\n",
      "Epoch: 2 / 3, Step: 480 / 750 Loss: 0.2899\n",
      "Epoch: 2 / 3, Step: 481 / 750 Loss: 0.4508\n",
      "Epoch: 2 / 3, Step: 482 / 750 Loss: 0.4080\n",
      "Epoch: 2 / 3, Step: 483 / 750 Loss: 0.3538\n",
      "Epoch: 2 / 3, Step: 484 / 750 Loss: 0.2796\n",
      "Epoch: 2 / 3, Step: 485 / 750 Loss: 0.1330\n",
      "Epoch: 2 / 3, Step: 486 / 750 Loss: 0.1502\n",
      "Epoch: 2 / 3, Step: 487 / 750 Loss: 0.1967\n",
      "Epoch: 2 / 3, Step: 488 / 750 Loss: 0.2267\n",
      "Epoch: 2 / 3, Step: 489 / 750 Loss: 0.3300\n",
      "Epoch: 2 / 3, Step: 490 / 750 Loss: 0.3236\n",
      "Epoch: 2 / 3, Step: 491 / 750 Loss: 0.1366\n",
      "Epoch: 2 / 3, Step: 492 / 750 Loss: 0.2841\n",
      "Epoch: 2 / 3, Step: 493 / 750 Loss: 0.1709\n",
      "Epoch: 2 / 3, Step: 494 / 750 Loss: 0.3593\n",
      "Epoch: 2 / 3, Step: 495 / 750 Loss: 0.2606\n",
      "Epoch: 2 / 3, Step: 496 / 750 Loss: 0.1415\n",
      "Epoch: 2 / 3, Step: 497 / 750 Loss: 0.2034\n",
      "Epoch: 2 / 3, Step: 498 / 750 Loss: 0.3088\n",
      "Epoch: 2 / 3, Step: 499 / 750 Loss: 0.2433\n",
      "Epoch: 2 / 3, Step: 500 / 750 Loss: 0.0842\n",
      "Epoch: 2 / 3, Step: 501 / 750 Loss: 0.1251\n",
      "Epoch: 2 / 3, Step: 502 / 750 Loss: 0.0543\n",
      "Epoch: 2 / 3, Step: 503 / 750 Loss: 0.1754\n",
      "Epoch: 2 / 3, Step: 504 / 750 Loss: 0.2061\n",
      "Epoch: 2 / 3, Step: 505 / 750 Loss: 0.3090\n",
      "Epoch: 2 / 3, Step: 506 / 750 Loss: 0.1123\n",
      "Epoch: 2 / 3, Step: 507 / 750 Loss: 0.1266\n",
      "Epoch: 2 / 3, Step: 508 / 750 Loss: 0.0584\n",
      "Epoch: 2 / 3, Step: 509 / 750 Loss: 0.1922\n",
      "Epoch: 2 / 3, Step: 510 / 750 Loss: 0.0790\n",
      "Epoch: 2 / 3, Step: 511 / 750 Loss: 0.4033\n",
      "Epoch: 2 / 3, Step: 512 / 750 Loss: 0.2659\n",
      "Epoch: 2 / 3, Step: 513 / 750 Loss: 0.1575\n",
      "Epoch: 2 / 3, Step: 514 / 750 Loss: 0.1698\n",
      "Epoch: 2 / 3, Step: 515 / 750 Loss: 0.6068\n",
      "Epoch: 2 / 3, Step: 516 / 750 Loss: 0.0960\n",
      "Epoch: 2 / 3, Step: 517 / 750 Loss: 0.1018\n",
      "Epoch: 2 / 3, Step: 518 / 750 Loss: 0.1880\n",
      "Epoch: 2 / 3, Step: 519 / 750 Loss: 0.2235\n",
      "Epoch: 2 / 3, Step: 520 / 750 Loss: 0.4026\n",
      "Epoch: 2 / 3, Step: 521 / 750 Loss: 0.1916\n",
      "Epoch: 2 / 3, Step: 522 / 750 Loss: 0.2646\n",
      "Epoch: 2 / 3, Step: 523 / 750 Loss: 0.0892\n",
      "Epoch: 2 / 3, Step: 524 / 750 Loss: 0.1126\n",
      "Epoch: 2 / 3, Step: 525 / 750 Loss: 0.3308\n",
      "Epoch: 2 / 3, Step: 526 / 750 Loss: 0.3723\n",
      "Epoch: 2 / 3, Step: 527 / 750 Loss: 0.2583\n",
      "Epoch: 2 / 3, Step: 528 / 750 Loss: 0.3898\n",
      "Epoch: 2 / 3, Step: 529 / 750 Loss: 0.3483\n",
      "Epoch: 2 / 3, Step: 530 / 750 Loss: 0.4057\n",
      "Epoch: 2 / 3, Step: 531 / 750 Loss: 0.1198\n",
      "Epoch: 2 / 3, Step: 532 / 750 Loss: 0.3701\n",
      "Epoch: 2 / 3, Step: 533 / 750 Loss: 0.2735\n",
      "Epoch: 2 / 3, Step: 534 / 750 Loss: 0.0878\n",
      "Epoch: 2 / 3, Step: 535 / 750 Loss: 0.2984\n",
      "Epoch: 2 / 3, Step: 536 / 750 Loss: 0.1685\n",
      "Epoch: 2 / 3, Step: 537 / 750 Loss: 0.1598\n",
      "Epoch: 2 / 3, Step: 538 / 750 Loss: 0.2072\n",
      "Epoch: 2 / 3, Step: 539 / 750 Loss: 0.2087\n",
      "Epoch: 2 / 3, Step: 540 / 750 Loss: 0.3196\n",
      "Epoch: 2 / 3, Step: 541 / 750 Loss: 0.1191\n",
      "Epoch: 2 / 3, Step: 542 / 750 Loss: 0.1631\n",
      "Epoch: 2 / 3, Step: 543 / 750 Loss: 0.1132\n",
      "Epoch: 2 / 3, Step: 544 / 750 Loss: 0.0781\n",
      "Epoch: 2 / 3, Step: 545 / 750 Loss: 0.4846\n",
      "Epoch: 2 / 3, Step: 546 / 750 Loss: 0.1347\n",
      "Epoch: 2 / 3, Step: 547 / 750 Loss: 0.2076\n",
      "Epoch: 2 / 3, Step: 548 / 750 Loss: 0.2605\n",
      "Epoch: 2 / 3, Step: 549 / 750 Loss: 0.1655\n",
      "Epoch: 2 / 3, Step: 550 / 750 Loss: 0.3201\n",
      "Epoch: 2 / 3, Step: 551 / 750 Loss: 0.2804\n",
      "Epoch: 2 / 3, Step: 552 / 750 Loss: 0.1462\n",
      "Epoch: 2 / 3, Step: 553 / 750 Loss: 0.6305\n",
      "Epoch: 2 / 3, Step: 554 / 750 Loss: 0.1433\n",
      "Epoch: 2 / 3, Step: 555 / 750 Loss: 0.1753\n",
      "Epoch: 2 / 3, Step: 556 / 750 Loss: 0.1554\n",
      "Epoch: 2 / 3, Step: 557 / 750 Loss: 0.1068\n",
      "Epoch: 2 / 3, Step: 558 / 750 Loss: 0.2801\n",
      "Epoch: 2 / 3, Step: 559 / 750 Loss: 0.1957\n",
      "Epoch: 2 / 3, Step: 560 / 750 Loss: 0.1448\n",
      "Epoch: 2 / 3, Step: 561 / 750 Loss: 0.1932\n",
      "Epoch: 2 / 3, Step: 562 / 750 Loss: 0.2040\n",
      "Epoch: 2 / 3, Step: 563 / 750 Loss: 0.1760\n",
      "Epoch: 2 / 3, Step: 564 / 750 Loss: 0.5197\n",
      "Epoch: 2 / 3, Step: 565 / 750 Loss: 0.1102\n",
      "Epoch: 2 / 3, Step: 566 / 750 Loss: 0.1875\n",
      "Epoch: 2 / 3, Step: 567 / 750 Loss: 0.3936\n",
      "Epoch: 2 / 3, Step: 568 / 750 Loss: 0.3461\n",
      "Epoch: 2 / 3, Step: 569 / 750 Loss: 0.3120\n",
      "Epoch: 2 / 3, Step: 570 / 750 Loss: 0.3224\n",
      "Epoch: 2 / 3, Step: 571 / 750 Loss: 0.2779\n",
      "Epoch: 2 / 3, Step: 572 / 750 Loss: 0.1468\n",
      "Epoch: 2 / 3, Step: 573 / 750 Loss: 0.0856\n",
      "Epoch: 2 / 3, Step: 574 / 750 Loss: 0.1500\n",
      "Epoch: 2 / 3, Step: 575 / 750 Loss: 0.2470\n",
      "Epoch: 2 / 3, Step: 576 / 750 Loss: 0.1424\n",
      "Epoch: 2 / 3, Step: 577 / 750 Loss: 0.1991\n",
      "Epoch: 2 / 3, Step: 578 / 750 Loss: 0.2090\n",
      "Epoch: 2 / 3, Step: 579 / 750 Loss: 0.2579\n",
      "Epoch: 2 / 3, Step: 580 / 750 Loss: 0.3487\n",
      "Epoch: 2 / 3, Step: 581 / 750 Loss: 0.1258\n",
      "Epoch: 2 / 3, Step: 582 / 750 Loss: 0.1754\n",
      "Epoch: 2 / 3, Step: 583 / 750 Loss: 0.1595\n",
      "Epoch: 2 / 3, Step: 584 / 750 Loss: 0.2025\n",
      "Epoch: 2 / 3, Step: 585 / 750 Loss: 0.3022\n",
      "Epoch: 2 / 3, Step: 586 / 750 Loss: 0.3969\n",
      "Epoch: 2 / 3, Step: 587 / 750 Loss: 0.1320\n",
      "Epoch: 2 / 3, Step: 588 / 750 Loss: 0.3058\n",
      "Epoch: 2 / 3, Step: 589 / 750 Loss: 0.1295\n",
      "Epoch: 2 / 3, Step: 590 / 750 Loss: 0.1743\n",
      "Epoch: 2 / 3, Step: 591 / 750 Loss: 0.2141\n",
      "Epoch: 2 / 3, Step: 592 / 750 Loss: 0.2880\n",
      "Epoch: 2 / 3, Step: 593 / 750 Loss: 0.1155\n",
      "Epoch: 2 / 3, Step: 594 / 750 Loss: 0.3292\n",
      "Epoch: 2 / 3, Step: 595 / 750 Loss: 0.1448\n",
      "Epoch: 2 / 3, Step: 596 / 750 Loss: 0.0702\n",
      "Epoch: 2 / 3, Step: 597 / 750 Loss: 0.2773\n",
      "Epoch: 2 / 3, Step: 598 / 750 Loss: 0.1003\n",
      "Epoch: 2 / 3, Step: 599 / 750 Loss: 0.2014\n",
      "Epoch: 2 / 3, Step: 600 / 750 Loss: 0.1684\n",
      "Epoch: 2 / 3, Step: 601 / 750 Loss: 0.1102\n",
      "Epoch: 2 / 3, Step: 602 / 750 Loss: 0.0897\n",
      "Epoch: 2 / 3, Step: 603 / 750 Loss: 0.1998\n",
      "Epoch: 2 / 3, Step: 604 / 750 Loss: 0.1396\n",
      "Epoch: 2 / 3, Step: 605 / 750 Loss: 0.3368\n",
      "Epoch: 2 / 3, Step: 606 / 750 Loss: 0.1388\n",
      "Epoch: 2 / 3, Step: 607 / 750 Loss: 0.1667\n",
      "Epoch: 2 / 3, Step: 608 / 750 Loss: 0.2490\n",
      "Epoch: 2 / 3, Step: 609 / 750 Loss: 0.5726\n",
      "Epoch: 2 / 3, Step: 610 / 750 Loss: 0.0812\n",
      "Epoch: 2 / 3, Step: 611 / 750 Loss: 0.3574\n",
      "Epoch: 2 / 3, Step: 612 / 750 Loss: 0.2496\n",
      "Epoch: 2 / 3, Step: 613 / 750 Loss: 0.3337\n",
      "Epoch: 2 / 3, Step: 614 / 750 Loss: 0.1011\n",
      "Epoch: 2 / 3, Step: 615 / 750 Loss: 0.1619\n",
      "Epoch: 2 / 3, Step: 616 / 750 Loss: 0.0971\n",
      "Epoch: 2 / 3, Step: 617 / 750 Loss: 0.4592\n",
      "Epoch: 2 / 3, Step: 618 / 750 Loss: 0.3133\n",
      "Epoch: 2 / 3, Step: 619 / 750 Loss: 0.1430\n",
      "Epoch: 2 / 3, Step: 620 / 750 Loss: 0.2969\n",
      "Epoch: 2 / 3, Step: 621 / 750 Loss: 0.3014\n",
      "Epoch: 2 / 3, Step: 622 / 750 Loss: 0.1651\n",
      "Epoch: 2 / 3, Step: 623 / 750 Loss: 0.0616\n",
      "Epoch: 2 / 3, Step: 624 / 750 Loss: 0.3064\n",
      "Epoch: 2 / 3, Step: 625 / 750 Loss: 0.2000\n",
      "Epoch: 2 / 3, Step: 626 / 750 Loss: 0.2882\n",
      "Epoch: 2 / 3, Step: 627 / 750 Loss: 0.3049\n",
      "Epoch: 2 / 3, Step: 628 / 750 Loss: 0.2259\n",
      "Epoch: 2 / 3, Step: 629 / 750 Loss: 0.0925\n",
      "Epoch: 2 / 3, Step: 630 / 750 Loss: 0.1555\n",
      "Epoch: 2 / 3, Step: 631 / 750 Loss: 0.3045\n",
      "Epoch: 2 / 3, Step: 632 / 750 Loss: 0.1660\n",
      "Epoch: 2 / 3, Step: 633 / 750 Loss: 0.4124\n",
      "Epoch: 2 / 3, Step: 634 / 750 Loss: 0.3427\n",
      "Epoch: 2 / 3, Step: 635 / 750 Loss: 0.0986\n",
      "Epoch: 2 / 3, Step: 636 / 750 Loss: 0.0936\n",
      "Epoch: 2 / 3, Step: 637 / 750 Loss: 0.2028\n",
      "Epoch: 2 / 3, Step: 638 / 750 Loss: 0.0777\n",
      "Epoch: 2 / 3, Step: 639 / 750 Loss: 0.0986\n",
      "Epoch: 2 / 3, Step: 640 / 750 Loss: 0.1855\n",
      "Epoch: 2 / 3, Step: 641 / 750 Loss: 0.1619\n",
      "Epoch: 2 / 3, Step: 642 / 750 Loss: 0.2284\n",
      "Epoch: 2 / 3, Step: 643 / 750 Loss: 0.3139\n",
      "Epoch: 2 / 3, Step: 644 / 750 Loss: 0.2011\n",
      "Epoch: 2 / 3, Step: 645 / 750 Loss: 0.2011\n",
      "Epoch: 2 / 3, Step: 646 / 750 Loss: 0.0814\n",
      "Epoch: 2 / 3, Step: 647 / 750 Loss: 0.4277\n",
      "Epoch: 2 / 3, Step: 648 / 750 Loss: 0.3239\n",
      "Epoch: 2 / 3, Step: 649 / 750 Loss: 0.2095\n",
      "Epoch: 2 / 3, Step: 650 / 750 Loss: 0.1434\n",
      "Epoch: 2 / 3, Step: 651 / 750 Loss: 0.1921\n",
      "Epoch: 2 / 3, Step: 652 / 750 Loss: 0.1415\n",
      "Epoch: 2 / 3, Step: 653 / 750 Loss: 0.2765\n",
      "Epoch: 2 / 3, Step: 654 / 750 Loss: 0.0971\n",
      "Epoch: 2 / 3, Step: 655 / 750 Loss: 0.2640\n",
      "Epoch: 2 / 3, Step: 656 / 750 Loss: 0.2468\n",
      "Epoch: 2 / 3, Step: 657 / 750 Loss: 0.4146\n",
      "Epoch: 2 / 3, Step: 658 / 750 Loss: 0.1249\n",
      "Epoch: 2 / 3, Step: 659 / 750 Loss: 0.3452\n",
      "Epoch: 2 / 3, Step: 660 / 750 Loss: 0.1274\n",
      "Epoch: 2 / 3, Step: 661 / 750 Loss: 0.1760\n",
      "Epoch: 2 / 3, Step: 662 / 750 Loss: 0.0877\n",
      "Epoch: 2 / 3, Step: 663 / 750 Loss: 0.0940\n",
      "Epoch: 2 / 3, Step: 664 / 750 Loss: 0.1222\n",
      "Epoch: 2 / 3, Step: 665 / 750 Loss: 0.0742\n",
      "Epoch: 2 / 3, Step: 666 / 750 Loss: 0.1205\n",
      "Epoch: 2 / 3, Step: 667 / 750 Loss: 0.2392\n",
      "Epoch: 2 / 3, Step: 668 / 750 Loss: 0.1264\n",
      "Epoch: 2 / 3, Step: 669 / 750 Loss: 0.2191\n",
      "Epoch: 2 / 3, Step: 670 / 750 Loss: 0.1054\n",
      "Epoch: 2 / 3, Step: 671 / 750 Loss: 0.5441\n",
      "Epoch: 2 / 3, Step: 672 / 750 Loss: 0.3435\n",
      "Epoch: 2 / 3, Step: 673 / 750 Loss: 0.2564\n",
      "Epoch: 2 / 3, Step: 674 / 750 Loss: 0.0968\n",
      "Epoch: 2 / 3, Step: 675 / 750 Loss: 0.0963\n",
      "Epoch: 2 / 3, Step: 676 / 750 Loss: 0.1848\n",
      "Epoch: 2 / 3, Step: 677 / 750 Loss: 0.1406\n",
      "Epoch: 2 / 3, Step: 678 / 750 Loss: 0.1404\n",
      "Epoch: 2 / 3, Step: 679 / 750 Loss: 0.3155\n",
      "Epoch: 2 / 3, Step: 680 / 750 Loss: 0.4427\n",
      "Epoch: 2 / 3, Step: 681 / 750 Loss: 0.1559\n",
      "Epoch: 2 / 3, Step: 682 / 750 Loss: 0.2815\n",
      "Epoch: 2 / 3, Step: 683 / 750 Loss: 0.3765\n",
      "Epoch: 2 / 3, Step: 684 / 750 Loss: 0.2335\n",
      "Epoch: 2 / 3, Step: 685 / 750 Loss: 0.0560\n",
      "Epoch: 2 / 3, Step: 686 / 750 Loss: 0.2222\n",
      "Epoch: 2 / 3, Step: 687 / 750 Loss: 0.2877\n",
      "Epoch: 2 / 3, Step: 688 / 750 Loss: 0.3126\n",
      "Epoch: 2 / 3, Step: 689 / 750 Loss: 0.2589\n",
      "Epoch: 2 / 3, Step: 690 / 750 Loss: 0.1880\n",
      "Epoch: 2 / 3, Step: 691 / 750 Loss: 0.0982\n",
      "Epoch: 2 / 3, Step: 692 / 750 Loss: 0.0696\n",
      "Epoch: 2 / 3, Step: 693 / 750 Loss: 0.1064\n",
      "Epoch: 2 / 3, Step: 694 / 750 Loss: 0.1461\n",
      "Epoch: 2 / 3, Step: 695 / 750 Loss: 0.2096\n",
      "Epoch: 2 / 3, Step: 696 / 750 Loss: 0.1707\n",
      "Epoch: 2 / 3, Step: 697 / 750 Loss: 0.2359\n",
      "Epoch: 2 / 3, Step: 698 / 750 Loss: 0.1064\n",
      "Epoch: 2 / 3, Step: 699 / 750 Loss: 0.3190\n",
      "Epoch: 2 / 3, Step: 700 / 750 Loss: 0.0942\n",
      "Epoch: 2 / 3, Step: 701 / 750 Loss: 0.2861\n",
      "Epoch: 2 / 3, Step: 702 / 750 Loss: 0.2440\n",
      "Epoch: 2 / 3, Step: 703 / 750 Loss: 0.1796\n",
      "Epoch: 2 / 3, Step: 704 / 750 Loss: 0.2164\n",
      "Epoch: 2 / 3, Step: 705 / 750 Loss: 0.1762\n",
      "Epoch: 2 / 3, Step: 706 / 750 Loss: 0.5006\n",
      "Epoch: 2 / 3, Step: 707 / 750 Loss: 0.4115\n",
      "Epoch: 2 / 3, Step: 708 / 750 Loss: 0.1006\n",
      "Epoch: 2 / 3, Step: 709 / 750 Loss: 0.0614\n",
      "Epoch: 2 / 3, Step: 710 / 750 Loss: 0.1180\n",
      "Epoch: 2 / 3, Step: 711 / 750 Loss: 0.3794\n",
      "Epoch: 2 / 3, Step: 712 / 750 Loss: 0.1400\n",
      "Epoch: 2 / 3, Step: 713 / 750 Loss: 0.0618\n",
      "Epoch: 2 / 3, Step: 714 / 750 Loss: 0.2705\n",
      "Epoch: 2 / 3, Step: 715 / 750 Loss: 0.4634\n",
      "Epoch: 2 / 3, Step: 716 / 750 Loss: 0.2720\n",
      "Epoch: 2 / 3, Step: 717 / 750 Loss: 0.2198\n",
      "Epoch: 2 / 3, Step: 718 / 750 Loss: 0.1183\n",
      "Epoch: 2 / 3, Step: 719 / 750 Loss: 0.3420\n",
      "Epoch: 2 / 3, Step: 720 / 750 Loss: 0.2258\n",
      "Epoch: 2 / 3, Step: 721 / 750 Loss: 0.1309\n",
      "Epoch: 2 / 3, Step: 722 / 750 Loss: 0.1151\n",
      "Epoch: 2 / 3, Step: 723 / 750 Loss: 0.0585\n",
      "Epoch: 2 / 3, Step: 724 / 750 Loss: 0.1259\n",
      "Epoch: 2 / 3, Step: 725 / 750 Loss: 0.0735\n",
      "Epoch: 2 / 3, Step: 726 / 750 Loss: 0.2405\n",
      "Epoch: 2 / 3, Step: 727 / 750 Loss: 0.1309\n",
      "Epoch: 2 / 3, Step: 728 / 750 Loss: 0.1734\n",
      "Epoch: 2 / 3, Step: 729 / 750 Loss: 0.1103\n",
      "Epoch: 2 / 3, Step: 730 / 750 Loss: 0.2289\n",
      "Epoch: 2 / 3, Step: 731 / 750 Loss: 0.1266\n",
      "Epoch: 2 / 3, Step: 732 / 750 Loss: 0.2376\n",
      "Epoch: 2 / 3, Step: 733 / 750 Loss: 0.1211\n",
      "Epoch: 2 / 3, Step: 734 / 750 Loss: 0.2334\n",
      "Epoch: 2 / 3, Step: 735 / 750 Loss: 0.2386\n",
      "Epoch: 2 / 3, Step: 736 / 750 Loss: 0.0365\n",
      "Epoch: 2 / 3, Step: 737 / 750 Loss: 0.4225\n",
      "Epoch: 2 / 3, Step: 738 / 750 Loss: 0.2335\n",
      "Epoch: 2 / 3, Step: 739 / 750 Loss: 0.1721\n",
      "Epoch: 2 / 3, Step: 740 / 750 Loss: 0.1993\n",
      "Epoch: 2 / 3, Step: 741 / 750 Loss: 0.4211\n",
      "Epoch: 2 / 3, Step: 742 / 750 Loss: 0.2082\n",
      "Epoch: 2 / 3, Step: 743 / 750 Loss: 0.0719\n",
      "Epoch: 2 / 3, Step: 744 / 750 Loss: 0.1089\n",
      "Epoch: 2 / 3, Step: 745 / 750 Loss: 0.4507\n",
      "Epoch: 2 / 3, Step: 746 / 750 Loss: 0.2066\n",
      "Epoch: 2 / 3, Step: 747 / 750 Loss: 0.1147\n",
      "Epoch: 2 / 3, Step: 748 / 750 Loss: 0.3600\n",
      "Epoch: 2 / 3, Step: 749 / 750 Loss: 0.2795\n",
      "Epoch: 3 / 3, Step: 0 / 750 Loss: 0.1641\n",
      "Epoch: 3 / 3, Step: 1 / 750 Loss: 0.1431\n",
      "Epoch: 3 / 3, Step: 2 / 750 Loss: 0.0959\n",
      "Epoch: 3 / 3, Step: 3 / 750 Loss: 0.3550\n",
      "Epoch: 3 / 3, Step: 4 / 750 Loss: 0.0679\n",
      "Epoch: 3 / 3, Step: 5 / 750 Loss: 0.0575\n",
      "Epoch: 3 / 3, Step: 6 / 750 Loss: 0.2485\n",
      "Epoch: 3 / 3, Step: 7 / 750 Loss: 0.0636\n",
      "Epoch: 3 / 3, Step: 8 / 750 Loss: 0.1176\n",
      "Epoch: 3 / 3, Step: 9 / 750 Loss: 0.1171\n",
      "Epoch: 3 / 3, Step: 10 / 750 Loss: 0.1247\n",
      "Epoch: 3 / 3, Step: 11 / 750 Loss: 0.1171\n",
      "Epoch: 3 / 3, Step: 12 / 750 Loss: 0.2353\n",
      "Epoch: 3 / 3, Step: 13 / 750 Loss: 0.1088\n",
      "Epoch: 3 / 3, Step: 14 / 750 Loss: 0.1219\n",
      "Epoch: 3 / 3, Step: 15 / 750 Loss: 0.0437\n",
      "Epoch: 3 / 3, Step: 16 / 750 Loss: 0.2635\n",
      "Epoch: 3 / 3, Step: 17 / 750 Loss: 0.0625\n",
      "Epoch: 3 / 3, Step: 18 / 750 Loss: 0.0931\n",
      "Epoch: 3 / 3, Step: 19 / 750 Loss: 0.2536\n",
      "Epoch: 3 / 3, Step: 20 / 750 Loss: 0.2651\n",
      "Epoch: 3 / 3, Step: 21 / 750 Loss: 0.0463\n",
      "Epoch: 3 / 3, Step: 22 / 750 Loss: 0.1755\n",
      "Epoch: 3 / 3, Step: 23 / 750 Loss: 0.1834\n",
      "Epoch: 3 / 3, Step: 24 / 750 Loss: 0.0462\n",
      "Epoch: 3 / 3, Step: 25 / 750 Loss: 0.1138\n",
      "Epoch: 3 / 3, Step: 26 / 750 Loss: 0.4022\n",
      "Epoch: 3 / 3, Step: 27 / 750 Loss: 0.2585\n",
      "Epoch: 3 / 3, Step: 28 / 750 Loss: 0.2413\n",
      "Epoch: 3 / 3, Step: 29 / 750 Loss: 0.2971\n",
      "Epoch: 3 / 3, Step: 30 / 750 Loss: 0.2195\n",
      "Epoch: 3 / 3, Step: 31 / 750 Loss: 0.2590\n",
      "Epoch: 3 / 3, Step: 32 / 750 Loss: 0.0545\n",
      "Epoch: 3 / 3, Step: 33 / 750 Loss: 0.2242\n",
      "Epoch: 3 / 3, Step: 34 / 750 Loss: 0.2400\n",
      "Epoch: 3 / 3, Step: 35 / 750 Loss: 0.0909\n",
      "Epoch: 3 / 3, Step: 36 / 750 Loss: 0.1570\n",
      "Epoch: 3 / 3, Step: 37 / 750 Loss: 0.1324\n",
      "Epoch: 3 / 3, Step: 38 / 750 Loss: 0.2456\n",
      "Epoch: 3 / 3, Step: 39 / 750 Loss: 0.2308\n",
      "Epoch: 3 / 3, Step: 40 / 750 Loss: 0.2312\n",
      "Epoch: 3 / 3, Step: 41 / 750 Loss: 0.2377\n",
      "Epoch: 3 / 3, Step: 42 / 750 Loss: 0.1337\n",
      "Epoch: 3 / 3, Step: 43 / 750 Loss: 0.0990\n",
      "Epoch: 3 / 3, Step: 44 / 750 Loss: 0.1674\n",
      "Epoch: 3 / 3, Step: 45 / 750 Loss: 0.3424\n",
      "Epoch: 3 / 3, Step: 46 / 750 Loss: 0.1830\n",
      "Epoch: 3 / 3, Step: 47 / 750 Loss: 0.0595\n",
      "Epoch: 3 / 3, Step: 48 / 750 Loss: 0.1323\n",
      "Epoch: 3 / 3, Step: 49 / 750 Loss: 0.1566\n",
      "Epoch: 3 / 3, Step: 50 / 750 Loss: 0.1802\n",
      "Epoch: 3 / 3, Step: 51 / 750 Loss: 0.0695\n",
      "Epoch: 3 / 3, Step: 52 / 750 Loss: 0.1659\n",
      "Epoch: 3 / 3, Step: 53 / 750 Loss: 0.0955\n",
      "Epoch: 3 / 3, Step: 54 / 750 Loss: 0.0635\n",
      "Epoch: 3 / 3, Step: 55 / 750 Loss: 0.1785\n",
      "Epoch: 3 / 3, Step: 56 / 750 Loss: 0.0902\n",
      "Epoch: 3 / 3, Step: 57 / 750 Loss: 0.0493\n",
      "Epoch: 3 / 3, Step: 58 / 750 Loss: 0.0760\n",
      "Epoch: 3 / 3, Step: 59 / 750 Loss: 0.0355\n",
      "Epoch: 3 / 3, Step: 60 / 750 Loss: 0.2266\n",
      "Epoch: 3 / 3, Step: 61 / 750 Loss: 0.3517\n",
      "Epoch: 3 / 3, Step: 62 / 750 Loss: 0.3502\n",
      "Epoch: 3 / 3, Step: 63 / 750 Loss: 0.0971\n",
      "Epoch: 3 / 3, Step: 64 / 750 Loss: 0.0743\n",
      "Epoch: 3 / 3, Step: 65 / 750 Loss: 0.1867\n",
      "Epoch: 3 / 3, Step: 66 / 750 Loss: 0.1560\n",
      "Epoch: 3 / 3, Step: 67 / 750 Loss: 0.1725\n",
      "Epoch: 3 / 3, Step: 68 / 750 Loss: 0.0603\n",
      "Epoch: 3 / 3, Step: 69 / 750 Loss: 0.1013\n",
      "Epoch: 3 / 3, Step: 70 / 750 Loss: 0.0519\n",
      "Epoch: 3 / 3, Step: 71 / 750 Loss: 0.2512\n",
      "Epoch: 3 / 3, Step: 72 / 750 Loss: 0.1318\n",
      "Epoch: 3 / 3, Step: 73 / 750 Loss: 0.1489\n",
      "Epoch: 3 / 3, Step: 74 / 750 Loss: 0.3662\n",
      "Epoch: 3 / 3, Step: 75 / 750 Loss: 0.1336\n",
      "Epoch: 3 / 3, Step: 76 / 750 Loss: 0.2448\n",
      "Epoch: 3 / 3, Step: 77 / 750 Loss: 0.0338\n",
      "Epoch: 3 / 3, Step: 78 / 750 Loss: 0.0677\n",
      "Epoch: 3 / 3, Step: 79 / 750 Loss: 0.2500\n",
      "Epoch: 3 / 3, Step: 80 / 750 Loss: 0.2264\n",
      "Epoch: 3 / 3, Step: 81 / 750 Loss: 0.0637\n",
      "Epoch: 3 / 3, Step: 82 / 750 Loss: 0.2435\n",
      "Epoch: 3 / 3, Step: 83 / 750 Loss: 0.0557\n",
      "Epoch: 3 / 3, Step: 84 / 750 Loss: 0.0877\n",
      "Epoch: 3 / 3, Step: 85 / 750 Loss: 0.1017\n",
      "Epoch: 3 / 3, Step: 86 / 750 Loss: 0.0750\n",
      "Epoch: 3 / 3, Step: 87 / 750 Loss: 0.1141\n",
      "Epoch: 3 / 3, Step: 88 / 750 Loss: 0.2333\n",
      "Epoch: 3 / 3, Step: 89 / 750 Loss: 0.0999\n",
      "Epoch: 3 / 3, Step: 90 / 750 Loss: 0.1776\n",
      "Epoch: 3 / 3, Step: 91 / 750 Loss: 0.1123\n",
      "Epoch: 3 / 3, Step: 92 / 750 Loss: 0.1949\n",
      "Epoch: 3 / 3, Step: 93 / 750 Loss: 0.0905\n",
      "Epoch: 3 / 3, Step: 94 / 750 Loss: 0.0371\n",
      "Epoch: 3 / 3, Step: 95 / 750 Loss: 0.3505\n",
      "Epoch: 3 / 3, Step: 96 / 750 Loss: 0.1036\n",
      "Epoch: 3 / 3, Step: 97 / 750 Loss: 0.1775\n",
      "Epoch: 3 / 3, Step: 98 / 750 Loss: 0.1141\n",
      "Epoch: 3 / 3, Step: 99 / 750 Loss: 0.1711\n",
      "Epoch: 3 / 3, Step: 100 / 750 Loss: 0.1233\n",
      "Epoch: 3 / 3, Step: 101 / 750 Loss: 0.2614\n",
      "Epoch: 3 / 3, Step: 102 / 750 Loss: 0.0872\n",
      "Epoch: 3 / 3, Step: 103 / 750 Loss: 0.3169\n",
      "Epoch: 3 / 3, Step: 104 / 750 Loss: 0.0497\n",
      "Epoch: 3 / 3, Step: 105 / 750 Loss: 0.0886\n",
      "Epoch: 3 / 3, Step: 106 / 750 Loss: 0.0804\n",
      "Epoch: 3 / 3, Step: 107 / 750 Loss: 0.3336\n",
      "Epoch: 3 / 3, Step: 108 / 750 Loss: 0.1337\n",
      "Epoch: 3 / 3, Step: 109 / 750 Loss: 0.1047\n",
      "Epoch: 3 / 3, Step: 110 / 750 Loss: 0.1628\n",
      "Epoch: 3 / 3, Step: 111 / 750 Loss: 0.0331\n",
      "Epoch: 3 / 3, Step: 112 / 750 Loss: 0.2182\n",
      "Epoch: 3 / 3, Step: 113 / 750 Loss: 0.2502\n",
      "Epoch: 3 / 3, Step: 114 / 750 Loss: 0.0735\n",
      "Epoch: 3 / 3, Step: 115 / 750 Loss: 0.0966\n",
      "Epoch: 3 / 3, Step: 116 / 750 Loss: 0.2366\n",
      "Epoch: 3 / 3, Step: 117 / 750 Loss: 0.1940\n",
      "Epoch: 3 / 3, Step: 118 / 750 Loss: 0.0613\n",
      "Epoch: 3 / 3, Step: 119 / 750 Loss: 0.0871\n",
      "Epoch: 3 / 3, Step: 120 / 750 Loss: 0.2738\n",
      "Epoch: 3 / 3, Step: 121 / 750 Loss: 0.0392\n",
      "Epoch: 3 / 3, Step: 122 / 750 Loss: 0.2058\n",
      "Epoch: 3 / 3, Step: 123 / 750 Loss: 0.0505\n",
      "Epoch: 3 / 3, Step: 124 / 750 Loss: 0.1280\n",
      "Epoch: 3 / 3, Step: 125 / 750 Loss: 0.0482\n",
      "Epoch: 3 / 3, Step: 126 / 750 Loss: 0.0339\n",
      "Epoch: 3 / 3, Step: 127 / 750 Loss: 0.1437\n",
      "Epoch: 3 / 3, Step: 128 / 750 Loss: 0.1673\n",
      "Epoch: 3 / 3, Step: 129 / 750 Loss: 0.0811\n",
      "Epoch: 3 / 3, Step: 130 / 750 Loss: 0.0939\n",
      "Epoch: 3 / 3, Step: 131 / 750 Loss: 0.0353\n",
      "Epoch: 3 / 3, Step: 132 / 750 Loss: 0.2604\n",
      "Epoch: 3 / 3, Step: 133 / 750 Loss: 0.1083\n",
      "Epoch: 3 / 3, Step: 134 / 750 Loss: 0.1750\n",
      "Epoch: 3 / 3, Step: 135 / 750 Loss: 0.0807\n",
      "Epoch: 3 / 3, Step: 136 / 750 Loss: 0.2104\n",
      "Epoch: 3 / 3, Step: 137 / 750 Loss: 0.0754\n",
      "Epoch: 3 / 3, Step: 138 / 750 Loss: 0.0276\n",
      "Epoch: 3 / 3, Step: 139 / 750 Loss: 0.3312\n",
      "Epoch: 3 / 3, Step: 140 / 750 Loss: 0.0547\n",
      "Epoch: 3 / 3, Step: 141 / 750 Loss: 0.1039\n",
      "Epoch: 3 / 3, Step: 142 / 750 Loss: 0.1814\n",
      "Epoch: 3 / 3, Step: 143 / 750 Loss: 0.0352\n",
      "Epoch: 3 / 3, Step: 144 / 750 Loss: 0.3388\n",
      "Epoch: 3 / 3, Step: 145 / 750 Loss: 0.1773\n",
      "Epoch: 3 / 3, Step: 146 / 750 Loss: 0.1796\n",
      "Epoch: 3 / 3, Step: 147 / 750 Loss: 0.2604\n",
      "Epoch: 3 / 3, Step: 148 / 750 Loss: 0.2125\n",
      "Epoch: 3 / 3, Step: 149 / 750 Loss: 0.1297\n",
      "Epoch: 3 / 3, Step: 150 / 750 Loss: 0.2957\n",
      "Epoch: 3 / 3, Step: 151 / 750 Loss: 0.1730\n",
      "Epoch: 3 / 3, Step: 152 / 750 Loss: 0.1091\n",
      "Epoch: 3 / 3, Step: 153 / 750 Loss: 0.0382\n",
      "Epoch: 3 / 3, Step: 154 / 750 Loss: 0.2262\n",
      "Epoch: 3 / 3, Step: 155 / 750 Loss: 0.2546\n",
      "Epoch: 3 / 3, Step: 156 / 750 Loss: 0.0795\n",
      "Epoch: 3 / 3, Step: 157 / 750 Loss: 0.1530\n",
      "Epoch: 3 / 3, Step: 158 / 750 Loss: 0.2633\n",
      "Epoch: 3 / 3, Step: 159 / 750 Loss: 0.0736\n",
      "Epoch: 3 / 3, Step: 160 / 750 Loss: 0.1700\n",
      "Epoch: 3 / 3, Step: 161 / 750 Loss: 0.0814\n",
      "Epoch: 3 / 3, Step: 162 / 750 Loss: 0.0466\n",
      "Epoch: 3 / 3, Step: 163 / 750 Loss: 0.1392\n",
      "Epoch: 3 / 3, Step: 164 / 750 Loss: 0.0345\n",
      "Epoch: 3 / 3, Step: 165 / 750 Loss: 0.1298\n",
      "Epoch: 3 / 3, Step: 166 / 750 Loss: 0.1136\n",
      "Epoch: 3 / 3, Step: 167 / 750 Loss: 0.1966\n",
      "Epoch: 3 / 3, Step: 168 / 750 Loss: 0.1135\n",
      "Epoch: 3 / 3, Step: 169 / 750 Loss: 0.1360\n",
      "Epoch: 3 / 3, Step: 170 / 750 Loss: 0.2619\n",
      "Epoch: 3 / 3, Step: 171 / 750 Loss: 0.0923\n",
      "Epoch: 3 / 3, Step: 172 / 750 Loss: 0.2186\n",
      "Epoch: 3 / 3, Step: 173 / 750 Loss: 0.0872\n",
      "Epoch: 3 / 3, Step: 174 / 750 Loss: 0.1127\n",
      "Epoch: 3 / 3, Step: 175 / 750 Loss: 0.1118\n",
      "Epoch: 3 / 3, Step: 176 / 750 Loss: 0.1246\n",
      "Epoch: 3 / 3, Step: 177 / 750 Loss: 0.1773\n",
      "Epoch: 3 / 3, Step: 178 / 750 Loss: 0.0969\n",
      "Epoch: 3 / 3, Step: 179 / 750 Loss: 0.1667\n",
      "Epoch: 3 / 3, Step: 180 / 750 Loss: 0.1533\n",
      "Epoch: 3 / 3, Step: 181 / 750 Loss: 0.0678\n",
      "Epoch: 3 / 3, Step: 182 / 750 Loss: 0.0427\n",
      "Epoch: 3 / 3, Step: 183 / 750 Loss: 0.1682\n",
      "Epoch: 3 / 3, Step: 184 / 750 Loss: 0.0505\n",
      "Epoch: 3 / 3, Step: 185 / 750 Loss: 0.2160\n",
      "Epoch: 3 / 3, Step: 186 / 750 Loss: 0.0897\n",
      "Epoch: 3 / 3, Step: 187 / 750 Loss: 0.1875\n",
      "Epoch: 3 / 3, Step: 188 / 750 Loss: 0.3426\n",
      "Epoch: 3 / 3, Step: 189 / 750 Loss: 0.2475\n",
      "Epoch: 3 / 3, Step: 190 / 750 Loss: 0.1101\n",
      "Epoch: 3 / 3, Step: 191 / 750 Loss: 0.1309\n",
      "Epoch: 3 / 3, Step: 192 / 750 Loss: 0.1274\n",
      "Epoch: 3 / 3, Step: 193 / 750 Loss: 0.3284\n",
      "Epoch: 3 / 3, Step: 194 / 750 Loss: 0.0453\n",
      "Epoch: 3 / 3, Step: 195 / 750 Loss: 0.1660\n",
      "Epoch: 3 / 3, Step: 196 / 750 Loss: 0.1172\n",
      "Epoch: 3 / 3, Step: 197 / 750 Loss: 0.0195\n",
      "Epoch: 3 / 3, Step: 198 / 750 Loss: 0.5528\n",
      "Epoch: 3 / 3, Step: 199 / 750 Loss: 0.1190\n",
      "Epoch: 3 / 3, Step: 200 / 750 Loss: 0.2093\n",
      "Epoch: 3 / 3, Step: 201 / 750 Loss: 0.1958\n",
      "Epoch: 3 / 3, Step: 202 / 750 Loss: 0.3839\n",
      "Epoch: 3 / 3, Step: 203 / 750 Loss: 0.0456\n",
      "Epoch: 3 / 3, Step: 204 / 750 Loss: 0.1515\n",
      "Epoch: 3 / 3, Step: 205 / 750 Loss: 0.0621\n",
      "Epoch: 3 / 3, Step: 206 / 750 Loss: 0.1241\n",
      "Epoch: 3 / 3, Step: 207 / 750 Loss: 0.0643\n",
      "Epoch: 3 / 3, Step: 208 / 750 Loss: 0.2269\n",
      "Epoch: 3 / 3, Step: 209 / 750 Loss: 0.0345\n",
      "Epoch: 3 / 3, Step: 210 / 750 Loss: 0.1325\n",
      "Epoch: 3 / 3, Step: 211 / 750 Loss: 0.1102\n",
      "Epoch: 3 / 3, Step: 212 / 750 Loss: 0.0526\n",
      "Epoch: 3 / 3, Step: 213 / 750 Loss: 0.4264\n",
      "Epoch: 3 / 3, Step: 214 / 750 Loss: 0.1356\n",
      "Epoch: 3 / 3, Step: 215 / 750 Loss: 0.1133\n",
      "Epoch: 3 / 3, Step: 216 / 750 Loss: 0.3536\n",
      "Epoch: 3 / 3, Step: 217 / 750 Loss: 0.1159\n",
      "Epoch: 3 / 3, Step: 218 / 750 Loss: 0.1946\n",
      "Epoch: 3 / 3, Step: 219 / 750 Loss: 0.1780\n",
      "Epoch: 3 / 3, Step: 220 / 750 Loss: 0.2595\n",
      "Epoch: 3 / 3, Step: 221 / 750 Loss: 0.1701\n",
      "Epoch: 3 / 3, Step: 222 / 750 Loss: 0.0858\n",
      "Epoch: 3 / 3, Step: 223 / 750 Loss: 0.1386\n",
      "Epoch: 3 / 3, Step: 224 / 750 Loss: 0.0915\n",
      "Epoch: 3 / 3, Step: 225 / 750 Loss: 0.2239\n",
      "Epoch: 3 / 3, Step: 226 / 750 Loss: 0.0799\n",
      "Epoch: 3 / 3, Step: 227 / 750 Loss: 0.2200\n",
      "Epoch: 3 / 3, Step: 228 / 750 Loss: 0.2908\n",
      "Epoch: 3 / 3, Step: 229 / 750 Loss: 0.2788\n",
      "Epoch: 3 / 3, Step: 230 / 750 Loss: 0.1502\n",
      "Epoch: 3 / 3, Step: 231 / 750 Loss: 0.1278\n",
      "Epoch: 3 / 3, Step: 232 / 750 Loss: 0.0673\n",
      "Epoch: 3 / 3, Step: 233 / 750 Loss: 0.0417\n",
      "Epoch: 3 / 3, Step: 234 / 750 Loss: 0.2017\n",
      "Epoch: 3 / 3, Step: 235 / 750 Loss: 0.1145\n",
      "Epoch: 3 / 3, Step: 236 / 750 Loss: 0.2784\n",
      "Epoch: 3 / 3, Step: 237 / 750 Loss: 0.0305\n",
      "Epoch: 3 / 3, Step: 238 / 750 Loss: 0.0818\n",
      "Epoch: 3 / 3, Step: 239 / 750 Loss: 0.1077\n",
      "Epoch: 3 / 3, Step: 240 / 750 Loss: 0.1427\n",
      "Epoch: 3 / 3, Step: 241 / 750 Loss: 0.1242\n",
      "Epoch: 3 / 3, Step: 242 / 750 Loss: 0.0940\n",
      "Epoch: 3 / 3, Step: 243 / 750 Loss: 0.1257\n",
      "Epoch: 3 / 3, Step: 244 / 750 Loss: 0.0923\n",
      "Epoch: 3 / 3, Step: 245 / 750 Loss: 0.0143\n",
      "Epoch: 3 / 3, Step: 246 / 750 Loss: 0.1930\n",
      "Epoch: 3 / 3, Step: 247 / 750 Loss: 0.1829\n",
      "Epoch: 3 / 3, Step: 248 / 750 Loss: 0.2260\n",
      "Epoch: 3 / 3, Step: 249 / 750 Loss: 0.0572\n",
      "Epoch: 3 / 3, Step: 250 / 750 Loss: 0.0454\n",
      "Epoch: 3 / 3, Step: 251 / 750 Loss: 0.2065\n",
      "Epoch: 3 / 3, Step: 252 / 750 Loss: 0.1104\n",
      "Epoch: 3 / 3, Step: 253 / 750 Loss: 0.2012\n",
      "Epoch: 3 / 3, Step: 254 / 750 Loss: 0.0369\n",
      "Epoch: 3 / 3, Step: 255 / 750 Loss: 0.2399\n",
      "Epoch: 3 / 3, Step: 256 / 750 Loss: 0.0274\n",
      "Epoch: 3 / 3, Step: 257 / 750 Loss: 0.0466\n",
      "Epoch: 3 / 3, Step: 258 / 750 Loss: 0.2090\n",
      "Epoch: 3 / 3, Step: 259 / 750 Loss: 0.0541\n",
      "Epoch: 3 / 3, Step: 260 / 750 Loss: 0.4689\n",
      "Epoch: 3 / 3, Step: 261 / 750 Loss: 0.2274\n",
      "Epoch: 3 / 3, Step: 262 / 750 Loss: 0.3965\n",
      "Epoch: 3 / 3, Step: 263 / 750 Loss: 0.1158\n",
      "Epoch: 3 / 3, Step: 264 / 750 Loss: 0.0403\n",
      "Epoch: 3 / 3, Step: 265 / 750 Loss: 0.0951\n",
      "Epoch: 3 / 3, Step: 266 / 750 Loss: 0.0507\n",
      "Epoch: 3 / 3, Step: 267 / 750 Loss: 0.1517\n",
      "Epoch: 3 / 3, Step: 268 / 750 Loss: 0.0787\n",
      "Epoch: 3 / 3, Step: 269 / 750 Loss: 0.0627\n",
      "Epoch: 3 / 3, Step: 270 / 750 Loss: 0.2845\n",
      "Epoch: 3 / 3, Step: 271 / 750 Loss: 0.0478\n",
      "Epoch: 3 / 3, Step: 272 / 750 Loss: 0.0741\n",
      "Epoch: 3 / 3, Step: 273 / 750 Loss: 0.0344\n",
      "Epoch: 3 / 3, Step: 274 / 750 Loss: 0.0987\n",
      "Epoch: 3 / 3, Step: 275 / 750 Loss: 0.0960\n",
      "Epoch: 3 / 3, Step: 276 / 750 Loss: 0.3602\n",
      "Epoch: 3 / 3, Step: 277 / 750 Loss: 0.1258\n",
      "Epoch: 3 / 3, Step: 278 / 750 Loss: 0.3179\n",
      "Epoch: 3 / 3, Step: 279 / 750 Loss: 0.1249\n",
      "Epoch: 3 / 3, Step: 280 / 750 Loss: 0.2387\n",
      "Epoch: 3 / 3, Step: 281 / 750 Loss: 0.1634\n",
      "Epoch: 3 / 3, Step: 282 / 750 Loss: 0.1953\n",
      "Epoch: 3 / 3, Step: 283 / 750 Loss: 0.2580\n",
      "Epoch: 3 / 3, Step: 284 / 750 Loss: 0.1654\n",
      "Epoch: 3 / 3, Step: 285 / 750 Loss: 0.3811\n",
      "Epoch: 3 / 3, Step: 286 / 750 Loss: 0.2057\n",
      "Epoch: 3 / 3, Step: 287 / 750 Loss: 0.0460\n",
      "Epoch: 3 / 3, Step: 288 / 750 Loss: 0.3139\n",
      "Epoch: 3 / 3, Step: 289 / 750 Loss: 0.3026\n",
      "Epoch: 3 / 3, Step: 290 / 750 Loss: 0.1164\n",
      "Epoch: 3 / 3, Step: 291 / 750 Loss: 0.2538\n",
      "Epoch: 3 / 3, Step: 292 / 750 Loss: 0.0519\n",
      "Epoch: 3 / 3, Step: 293 / 750 Loss: 0.2783\n",
      "Epoch: 3 / 3, Step: 294 / 750 Loss: 0.2546\n",
      "Epoch: 3 / 3, Step: 295 / 750 Loss: 0.1379\n",
      "Epoch: 3 / 3, Step: 296 / 750 Loss: 0.2378\n",
      "Epoch: 3 / 3, Step: 297 / 750 Loss: 0.2618\n",
      "Epoch: 3 / 3, Step: 298 / 750 Loss: 0.1162\n",
      "Epoch: 3 / 3, Step: 299 / 750 Loss: 0.2046\n",
      "Epoch: 3 / 3, Step: 300 / 750 Loss: 0.2342\n",
      "Epoch: 3 / 3, Step: 301 / 750 Loss: 0.1087\n",
      "Epoch: 3 / 3, Step: 302 / 750 Loss: 0.1764\n",
      "Epoch: 3 / 3, Step: 303 / 750 Loss: 0.1591\n",
      "Epoch: 3 / 3, Step: 304 / 750 Loss: 0.3419\n",
      "Epoch: 3 / 3, Step: 305 / 750 Loss: 0.1074\n",
      "Epoch: 3 / 3, Step: 306 / 750 Loss: 0.1940\n",
      "Epoch: 3 / 3, Step: 307 / 750 Loss: 0.0850\n",
      "Epoch: 3 / 3, Step: 308 / 750 Loss: 0.1087\n",
      "Epoch: 3 / 3, Step: 309 / 750 Loss: 0.1528\n",
      "Epoch: 3 / 3, Step: 310 / 750 Loss: 0.3619\n",
      "Epoch: 3 / 3, Step: 311 / 750 Loss: 0.0494\n",
      "Epoch: 3 / 3, Step: 312 / 750 Loss: 0.1701\n",
      "Epoch: 3 / 3, Step: 313 / 750 Loss: 0.2066\n",
      "Epoch: 3 / 3, Step: 314 / 750 Loss: 0.1911\n",
      "Epoch: 3 / 3, Step: 315 / 750 Loss: 0.1931\n",
      "Epoch: 3 / 3, Step: 316 / 750 Loss: 0.2579\n",
      "Epoch: 3 / 3, Step: 317 / 750 Loss: 0.2443\n",
      "Epoch: 3 / 3, Step: 318 / 750 Loss: 0.1081\n",
      "Epoch: 3 / 3, Step: 319 / 750 Loss: 0.1057\n",
      "Epoch: 3 / 3, Step: 320 / 750 Loss: 0.2441\n",
      "Epoch: 3 / 3, Step: 321 / 750 Loss: 0.3332\n",
      "Epoch: 3 / 3, Step: 322 / 750 Loss: 0.3282\n",
      "Epoch: 3 / 3, Step: 323 / 750 Loss: 0.0919\n",
      "Epoch: 3 / 3, Step: 324 / 750 Loss: 0.3474\n",
      "Epoch: 3 / 3, Step: 325 / 750 Loss: 0.2170\n",
      "Epoch: 3 / 3, Step: 326 / 750 Loss: 0.1609\n",
      "Epoch: 3 / 3, Step: 327 / 750 Loss: 0.1955\n",
      "Epoch: 3 / 3, Step: 328 / 750 Loss: 0.1610\n",
      "Epoch: 3 / 3, Step: 329 / 750 Loss: 0.2792\n",
      "Epoch: 3 / 3, Step: 330 / 750 Loss: 0.5051\n",
      "Epoch: 3 / 3, Step: 331 / 750 Loss: 0.2037\n",
      "Epoch: 3 / 3, Step: 332 / 750 Loss: 0.1911\n",
      "Epoch: 3 / 3, Step: 333 / 750 Loss: 0.0781\n",
      "Epoch: 3 / 3, Step: 334 / 750 Loss: 0.0508\n",
      "Epoch: 3 / 3, Step: 335 / 750 Loss: 0.1308\n",
      "Epoch: 3 / 3, Step: 336 / 750 Loss: 0.1040\n",
      "Epoch: 3 / 3, Step: 337 / 750 Loss: 0.0688\n",
      "Epoch: 3 / 3, Step: 338 / 750 Loss: 0.1378\n",
      "Epoch: 3 / 3, Step: 339 / 750 Loss: 0.1215\n",
      "Epoch: 3 / 3, Step: 340 / 750 Loss: 0.0725\n",
      "Epoch: 3 / 3, Step: 341 / 750 Loss: 0.0965\n",
      "Epoch: 3 / 3, Step: 342 / 750 Loss: 0.1146\n",
      "Epoch: 3 / 3, Step: 343 / 750 Loss: 0.0363\n",
      "Epoch: 3 / 3, Step: 344 / 750 Loss: 0.0957\n",
      "Epoch: 3 / 3, Step: 345 / 750 Loss: 0.2105\n",
      "Epoch: 3 / 3, Step: 346 / 750 Loss: 0.1182\n",
      "Epoch: 3 / 3, Step: 347 / 750 Loss: 0.3908\n",
      "Epoch: 3 / 3, Step: 348 / 750 Loss: 0.3360\n",
      "Epoch: 3 / 3, Step: 349 / 750 Loss: 0.2108\n",
      "Epoch: 3 / 3, Step: 350 / 750 Loss: 0.2406\n",
      "Epoch: 3 / 3, Step: 351 / 750 Loss: 0.1912\n",
      "Epoch: 3 / 3, Step: 352 / 750 Loss: 0.0400\n",
      "Epoch: 3 / 3, Step: 353 / 750 Loss: 0.2755\n",
      "Epoch: 3 / 3, Step: 354 / 750 Loss: 0.2591\n",
      "Epoch: 3 / 3, Step: 355 / 750 Loss: 0.1547\n",
      "Epoch: 3 / 3, Step: 356 / 750 Loss: 0.1317\n",
      "Epoch: 3 / 3, Step: 357 / 750 Loss: 0.0254\n",
      "Epoch: 3 / 3, Step: 358 / 750 Loss: 0.2473\n",
      "Epoch: 3 / 3, Step: 359 / 750 Loss: 0.0348\n",
      "Epoch: 3 / 3, Step: 360 / 750 Loss: 0.0408\n",
      "Epoch: 3 / 3, Step: 361 / 750 Loss: 0.2652\n",
      "Epoch: 3 / 3, Step: 362 / 750 Loss: 0.3211\n",
      "Epoch: 3 / 3, Step: 363 / 750 Loss: 0.0526\n",
      "Epoch: 3 / 3, Step: 364 / 750 Loss: 0.3970\n",
      "Epoch: 3 / 3, Step: 365 / 750 Loss: 0.0712\n",
      "Epoch: 3 / 3, Step: 366 / 750 Loss: 0.0690\n",
      "Epoch: 3 / 3, Step: 367 / 750 Loss: 0.3102\n",
      "Epoch: 3 / 3, Step: 368 / 750 Loss: 0.1816\n",
      "Epoch: 3 / 3, Step: 369 / 750 Loss: 0.0506\n",
      "Epoch: 3 / 3, Step: 370 / 750 Loss: 0.0255\n",
      "Epoch: 3 / 3, Step: 371 / 750 Loss: 0.1019\n",
      "Epoch: 3 / 3, Step: 372 / 750 Loss: 0.1492\n",
      "Epoch: 3 / 3, Step: 373 / 750 Loss: 0.0491\n",
      "Epoch: 3 / 3, Step: 374 / 750 Loss: 0.2099\n",
      "Epoch: 3 / 3, Step: 375 / 750 Loss: 0.3917\n",
      "Epoch: 3 / 3, Step: 376 / 750 Loss: 0.1974\n",
      "Epoch: 3 / 3, Step: 377 / 750 Loss: 0.2135\n",
      "Epoch: 3 / 3, Step: 378 / 750 Loss: 0.1892\n",
      "Epoch: 3 / 3, Step: 379 / 750 Loss: 0.4077\n",
      "Epoch: 3 / 3, Step: 380 / 750 Loss: 0.0452\n",
      "Epoch: 3 / 3, Step: 381 / 750 Loss: 0.0386\n",
      "Epoch: 3 / 3, Step: 382 / 750 Loss: 0.1375\n",
      "Epoch: 3 / 3, Step: 383 / 750 Loss: 0.0607\n",
      "Epoch: 3 / 3, Step: 384 / 750 Loss: 0.1056\n",
      "Epoch: 3 / 3, Step: 385 / 750 Loss: 0.0440\n",
      "Epoch: 3 / 3, Step: 386 / 750 Loss: 0.2864\n",
      "Epoch: 3 / 3, Step: 387 / 750 Loss: 0.0771\n",
      "Epoch: 3 / 3, Step: 388 / 750 Loss: 0.4445\n",
      "Epoch: 3 / 3, Step: 389 / 750 Loss: 0.1990\n",
      "Epoch: 3 / 3, Step: 390 / 750 Loss: 0.0927\n",
      "Epoch: 3 / 3, Step: 391 / 750 Loss: 0.5479\n",
      "Epoch: 3 / 3, Step: 392 / 750 Loss: 0.0591\n",
      "Epoch: 3 / 3, Step: 393 / 750 Loss: 0.2018\n",
      "Epoch: 3 / 3, Step: 394 / 750 Loss: 0.1557\n",
      "Epoch: 3 / 3, Step: 395 / 750 Loss: 0.0621\n",
      "Epoch: 3 / 3, Step: 396 / 750 Loss: 0.0898\n",
      "Epoch: 3 / 3, Step: 397 / 750 Loss: 0.1252\n",
      "Epoch: 3 / 3, Step: 398 / 750 Loss: 0.0548\n",
      "Epoch: 3 / 3, Step: 399 / 750 Loss: 0.2301\n",
      "Epoch: 3 / 3, Step: 400 / 750 Loss: 0.5546\n",
      "Epoch: 3 / 3, Step: 401 / 750 Loss: 0.0882\n",
      "Epoch: 3 / 3, Step: 402 / 750 Loss: 0.0754\n",
      "Epoch: 3 / 3, Step: 403 / 750 Loss: 0.0839\n",
      "Epoch: 3 / 3, Step: 404 / 750 Loss: 0.0488\n",
      "Epoch: 3 / 3, Step: 405 / 750 Loss: 0.0708\n",
      "Epoch: 3 / 3, Step: 406 / 750 Loss: 0.1252\n",
      "Epoch: 3 / 3, Step: 407 / 750 Loss: 0.1212\n",
      "Epoch: 3 / 3, Step: 408 / 750 Loss: 0.2543\n",
      "Epoch: 3 / 3, Step: 409 / 750 Loss: 0.0601\n",
      "Epoch: 3 / 3, Step: 410 / 750 Loss: 0.0509\n",
      "Epoch: 3 / 3, Step: 411 / 750 Loss: 0.0356\n",
      "Epoch: 3 / 3, Step: 412 / 750 Loss: 0.0343\n",
      "Epoch: 3 / 3, Step: 413 / 750 Loss: 0.2610\n",
      "Epoch: 3 / 3, Step: 414 / 750 Loss: 0.0686\n",
      "Epoch: 3 / 3, Step: 415 / 750 Loss: 0.1828\n",
      "Epoch: 3 / 3, Step: 416 / 750 Loss: 0.0479\n",
      "Epoch: 3 / 3, Step: 417 / 750 Loss: 0.0168\n",
      "Epoch: 3 / 3, Step: 418 / 750 Loss: 0.0379\n",
      "Epoch: 3 / 3, Step: 419 / 750 Loss: 0.1623\n",
      "Epoch: 3 / 3, Step: 420 / 750 Loss: 0.0305\n",
      "Epoch: 3 / 3, Step: 421 / 750 Loss: 0.0332\n",
      "Epoch: 3 / 3, Step: 422 / 750 Loss: 0.1694\n",
      "Epoch: 3 / 3, Step: 423 / 750 Loss: 0.0904\n",
      "Epoch: 3 / 3, Step: 424 / 750 Loss: 0.0296\n",
      "Epoch: 3 / 3, Step: 425 / 750 Loss: 0.1457\n",
      "Epoch: 3 / 3, Step: 426 / 750 Loss: 0.0342\n",
      "Epoch: 3 / 3, Step: 427 / 750 Loss: 0.0498\n",
      "Epoch: 3 / 3, Step: 428 / 750 Loss: 0.1588\n",
      "Epoch: 3 / 3, Step: 429 / 750 Loss: 0.2313\n",
      "Epoch: 3 / 3, Step: 430 / 750 Loss: 0.1124\n",
      "Epoch: 3 / 3, Step: 431 / 750 Loss: 0.0874\n",
      "Epoch: 3 / 3, Step: 432 / 750 Loss: 0.2119\n",
      "Epoch: 3 / 3, Step: 433 / 750 Loss: 0.1970\n",
      "Epoch: 3 / 3, Step: 434 / 750 Loss: 0.0296\n",
      "Epoch: 3 / 3, Step: 435 / 750 Loss: 0.1267\n",
      "Epoch: 3 / 3, Step: 436 / 750 Loss: 0.0818\n",
      "Epoch: 3 / 3, Step: 437 / 750 Loss: 0.3755\n",
      "Epoch: 3 / 3, Step: 438 / 750 Loss: 0.2382\n",
      "Epoch: 3 / 3, Step: 439 / 750 Loss: 0.0966\n",
      "Epoch: 3 / 3, Step: 440 / 750 Loss: 0.0727\n",
      "Epoch: 3 / 3, Step: 441 / 750 Loss: 0.0609\n",
      "Epoch: 3 / 3, Step: 442 / 750 Loss: 0.3897\n",
      "Epoch: 3 / 3, Step: 443 / 750 Loss: 0.2072\n",
      "Epoch: 3 / 3, Step: 444 / 750 Loss: 0.0485\n",
      "Epoch: 3 / 3, Step: 445 / 750 Loss: 0.2584\n",
      "Epoch: 3 / 3, Step: 446 / 750 Loss: 0.0523\n",
      "Epoch: 3 / 3, Step: 447 / 750 Loss: 0.3301\n",
      "Epoch: 3 / 3, Step: 448 / 750 Loss: 0.0195\n",
      "Epoch: 3 / 3, Step: 449 / 750 Loss: 0.0462\n",
      "Epoch: 3 / 3, Step: 450 / 750 Loss: 0.1982\n",
      "Epoch: 3 / 3, Step: 451 / 750 Loss: 0.3527\n",
      "Epoch: 3 / 3, Step: 452 / 750 Loss: 0.1128\n",
      "Epoch: 3 / 3, Step: 453 / 750 Loss: 0.2197\n",
      "Epoch: 3 / 3, Step: 454 / 750 Loss: 0.1601\n",
      "Epoch: 3 / 3, Step: 455 / 750 Loss: 0.2000\n",
      "Epoch: 3 / 3, Step: 456 / 750 Loss: 0.2542\n",
      "Epoch: 3 / 3, Step: 457 / 750 Loss: 0.1276\n",
      "Epoch: 3 / 3, Step: 458 / 750 Loss: 0.1530\n",
      "Epoch: 3 / 3, Step: 459 / 750 Loss: 0.0791\n",
      "Epoch: 3 / 3, Step: 460 / 750 Loss: 0.0689\n",
      "Epoch: 3 / 3, Step: 461 / 750 Loss: 0.1088\n",
      "Epoch: 3 / 3, Step: 462 / 750 Loss: 0.3153\n",
      "Epoch: 3 / 3, Step: 463 / 750 Loss: 0.0973\n",
      "Epoch: 3 / 3, Step: 464 / 750 Loss: 0.3293\n",
      "Epoch: 3 / 3, Step: 465 / 750 Loss: 0.0503\n",
      "Epoch: 3 / 3, Step: 466 / 750 Loss: 0.0722\n",
      "Epoch: 3 / 3, Step: 467 / 750 Loss: 0.0643\n",
      "Epoch: 3 / 3, Step: 468 / 750 Loss: 0.1355\n",
      "Epoch: 3 / 3, Step: 469 / 750 Loss: 0.0750\n",
      "Epoch: 3 / 3, Step: 470 / 750 Loss: 0.1664\n",
      "Epoch: 3 / 3, Step: 471 / 750 Loss: 0.1639\n",
      "Epoch: 3 / 3, Step: 472 / 750 Loss: 0.3352\n",
      "Epoch: 3 / 3, Step: 473 / 750 Loss: 0.0476\n",
      "Epoch: 3 / 3, Step: 474 / 750 Loss: 0.3579\n",
      "Epoch: 3 / 3, Step: 475 / 750 Loss: 0.3479\n",
      "Epoch: 3 / 3, Step: 476 / 750 Loss: 0.1065\n",
      "Epoch: 3 / 3, Step: 477 / 750 Loss: 0.0327\n",
      "Epoch: 3 / 3, Step: 478 / 750 Loss: 0.3525\n",
      "Epoch: 3 / 3, Step: 479 / 750 Loss: 0.3771\n",
      "Epoch: 3 / 3, Step: 480 / 750 Loss: 0.1815\n",
      "Epoch: 3 / 3, Step: 481 / 750 Loss: 0.0462\n",
      "Epoch: 3 / 3, Step: 482 / 750 Loss: 0.4513\n",
      "Epoch: 3 / 3, Step: 483 / 750 Loss: 0.3427\n",
      "Epoch: 3 / 3, Step: 484 / 750 Loss: 0.0579\n",
      "Epoch: 3 / 3, Step: 485 / 750 Loss: 0.0684\n",
      "Epoch: 3 / 3, Step: 486 / 750 Loss: 0.0637\n",
      "Epoch: 3 / 3, Step: 487 / 750 Loss: 0.0336\n",
      "Epoch: 3 / 3, Step: 488 / 750 Loss: 0.0980\n",
      "Epoch: 3 / 3, Step: 489 / 750 Loss: 0.1358\n",
      "Epoch: 3 / 3, Step: 490 / 750 Loss: 0.0519\n",
      "Epoch: 3 / 3, Step: 491 / 750 Loss: 0.1555\n",
      "Epoch: 3 / 3, Step: 492 / 750 Loss: 0.0419\n",
      "Epoch: 3 / 3, Step: 493 / 750 Loss: 0.3155\n",
      "Epoch: 3 / 3, Step: 494 / 750 Loss: 0.1961\n",
      "Epoch: 3 / 3, Step: 495 / 750 Loss: 0.1646\n",
      "Epoch: 3 / 3, Step: 496 / 750 Loss: 0.2002\n",
      "Epoch: 3 / 3, Step: 497 / 750 Loss: 0.1592\n",
      "Epoch: 3 / 3, Step: 498 / 750 Loss: 0.3301\n",
      "Epoch: 3 / 3, Step: 499 / 750 Loss: 0.2114\n",
      "Epoch: 3 / 3, Step: 500 / 750 Loss: 0.2009\n",
      "Epoch: 3 / 3, Step: 501 / 750 Loss: 0.0620\n",
      "Epoch: 3 / 3, Step: 502 / 750 Loss: 0.2904\n",
      "Epoch: 3 / 3, Step: 503 / 750 Loss: 0.1783\n",
      "Epoch: 3 / 3, Step: 504 / 750 Loss: 0.3726\n",
      "Epoch: 3 / 3, Step: 505 / 750 Loss: 0.0630\n",
      "Epoch: 3 / 3, Step: 506 / 750 Loss: 0.0429\n",
      "Epoch: 3 / 3, Step: 507 / 750 Loss: 0.0873\n",
      "Epoch: 3 / 3, Step: 508 / 750 Loss: 0.0849\n",
      "Epoch: 3 / 3, Step: 509 / 750 Loss: 0.2263\n",
      "Epoch: 3 / 3, Step: 510 / 750 Loss: 0.3286\n",
      "Epoch: 3 / 3, Step: 511 / 750 Loss: 0.1140\n",
      "Epoch: 3 / 3, Step: 512 / 750 Loss: 0.1323\n",
      "Epoch: 3 / 3, Step: 513 / 750 Loss: 0.1757\n",
      "Epoch: 3 / 3, Step: 514 / 750 Loss: 0.1218\n",
      "Epoch: 3 / 3, Step: 515 / 750 Loss: 0.0994\n",
      "Epoch: 3 / 3, Step: 516 / 750 Loss: 0.1055\n",
      "Epoch: 3 / 3, Step: 517 / 750 Loss: 0.0858\n",
      "Epoch: 3 / 3, Step: 518 / 750 Loss: 0.2420\n",
      "Epoch: 3 / 3, Step: 519 / 750 Loss: 0.0773\n",
      "Epoch: 3 / 3, Step: 520 / 750 Loss: 0.1221\n",
      "Epoch: 3 / 3, Step: 521 / 750 Loss: 0.2138\n",
      "Epoch: 3 / 3, Step: 522 / 750 Loss: 0.0899\n",
      "Epoch: 3 / 3, Step: 523 / 750 Loss: 0.2098\n",
      "Epoch: 3 / 3, Step: 524 / 750 Loss: 0.0593\n",
      "Epoch: 3 / 3, Step: 525 / 750 Loss: 0.1139\n",
      "Epoch: 3 / 3, Step: 526 / 750 Loss: 0.0247\n",
      "Epoch: 3 / 3, Step: 527 / 750 Loss: 0.2330\n",
      "Epoch: 3 / 3, Step: 528 / 750 Loss: 0.0379\n",
      "Epoch: 3 / 3, Step: 529 / 750 Loss: 0.0566\n",
      "Epoch: 3 / 3, Step: 530 / 750 Loss: 0.1488\n",
      "Epoch: 3 / 3, Step: 531 / 750 Loss: 0.0619\n",
      "Epoch: 3 / 3, Step: 532 / 750 Loss: 0.3487\n",
      "Epoch: 3 / 3, Step: 533 / 750 Loss: 0.2403\n",
      "Epoch: 3 / 3, Step: 534 / 750 Loss: 0.0731\n",
      "Epoch: 3 / 3, Step: 535 / 750 Loss: 0.2506\n",
      "Epoch: 3 / 3, Step: 536 / 750 Loss: 0.1899\n",
      "Epoch: 3 / 3, Step: 537 / 750 Loss: 0.0648\n",
      "Epoch: 3 / 3, Step: 538 / 750 Loss: 0.0643\n",
      "Epoch: 3 / 3, Step: 539 / 750 Loss: 0.2078\n",
      "Epoch: 3 / 3, Step: 540 / 750 Loss: 0.0747\n",
      "Epoch: 3 / 3, Step: 541 / 750 Loss: 0.0435\n",
      "Epoch: 3 / 3, Step: 542 / 750 Loss: 0.3058\n",
      "Epoch: 3 / 3, Step: 543 / 750 Loss: 0.2649\n",
      "Epoch: 3 / 3, Step: 544 / 750 Loss: 0.0722\n",
      "Epoch: 3 / 3, Step: 545 / 750 Loss: 0.2527\n",
      "Epoch: 3 / 3, Step: 546 / 750 Loss: 0.2460\n",
      "Epoch: 3 / 3, Step: 547 / 750 Loss: 0.1991\n",
      "Epoch: 3 / 3, Step: 548 / 750 Loss: 0.1422\n",
      "Epoch: 3 / 3, Step: 549 / 750 Loss: 0.2959\n",
      "Epoch: 3 / 3, Step: 550 / 750 Loss: 0.0484\n",
      "Epoch: 3 / 3, Step: 551 / 750 Loss: 0.2633\n",
      "Epoch: 3 / 3, Step: 552 / 750 Loss: 0.1978\n",
      "Epoch: 3 / 3, Step: 553 / 750 Loss: 0.1382\n",
      "Epoch: 3 / 3, Step: 554 / 750 Loss: 0.2195\n",
      "Epoch: 3 / 3, Step: 555 / 750 Loss: 0.2434\n",
      "Epoch: 3 / 3, Step: 556 / 750 Loss: 0.0615\n",
      "Epoch: 3 / 3, Step: 557 / 750 Loss: 0.1301\n",
      "Epoch: 3 / 3, Step: 558 / 750 Loss: 0.0919\n",
      "Epoch: 3 / 3, Step: 559 / 750 Loss: 0.1400\n",
      "Epoch: 3 / 3, Step: 560 / 750 Loss: 0.1558\n",
      "Epoch: 3 / 3, Step: 561 / 750 Loss: 0.0362\n",
      "Epoch: 3 / 3, Step: 562 / 750 Loss: 0.4158\n",
      "Epoch: 3 / 3, Step: 563 / 750 Loss: 0.1844\n",
      "Epoch: 3 / 3, Step: 564 / 750 Loss: 0.0925\n",
      "Epoch: 3 / 3, Step: 565 / 750 Loss: 0.0986\n",
      "Epoch: 3 / 3, Step: 566 / 750 Loss: 0.0522\n",
      "Epoch: 3 / 3, Step: 567 / 750 Loss: 0.0845\n",
      "Epoch: 3 / 3, Step: 568 / 750 Loss: 0.1527\n",
      "Epoch: 3 / 3, Step: 569 / 750 Loss: 0.2679\n",
      "Epoch: 3 / 3, Step: 570 / 750 Loss: 0.0929\n",
      "Epoch: 3 / 3, Step: 571 / 750 Loss: 0.1453\n",
      "Epoch: 3 / 3, Step: 572 / 750 Loss: 0.1442\n",
      "Epoch: 3 / 3, Step: 573 / 750 Loss: 0.0369\n",
      "Epoch: 3 / 3, Step: 574 / 750 Loss: 0.0729\n",
      "Epoch: 3 / 3, Step: 575 / 750 Loss: 0.4624\n",
      "Epoch: 3 / 3, Step: 576 / 750 Loss: 0.1162\n",
      "Epoch: 3 / 3, Step: 577 / 750 Loss: 0.2170\n",
      "Epoch: 3 / 3, Step: 578 / 750 Loss: 0.1341\n",
      "Epoch: 3 / 3, Step: 579 / 750 Loss: 0.0661\n",
      "Epoch: 3 / 3, Step: 580 / 750 Loss: 0.1895\n",
      "Epoch: 3 / 3, Step: 581 / 750 Loss: 0.1950\n",
      "Epoch: 3 / 3, Step: 582 / 750 Loss: 0.1113\n",
      "Epoch: 3 / 3, Step: 583 / 750 Loss: 0.0774\n",
      "Epoch: 3 / 3, Step: 584 / 750 Loss: 0.0809\n",
      "Epoch: 3 / 3, Step: 585 / 750 Loss: 0.1817\n",
      "Epoch: 3 / 3, Step: 586 / 750 Loss: 0.1947\n",
      "Epoch: 3 / 3, Step: 587 / 750 Loss: 0.1463\n",
      "Epoch: 3 / 3, Step: 588 / 750 Loss: 0.2431\n",
      "Epoch: 3 / 3, Step: 589 / 750 Loss: 0.3215\n",
      "Epoch: 3 / 3, Step: 590 / 750 Loss: 0.2215\n",
      "Epoch: 3 / 3, Step: 591 / 750 Loss: 0.0621\n",
      "Epoch: 3 / 3, Step: 592 / 750 Loss: 0.0300\n",
      "Epoch: 3 / 3, Step: 593 / 750 Loss: 0.2231\n",
      "Epoch: 3 / 3, Step: 594 / 750 Loss: 0.2610\n",
      "Epoch: 3 / 3, Step: 595 / 750 Loss: 0.3640\n",
      "Epoch: 3 / 3, Step: 596 / 750 Loss: 0.0406\n",
      "Epoch: 3 / 3, Step: 597 / 750 Loss: 0.2528\n",
      "Epoch: 3 / 3, Step: 598 / 750 Loss: 0.1584\n",
      "Epoch: 3 / 3, Step: 599 / 750 Loss: 0.0523\n",
      "Epoch: 3 / 3, Step: 600 / 750 Loss: 0.0734\n",
      "Epoch: 3 / 3, Step: 601 / 750 Loss: 0.1810\n",
      "Epoch: 3 / 3, Step: 602 / 750 Loss: 0.2359\n",
      "Epoch: 3 / 3, Step: 603 / 750 Loss: 0.2846\n",
      "Epoch: 3 / 3, Step: 604 / 750 Loss: 0.1928\n",
      "Epoch: 3 / 3, Step: 605 / 750 Loss: 0.0951\n",
      "Epoch: 3 / 3, Step: 606 / 750 Loss: 0.1854\n",
      "Epoch: 3 / 3, Step: 607 / 750 Loss: 0.1691\n",
      "Epoch: 3 / 3, Step: 608 / 750 Loss: 0.0490\n",
      "Epoch: 3 / 3, Step: 609 / 750 Loss: 0.0950\n",
      "Epoch: 3 / 3, Step: 610 / 750 Loss: 0.1443\n",
      "Epoch: 3 / 3, Step: 611 / 750 Loss: 0.2918\n",
      "Epoch: 3 / 3, Step: 612 / 750 Loss: 0.0546\n",
      "Epoch: 3 / 3, Step: 613 / 750 Loss: 0.2255\n",
      "Epoch: 3 / 3, Step: 614 / 750 Loss: 0.0681\n",
      "Epoch: 3 / 3, Step: 615 / 750 Loss: 0.0795\n",
      "Epoch: 3 / 3, Step: 616 / 750 Loss: 0.0847\n",
      "Epoch: 3 / 3, Step: 617 / 750 Loss: 0.0576\n",
      "Epoch: 3 / 3, Step: 618 / 750 Loss: 0.2228\n",
      "Epoch: 3 / 3, Step: 619 / 750 Loss: 0.0952\n",
      "Epoch: 3 / 3, Step: 620 / 750 Loss: 0.0238\n",
      "Epoch: 3 / 3, Step: 621 / 750 Loss: 0.1839\n",
      "Epoch: 3 / 3, Step: 622 / 750 Loss: 0.0392\n",
      "Epoch: 3 / 3, Step: 623 / 750 Loss: 0.1235\n",
      "Epoch: 3 / 3, Step: 624 / 750 Loss: 0.0998\n",
      "Epoch: 3 / 3, Step: 625 / 750 Loss: 0.0955\n",
      "Epoch: 3 / 3, Step: 626 / 750 Loss: 0.0435\n",
      "Epoch: 3 / 3, Step: 627 / 750 Loss: 0.0815\n",
      "Epoch: 3 / 3, Step: 628 / 750 Loss: 0.0178\n",
      "Epoch: 3 / 3, Step: 629 / 750 Loss: 0.0701\n",
      "Epoch: 3 / 3, Step: 630 / 750 Loss: 0.1649\n",
      "Epoch: 3 / 3, Step: 631 / 750 Loss: 0.1739\n",
      "Epoch: 3 / 3, Step: 632 / 750 Loss: 0.0886\n",
      "Epoch: 3 / 3, Step: 633 / 750 Loss: 0.0334\n",
      "Epoch: 3 / 3, Step: 634 / 750 Loss: 0.1226\n",
      "Epoch: 3 / 3, Step: 635 / 750 Loss: 0.2574\n",
      "Epoch: 3 / 3, Step: 636 / 750 Loss: 0.1039\n",
      "Epoch: 3 / 3, Step: 637 / 750 Loss: 0.1088\n",
      "Epoch: 3 / 3, Step: 638 / 750 Loss: 0.0535\n",
      "Epoch: 3 / 3, Step: 639 / 750 Loss: 0.0663\n",
      "Epoch: 3 / 3, Step: 640 / 750 Loss: 0.0489\n",
      "Epoch: 3 / 3, Step: 641 / 750 Loss: 0.3060\n",
      "Epoch: 3 / 3, Step: 642 / 750 Loss: 0.2374\n",
      "Epoch: 3 / 3, Step: 643 / 750 Loss: 0.0266\n",
      "Epoch: 3 / 3, Step: 644 / 750 Loss: 0.0217\n",
      "Epoch: 3 / 3, Step: 645 / 750 Loss: 0.3523\n",
      "Epoch: 3 / 3, Step: 646 / 750 Loss: 0.0515\n",
      "Epoch: 3 / 3, Step: 647 / 750 Loss: 0.1440\n",
      "Epoch: 3 / 3, Step: 648 / 750 Loss: 0.1306\n",
      "Epoch: 3 / 3, Step: 649 / 750 Loss: 0.0938\n",
      "Epoch: 3 / 3, Step: 650 / 750 Loss: 0.0233\n",
      "Epoch: 3 / 3, Step: 651 / 750 Loss: 0.3413\n",
      "Epoch: 3 / 3, Step: 652 / 750 Loss: 0.1861\n",
      "Epoch: 3 / 3, Step: 653 / 750 Loss: 0.3357\n",
      "Epoch: 3 / 3, Step: 654 / 750 Loss: 0.1924\n",
      "Epoch: 3 / 3, Step: 655 / 750 Loss: 0.3088\n",
      "Epoch: 3 / 3, Step: 656 / 750 Loss: 0.0158\n",
      "Epoch: 3 / 3, Step: 657 / 750 Loss: 0.0499\n",
      "Epoch: 3 / 3, Step: 658 / 750 Loss: 0.1792\n",
      "Epoch: 3 / 3, Step: 659 / 750 Loss: 0.0780\n",
      "Epoch: 3 / 3, Step: 660 / 750 Loss: 0.0256\n",
      "Epoch: 3 / 3, Step: 661 / 750 Loss: 0.1081\n",
      "Epoch: 3 / 3, Step: 662 / 750 Loss: 0.0646\n",
      "Epoch: 3 / 3, Step: 663 / 750 Loss: 0.0791\n",
      "Epoch: 3 / 3, Step: 664 / 750 Loss: 0.0271\n",
      "Epoch: 3 / 3, Step: 665 / 750 Loss: 0.2945\n",
      "Epoch: 3 / 3, Step: 666 / 750 Loss: 0.2063\n",
      "Epoch: 3 / 3, Step: 667 / 750 Loss: 0.1056\n",
      "Epoch: 3 / 3, Step: 668 / 750 Loss: 0.1054\n",
      "Epoch: 3 / 3, Step: 669 / 750 Loss: 0.0430\n",
      "Epoch: 3 / 3, Step: 670 / 750 Loss: 0.2001\n",
      "Epoch: 3 / 3, Step: 671 / 750 Loss: 0.1515\n",
      "Epoch: 3 / 3, Step: 672 / 750 Loss: 0.2622\n",
      "Epoch: 3 / 3, Step: 673 / 750 Loss: 0.0891\n",
      "Epoch: 3 / 3, Step: 674 / 750 Loss: 0.1361\n",
      "Epoch: 3 / 3, Step: 675 / 750 Loss: 0.1746\n",
      "Epoch: 3 / 3, Step: 676 / 750 Loss: 0.1794\n",
      "Epoch: 3 / 3, Step: 677 / 750 Loss: 0.3148\n",
      "Epoch: 3 / 3, Step: 678 / 750 Loss: 0.3966\n",
      "Epoch: 3 / 3, Step: 679 / 750 Loss: 0.4234\n",
      "Epoch: 3 / 3, Step: 680 / 750 Loss: 0.0308\n",
      "Epoch: 3 / 3, Step: 681 / 750 Loss: 0.0480\n",
      "Epoch: 3 / 3, Step: 682 / 750 Loss: 0.2490\n",
      "Epoch: 3 / 3, Step: 683 / 750 Loss: 0.0891\n",
      "Epoch: 3 / 3, Step: 684 / 750 Loss: 0.2724\n",
      "Epoch: 3 / 3, Step: 685 / 750 Loss: 0.0871\n",
      "Epoch: 3 / 3, Step: 686 / 750 Loss: 0.0642\n",
      "Epoch: 3 / 3, Step: 687 / 750 Loss: 0.0645\n",
      "Epoch: 3 / 3, Step: 688 / 750 Loss: 0.1319\n",
      "Epoch: 3 / 3, Step: 689 / 750 Loss: 0.0878\n",
      "Epoch: 3 / 3, Step: 690 / 750 Loss: 0.1961\n",
      "Epoch: 3 / 3, Step: 691 / 750 Loss: 0.1797\n",
      "Epoch: 3 / 3, Step: 692 / 750 Loss: 0.1358\n",
      "Epoch: 3 / 3, Step: 693 / 750 Loss: 0.1565\n",
      "Epoch: 3 / 3, Step: 694 / 750 Loss: 0.1405\n",
      "Epoch: 3 / 3, Step: 695 / 750 Loss: 0.2276\n",
      "Epoch: 3 / 3, Step: 696 / 750 Loss: 0.0749\n",
      "Epoch: 3 / 3, Step: 697 / 750 Loss: 0.0863\n",
      "Epoch: 3 / 3, Step: 698 / 750 Loss: 0.0595\n",
      "Epoch: 3 / 3, Step: 699 / 750 Loss: 0.1617\n",
      "Epoch: 3 / 3, Step: 700 / 750 Loss: 0.1674\n",
      "Epoch: 3 / 3, Step: 701 / 750 Loss: 0.0424\n",
      "Epoch: 3 / 3, Step: 702 / 750 Loss: 0.0554\n",
      "Epoch: 3 / 3, Step: 703 / 750 Loss: 0.2115\n",
      "Epoch: 3 / 3, Step: 704 / 750 Loss: 0.2713\n",
      "Epoch: 3 / 3, Step: 705 / 750 Loss: 0.2494\n",
      "Epoch: 3 / 3, Step: 706 / 750 Loss: 0.1357\n",
      "Epoch: 3 / 3, Step: 707 / 750 Loss: 0.1967\n",
      "Epoch: 3 / 3, Step: 708 / 750 Loss: 0.1484\n",
      "Epoch: 3 / 3, Step: 709 / 750 Loss: 0.1771\n",
      "Epoch: 3 / 3, Step: 710 / 750 Loss: 0.3504\n",
      "Epoch: 3 / 3, Step: 711 / 750 Loss: 0.0908\n",
      "Epoch: 3 / 3, Step: 712 / 750 Loss: 0.1434\n",
      "Epoch: 3 / 3, Step: 713 / 750 Loss: 0.2162\n",
      "Epoch: 3 / 3, Step: 714 / 750 Loss: 0.1127\n",
      "Epoch: 3 / 3, Step: 715 / 750 Loss: 0.4123\n",
      "Epoch: 3 / 3, Step: 716 / 750 Loss: 0.1897\n",
      "Epoch: 3 / 3, Step: 717 / 750 Loss: 0.0876\n",
      "Epoch: 3 / 3, Step: 718 / 750 Loss: 0.0867\n",
      "Epoch: 3 / 3, Step: 719 / 750 Loss: 0.2211\n",
      "Epoch: 3 / 3, Step: 720 / 750 Loss: 0.0335\n",
      "Epoch: 3 / 3, Step: 721 / 750 Loss: 0.0302\n",
      "Epoch: 3 / 3, Step: 722 / 750 Loss: 0.1738\n",
      "Epoch: 3 / 3, Step: 723 / 750 Loss: 0.0358\n",
      "Epoch: 3 / 3, Step: 724 / 750 Loss: 0.1575\n",
      "Epoch: 3 / 3, Step: 725 / 750 Loss: 0.0887\n",
      "Epoch: 3 / 3, Step: 726 / 750 Loss: 0.1836\n",
      "Epoch: 3 / 3, Step: 727 / 750 Loss: 0.0227\n",
      "Epoch: 3 / 3, Step: 728 / 750 Loss: 0.0521\n",
      "Epoch: 3 / 3, Step: 729 / 750 Loss: 0.1791\n",
      "Epoch: 3 / 3, Step: 730 / 750 Loss: 0.1403\n",
      "Epoch: 3 / 3, Step: 731 / 750 Loss: 0.1647\n",
      "Epoch: 3 / 3, Step: 732 / 750 Loss: 0.0260\n",
      "Epoch: 3 / 3, Step: 733 / 750 Loss: 0.1736\n",
      "Epoch: 3 / 3, Step: 734 / 750 Loss: 0.0771\n",
      "Epoch: 3 / 3, Step: 735 / 750 Loss: 0.1749\n",
      "Epoch: 3 / 3, Step: 736 / 750 Loss: 0.0719\n",
      "Epoch: 3 / 3, Step: 737 / 750 Loss: 0.2272\n",
      "Epoch: 3 / 3, Step: 738 / 750 Loss: 0.1230\n",
      "Epoch: 3 / 3, Step: 739 / 750 Loss: 0.2339\n",
      "Epoch: 3 / 3, Step: 740 / 750 Loss: 0.0500\n",
      "Epoch: 3 / 3, Step: 741 / 750 Loss: 0.0532\n",
      "Epoch: 3 / 3, Step: 742 / 750 Loss: 0.3027\n",
      "Epoch: 3 / 3, Step: 743 / 750 Loss: 0.1309\n",
      "Epoch: 3 / 3, Step: 744 / 750 Loss: 0.0901\n",
      "Epoch: 3 / 3, Step: 745 / 750 Loss: 0.1021\n",
      "Epoch: 3 / 3, Step: 746 / 750 Loss: 0.0294\n",
      "Epoch: 3 / 3, Step: 747 / 750 Loss: 0.3172\n",
      "Epoch: 3 / 3, Step: 748 / 750 Loss: 0.3739\n",
      "Epoch: 3 / 3, Step: 749 / 750 Loss: 0.1790\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "loss_total = 0\n",
    "model.train()\n",
    "for i in range(3):\n",
    "    for j, data in enumerate(train_dataloader):\n",
    "        inputs = {'input_ids': data[0].cuda(), \n",
    "                      'attention_mask': data[1].cuda(), \n",
    "                      'labels': data[2].cuda()}\n",
    "        output = model(**inputs)\n",
    "        loss = output[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Epoch: {} / {}, Step: {} / {} Loss: {:.4f}\".format(i+1, 3, j, len(train_dataloader),\n",
    "                                                                      loss))\n",
    "torch.save(model.state_dict(), 'modelC.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a2f26bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('modelC.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "loss_total = 0\n",
    "results = defaultdict(dict)\n",
    "predictions, true_vals = [], []\n",
    "for j, data in enumerate(test_dataloader):\n",
    "    inputs = {'input_ids': data[0].cuda(), \n",
    "              'attention_mask': data[1].cuda(), \n",
    "              'labels': data[2].cuda()}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "    loss = output[0]\n",
    "    logits = output[1]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    labels = inputs['labels'].cpu().numpy()\n",
    "    loss_total += loss.item()\n",
    "    predictions.append(logits)\n",
    "    true_vals.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "641e375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_vals = np.concatenate(true_vals, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "62b9efea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "preds_flat = np.argmax(predictions, axis = 1).flatten()\n",
    "labels_flat = true_vals.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1db08967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.86      0.91      0.89      1156\n",
      "         pos       0.81      0.84      0.82       456\n",
      "         neu       0.96      0.94      0.95      4388\n",
      "\n",
      "    accuracy                           0.93      6000\n",
      "   macro avg       0.88      0.90      0.89      6000\n",
      "weighted avg       0.93      0.93      0.93      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['neg', 'pos', 'neu']\n",
    "print(classification_report(labels_flat, preds_flat, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669ca81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8deb086",
   "metadata": {},
   "source": [
    "# DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "fc6ecb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertConfig,DistilBertTokenizer,DistilBertModel\n",
    "distil_berttokenizer = DistilBertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b3637cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/a/grad/sanket96/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2322: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = distil_berttokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='train'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')\n",
    "\n",
    "encoded_data_test = distil_berttokenizer.batch_encode_plus(covid_senti[covid_senti.data_type=='test'].tweet.values, add_special_tokens=True,\n",
    "                                                return_attention_mask=True, padding=True,\n",
    "                                                max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6a818c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train set\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(covid_senti[covid_senti.data_type == 'train'].label_num.values)\n",
    "\n",
    "#validation set\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(covid_senti[covid_senti.data_type == 'test'].label_num.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f4d5d172",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.5.output.dense.bias', 'bert.pooler.dense.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.embeddings.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.output.dense.bias', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'cls.predictions.decoder.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.key.bias', 'cls.predictions.transform.dense.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.weight', 'cls.seq_relationship.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.embeddings.LayerNorm.weight', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.embeddings.token_type_embeddings.weight', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.9.attention.self.key.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.pooler.dense.weight', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.self.key.weight', 'cls.seq_relationship.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.bias', 'cls.predictions.transform.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.output.dense.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['transformer.layer.7.ffn.lin1.bias', 'transformer.layer.11.ffn.lin1.weight', 'transformer.layer.9.attention.out_lin.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.10.attention.q_lin.bias', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.9.attention.v_lin.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.10.ffn.lin1.bias', 'transformer.layer.9.ffn.lin1.weight', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.7.ffn.lin2.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.10.ffn.lin2.bias', 'transformer.layer.9.attention.v_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.7.attention.v_lin.weight', 'transformer.layer.8.ffn.lin2.weight', 'transformer.layer.9.output_layer_norm.weight', 'pre_classifier.weight', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.6.attention.out_lin.weight', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.7.output_layer_norm.bias', 'transformer.layer.10.attention.v_lin.weight', 'transformer.layer.7.attention.k_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.8.ffn.lin2.bias', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.6.ffn.lin2.weight', 'transformer.layer.9.attention.q_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.9.sa_layer_norm.bias', 'transformer.layer.11.attention.v_lin.weight', 'classifier.weight', 'transformer.layer.10.attention.out_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.10.attention.q_lin.weight', 'transformer.layer.9.ffn.lin2.bias', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.6.attention.q_lin.bias', 'transformer.layer.6.ffn.lin2.bias', 'transformer.layer.9.attention.k_lin.weight', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.11.attention.out_lin.bias', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.10.attention.v_lin.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.11.ffn.lin1.bias', 'transformer.layer.8.output_layer_norm.bias', 'transformer.layer.6.output_layer_norm.weight', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.9.ffn.lin2.weight', 'transformer.layer.7.attention.out_lin.bias', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.7.ffn.lin2.bias', 'transformer.layer.11.sa_layer_norm.bias', 'transformer.layer.10.attention.k_lin.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.0.ffn.lin1.weight', 'embeddings.LayerNorm.bias', 'transformer.layer.1.attention.q_lin.bias', 'pre_classifier.bias', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.10.sa_layer_norm.bias', 'transformer.layer.7.attention.k_lin.weight', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.9.attention.q_lin.bias', 'transformer.layer.11.sa_layer_norm.weight', 'transformer.layer.6.attention.k_lin.bias', 'transformer.layer.11.output_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.7.output_layer_norm.weight', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.8.attention.v_lin.bias', 'transformer.layer.6.output_layer_norm.bias', 'transformer.layer.9.output_layer_norm.bias', 'transformer.layer.0.ffn.lin2.bias', 'embeddings.position_embeddings.weight', 'transformer.layer.10.sa_layer_norm.weight', 'transformer.layer.11.attention.k_lin.bias', 'transformer.layer.7.attention.out_lin.weight', 'transformer.layer.8.attention.q_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.9.attention.out_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.8.attention.out_lin.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.7.sa_layer_norm.weight', 'transformer.layer.10.attention.out_lin.bias', 'transformer.layer.6.attention.q_lin.weight', 'classifier.bias', 'transformer.layer.10.ffn.lin1.weight', 'transformer.layer.7.sa_layer_norm.bias', 'transformer.layer.9.attention.k_lin.bias', 'transformer.layer.11.output_layer_norm.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.8.ffn.lin1.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.6.attention.out_lin.bias', 'transformer.layer.11.attention.k_lin.weight', 'transformer.layer.11.attention.out_lin.weight', 'transformer.layer.6.attention.v_lin.weight', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.6.sa_layer_norm.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.11.attention.q_lin.bias', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.7.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.8.attention.k_lin.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.10.ffn.lin2.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.8.attention.out_lin.weight', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.6.ffn.lin1.weight', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.10.attention.k_lin.weight', 'transformer.layer.9.sa_layer_norm.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.11.ffn.lin2.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.7.attention.q_lin.bias', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.6.ffn.lin1.bias', 'transformer.layer.8.output_layer_norm.weight', 'transformer.layer.6.sa_layer_norm.bias', 'transformer.layer.8.attention.v_lin.weight', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.9.ffn.lin1.bias', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.11.ffn.lin2.bias', 'transformer.layer.7.attention.v_lin.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.8.ffn.lin1.weight', 'transformer.layer.8.attention.k_lin.bias', 'transformer.layer.8.attention.q_lin.bias', 'transformer.layer.11.attention.q_lin.weight', 'transformer.layer.8.sa_layer_norm.weight', 'embeddings.LayerNorm.weight', 'transformer.layer.5.sa_layer_norm.weight', 'embeddings.word_embeddings.weight', 'transformer.layer.10.output_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.10.output_layer_norm.bias', 'transformer.layer.6.attention.k_lin.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.6.attention.v_lin.bias', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.8.sa_layer_norm.bias', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.11.attention.v_lin.bias', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.7.attention.q_lin.weight', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.4.output_layer_norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(class_dict),\n",
    "                                                     output_attentions = False,\n",
    "                                                      output_hidden_states = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fffd07a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from torch import nn, optim\n",
    "\n",
    "train_dataset = TensorDataset(input_ids_train, attention_masks_train,labels_train)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_masks_test,labels_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=32)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "33de886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2d381d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 3, Step: 0 / 750 Loss: 1.3260\n",
      "Epoch: 1 / 3, Step: 1 / 750 Loss: 0.9201\n",
      "Epoch: 1 / 3, Step: 2 / 750 Loss: 0.6835\n",
      "Epoch: 1 / 3, Step: 3 / 750 Loss: 0.8440\n",
      "Epoch: 1 / 3, Step: 4 / 750 Loss: 0.6767\n",
      "Epoch: 1 / 3, Step: 5 / 750 Loss: 0.6434\n",
      "Epoch: 1 / 3, Step: 6 / 750 Loss: 0.8689\n",
      "Epoch: 1 / 3, Step: 7 / 750 Loss: 0.7314\n",
      "Epoch: 1 / 3, Step: 8 / 750 Loss: 0.6250\n",
      "Epoch: 1 / 3, Step: 9 / 750 Loss: 0.7379\n",
      "Epoch: 1 / 3, Step: 10 / 750 Loss: 0.8152\n",
      "Epoch: 1 / 3, Step: 11 / 750 Loss: 0.8038\n",
      "Epoch: 1 / 3, Step: 12 / 750 Loss: 0.5505\n",
      "Epoch: 1 / 3, Step: 13 / 750 Loss: 0.5375\n",
      "Epoch: 1 / 3, Step: 14 / 750 Loss: 0.7962\n",
      "Epoch: 1 / 3, Step: 15 / 750 Loss: 0.9293\n",
      "Epoch: 1 / 3, Step: 16 / 750 Loss: 0.5782\n",
      "Epoch: 1 / 3, Step: 17 / 750 Loss: 0.8427\n",
      "Epoch: 1 / 3, Step: 18 / 750 Loss: 0.6130\n",
      "Epoch: 1 / 3, Step: 19 / 750 Loss: 0.6882\n",
      "Epoch: 1 / 3, Step: 20 / 750 Loss: 0.7653\n",
      "Epoch: 1 / 3, Step: 21 / 750 Loss: 0.8577\n",
      "Epoch: 1 / 3, Step: 22 / 750 Loss: 0.7319\n",
      "Epoch: 1 / 3, Step: 23 / 750 Loss: 0.6615\n",
      "Epoch: 1 / 3, Step: 24 / 750 Loss: 0.6055\n",
      "Epoch: 1 / 3, Step: 25 / 750 Loss: 0.6882\n",
      "Epoch: 1 / 3, Step: 26 / 750 Loss: 0.9423\n",
      "Epoch: 1 / 3, Step: 27 / 750 Loss: 0.5344\n",
      "Epoch: 1 / 3, Step: 28 / 750 Loss: 0.5374\n",
      "Epoch: 1 / 3, Step: 29 / 750 Loss: 0.6085\n",
      "Epoch: 1 / 3, Step: 30 / 750 Loss: 1.0261\n",
      "Epoch: 1 / 3, Step: 31 / 750 Loss: 0.8426\n",
      "Epoch: 1 / 3, Step: 32 / 750 Loss: 0.3486\n",
      "Epoch: 1 / 3, Step: 33 / 750 Loss: 0.5905\n",
      "Epoch: 1 / 3, Step: 34 / 750 Loss: 0.5892\n",
      "Epoch: 1 / 3, Step: 35 / 750 Loss: 0.7036\n",
      "Epoch: 1 / 3, Step: 36 / 750 Loss: 0.6442\n",
      "Epoch: 1 / 3, Step: 37 / 750 Loss: 0.7235\n",
      "Epoch: 1 / 3, Step: 38 / 750 Loss: 0.9366\n",
      "Epoch: 1 / 3, Step: 39 / 750 Loss: 0.5826\n",
      "Epoch: 1 / 3, Step: 40 / 750 Loss: 0.7753\n",
      "Epoch: 1 / 3, Step: 41 / 750 Loss: 0.7048\n",
      "Epoch: 1 / 3, Step: 42 / 750 Loss: 0.9092\n",
      "Epoch: 1 / 3, Step: 43 / 750 Loss: 0.5952\n",
      "Epoch: 1 / 3, Step: 44 / 750 Loss: 0.7201\n",
      "Epoch: 1 / 3, Step: 45 / 750 Loss: 0.7374\n",
      "Epoch: 1 / 3, Step: 46 / 750 Loss: 0.8553\n",
      "Epoch: 1 / 3, Step: 47 / 750 Loss: 0.6243\n",
      "Epoch: 1 / 3, Step: 48 / 750 Loss: 0.7511\n",
      "Epoch: 1 / 3, Step: 49 / 750 Loss: 0.8410\n",
      "Epoch: 1 / 3, Step: 50 / 750 Loss: 0.7731\n",
      "Epoch: 1 / 3, Step: 51 / 750 Loss: 0.9013\n",
      "Epoch: 1 / 3, Step: 52 / 750 Loss: 0.6701\n",
      "Epoch: 1 / 3, Step: 53 / 750 Loss: 0.7875\n",
      "Epoch: 1 / 3, Step: 54 / 750 Loss: 0.9319\n",
      "Epoch: 1 / 3, Step: 55 / 750 Loss: 0.7894\n",
      "Epoch: 1 / 3, Step: 56 / 750 Loss: 0.7147\n",
      "Epoch: 1 / 3, Step: 57 / 750 Loss: 0.6959\n",
      "Epoch: 1 / 3, Step: 58 / 750 Loss: 0.6403\n",
      "Epoch: 1 / 3, Step: 59 / 750 Loss: 0.6126\n",
      "Epoch: 1 / 3, Step: 60 / 750 Loss: 0.9250\n",
      "Epoch: 1 / 3, Step: 61 / 750 Loss: 0.7451\n",
      "Epoch: 1 / 3, Step: 62 / 750 Loss: 0.7984\n",
      "Epoch: 1 / 3, Step: 63 / 750 Loss: 0.6334\n",
      "Epoch: 1 / 3, Step: 64 / 750 Loss: 0.8314\n",
      "Epoch: 1 / 3, Step: 65 / 750 Loss: 0.8313\n",
      "Epoch: 1 / 3, Step: 66 / 750 Loss: 0.7494\n",
      "Epoch: 1 / 3, Step: 67 / 750 Loss: 0.8429\n",
      "Epoch: 1 / 3, Step: 68 / 750 Loss: 0.9939\n",
      "Epoch: 1 / 3, Step: 69 / 750 Loss: 0.6473\n",
      "Epoch: 1 / 3, Step: 70 / 750 Loss: 0.7827\n",
      "Epoch: 1 / 3, Step: 71 / 750 Loss: 0.7193\n",
      "Epoch: 1 / 3, Step: 72 / 750 Loss: 0.7583\n",
      "Epoch: 1 / 3, Step: 73 / 750 Loss: 0.7159\n",
      "Epoch: 1 / 3, Step: 74 / 750 Loss: 0.8006\n",
      "Epoch: 1 / 3, Step: 75 / 750 Loss: 0.8217\n",
      "Epoch: 1 / 3, Step: 76 / 750 Loss: 0.7279\n",
      "Epoch: 1 / 3, Step: 77 / 750 Loss: 0.5818\n",
      "Epoch: 1 / 3, Step: 78 / 750 Loss: 0.8125\n",
      "Epoch: 1 / 3, Step: 79 / 750 Loss: 0.6830\n",
      "Epoch: 1 / 3, Step: 80 / 750 Loss: 0.8371\n",
      "Epoch: 1 / 3, Step: 81 / 750 Loss: 0.5348\n",
      "Epoch: 1 / 3, Step: 82 / 750 Loss: 0.9017\n",
      "Epoch: 1 / 3, Step: 83 / 750 Loss: 0.7998\n",
      "Epoch: 1 / 3, Step: 84 / 750 Loss: 0.8081\n",
      "Epoch: 1 / 3, Step: 85 / 750 Loss: 0.7995\n",
      "Epoch: 1 / 3, Step: 86 / 750 Loss: 0.6923\n",
      "Epoch: 1 / 3, Step: 87 / 750 Loss: 0.8026\n",
      "Epoch: 1 / 3, Step: 88 / 750 Loss: 0.7305\n",
      "Epoch: 1 / 3, Step: 89 / 750 Loss: 0.5686\n",
      "Epoch: 1 / 3, Step: 90 / 750 Loss: 0.7147\n",
      "Epoch: 1 / 3, Step: 91 / 750 Loss: 0.9021\n",
      "Epoch: 1 / 3, Step: 92 / 750 Loss: 0.7038\n",
      "Epoch: 1 / 3, Step: 93 / 750 Loss: 0.8263\n",
      "Epoch: 1 / 3, Step: 94 / 750 Loss: 0.9440\n",
      "Epoch: 1 / 3, Step: 95 / 750 Loss: 0.6757\n",
      "Epoch: 1 / 3, Step: 96 / 750 Loss: 0.5325\n",
      "Epoch: 1 / 3, Step: 97 / 750 Loss: 0.9990\n",
      "Epoch: 1 / 3, Step: 98 / 750 Loss: 0.8341\n",
      "Epoch: 1 / 3, Step: 99 / 750 Loss: 0.6131\n",
      "Epoch: 1 / 3, Step: 100 / 750 Loss: 0.7939\n",
      "Epoch: 1 / 3, Step: 101 / 750 Loss: 0.6347\n",
      "Epoch: 1 / 3, Step: 102 / 750 Loss: 0.7955\n",
      "Epoch: 1 / 3, Step: 103 / 750 Loss: 0.9442\n",
      "Epoch: 1 / 3, Step: 104 / 750 Loss: 0.9324\n",
      "Epoch: 1 / 3, Step: 105 / 750 Loss: 0.7016\n",
      "Epoch: 1 / 3, Step: 106 / 750 Loss: 0.7881\n",
      "Epoch: 1 / 3, Step: 107 / 750 Loss: 0.6670\n",
      "Epoch: 1 / 3, Step: 108 / 750 Loss: 0.7140\n",
      "Epoch: 1 / 3, Step: 109 / 750 Loss: 0.6683\n",
      "Epoch: 1 / 3, Step: 110 / 750 Loss: 0.8600\n",
      "Epoch: 1 / 3, Step: 111 / 750 Loss: 0.8912\n",
      "Epoch: 1 / 3, Step: 112 / 750 Loss: 0.7414\n",
      "Epoch: 1 / 3, Step: 113 / 750 Loss: 0.6759\n",
      "Epoch: 1 / 3, Step: 114 / 750 Loss: 1.0099\n",
      "Epoch: 1 / 3, Step: 115 / 750 Loss: 0.8568\n",
      "Epoch: 1 / 3, Step: 116 / 750 Loss: 0.8541\n",
      "Epoch: 1 / 3, Step: 117 / 750 Loss: 0.7344\n",
      "Epoch: 1 / 3, Step: 118 / 750 Loss: 0.8789\n",
      "Epoch: 1 / 3, Step: 119 / 750 Loss: 0.9530\n",
      "Epoch: 1 / 3, Step: 120 / 750 Loss: 1.0501\n",
      "Epoch: 1 / 3, Step: 121 / 750 Loss: 0.8860\n",
      "Epoch: 1 / 3, Step: 122 / 750 Loss: 0.7657\n",
      "Epoch: 1 / 3, Step: 123 / 750 Loss: 0.7448\n",
      "Epoch: 1 / 3, Step: 124 / 750 Loss: 0.8643\n",
      "Epoch: 1 / 3, Step: 125 / 750 Loss: 0.7531\n",
      "Epoch: 1 / 3, Step: 126 / 750 Loss: 0.8260\n",
      "Epoch: 1 / 3, Step: 127 / 750 Loss: 0.7594\n",
      "Epoch: 1 / 3, Step: 128 / 750 Loss: 0.5872\n",
      "Epoch: 1 / 3, Step: 129 / 750 Loss: 0.8103\n",
      "Epoch: 1 / 3, Step: 130 / 750 Loss: 0.6335\n",
      "Epoch: 1 / 3, Step: 131 / 750 Loss: 0.9042\n",
      "Epoch: 1 / 3, Step: 132 / 750 Loss: 0.9777\n",
      "Epoch: 1 / 3, Step: 133 / 750 Loss: 1.1974\n",
      "Epoch: 1 / 3, Step: 134 / 750 Loss: 0.6244\n",
      "Epoch: 1 / 3, Step: 135 / 750 Loss: 0.8850\n",
      "Epoch: 1 / 3, Step: 136 / 750 Loss: 0.6087\n",
      "Epoch: 1 / 3, Step: 137 / 750 Loss: 0.7779\n",
      "Epoch: 1 / 3, Step: 138 / 750 Loss: 0.6768\n",
      "Epoch: 1 / 3, Step: 139 / 750 Loss: 0.9040\n",
      "Epoch: 1 / 3, Step: 140 / 750 Loss: 0.5025\n",
      "Epoch: 1 / 3, Step: 141 / 750 Loss: 0.6641\n",
      "Epoch: 1 / 3, Step: 142 / 750 Loss: 0.8525\n",
      "Epoch: 1 / 3, Step: 143 / 750 Loss: 0.8404\n",
      "Epoch: 1 / 3, Step: 144 / 750 Loss: 0.6937\n",
      "Epoch: 1 / 3, Step: 145 / 750 Loss: 0.6951\n",
      "Epoch: 1 / 3, Step: 146 / 750 Loss: 0.7025\n",
      "Epoch: 1 / 3, Step: 147 / 750 Loss: 0.7588\n",
      "Epoch: 1 / 3, Step: 148 / 750 Loss: 0.9049\n",
      "Epoch: 1 / 3, Step: 149 / 750 Loss: 0.6462\n",
      "Epoch: 1 / 3, Step: 150 / 750 Loss: 0.8846\n",
      "Epoch: 1 / 3, Step: 151 / 750 Loss: 0.7926\n",
      "Epoch: 1 / 3, Step: 152 / 750 Loss: 0.8924\n",
      "Epoch: 1 / 3, Step: 153 / 750 Loss: 0.8109\n",
      "Epoch: 1 / 3, Step: 154 / 750 Loss: 0.7627\n",
      "Epoch: 1 / 3, Step: 155 / 750 Loss: 0.8853\n",
      "Epoch: 1 / 3, Step: 156 / 750 Loss: 0.8259\n",
      "Epoch: 1 / 3, Step: 157 / 750 Loss: 0.5984\n",
      "Epoch: 1 / 3, Step: 158 / 750 Loss: 0.6698\n",
      "Epoch: 1 / 3, Step: 159 / 750 Loss: 0.5847\n",
      "Epoch: 1 / 3, Step: 160 / 750 Loss: 0.7049\n",
      "Epoch: 1 / 3, Step: 161 / 750 Loss: 0.5318\n",
      "Epoch: 1 / 3, Step: 162 / 750 Loss: 0.9160\n",
      "Epoch: 1 / 3, Step: 163 / 750 Loss: 0.9838\n",
      "Epoch: 1 / 3, Step: 164 / 750 Loss: 0.7262\n",
      "Epoch: 1 / 3, Step: 165 / 750 Loss: 0.8475\n",
      "Epoch: 1 / 3, Step: 166 / 750 Loss: 0.7882\n",
      "Epoch: 1 / 3, Step: 167 / 750 Loss: 0.6035\n",
      "Epoch: 1 / 3, Step: 168 / 750 Loss: 0.6083\n",
      "Epoch: 1 / 3, Step: 169 / 750 Loss: 0.8300\n",
      "Epoch: 1 / 3, Step: 170 / 750 Loss: 0.8192\n",
      "Epoch: 1 / 3, Step: 171 / 750 Loss: 0.8043\n",
      "Epoch: 1 / 3, Step: 172 / 750 Loss: 0.6315\n",
      "Epoch: 1 / 3, Step: 173 / 750 Loss: 0.6073\n",
      "Epoch: 1 / 3, Step: 174 / 750 Loss: 0.6307\n",
      "Epoch: 1 / 3, Step: 175 / 750 Loss: 0.6704\n",
      "Epoch: 1 / 3, Step: 176 / 750 Loss: 0.7483\n",
      "Epoch: 1 / 3, Step: 177 / 750 Loss: 0.7693\n",
      "Epoch: 1 / 3, Step: 178 / 750 Loss: 0.8125\n",
      "Epoch: 1 / 3, Step: 179 / 750 Loss: 0.6839\n",
      "Epoch: 1 / 3, Step: 180 / 750 Loss: 0.6506\n",
      "Epoch: 1 / 3, Step: 181 / 750 Loss: 0.7803\n",
      "Epoch: 1 / 3, Step: 182 / 750 Loss: 0.5962\n",
      "Epoch: 1 / 3, Step: 183 / 750 Loss: 0.9530\n",
      "Epoch: 1 / 3, Step: 184 / 750 Loss: 0.7588\n",
      "Epoch: 1 / 3, Step: 185 / 750 Loss: 0.7747\n",
      "Epoch: 1 / 3, Step: 186 / 750 Loss: 0.7861\n",
      "Epoch: 1 / 3, Step: 187 / 750 Loss: 0.7483\n",
      "Epoch: 1 / 3, Step: 188 / 750 Loss: 0.6585\n",
      "Epoch: 1 / 3, Step: 189 / 750 Loss: 0.7384\n",
      "Epoch: 1 / 3, Step: 190 / 750 Loss: 0.7538\n",
      "Epoch: 1 / 3, Step: 191 / 750 Loss: 0.5938\n",
      "Epoch: 1 / 3, Step: 192 / 750 Loss: 0.6724\n",
      "Epoch: 1 / 3, Step: 193 / 750 Loss: 0.7302\n",
      "Epoch: 1 / 3, Step: 194 / 750 Loss: 0.9190\n",
      "Epoch: 1 / 3, Step: 195 / 750 Loss: 0.9318\n",
      "Epoch: 1 / 3, Step: 196 / 750 Loss: 0.7463\n",
      "Epoch: 1 / 3, Step: 197 / 750 Loss: 0.7025\n",
      "Epoch: 1 / 3, Step: 198 / 750 Loss: 0.8227\n",
      "Epoch: 1 / 3, Step: 199 / 750 Loss: 0.6837\n",
      "Epoch: 1 / 3, Step: 200 / 750 Loss: 0.8025\n",
      "Epoch: 1 / 3, Step: 201 / 750 Loss: 0.5692\n",
      "Epoch: 1 / 3, Step: 202 / 750 Loss: 0.8048\n",
      "Epoch: 1 / 3, Step: 203 / 750 Loss: 0.9940\n",
      "Epoch: 1 / 3, Step: 204 / 750 Loss: 0.7271\n",
      "Epoch: 1 / 3, Step: 205 / 750 Loss: 0.7293\n",
      "Epoch: 1 / 3, Step: 206 / 750 Loss: 0.7799\n",
      "Epoch: 1 / 3, Step: 207 / 750 Loss: 0.6683\n",
      "Epoch: 1 / 3, Step: 208 / 750 Loss: 0.8247\n",
      "Epoch: 1 / 3, Step: 209 / 750 Loss: 0.7704\n",
      "Epoch: 1 / 3, Step: 210 / 750 Loss: 0.7184\n",
      "Epoch: 1 / 3, Step: 211 / 750 Loss: 0.6146\n",
      "Epoch: 1 / 3, Step: 212 / 750 Loss: 0.7135\n",
      "Epoch: 1 / 3, Step: 213 / 750 Loss: 0.6032\n",
      "Epoch: 1 / 3, Step: 214 / 750 Loss: 1.0224\n",
      "Epoch: 1 / 3, Step: 215 / 750 Loss: 0.9588\n",
      "Epoch: 1 / 3, Step: 216 / 750 Loss: 0.8796\n",
      "Epoch: 1 / 3, Step: 217 / 750 Loss: 0.8468\n",
      "Epoch: 1 / 3, Step: 218 / 750 Loss: 0.6152\n",
      "Epoch: 1 / 3, Step: 219 / 750 Loss: 0.6075\n",
      "Epoch: 1 / 3, Step: 220 / 750 Loss: 0.5338\n",
      "Epoch: 1 / 3, Step: 221 / 750 Loss: 0.7133\n",
      "Epoch: 1 / 3, Step: 222 / 750 Loss: 0.6091\n",
      "Epoch: 1 / 3, Step: 223 / 750 Loss: 0.8678\n",
      "Epoch: 1 / 3, Step: 224 / 750 Loss: 0.7037\n",
      "Epoch: 1 / 3, Step: 225 / 750 Loss: 0.6691\n",
      "Epoch: 1 / 3, Step: 226 / 750 Loss: 0.6052\n",
      "Epoch: 1 / 3, Step: 227 / 750 Loss: 0.7045\n",
      "Epoch: 1 / 3, Step: 228 / 750 Loss: 0.5744\n",
      "Epoch: 1 / 3, Step: 229 / 750 Loss: 0.7647\n",
      "Epoch: 1 / 3, Step: 230 / 750 Loss: 0.6194\n",
      "Epoch: 1 / 3, Step: 231 / 750 Loss: 0.7111\n",
      "Epoch: 1 / 3, Step: 232 / 750 Loss: 0.8129\n",
      "Epoch: 1 / 3, Step: 233 / 750 Loss: 0.6629\n",
      "Epoch: 1 / 3, Step: 234 / 750 Loss: 0.4935\n",
      "Epoch: 1 / 3, Step: 235 / 750 Loss: 0.5497\n",
      "Epoch: 1 / 3, Step: 236 / 750 Loss: 0.5403\n",
      "Epoch: 1 / 3, Step: 237 / 750 Loss: 0.5763\n",
      "Epoch: 1 / 3, Step: 238 / 750 Loss: 0.8331\n",
      "Epoch: 1 / 3, Step: 239 / 750 Loss: 0.4272\n",
      "Epoch: 1 / 3, Step: 240 / 750 Loss: 0.8476\n",
      "Epoch: 1 / 3, Step: 241 / 750 Loss: 0.5201\n",
      "Epoch: 1 / 3, Step: 242 / 750 Loss: 0.6400\n",
      "Epoch: 1 / 3, Step: 243 / 750 Loss: 0.6063\n",
      "Epoch: 1 / 3, Step: 244 / 750 Loss: 0.8876\n",
      "Epoch: 1 / 3, Step: 245 / 750 Loss: 0.9362\n",
      "Epoch: 1 / 3, Step: 246 / 750 Loss: 0.9900\n",
      "Epoch: 1 / 3, Step: 247 / 750 Loss: 0.6636\n",
      "Epoch: 1 / 3, Step: 248 / 750 Loss: 0.6740\n",
      "Epoch: 1 / 3, Step: 249 / 750 Loss: 0.6238\n",
      "Epoch: 1 / 3, Step: 250 / 750 Loss: 0.5914\n",
      "Epoch: 1 / 3, Step: 251 / 750 Loss: 0.8359\n",
      "Epoch: 1 / 3, Step: 252 / 750 Loss: 0.6481\n",
      "Epoch: 1 / 3, Step: 253 / 750 Loss: 0.6192\n",
      "Epoch: 1 / 3, Step: 254 / 750 Loss: 0.6708\n",
      "Epoch: 1 / 3, Step: 255 / 750 Loss: 0.8029\n",
      "Epoch: 1 / 3, Step: 256 / 750 Loss: 0.5136\n",
      "Epoch: 1 / 3, Step: 257 / 750 Loss: 0.6827\n",
      "Epoch: 1 / 3, Step: 258 / 750 Loss: 0.7293\n",
      "Epoch: 1 / 3, Step: 259 / 750 Loss: 0.7560\n",
      "Epoch: 1 / 3, Step: 260 / 750 Loss: 0.5712\n",
      "Epoch: 1 / 3, Step: 261 / 750 Loss: 0.4018\n",
      "Epoch: 1 / 3, Step: 262 / 750 Loss: 0.7135\n",
      "Epoch: 1 / 3, Step: 263 / 750 Loss: 0.7741\n",
      "Epoch: 1 / 3, Step: 264 / 750 Loss: 0.8322\n",
      "Epoch: 1 / 3, Step: 265 / 750 Loss: 0.7567\n",
      "Epoch: 1 / 3, Step: 266 / 750 Loss: 0.8058\n",
      "Epoch: 1 / 3, Step: 267 / 750 Loss: 0.9517\n",
      "Epoch: 1 / 3, Step: 268 / 750 Loss: 0.8998\n",
      "Epoch: 1 / 3, Step: 269 / 750 Loss: 0.9184\n",
      "Epoch: 1 / 3, Step: 270 / 750 Loss: 0.6752\n",
      "Epoch: 1 / 3, Step: 271 / 750 Loss: 0.6055\n",
      "Epoch: 1 / 3, Step: 272 / 750 Loss: 0.7865\n",
      "Epoch: 1 / 3, Step: 273 / 750 Loss: 0.6676\n",
      "Epoch: 1 / 3, Step: 274 / 750 Loss: 0.8264\n",
      "Epoch: 1 / 3, Step: 275 / 750 Loss: 0.8035\n",
      "Epoch: 1 / 3, Step: 276 / 750 Loss: 0.9818\n",
      "Epoch: 1 / 3, Step: 277 / 750 Loss: 0.7189\n",
      "Epoch: 1 / 3, Step: 278 / 750 Loss: 0.7700\n",
      "Epoch: 1 / 3, Step: 279 / 750 Loss: 0.7641\n",
      "Epoch: 1 / 3, Step: 280 / 750 Loss: 0.6153\n",
      "Epoch: 1 / 3, Step: 281 / 750 Loss: 0.5806\n",
      "Epoch: 1 / 3, Step: 282 / 750 Loss: 0.8188\n",
      "Epoch: 1 / 3, Step: 283 / 750 Loss: 0.6703\n",
      "Epoch: 1 / 3, Step: 284 / 750 Loss: 0.6838\n",
      "Epoch: 1 / 3, Step: 285 / 750 Loss: 1.1014\n",
      "Epoch: 1 / 3, Step: 286 / 750 Loss: 0.6785\n",
      "Epoch: 1 / 3, Step: 287 / 750 Loss: 0.6407\n",
      "Epoch: 1 / 3, Step: 288 / 750 Loss: 0.6833\n",
      "Epoch: 1 / 3, Step: 289 / 750 Loss: 0.6472\n",
      "Epoch: 1 / 3, Step: 290 / 750 Loss: 0.6823\n",
      "Epoch: 1 / 3, Step: 291 / 750 Loss: 0.7842\n",
      "Epoch: 1 / 3, Step: 292 / 750 Loss: 0.8012\n",
      "Epoch: 1 / 3, Step: 293 / 750 Loss: 0.6562\n",
      "Epoch: 1 / 3, Step: 294 / 750 Loss: 0.6337\n",
      "Epoch: 1 / 3, Step: 295 / 750 Loss: 0.4577\n",
      "Epoch: 1 / 3, Step: 296 / 750 Loss: 0.6155\n",
      "Epoch: 1 / 3, Step: 297 / 750 Loss: 0.9852\n",
      "Epoch: 1 / 3, Step: 298 / 750 Loss: 0.7849\n",
      "Epoch: 1 / 3, Step: 299 / 750 Loss: 0.7284\n",
      "Epoch: 1 / 3, Step: 300 / 750 Loss: 0.6416\n",
      "Epoch: 1 / 3, Step: 301 / 750 Loss: 0.4935\n",
      "Epoch: 1 / 3, Step: 302 / 750 Loss: 0.7286\n",
      "Epoch: 1 / 3, Step: 303 / 750 Loss: 0.5455\n",
      "Epoch: 1 / 3, Step: 304 / 750 Loss: 0.6570\n",
      "Epoch: 1 / 3, Step: 305 / 750 Loss: 1.0558\n",
      "Epoch: 1 / 3, Step: 306 / 750 Loss: 0.9103\n",
      "Epoch: 1 / 3, Step: 307 / 750 Loss: 0.8503\n",
      "Epoch: 1 / 3, Step: 308 / 750 Loss: 0.5405\n",
      "Epoch: 1 / 3, Step: 309 / 750 Loss: 0.7229\n",
      "Epoch: 1 / 3, Step: 310 / 750 Loss: 0.7943\n",
      "Epoch: 1 / 3, Step: 311 / 750 Loss: 0.6260\n",
      "Epoch: 1 / 3, Step: 312 / 750 Loss: 0.8618\n",
      "Epoch: 1 / 3, Step: 313 / 750 Loss: 0.5286\n",
      "Epoch: 1 / 3, Step: 314 / 750 Loss: 0.8307\n",
      "Epoch: 1 / 3, Step: 315 / 750 Loss: 0.5939\n",
      "Epoch: 1 / 3, Step: 316 / 750 Loss: 1.1137\n",
      "Epoch: 1 / 3, Step: 317 / 750 Loss: 0.5899\n",
      "Epoch: 1 / 3, Step: 318 / 750 Loss: 0.6683\n",
      "Epoch: 1 / 3, Step: 319 / 750 Loss: 0.9412\n",
      "Epoch: 1 / 3, Step: 320 / 750 Loss: 0.6793\n",
      "Epoch: 1 / 3, Step: 321 / 750 Loss: 0.6813\n",
      "Epoch: 1 / 3, Step: 322 / 750 Loss: 0.5560\n",
      "Epoch: 1 / 3, Step: 323 / 750 Loss: 0.7108\n",
      "Epoch: 1 / 3, Step: 324 / 750 Loss: 0.9005\n",
      "Epoch: 1 / 3, Step: 325 / 750 Loss: 0.7931\n",
      "Epoch: 1 / 3, Step: 326 / 750 Loss: 0.8074\n",
      "Epoch: 1 / 3, Step: 327 / 750 Loss: 0.6286\n",
      "Epoch: 1 / 3, Step: 328 / 750 Loss: 0.7752\n",
      "Epoch: 1 / 3, Step: 329 / 750 Loss: 0.7470\n",
      "Epoch: 1 / 3, Step: 330 / 750 Loss: 0.7549\n",
      "Epoch: 1 / 3, Step: 331 / 750 Loss: 0.9383\n",
      "Epoch: 1 / 3, Step: 332 / 750 Loss: 0.6771\n",
      "Epoch: 1 / 3, Step: 333 / 750 Loss: 0.6033\n",
      "Epoch: 1 / 3, Step: 334 / 750 Loss: 0.6101\n",
      "Epoch: 1 / 3, Step: 335 / 750 Loss: 0.6903\n",
      "Epoch: 1 / 3, Step: 336 / 750 Loss: 0.6563\n",
      "Epoch: 1 / 3, Step: 337 / 750 Loss: 0.7254\n",
      "Epoch: 1 / 3, Step: 338 / 750 Loss: 0.8045\n",
      "Epoch: 1 / 3, Step: 339 / 750 Loss: 0.5887\n",
      "Epoch: 1 / 3, Step: 340 / 750 Loss: 0.8144\n",
      "Epoch: 1 / 3, Step: 341 / 750 Loss: 0.7322\n",
      "Epoch: 1 / 3, Step: 342 / 750 Loss: 0.5751\n",
      "Epoch: 1 / 3, Step: 343 / 750 Loss: 0.6188\n",
      "Epoch: 1 / 3, Step: 344 / 750 Loss: 0.4593\n",
      "Epoch: 1 / 3, Step: 345 / 750 Loss: 0.7000\n",
      "Epoch: 1 / 3, Step: 346 / 750 Loss: 0.4653\n",
      "Epoch: 1 / 3, Step: 347 / 750 Loss: 0.8001\n",
      "Epoch: 1 / 3, Step: 348 / 750 Loss: 0.6789\n",
      "Epoch: 1 / 3, Step: 349 / 750 Loss: 0.5648\n",
      "Epoch: 1 / 3, Step: 350 / 750 Loss: 0.9499\n",
      "Epoch: 1 / 3, Step: 351 / 750 Loss: 0.6221\n",
      "Epoch: 1 / 3, Step: 352 / 750 Loss: 0.6599\n",
      "Epoch: 1 / 3, Step: 353 / 750 Loss: 0.7501\n",
      "Epoch: 1 / 3, Step: 354 / 750 Loss: 0.5104\n",
      "Epoch: 1 / 3, Step: 355 / 750 Loss: 0.6285\n",
      "Epoch: 1 / 3, Step: 356 / 750 Loss: 0.7006\n",
      "Epoch: 1 / 3, Step: 357 / 750 Loss: 0.9217\n",
      "Epoch: 1 / 3, Step: 358 / 750 Loss: 0.7032\n",
      "Epoch: 1 / 3, Step: 359 / 750 Loss: 0.7722\n",
      "Epoch: 1 / 3, Step: 360 / 750 Loss: 0.6080\n",
      "Epoch: 1 / 3, Step: 361 / 750 Loss: 0.8678\n",
      "Epoch: 1 / 3, Step: 362 / 750 Loss: 0.7487\n",
      "Epoch: 1 / 3, Step: 363 / 750 Loss: 0.6739\n",
      "Epoch: 1 / 3, Step: 364 / 750 Loss: 0.5723\n",
      "Epoch: 1 / 3, Step: 365 / 750 Loss: 0.6682\n",
      "Epoch: 1 / 3, Step: 366 / 750 Loss: 0.5124\n",
      "Epoch: 1 / 3, Step: 367 / 750 Loss: 0.8915\n",
      "Epoch: 1 / 3, Step: 368 / 750 Loss: 0.6701\n",
      "Epoch: 1 / 3, Step: 369 / 750 Loss: 0.6327\n",
      "Epoch: 1 / 3, Step: 370 / 750 Loss: 0.7033\n",
      "Epoch: 1 / 3, Step: 371 / 750 Loss: 0.7909\n",
      "Epoch: 1 / 3, Step: 372 / 750 Loss: 0.7799\n",
      "Epoch: 1 / 3, Step: 373 / 750 Loss: 0.5522\n",
      "Epoch: 1 / 3, Step: 374 / 750 Loss: 0.7394\n",
      "Epoch: 1 / 3, Step: 375 / 750 Loss: 0.9176\n",
      "Epoch: 1 / 3, Step: 376 / 750 Loss: 0.7474\n",
      "Epoch: 1 / 3, Step: 377 / 750 Loss: 0.6727\n",
      "Epoch: 1 / 3, Step: 378 / 750 Loss: 0.7260\n",
      "Epoch: 1 / 3, Step: 379 / 750 Loss: 0.7455\n",
      "Epoch: 1 / 3, Step: 380 / 750 Loss: 0.6351\n",
      "Epoch: 1 / 3, Step: 381 / 750 Loss: 0.5059\n",
      "Epoch: 1 / 3, Step: 382 / 750 Loss: 0.5685\n",
      "Epoch: 1 / 3, Step: 383 / 750 Loss: 0.7864\n",
      "Epoch: 1 / 3, Step: 384 / 750 Loss: 1.0551\n",
      "Epoch: 1 / 3, Step: 385 / 750 Loss: 0.6762\n",
      "Epoch: 1 / 3, Step: 386 / 750 Loss: 0.5512\n",
      "Epoch: 1 / 3, Step: 387 / 750 Loss: 0.7729\n",
      "Epoch: 1 / 3, Step: 388 / 750 Loss: 1.0455\n",
      "Epoch: 1 / 3, Step: 389 / 750 Loss: 0.8849\n",
      "Epoch: 1 / 3, Step: 390 / 750 Loss: 0.8012\n",
      "Epoch: 1 / 3, Step: 391 / 750 Loss: 0.7185\n",
      "Epoch: 1 / 3, Step: 392 / 750 Loss: 0.6338\n",
      "Epoch: 1 / 3, Step: 393 / 750 Loss: 0.8115\n",
      "Epoch: 1 / 3, Step: 394 / 750 Loss: 0.6245\n",
      "Epoch: 1 / 3, Step: 395 / 750 Loss: 0.9738\n",
      "Epoch: 1 / 3, Step: 396 / 750 Loss: 0.7547\n",
      "Epoch: 1 / 3, Step: 397 / 750 Loss: 0.6752\n",
      "Epoch: 1 / 3, Step: 398 / 750 Loss: 0.8376\n",
      "Epoch: 1 / 3, Step: 399 / 750 Loss: 0.7092\n",
      "Epoch: 1 / 3, Step: 400 / 750 Loss: 0.6858\n",
      "Epoch: 1 / 3, Step: 401 / 750 Loss: 0.7158\n",
      "Epoch: 1 / 3, Step: 402 / 750 Loss: 0.6301\n",
      "Epoch: 1 / 3, Step: 403 / 750 Loss: 0.8425\n",
      "Epoch: 1 / 3, Step: 404 / 750 Loss: 0.7777\n",
      "Epoch: 1 / 3, Step: 405 / 750 Loss: 0.8298\n",
      "Epoch: 1 / 3, Step: 406 / 750 Loss: 0.6232\n",
      "Epoch: 1 / 3, Step: 407 / 750 Loss: 0.5741\n",
      "Epoch: 1 / 3, Step: 408 / 750 Loss: 1.1347\n",
      "Epoch: 1 / 3, Step: 409 / 750 Loss: 0.5182\n",
      "Epoch: 1 / 3, Step: 410 / 750 Loss: 0.8026\n",
      "Epoch: 1 / 3, Step: 411 / 750 Loss: 0.7738\n",
      "Epoch: 1 / 3, Step: 412 / 750 Loss: 0.8107\n",
      "Epoch: 1 / 3, Step: 413 / 750 Loss: 0.5886\n",
      "Epoch: 1 / 3, Step: 414 / 750 Loss: 0.6191\n",
      "Epoch: 1 / 3, Step: 415 / 750 Loss: 0.5710\n",
      "Epoch: 1 / 3, Step: 416 / 750 Loss: 0.9439\n",
      "Epoch: 1 / 3, Step: 417 / 750 Loss: 0.8524\n",
      "Epoch: 1 / 3, Step: 418 / 750 Loss: 0.9109\n",
      "Epoch: 1 / 3, Step: 419 / 750 Loss: 0.7879\n",
      "Epoch: 1 / 3, Step: 420 / 750 Loss: 0.8501\n",
      "Epoch: 1 / 3, Step: 421 / 750 Loss: 0.9328\n",
      "Epoch: 1 / 3, Step: 422 / 750 Loss: 0.8179\n",
      "Epoch: 1 / 3, Step: 423 / 750 Loss: 0.6832\n",
      "Epoch: 1 / 3, Step: 424 / 750 Loss: 0.7374\n",
      "Epoch: 1 / 3, Step: 425 / 750 Loss: 0.8754\n",
      "Epoch: 1 / 3, Step: 426 / 750 Loss: 0.6645\n",
      "Epoch: 1 / 3, Step: 427 / 750 Loss: 0.7253\n",
      "Epoch: 1 / 3, Step: 428 / 750 Loss: 0.7561\n",
      "Epoch: 1 / 3, Step: 429 / 750 Loss: 0.9280\n",
      "Epoch: 1 / 3, Step: 430 / 750 Loss: 0.5856\n",
      "Epoch: 1 / 3, Step: 431 / 750 Loss: 0.7623\n",
      "Epoch: 1 / 3, Step: 432 / 750 Loss: 0.6766\n",
      "Epoch: 1 / 3, Step: 433 / 750 Loss: 0.5722\n",
      "Epoch: 1 / 3, Step: 434 / 750 Loss: 0.4821\n",
      "Epoch: 1 / 3, Step: 435 / 750 Loss: 0.7070\n",
      "Epoch: 1 / 3, Step: 436 / 750 Loss: 0.5504\n",
      "Epoch: 1 / 3, Step: 437 / 750 Loss: 0.8617\n",
      "Epoch: 1 / 3, Step: 438 / 750 Loss: 0.8996\n",
      "Epoch: 1 / 3, Step: 439 / 750 Loss: 0.9464\n",
      "Epoch: 1 / 3, Step: 440 / 750 Loss: 0.6971\n",
      "Epoch: 1 / 3, Step: 441 / 750 Loss: 0.6361\n",
      "Epoch: 1 / 3, Step: 442 / 750 Loss: 0.6968\n",
      "Epoch: 1 / 3, Step: 443 / 750 Loss: 0.8759\n",
      "Epoch: 1 / 3, Step: 444 / 750 Loss: 0.5539\n",
      "Epoch: 1 / 3, Step: 445 / 750 Loss: 0.7369\n",
      "Epoch: 1 / 3, Step: 446 / 750 Loss: 0.7375\n",
      "Epoch: 1 / 3, Step: 447 / 750 Loss: 0.4514\n",
      "Epoch: 1 / 3, Step: 448 / 750 Loss: 0.9760\n",
      "Epoch: 1 / 3, Step: 449 / 750 Loss: 0.7148\n",
      "Epoch: 1 / 3, Step: 450 / 750 Loss: 0.9292\n",
      "Epoch: 1 / 3, Step: 451 / 750 Loss: 0.5590\n",
      "Epoch: 1 / 3, Step: 452 / 750 Loss: 0.6094\n",
      "Epoch: 1 / 3, Step: 453 / 750 Loss: 0.4697\n",
      "Epoch: 1 / 3, Step: 454 / 750 Loss: 0.7388\n",
      "Epoch: 1 / 3, Step: 455 / 750 Loss: 0.7041\n",
      "Epoch: 1 / 3, Step: 456 / 750 Loss: 0.6549\n",
      "Epoch: 1 / 3, Step: 457 / 750 Loss: 0.6948\n",
      "Epoch: 1 / 3, Step: 458 / 750 Loss: 0.8755\n",
      "Epoch: 1 / 3, Step: 459 / 750 Loss: 0.8707\n",
      "Epoch: 1 / 3, Step: 460 / 750 Loss: 0.6735\n",
      "Epoch: 1 / 3, Step: 461 / 750 Loss: 0.7746\n",
      "Epoch: 1 / 3, Step: 462 / 750 Loss: 0.7111\n",
      "Epoch: 1 / 3, Step: 463 / 750 Loss: 0.8084\n",
      "Epoch: 1 / 3, Step: 464 / 750 Loss: 0.6159\n",
      "Epoch: 1 / 3, Step: 465 / 750 Loss: 0.7688\n",
      "Epoch: 1 / 3, Step: 466 / 750 Loss: 0.7523\n",
      "Epoch: 1 / 3, Step: 467 / 750 Loss: 0.6940\n",
      "Epoch: 1 / 3, Step: 468 / 750 Loss: 0.7542\n",
      "Epoch: 1 / 3, Step: 469 / 750 Loss: 0.5843\n",
      "Epoch: 1 / 3, Step: 470 / 750 Loss: 0.5654\n",
      "Epoch: 1 / 3, Step: 471 / 750 Loss: 0.7974\n",
      "Epoch: 1 / 3, Step: 472 / 750 Loss: 0.4365\n",
      "Epoch: 1 / 3, Step: 473 / 750 Loss: 0.7644\n",
      "Epoch: 1 / 3, Step: 474 / 750 Loss: 0.8085\n",
      "Epoch: 1 / 3, Step: 475 / 750 Loss: 0.8771\n",
      "Epoch: 1 / 3, Step: 476 / 750 Loss: 1.0306\n",
      "Epoch: 1 / 3, Step: 477 / 750 Loss: 0.4057\n",
      "Epoch: 1 / 3, Step: 478 / 750 Loss: 0.6716\n",
      "Epoch: 1 / 3, Step: 479 / 750 Loss: 0.8581\n",
      "Epoch: 1 / 3, Step: 480 / 750 Loss: 0.6859\n",
      "Epoch: 1 / 3, Step: 481 / 750 Loss: 0.7075\n",
      "Epoch: 1 / 3, Step: 482 / 750 Loss: 0.7000\n",
      "Epoch: 1 / 3, Step: 483 / 750 Loss: 0.5792\n",
      "Epoch: 1 / 3, Step: 484 / 750 Loss: 0.9035\n",
      "Epoch: 1 / 3, Step: 485 / 750 Loss: 0.5297\n",
      "Epoch: 1 / 3, Step: 486 / 750 Loss: 0.4458\n",
      "Epoch: 1 / 3, Step: 487 / 750 Loss: 0.6641\n",
      "Epoch: 1 / 3, Step: 488 / 750 Loss: 0.8263\n",
      "Epoch: 1 / 3, Step: 489 / 750 Loss: 0.6162\n",
      "Epoch: 1 / 3, Step: 490 / 750 Loss: 0.7404\n",
      "Epoch: 1 / 3, Step: 491 / 750 Loss: 0.6336\n",
      "Epoch: 1 / 3, Step: 492 / 750 Loss: 0.7903\n",
      "Epoch: 1 / 3, Step: 493 / 750 Loss: 0.6167\n",
      "Epoch: 1 / 3, Step: 494 / 750 Loss: 0.6666\n",
      "Epoch: 1 / 3, Step: 495 / 750 Loss: 0.8637\n",
      "Epoch: 1 / 3, Step: 496 / 750 Loss: 0.6717\n",
      "Epoch: 1 / 3, Step: 497 / 750 Loss: 0.6447\n",
      "Epoch: 1 / 3, Step: 498 / 750 Loss: 0.8472\n",
      "Epoch: 1 / 3, Step: 499 / 750 Loss: 0.7720\n",
      "Epoch: 1 / 3, Step: 500 / 750 Loss: 0.8108\n",
      "Epoch: 1 / 3, Step: 501 / 750 Loss: 0.7041\n",
      "Epoch: 1 / 3, Step: 502 / 750 Loss: 0.6153\n",
      "Epoch: 1 / 3, Step: 503 / 750 Loss: 0.7899\n",
      "Epoch: 1 / 3, Step: 504 / 750 Loss: 0.6322\n",
      "Epoch: 1 / 3, Step: 505 / 750 Loss: 0.6394\n",
      "Epoch: 1 / 3, Step: 506 / 750 Loss: 0.5160\n",
      "Epoch: 1 / 3, Step: 507 / 750 Loss: 0.4687\n",
      "Epoch: 1 / 3, Step: 508 / 750 Loss: 0.4121\n",
      "Epoch: 1 / 3, Step: 509 / 750 Loss: 0.5109\n",
      "Epoch: 1 / 3, Step: 510 / 750 Loss: 0.6319\n",
      "Epoch: 1 / 3, Step: 511 / 750 Loss: 0.7590\n",
      "Epoch: 1 / 3, Step: 512 / 750 Loss: 0.9850\n",
      "Epoch: 1 / 3, Step: 513 / 750 Loss: 0.6876\n",
      "Epoch: 1 / 3, Step: 514 / 750 Loss: 0.7314\n",
      "Epoch: 1 / 3, Step: 515 / 750 Loss: 0.7920\n",
      "Epoch: 1 / 3, Step: 516 / 750 Loss: 0.9687\n",
      "Epoch: 1 / 3, Step: 517 / 750 Loss: 0.7469\n",
      "Epoch: 1 / 3, Step: 518 / 750 Loss: 0.7805\n",
      "Epoch: 1 / 3, Step: 519 / 750 Loss: 0.8224\n",
      "Epoch: 1 / 3, Step: 520 / 750 Loss: 0.7236\n",
      "Epoch: 1 / 3, Step: 521 / 750 Loss: 0.6512\n",
      "Epoch: 1 / 3, Step: 522 / 750 Loss: 0.7895\n",
      "Epoch: 1 / 3, Step: 523 / 750 Loss: 0.6029\n",
      "Epoch: 1 / 3, Step: 524 / 750 Loss: 0.5020\n",
      "Epoch: 1 / 3, Step: 525 / 750 Loss: 0.8867\n",
      "Epoch: 1 / 3, Step: 526 / 750 Loss: 0.6504\n",
      "Epoch: 1 / 3, Step: 527 / 750 Loss: 0.6577\n",
      "Epoch: 1 / 3, Step: 528 / 750 Loss: 0.4792\n",
      "Epoch: 1 / 3, Step: 529 / 750 Loss: 0.5454\n",
      "Epoch: 1 / 3, Step: 530 / 750 Loss: 0.8498\n",
      "Epoch: 1 / 3, Step: 531 / 750 Loss: 0.6371\n",
      "Epoch: 1 / 3, Step: 532 / 750 Loss: 0.5424\n",
      "Epoch: 1 / 3, Step: 533 / 750 Loss: 0.6474\n",
      "Epoch: 1 / 3, Step: 534 / 750 Loss: 0.3243\n",
      "Epoch: 1 / 3, Step: 535 / 750 Loss: 0.7093\n",
      "Epoch: 1 / 3, Step: 536 / 750 Loss: 0.7280\n",
      "Epoch: 1 / 3, Step: 537 / 750 Loss: 0.8305\n",
      "Epoch: 1 / 3, Step: 538 / 750 Loss: 0.6842\n",
      "Epoch: 1 / 3, Step: 539 / 750 Loss: 0.8542\n",
      "Epoch: 1 / 3, Step: 540 / 750 Loss: 0.8019\n",
      "Epoch: 1 / 3, Step: 541 / 750 Loss: 0.5391\n",
      "Epoch: 1 / 3, Step: 542 / 750 Loss: 0.5688\n",
      "Epoch: 1 / 3, Step: 543 / 750 Loss: 0.6485\n",
      "Epoch: 1 / 3, Step: 544 / 750 Loss: 0.5900\n",
      "Epoch: 1 / 3, Step: 545 / 750 Loss: 0.8072\n",
      "Epoch: 1 / 3, Step: 546 / 750 Loss: 0.8101\n",
      "Epoch: 1 / 3, Step: 547 / 750 Loss: 0.8060\n",
      "Epoch: 1 / 3, Step: 548 / 750 Loss: 0.6411\n",
      "Epoch: 1 / 3, Step: 549 / 750 Loss: 0.6683\n",
      "Epoch: 1 / 3, Step: 550 / 750 Loss: 0.5365\n",
      "Epoch: 1 / 3, Step: 551 / 750 Loss: 0.9336\n",
      "Epoch: 1 / 3, Step: 552 / 750 Loss: 0.6673\n",
      "Epoch: 1 / 3, Step: 553 / 750 Loss: 0.7281\n",
      "Epoch: 1 / 3, Step: 554 / 750 Loss: 0.7959\n",
      "Epoch: 1 / 3, Step: 555 / 750 Loss: 0.6844\n",
      "Epoch: 1 / 3, Step: 556 / 750 Loss: 0.5917\n",
      "Epoch: 1 / 3, Step: 557 / 750 Loss: 0.5854\n",
      "Epoch: 1 / 3, Step: 558 / 750 Loss: 0.7181\n",
      "Epoch: 1 / 3, Step: 559 / 750 Loss: 0.9420\n",
      "Epoch: 1 / 3, Step: 560 / 750 Loss: 0.6732\n",
      "Epoch: 1 / 3, Step: 561 / 750 Loss: 0.5681\n",
      "Epoch: 1 / 3, Step: 562 / 750 Loss: 0.6064\n",
      "Epoch: 1 / 3, Step: 563 / 750 Loss: 0.6964\n",
      "Epoch: 1 / 3, Step: 564 / 750 Loss: 0.8251\n",
      "Epoch: 1 / 3, Step: 565 / 750 Loss: 0.6850\n",
      "Epoch: 1 / 3, Step: 566 / 750 Loss: 0.7073\n",
      "Epoch: 1 / 3, Step: 567 / 750 Loss: 0.8063\n",
      "Epoch: 1 / 3, Step: 568 / 750 Loss: 0.7048\n",
      "Epoch: 1 / 3, Step: 569 / 750 Loss: 0.5342\n",
      "Epoch: 1 / 3, Step: 570 / 750 Loss: 0.9582\n",
      "Epoch: 1 / 3, Step: 571 / 750 Loss: 0.7013\n",
      "Epoch: 1 / 3, Step: 572 / 750 Loss: 0.6448\n",
      "Epoch: 1 / 3, Step: 573 / 750 Loss: 0.5539\n",
      "Epoch: 1 / 3, Step: 574 / 750 Loss: 0.5260\n",
      "Epoch: 1 / 3, Step: 575 / 750 Loss: 0.6136\n",
      "Epoch: 1 / 3, Step: 576 / 750 Loss: 0.8512\n",
      "Epoch: 1 / 3, Step: 577 / 750 Loss: 0.7978\n",
      "Epoch: 1 / 3, Step: 578 / 750 Loss: 0.4629\n",
      "Epoch: 1 / 3, Step: 579 / 750 Loss: 0.9685\n",
      "Epoch: 1 / 3, Step: 580 / 750 Loss: 0.7376\n",
      "Epoch: 1 / 3, Step: 581 / 750 Loss: 0.5512\n",
      "Epoch: 1 / 3, Step: 582 / 750 Loss: 0.8243\n",
      "Epoch: 1 / 3, Step: 583 / 750 Loss: 0.6201\n",
      "Epoch: 1 / 3, Step: 584 / 750 Loss: 0.6423\n",
      "Epoch: 1 / 3, Step: 585 / 750 Loss: 0.9104\n",
      "Epoch: 1 / 3, Step: 586 / 750 Loss: 0.9879\n",
      "Epoch: 1 / 3, Step: 587 / 750 Loss: 0.6473\n",
      "Epoch: 1 / 3, Step: 588 / 750 Loss: 0.6902\n",
      "Epoch: 1 / 3, Step: 589 / 750 Loss: 0.6140\n",
      "Epoch: 1 / 3, Step: 590 / 750 Loss: 0.7281\n",
      "Epoch: 1 / 3, Step: 591 / 750 Loss: 0.7058\n",
      "Epoch: 1 / 3, Step: 592 / 750 Loss: 0.7298\n",
      "Epoch: 1 / 3, Step: 593 / 750 Loss: 0.7173\n",
      "Epoch: 1 / 3, Step: 594 / 750 Loss: 0.5266\n",
      "Epoch: 1 / 3, Step: 595 / 750 Loss: 0.7669\n",
      "Epoch: 1 / 3, Step: 596 / 750 Loss: 0.6499\n",
      "Epoch: 1 / 3, Step: 597 / 750 Loss: 0.7612\n",
      "Epoch: 1 / 3, Step: 598 / 750 Loss: 0.5201\n",
      "Epoch: 1 / 3, Step: 599 / 750 Loss: 0.6124\n",
      "Epoch: 1 / 3, Step: 600 / 750 Loss: 0.6772\n",
      "Epoch: 1 / 3, Step: 601 / 750 Loss: 1.0158\n",
      "Epoch: 1 / 3, Step: 602 / 750 Loss: 0.8536\n",
      "Epoch: 1 / 3, Step: 603 / 750 Loss: 0.8909\n",
      "Epoch: 1 / 3, Step: 604 / 750 Loss: 0.8491\n",
      "Epoch: 1 / 3, Step: 605 / 750 Loss: 0.6325\n",
      "Epoch: 1 / 3, Step: 606 / 750 Loss: 0.6734\n",
      "Epoch: 1 / 3, Step: 607 / 750 Loss: 0.6070\n",
      "Epoch: 1 / 3, Step: 608 / 750 Loss: 0.8854\n",
      "Epoch: 1 / 3, Step: 609 / 750 Loss: 0.6598\n",
      "Epoch: 1 / 3, Step: 610 / 750 Loss: 0.6891\n",
      "Epoch: 1 / 3, Step: 611 / 750 Loss: 0.8322\n",
      "Epoch: 1 / 3, Step: 612 / 750 Loss: 0.6496\n",
      "Epoch: 1 / 3, Step: 613 / 750 Loss: 0.5550\n",
      "Epoch: 1 / 3, Step: 614 / 750 Loss: 0.8110\n",
      "Epoch: 1 / 3, Step: 615 / 750 Loss: 0.6755\n",
      "Epoch: 1 / 3, Step: 616 / 750 Loss: 0.5110\n",
      "Epoch: 1 / 3, Step: 617 / 750 Loss: 1.0681\n",
      "Epoch: 1 / 3, Step: 618 / 750 Loss: 0.8050\n",
      "Epoch: 1 / 3, Step: 619 / 750 Loss: 0.6559\n",
      "Epoch: 1 / 3, Step: 620 / 750 Loss: 0.6954\n",
      "Epoch: 1 / 3, Step: 621 / 750 Loss: 0.6483\n",
      "Epoch: 1 / 3, Step: 622 / 750 Loss: 0.5820\n",
      "Epoch: 1 / 3, Step: 623 / 750 Loss: 0.7148\n",
      "Epoch: 1 / 3, Step: 624 / 750 Loss: 0.5520\n",
      "Epoch: 1 / 3, Step: 625 / 750 Loss: 0.7523\n",
      "Epoch: 1 / 3, Step: 626 / 750 Loss: 0.6081\n",
      "Epoch: 1 / 3, Step: 627 / 750 Loss: 0.9621\n",
      "Epoch: 1 / 3, Step: 628 / 750 Loss: 0.5099\n",
      "Epoch: 1 / 3, Step: 629 / 750 Loss: 0.7902\n",
      "Epoch: 1 / 3, Step: 630 / 750 Loss: 0.7572\n",
      "Epoch: 1 / 3, Step: 631 / 750 Loss: 0.6355\n",
      "Epoch: 1 / 3, Step: 632 / 750 Loss: 0.6783\n",
      "Epoch: 1 / 3, Step: 633 / 750 Loss: 0.7502\n",
      "Epoch: 1 / 3, Step: 634 / 750 Loss: 0.8814\n",
      "Epoch: 1 / 3, Step: 635 / 750 Loss: 0.5872\n",
      "Epoch: 1 / 3, Step: 636 / 750 Loss: 0.6799\n",
      "Epoch: 1 / 3, Step: 637 / 750 Loss: 0.8094\n",
      "Epoch: 1 / 3, Step: 638 / 750 Loss: 0.7987\n",
      "Epoch: 1 / 3, Step: 639 / 750 Loss: 0.6133\n",
      "Epoch: 1 / 3, Step: 640 / 750 Loss: 0.6207\n",
      "Epoch: 1 / 3, Step: 641 / 750 Loss: 0.6607\n",
      "Epoch: 1 / 3, Step: 642 / 750 Loss: 0.6007\n",
      "Epoch: 1 / 3, Step: 643 / 750 Loss: 0.5973\n",
      "Epoch: 1 / 3, Step: 644 / 750 Loss: 0.8744\n",
      "Epoch: 1 / 3, Step: 645 / 750 Loss: 0.5817\n",
      "Epoch: 1 / 3, Step: 646 / 750 Loss: 0.8544\n",
      "Epoch: 1 / 3, Step: 647 / 750 Loss: 0.8726\n",
      "Epoch: 1 / 3, Step: 648 / 750 Loss: 0.8207\n",
      "Epoch: 1 / 3, Step: 649 / 750 Loss: 0.4835\n",
      "Epoch: 1 / 3, Step: 650 / 750 Loss: 0.5742\n",
      "Epoch: 1 / 3, Step: 651 / 750 Loss: 0.6937\n",
      "Epoch: 1 / 3, Step: 652 / 750 Loss: 0.6872\n",
      "Epoch: 1 / 3, Step: 653 / 750 Loss: 0.8652\n",
      "Epoch: 1 / 3, Step: 654 / 750 Loss: 0.6922\n",
      "Epoch: 1 / 3, Step: 655 / 750 Loss: 0.8988\n",
      "Epoch: 1 / 3, Step: 656 / 750 Loss: 0.6349\n",
      "Epoch: 1 / 3, Step: 657 / 750 Loss: 0.6180\n",
      "Epoch: 1 / 3, Step: 658 / 750 Loss: 0.5265\n",
      "Epoch: 1 / 3, Step: 659 / 750 Loss: 0.7896\n",
      "Epoch: 1 / 3, Step: 660 / 750 Loss: 0.6169\n",
      "Epoch: 1 / 3, Step: 661 / 750 Loss: 0.8857\n",
      "Epoch: 1 / 3, Step: 662 / 750 Loss: 0.6456\n",
      "Epoch: 1 / 3, Step: 663 / 750 Loss: 0.9990\n",
      "Epoch: 1 / 3, Step: 664 / 750 Loss: 0.7217\n",
      "Epoch: 1 / 3, Step: 665 / 750 Loss: 0.6705\n",
      "Epoch: 1 / 3, Step: 666 / 750 Loss: 0.7834\n",
      "Epoch: 1 / 3, Step: 667 / 750 Loss: 0.8808\n",
      "Epoch: 1 / 3, Step: 668 / 750 Loss: 0.7907\n",
      "Epoch: 1 / 3, Step: 669 / 750 Loss: 0.9641\n",
      "Epoch: 1 / 3, Step: 670 / 750 Loss: 0.6455\n",
      "Epoch: 1 / 3, Step: 671 / 750 Loss: 0.7288\n",
      "Epoch: 1 / 3, Step: 672 / 750 Loss: 0.7627\n",
      "Epoch: 1 / 3, Step: 673 / 750 Loss: 0.8104\n",
      "Epoch: 1 / 3, Step: 674 / 750 Loss: 0.5049\n",
      "Epoch: 1 / 3, Step: 675 / 750 Loss: 0.5212\n",
      "Epoch: 1 / 3, Step: 676 / 750 Loss: 0.9071\n",
      "Epoch: 1 / 3, Step: 677 / 750 Loss: 0.7776\n",
      "Epoch: 1 / 3, Step: 678 / 750 Loss: 0.7665\n",
      "Epoch: 1 / 3, Step: 679 / 750 Loss: 0.7391\n",
      "Epoch: 1 / 3, Step: 680 / 750 Loss: 0.9803\n",
      "Epoch: 1 / 3, Step: 681 / 750 Loss: 0.8057\n",
      "Epoch: 1 / 3, Step: 682 / 750 Loss: 0.8248\n",
      "Epoch: 1 / 3, Step: 683 / 750 Loss: 0.6726\n",
      "Epoch: 1 / 3, Step: 684 / 750 Loss: 0.9095\n",
      "Epoch: 1 / 3, Step: 685 / 750 Loss: 0.7171\n",
      "Epoch: 1 / 3, Step: 686 / 750 Loss: 0.8433\n",
      "Epoch: 1 / 3, Step: 687 / 750 Loss: 0.7993\n",
      "Epoch: 1 / 3, Step: 688 / 750 Loss: 0.7564\n",
      "Epoch: 1 / 3, Step: 689 / 750 Loss: 0.6773\n",
      "Epoch: 1 / 3, Step: 690 / 750 Loss: 0.8011\n",
      "Epoch: 1 / 3, Step: 691 / 750 Loss: 0.6286\n",
      "Epoch: 1 / 3, Step: 692 / 750 Loss: 0.7085\n",
      "Epoch: 1 / 3, Step: 693 / 750 Loss: 0.5990\n",
      "Epoch: 1 / 3, Step: 694 / 750 Loss: 1.0287\n",
      "Epoch: 1 / 3, Step: 695 / 750 Loss: 0.6182\n",
      "Epoch: 1 / 3, Step: 696 / 750 Loss: 0.7406\n",
      "Epoch: 1 / 3, Step: 697 / 750 Loss: 0.6011\n",
      "Epoch: 1 / 3, Step: 698 / 750 Loss: 0.7342\n",
      "Epoch: 1 / 3, Step: 699 / 750 Loss: 0.7400\n",
      "Epoch: 1 / 3, Step: 700 / 750 Loss: 0.8350\n",
      "Epoch: 1 / 3, Step: 701 / 750 Loss: 0.7979\n",
      "Epoch: 1 / 3, Step: 702 / 750 Loss: 0.6016\n",
      "Epoch: 1 / 3, Step: 703 / 750 Loss: 0.7196\n",
      "Epoch: 1 / 3, Step: 704 / 750 Loss: 0.4938\n",
      "Epoch: 1 / 3, Step: 705 / 750 Loss: 0.6419\n",
      "Epoch: 1 / 3, Step: 706 / 750 Loss: 0.6811\n",
      "Epoch: 1 / 3, Step: 707 / 750 Loss: 0.8563\n",
      "Epoch: 1 / 3, Step: 708 / 750 Loss: 0.6263\n",
      "Epoch: 1 / 3, Step: 709 / 750 Loss: 0.5525\n",
      "Epoch: 1 / 3, Step: 710 / 750 Loss: 0.5993\n",
      "Epoch: 1 / 3, Step: 711 / 750 Loss: 0.5050\n",
      "Epoch: 1 / 3, Step: 712 / 750 Loss: 0.6909\n",
      "Epoch: 1 / 3, Step: 713 / 750 Loss: 0.7454\n",
      "Epoch: 1 / 3, Step: 714 / 750 Loss: 0.6229\n",
      "Epoch: 1 / 3, Step: 715 / 750 Loss: 0.8739\n",
      "Epoch: 1 / 3, Step: 716 / 750 Loss: 0.6238\n",
      "Epoch: 1 / 3, Step: 717 / 750 Loss: 0.5104\n",
      "Epoch: 1 / 3, Step: 718 / 750 Loss: 1.0143\n",
      "Epoch: 1 / 3, Step: 719 / 750 Loss: 0.6525\n",
      "Epoch: 1 / 3, Step: 720 / 750 Loss: 0.6441\n",
      "Epoch: 1 / 3, Step: 721 / 750 Loss: 0.6127\n",
      "Epoch: 1 / 3, Step: 722 / 750 Loss: 0.6626\n",
      "Epoch: 1 / 3, Step: 723 / 750 Loss: 0.7802\n",
      "Epoch: 1 / 3, Step: 724 / 750 Loss: 0.7447\n",
      "Epoch: 1 / 3, Step: 725 / 750 Loss: 0.6836\n",
      "Epoch: 1 / 3, Step: 726 / 750 Loss: 0.4877\n",
      "Epoch: 1 / 3, Step: 727 / 750 Loss: 0.5814\n",
      "Epoch: 1 / 3, Step: 728 / 750 Loss: 0.6413\n",
      "Epoch: 1 / 3, Step: 729 / 750 Loss: 0.5484\n",
      "Epoch: 1 / 3, Step: 730 / 750 Loss: 0.8813\n",
      "Epoch: 1 / 3, Step: 731 / 750 Loss: 0.6362\n",
      "Epoch: 1 / 3, Step: 732 / 750 Loss: 0.5022\n",
      "Epoch: 1 / 3, Step: 733 / 750 Loss: 0.8292\n",
      "Epoch: 1 / 3, Step: 734 / 750 Loss: 1.0044\n",
      "Epoch: 1 / 3, Step: 735 / 750 Loss: 0.8261\n",
      "Epoch: 1 / 3, Step: 736 / 750 Loss: 0.8352\n",
      "Epoch: 1 / 3, Step: 737 / 750 Loss: 0.7806\n",
      "Epoch: 1 / 3, Step: 738 / 750 Loss: 0.6031\n",
      "Epoch: 1 / 3, Step: 739 / 750 Loss: 0.7298\n",
      "Epoch: 1 / 3, Step: 740 / 750 Loss: 0.6775\n",
      "Epoch: 1 / 3, Step: 741 / 750 Loss: 0.8072\n",
      "Epoch: 1 / 3, Step: 742 / 750 Loss: 0.8108\n",
      "Epoch: 1 / 3, Step: 743 / 750 Loss: 0.6388\n",
      "Epoch: 1 / 3, Step: 744 / 750 Loss: 0.5461\n",
      "Epoch: 1 / 3, Step: 745 / 750 Loss: 0.5655\n",
      "Epoch: 1 / 3, Step: 746 / 750 Loss: 0.5548\n",
      "Epoch: 1 / 3, Step: 747 / 750 Loss: 1.0448\n",
      "Epoch: 1 / 3, Step: 748 / 750 Loss: 0.9982\n",
      "Epoch: 1 / 3, Step: 749 / 750 Loss: 0.7080\n",
      "Epoch: 2 / 3, Step: 0 / 750 Loss: 0.6082\n",
      "Epoch: 2 / 3, Step: 1 / 750 Loss: 0.7387\n",
      "Epoch: 2 / 3, Step: 2 / 750 Loss: 0.8054\n",
      "Epoch: 2 / 3, Step: 3 / 750 Loss: 0.6664\n",
      "Epoch: 2 / 3, Step: 4 / 750 Loss: 0.7165\n",
      "Epoch: 2 / 3, Step: 5 / 750 Loss: 0.5962\n",
      "Epoch: 2 / 3, Step: 6 / 750 Loss: 0.7615\n",
      "Epoch: 2 / 3, Step: 7 / 750 Loss: 0.9708\n",
      "Epoch: 2 / 3, Step: 8 / 750 Loss: 0.5949\n",
      "Epoch: 2 / 3, Step: 9 / 750 Loss: 0.5793\n",
      "Epoch: 2 / 3, Step: 10 / 750 Loss: 0.6404\n",
      "Epoch: 2 / 3, Step: 11 / 750 Loss: 0.6051\n",
      "Epoch: 2 / 3, Step: 12 / 750 Loss: 0.3998\n",
      "Epoch: 2 / 3, Step: 13 / 750 Loss: 0.4775\n",
      "Epoch: 2 / 3, Step: 14 / 750 Loss: 0.4850\n",
      "Epoch: 2 / 3, Step: 15 / 750 Loss: 0.9063\n",
      "Epoch: 2 / 3, Step: 16 / 750 Loss: 0.7620\n",
      "Epoch: 2 / 3, Step: 17 / 750 Loss: 0.7626\n",
      "Epoch: 2 / 3, Step: 18 / 750 Loss: 0.8734\n",
      "Epoch: 2 / 3, Step: 19 / 750 Loss: 0.6912\n",
      "Epoch: 2 / 3, Step: 20 / 750 Loss: 0.5556\n",
      "Epoch: 2 / 3, Step: 21 / 750 Loss: 0.5019\n",
      "Epoch: 2 / 3, Step: 22 / 750 Loss: 0.5717\n",
      "Epoch: 2 / 3, Step: 23 / 750 Loss: 0.6985\n",
      "Epoch: 2 / 3, Step: 24 / 750 Loss: 0.5354\n",
      "Epoch: 2 / 3, Step: 25 / 750 Loss: 0.6604\n",
      "Epoch: 2 / 3, Step: 26 / 750 Loss: 0.6768\n",
      "Epoch: 2 / 3, Step: 27 / 750 Loss: 0.4617\n",
      "Epoch: 2 / 3, Step: 28 / 750 Loss: 0.6877\n",
      "Epoch: 2 / 3, Step: 29 / 750 Loss: 0.5395\n",
      "Epoch: 2 / 3, Step: 30 / 750 Loss: 0.6969\n",
      "Epoch: 2 / 3, Step: 31 / 750 Loss: 0.6441\n",
      "Epoch: 2 / 3, Step: 32 / 750 Loss: 0.6963\n",
      "Epoch: 2 / 3, Step: 33 / 750 Loss: 0.4418\n",
      "Epoch: 2 / 3, Step: 34 / 750 Loss: 0.7818\n",
      "Epoch: 2 / 3, Step: 35 / 750 Loss: 0.6936\n",
      "Epoch: 2 / 3, Step: 36 / 750 Loss: 0.4451\n",
      "Epoch: 2 / 3, Step: 37 / 750 Loss: 0.8200\n",
      "Epoch: 2 / 3, Step: 38 / 750 Loss: 0.7150\n",
      "Epoch: 2 / 3, Step: 39 / 750 Loss: 0.5929\n",
      "Epoch: 2 / 3, Step: 40 / 750 Loss: 0.7382\n",
      "Epoch: 2 / 3, Step: 41 / 750 Loss: 0.7567\n",
      "Epoch: 2 / 3, Step: 42 / 750 Loss: 0.7513\n",
      "Epoch: 2 / 3, Step: 43 / 750 Loss: 0.7067\n",
      "Epoch: 2 / 3, Step: 44 / 750 Loss: 0.6506\n",
      "Epoch: 2 / 3, Step: 45 / 750 Loss: 0.4520\n",
      "Epoch: 2 / 3, Step: 46 / 750 Loss: 0.6203\n",
      "Epoch: 2 / 3, Step: 47 / 750 Loss: 0.6990\n",
      "Epoch: 2 / 3, Step: 48 / 750 Loss: 0.4890\n",
      "Epoch: 2 / 3, Step: 49 / 750 Loss: 0.3637\n",
      "Epoch: 2 / 3, Step: 50 / 750 Loss: 0.5304\n",
      "Epoch: 2 / 3, Step: 51 / 750 Loss: 0.5416\n",
      "Epoch: 2 / 3, Step: 52 / 750 Loss: 0.5614\n",
      "Epoch: 2 / 3, Step: 53 / 750 Loss: 0.4689\n",
      "Epoch: 2 / 3, Step: 54 / 750 Loss: 0.8841\n",
      "Epoch: 2 / 3, Step: 55 / 750 Loss: 0.6838\n",
      "Epoch: 2 / 3, Step: 56 / 750 Loss: 0.5525\n",
      "Epoch: 2 / 3, Step: 57 / 750 Loss: 0.5038\n",
      "Epoch: 2 / 3, Step: 58 / 750 Loss: 0.6614\n",
      "Epoch: 2 / 3, Step: 59 / 750 Loss: 0.9039\n",
      "Epoch: 2 / 3, Step: 60 / 750 Loss: 0.6857\n",
      "Epoch: 2 / 3, Step: 61 / 750 Loss: 0.5724\n",
      "Epoch: 2 / 3, Step: 62 / 750 Loss: 0.5383\n",
      "Epoch: 2 / 3, Step: 63 / 750 Loss: 0.6855\n",
      "Epoch: 2 / 3, Step: 64 / 750 Loss: 0.4779\n",
      "Epoch: 2 / 3, Step: 65 / 750 Loss: 0.7751\n",
      "Epoch: 2 / 3, Step: 66 / 750 Loss: 0.7714\n",
      "Epoch: 2 / 3, Step: 67 / 750 Loss: 0.7228\n",
      "Epoch: 2 / 3, Step: 68 / 750 Loss: 0.4831\n",
      "Epoch: 2 / 3, Step: 69 / 750 Loss: 0.4861\n",
      "Epoch: 2 / 3, Step: 70 / 750 Loss: 0.7801\n",
      "Epoch: 2 / 3, Step: 71 / 750 Loss: 0.6247\n",
      "Epoch: 2 / 3, Step: 72 / 750 Loss: 0.5682\n",
      "Epoch: 2 / 3, Step: 73 / 750 Loss: 0.6651\n",
      "Epoch: 2 / 3, Step: 74 / 750 Loss: 0.6366\n",
      "Epoch: 2 / 3, Step: 75 / 750 Loss: 0.6240\n",
      "Epoch: 2 / 3, Step: 76 / 750 Loss: 0.7672\n",
      "Epoch: 2 / 3, Step: 77 / 750 Loss: 0.4742\n",
      "Epoch: 2 / 3, Step: 78 / 750 Loss: 0.6229\n",
      "Epoch: 2 / 3, Step: 79 / 750 Loss: 0.6909\n",
      "Epoch: 2 / 3, Step: 80 / 750 Loss: 0.6348\n",
      "Epoch: 2 / 3, Step: 81 / 750 Loss: 0.6414\n",
      "Epoch: 2 / 3, Step: 82 / 750 Loss: 0.3527\n",
      "Epoch: 2 / 3, Step: 83 / 750 Loss: 0.6100\n",
      "Epoch: 2 / 3, Step: 84 / 750 Loss: 0.7874\n",
      "Epoch: 2 / 3, Step: 85 / 750 Loss: 0.3246\n",
      "Epoch: 2 / 3, Step: 86 / 750 Loss: 0.5258\n",
      "Epoch: 2 / 3, Step: 87 / 750 Loss: 0.5214\n",
      "Epoch: 2 / 3, Step: 88 / 750 Loss: 0.7154\n",
      "Epoch: 2 / 3, Step: 89 / 750 Loss: 0.7097\n",
      "Epoch: 2 / 3, Step: 90 / 750 Loss: 0.8337\n",
      "Epoch: 2 / 3, Step: 91 / 750 Loss: 0.6433\n",
      "Epoch: 2 / 3, Step: 92 / 750 Loss: 0.6826\n",
      "Epoch: 2 / 3, Step: 93 / 750 Loss: 0.7514\n",
      "Epoch: 2 / 3, Step: 94 / 750 Loss: 0.5476\n",
      "Epoch: 2 / 3, Step: 95 / 750 Loss: 0.5620\n",
      "Epoch: 2 / 3, Step: 96 / 750 Loss: 0.4903\n",
      "Epoch: 2 / 3, Step: 97 / 750 Loss: 0.6936\n",
      "Epoch: 2 / 3, Step: 98 / 750 Loss: 0.3808\n",
      "Epoch: 2 / 3, Step: 99 / 750 Loss: 0.7948\n",
      "Epoch: 2 / 3, Step: 100 / 750 Loss: 0.4058\n",
      "Epoch: 2 / 3, Step: 101 / 750 Loss: 0.5245\n",
      "Epoch: 2 / 3, Step: 102 / 750 Loss: 0.7627\n",
      "Epoch: 2 / 3, Step: 103 / 750 Loss: 0.6935\n",
      "Epoch: 2 / 3, Step: 104 / 750 Loss: 0.6025\n",
      "Epoch: 2 / 3, Step: 105 / 750 Loss: 0.3727\n",
      "Epoch: 2 / 3, Step: 106 / 750 Loss: 0.7280\n",
      "Epoch: 2 / 3, Step: 107 / 750 Loss: 0.6435\n",
      "Epoch: 2 / 3, Step: 108 / 750 Loss: 0.6654\n",
      "Epoch: 2 / 3, Step: 109 / 750 Loss: 0.5229\n",
      "Epoch: 2 / 3, Step: 110 / 750 Loss: 0.5892\n",
      "Epoch: 2 / 3, Step: 111 / 750 Loss: 0.9841\n",
      "Epoch: 2 / 3, Step: 112 / 750 Loss: 0.5880\n",
      "Epoch: 2 / 3, Step: 113 / 750 Loss: 0.8256\n",
      "Epoch: 2 / 3, Step: 114 / 750 Loss: 0.6463\n",
      "Epoch: 2 / 3, Step: 115 / 750 Loss: 0.5796\n",
      "Epoch: 2 / 3, Step: 116 / 750 Loss: 0.5324\n",
      "Epoch: 2 / 3, Step: 117 / 750 Loss: 0.7515\n",
      "Epoch: 2 / 3, Step: 118 / 750 Loss: 0.5823\n",
      "Epoch: 2 / 3, Step: 119 / 750 Loss: 0.5978\n",
      "Epoch: 2 / 3, Step: 120 / 750 Loss: 0.4927\n",
      "Epoch: 2 / 3, Step: 121 / 750 Loss: 0.4769\n",
      "Epoch: 2 / 3, Step: 122 / 750 Loss: 0.8657\n",
      "Epoch: 2 / 3, Step: 123 / 750 Loss: 0.7079\n",
      "Epoch: 2 / 3, Step: 124 / 750 Loss: 0.5514\n",
      "Epoch: 2 / 3, Step: 125 / 750 Loss: 0.8601\n",
      "Epoch: 2 / 3, Step: 126 / 750 Loss: 0.6228\n",
      "Epoch: 2 / 3, Step: 127 / 750 Loss: 0.7610\n",
      "Epoch: 2 / 3, Step: 128 / 750 Loss: 0.6677\n",
      "Epoch: 2 / 3, Step: 129 / 750 Loss: 0.7437\n",
      "Epoch: 2 / 3, Step: 130 / 750 Loss: 0.5266\n",
      "Epoch: 2 / 3, Step: 131 / 750 Loss: 0.6416\n",
      "Epoch: 2 / 3, Step: 132 / 750 Loss: 0.6449\n",
      "Epoch: 2 / 3, Step: 133 / 750 Loss: 0.5694\n",
      "Epoch: 2 / 3, Step: 134 / 750 Loss: 0.5387\n",
      "Epoch: 2 / 3, Step: 135 / 750 Loss: 0.6925\n",
      "Epoch: 2 / 3, Step: 136 / 750 Loss: 0.4918\n",
      "Epoch: 2 / 3, Step: 137 / 750 Loss: 0.6169\n",
      "Epoch: 2 / 3, Step: 138 / 750 Loss: 0.6163\n",
      "Epoch: 2 / 3, Step: 139 / 750 Loss: 0.8075\n",
      "Epoch: 2 / 3, Step: 140 / 750 Loss: 0.4864\n",
      "Epoch: 2 / 3, Step: 141 / 750 Loss: 0.3494\n",
      "Epoch: 2 / 3, Step: 142 / 750 Loss: 0.6578\n",
      "Epoch: 2 / 3, Step: 143 / 750 Loss: 0.4212\n",
      "Epoch: 2 / 3, Step: 144 / 750 Loss: 0.4630\n",
      "Epoch: 2 / 3, Step: 145 / 750 Loss: 0.3833\n",
      "Epoch: 2 / 3, Step: 146 / 750 Loss: 0.5632\n",
      "Epoch: 2 / 3, Step: 147 / 750 Loss: 0.6116\n",
      "Epoch: 2 / 3, Step: 148 / 750 Loss: 0.6398\n",
      "Epoch: 2 / 3, Step: 149 / 750 Loss: 0.7780\n",
      "Epoch: 2 / 3, Step: 150 / 750 Loss: 0.5038\n",
      "Epoch: 2 / 3, Step: 151 / 750 Loss: 0.4558\n",
      "Epoch: 2 / 3, Step: 152 / 750 Loss: 0.7202\n",
      "Epoch: 2 / 3, Step: 153 / 750 Loss: 0.6759\n",
      "Epoch: 2 / 3, Step: 154 / 750 Loss: 0.6580\n",
      "Epoch: 2 / 3, Step: 155 / 750 Loss: 0.7415\n",
      "Epoch: 2 / 3, Step: 156 / 750 Loss: 0.6015\n",
      "Epoch: 2 / 3, Step: 157 / 750 Loss: 0.4059\n",
      "Epoch: 2 / 3, Step: 158 / 750 Loss: 0.6167\n",
      "Epoch: 2 / 3, Step: 159 / 750 Loss: 0.3764\n",
      "Epoch: 2 / 3, Step: 160 / 750 Loss: 0.5392\n",
      "Epoch: 2 / 3, Step: 161 / 750 Loss: 0.7597\n",
      "Epoch: 2 / 3, Step: 162 / 750 Loss: 0.4396\n",
      "Epoch: 2 / 3, Step: 163 / 750 Loss: 0.7963\n",
      "Epoch: 2 / 3, Step: 164 / 750 Loss: 0.7501\n",
      "Epoch: 2 / 3, Step: 165 / 750 Loss: 0.4010\n",
      "Epoch: 2 / 3, Step: 166 / 750 Loss: 0.6107\n",
      "Epoch: 2 / 3, Step: 167 / 750 Loss: 0.7167\n",
      "Epoch: 2 / 3, Step: 168 / 750 Loss: 0.4311\n",
      "Epoch: 2 / 3, Step: 169 / 750 Loss: 0.6260\n",
      "Epoch: 2 / 3, Step: 170 / 750 Loss: 0.6225\n",
      "Epoch: 2 / 3, Step: 171 / 750 Loss: 0.6129\n",
      "Epoch: 2 / 3, Step: 172 / 750 Loss: 0.5604\n",
      "Epoch: 2 / 3, Step: 173 / 750 Loss: 0.4781\n",
      "Epoch: 2 / 3, Step: 174 / 750 Loss: 0.7361\n",
      "Epoch: 2 / 3, Step: 175 / 750 Loss: 0.5634\n",
      "Epoch: 2 / 3, Step: 176 / 750 Loss: 0.5605\n",
      "Epoch: 2 / 3, Step: 177 / 750 Loss: 0.8215\n",
      "Epoch: 2 / 3, Step: 178 / 750 Loss: 0.6034\n",
      "Epoch: 2 / 3, Step: 179 / 750 Loss: 0.5981\n",
      "Epoch: 2 / 3, Step: 180 / 750 Loss: 0.4960\n",
      "Epoch: 2 / 3, Step: 181 / 750 Loss: 0.4524\n",
      "Epoch: 2 / 3, Step: 182 / 750 Loss: 0.5855\n",
      "Epoch: 2 / 3, Step: 183 / 750 Loss: 0.5675\n",
      "Epoch: 2 / 3, Step: 184 / 750 Loss: 0.3836\n",
      "Epoch: 2 / 3, Step: 185 / 750 Loss: 0.7077\n",
      "Epoch: 2 / 3, Step: 186 / 750 Loss: 0.5100\n",
      "Epoch: 2 / 3, Step: 187 / 750 Loss: 0.8567\n",
      "Epoch: 2 / 3, Step: 188 / 750 Loss: 1.0797\n",
      "Epoch: 2 / 3, Step: 189 / 750 Loss: 0.4047\n",
      "Epoch: 2 / 3, Step: 190 / 750 Loss: 0.7558\n",
      "Epoch: 2 / 3, Step: 191 / 750 Loss: 0.7212\n",
      "Epoch: 2 / 3, Step: 192 / 750 Loss: 0.6677\n",
      "Epoch: 2 / 3, Step: 193 / 750 Loss: 0.5728\n",
      "Epoch: 2 / 3, Step: 194 / 750 Loss: 0.7124\n",
      "Epoch: 2 / 3, Step: 195 / 750 Loss: 0.5071\n",
      "Epoch: 2 / 3, Step: 196 / 750 Loss: 0.5233\n",
      "Epoch: 2 / 3, Step: 197 / 750 Loss: 0.4967\n",
      "Epoch: 2 / 3, Step: 198 / 750 Loss: 0.6328\n",
      "Epoch: 2 / 3, Step: 199 / 750 Loss: 0.5201\n",
      "Epoch: 2 / 3, Step: 200 / 750 Loss: 0.4815\n",
      "Epoch: 2 / 3, Step: 201 / 750 Loss: 0.5259\n",
      "Epoch: 2 / 3, Step: 202 / 750 Loss: 0.6123\n",
      "Epoch: 2 / 3, Step: 203 / 750 Loss: 0.5261\n",
      "Epoch: 2 / 3, Step: 204 / 750 Loss: 0.6659\n",
      "Epoch: 2 / 3, Step: 205 / 750 Loss: 0.3827\n",
      "Epoch: 2 / 3, Step: 206 / 750 Loss: 0.5194\n",
      "Epoch: 2 / 3, Step: 207 / 750 Loss: 0.8147\n",
      "Epoch: 2 / 3, Step: 208 / 750 Loss: 0.6089\n",
      "Epoch: 2 / 3, Step: 209 / 750 Loss: 0.6666\n",
      "Epoch: 2 / 3, Step: 210 / 750 Loss: 0.5414\n",
      "Epoch: 2 / 3, Step: 211 / 750 Loss: 0.4744\n",
      "Epoch: 2 / 3, Step: 212 / 750 Loss: 0.5188\n",
      "Epoch: 2 / 3, Step: 213 / 750 Loss: 0.5391\n",
      "Epoch: 2 / 3, Step: 214 / 750 Loss: 0.7119\n",
      "Epoch: 2 / 3, Step: 215 / 750 Loss: 0.5837\n",
      "Epoch: 2 / 3, Step: 216 / 750 Loss: 0.6070\n",
      "Epoch: 2 / 3, Step: 217 / 750 Loss: 0.6803\n",
      "Epoch: 2 / 3, Step: 218 / 750 Loss: 0.5080\n",
      "Epoch: 2 / 3, Step: 219 / 750 Loss: 0.6996\n",
      "Epoch: 2 / 3, Step: 220 / 750 Loss: 0.3977\n",
      "Epoch: 2 / 3, Step: 221 / 750 Loss: 0.7367\n",
      "Epoch: 2 / 3, Step: 222 / 750 Loss: 0.5090\n",
      "Epoch: 2 / 3, Step: 223 / 750 Loss: 0.3596\n",
      "Epoch: 2 / 3, Step: 224 / 750 Loss: 0.3950\n",
      "Epoch: 2 / 3, Step: 225 / 750 Loss: 0.7565\n",
      "Epoch: 2 / 3, Step: 226 / 750 Loss: 0.3783\n",
      "Epoch: 2 / 3, Step: 227 / 750 Loss: 0.3983\n",
      "Epoch: 2 / 3, Step: 228 / 750 Loss: 0.7057\n",
      "Epoch: 2 / 3, Step: 229 / 750 Loss: 0.6024\n",
      "Epoch: 2 / 3, Step: 230 / 750 Loss: 0.6253\n",
      "Epoch: 2 / 3, Step: 231 / 750 Loss: 0.5919\n",
      "Epoch: 2 / 3, Step: 232 / 750 Loss: 0.5164\n",
      "Epoch: 2 / 3, Step: 233 / 750 Loss: 0.6091\n",
      "Epoch: 2 / 3, Step: 234 / 750 Loss: 0.5292\n",
      "Epoch: 2 / 3, Step: 235 / 750 Loss: 0.4919\n",
      "Epoch: 2 / 3, Step: 236 / 750 Loss: 0.6278\n",
      "Epoch: 2 / 3, Step: 237 / 750 Loss: 0.4315\n",
      "Epoch: 2 / 3, Step: 238 / 750 Loss: 0.4155\n",
      "Epoch: 2 / 3, Step: 239 / 750 Loss: 0.3521\n",
      "Epoch: 2 / 3, Step: 240 / 750 Loss: 0.9671\n",
      "Epoch: 2 / 3, Step: 241 / 750 Loss: 0.7965\n",
      "Epoch: 2 / 3, Step: 242 / 750 Loss: 0.4091\n",
      "Epoch: 2 / 3, Step: 243 / 750 Loss: 0.6156\n",
      "Epoch: 2 / 3, Step: 244 / 750 Loss: 0.3902\n",
      "Epoch: 2 / 3, Step: 245 / 750 Loss: 0.6051\n",
      "Epoch: 2 / 3, Step: 246 / 750 Loss: 0.3471\n",
      "Epoch: 2 / 3, Step: 247 / 750 Loss: 0.4654\n",
      "Epoch: 2 / 3, Step: 248 / 750 Loss: 0.4974\n",
      "Epoch: 2 / 3, Step: 249 / 750 Loss: 0.4177\n",
      "Epoch: 2 / 3, Step: 250 / 750 Loss: 0.3605\n",
      "Epoch: 2 / 3, Step: 251 / 750 Loss: 0.3990\n",
      "Epoch: 2 / 3, Step: 252 / 750 Loss: 0.5458\n",
      "Epoch: 2 / 3, Step: 253 / 750 Loss: 0.3744\n",
      "Epoch: 2 / 3, Step: 254 / 750 Loss: 0.5387\n",
      "Epoch: 2 / 3, Step: 255 / 750 Loss: 0.5405\n",
      "Epoch: 2 / 3, Step: 256 / 750 Loss: 0.4142\n",
      "Epoch: 2 / 3, Step: 257 / 750 Loss: 0.4416\n",
      "Epoch: 2 / 3, Step: 258 / 750 Loss: 0.3984\n",
      "Epoch: 2 / 3, Step: 259 / 750 Loss: 0.4744\n",
      "Epoch: 2 / 3, Step: 260 / 750 Loss: 0.5329\n",
      "Epoch: 2 / 3, Step: 261 / 750 Loss: 0.7520\n",
      "Epoch: 2 / 3, Step: 262 / 750 Loss: 0.3706\n",
      "Epoch: 2 / 3, Step: 263 / 750 Loss: 0.6912\n",
      "Epoch: 2 / 3, Step: 264 / 750 Loss: 0.4456\n",
      "Epoch: 2 / 3, Step: 265 / 750 Loss: 0.2266\n",
      "Epoch: 2 / 3, Step: 266 / 750 Loss: 0.3441\n",
      "Epoch: 2 / 3, Step: 267 / 750 Loss: 0.2928\n",
      "Epoch: 2 / 3, Step: 268 / 750 Loss: 0.5741\n",
      "Epoch: 2 / 3, Step: 269 / 750 Loss: 0.9561\n",
      "Epoch: 2 / 3, Step: 270 / 750 Loss: 0.5090\n",
      "Epoch: 2 / 3, Step: 271 / 750 Loss: 0.5121\n",
      "Epoch: 2 / 3, Step: 272 / 750 Loss: 0.4052\n",
      "Epoch: 2 / 3, Step: 273 / 750 Loss: 0.6394\n",
      "Epoch: 2 / 3, Step: 274 / 750 Loss: 0.5245\n",
      "Epoch: 2 / 3, Step: 275 / 750 Loss: 0.5231\n",
      "Epoch: 2 / 3, Step: 276 / 750 Loss: 0.5729\n",
      "Epoch: 2 / 3, Step: 277 / 750 Loss: 0.5810\n",
      "Epoch: 2 / 3, Step: 278 / 750 Loss: 0.6252\n",
      "Epoch: 2 / 3, Step: 279 / 750 Loss: 0.4636\n",
      "Epoch: 2 / 3, Step: 280 / 750 Loss: 0.7218\n",
      "Epoch: 2 / 3, Step: 281 / 750 Loss: 0.6885\n",
      "Epoch: 2 / 3, Step: 282 / 750 Loss: 0.5456\n",
      "Epoch: 2 / 3, Step: 283 / 750 Loss: 0.4425\n",
      "Epoch: 2 / 3, Step: 284 / 750 Loss: 0.4347\n",
      "Epoch: 2 / 3, Step: 285 / 750 Loss: 0.5191\n",
      "Epoch: 2 / 3, Step: 286 / 750 Loss: 0.4584\n",
      "Epoch: 2 / 3, Step: 287 / 750 Loss: 0.6706\n",
      "Epoch: 2 / 3, Step: 288 / 750 Loss: 0.3025\n",
      "Epoch: 2 / 3, Step: 289 / 750 Loss: 0.4398\n",
      "Epoch: 2 / 3, Step: 290 / 750 Loss: 0.3308\n",
      "Epoch: 2 / 3, Step: 291 / 750 Loss: 0.6308\n",
      "Epoch: 2 / 3, Step: 292 / 750 Loss: 0.6116\n",
      "Epoch: 2 / 3, Step: 293 / 750 Loss: 0.5466\n",
      "Epoch: 2 / 3, Step: 294 / 750 Loss: 0.4240\n",
      "Epoch: 2 / 3, Step: 295 / 750 Loss: 0.4179\n",
      "Epoch: 2 / 3, Step: 296 / 750 Loss: 0.3827\n",
      "Epoch: 2 / 3, Step: 297 / 750 Loss: 0.5664\n",
      "Epoch: 2 / 3, Step: 298 / 750 Loss: 0.6158\n",
      "Epoch: 2 / 3, Step: 299 / 750 Loss: 0.6874\n",
      "Epoch: 2 / 3, Step: 300 / 750 Loss: 0.3579\n",
      "Epoch: 2 / 3, Step: 301 / 750 Loss: 0.4775\n",
      "Epoch: 2 / 3, Step: 302 / 750 Loss: 0.3794\n",
      "Epoch: 2 / 3, Step: 303 / 750 Loss: 0.4582\n",
      "Epoch: 2 / 3, Step: 304 / 750 Loss: 0.2603\n",
      "Epoch: 2 / 3, Step: 305 / 750 Loss: 0.4459\n",
      "Epoch: 2 / 3, Step: 306 / 750 Loss: 0.4544\n",
      "Epoch: 2 / 3, Step: 307 / 750 Loss: 0.4567\n",
      "Epoch: 2 / 3, Step: 308 / 750 Loss: 0.4985\n",
      "Epoch: 2 / 3, Step: 309 / 750 Loss: 0.6159\n",
      "Epoch: 2 / 3, Step: 310 / 750 Loss: 0.3791\n",
      "Epoch: 2 / 3, Step: 311 / 750 Loss: 0.4398\n",
      "Epoch: 2 / 3, Step: 312 / 750 Loss: 0.4086\n",
      "Epoch: 2 / 3, Step: 313 / 750 Loss: 0.5719\n",
      "Epoch: 2 / 3, Step: 314 / 750 Loss: 0.5880\n",
      "Epoch: 2 / 3, Step: 315 / 750 Loss: 0.4279\n",
      "Epoch: 2 / 3, Step: 316 / 750 Loss: 0.3068\n",
      "Epoch: 2 / 3, Step: 317 / 750 Loss: 0.2943\n",
      "Epoch: 2 / 3, Step: 318 / 750 Loss: 0.4251\n",
      "Epoch: 2 / 3, Step: 319 / 750 Loss: 0.5977\n",
      "Epoch: 2 / 3, Step: 320 / 750 Loss: 0.1887\n",
      "Epoch: 2 / 3, Step: 321 / 750 Loss: 0.6109\n",
      "Epoch: 2 / 3, Step: 322 / 750 Loss: 0.3689\n",
      "Epoch: 2 / 3, Step: 323 / 750 Loss: 0.5354\n",
      "Epoch: 2 / 3, Step: 324 / 750 Loss: 0.3636\n",
      "Epoch: 2 / 3, Step: 325 / 750 Loss: 0.4737\n",
      "Epoch: 2 / 3, Step: 326 / 750 Loss: 0.5589\n",
      "Epoch: 2 / 3, Step: 327 / 750 Loss: 0.4042\n",
      "Epoch: 2 / 3, Step: 328 / 750 Loss: 0.4880\n",
      "Epoch: 2 / 3, Step: 329 / 750 Loss: 0.5800\n",
      "Epoch: 2 / 3, Step: 330 / 750 Loss: 0.4342\n",
      "Epoch: 2 / 3, Step: 331 / 750 Loss: 0.5993\n",
      "Epoch: 2 / 3, Step: 332 / 750 Loss: 0.8298\n",
      "Epoch: 2 / 3, Step: 333 / 750 Loss: 0.5751\n",
      "Epoch: 2 / 3, Step: 334 / 750 Loss: 0.2840\n",
      "Epoch: 2 / 3, Step: 335 / 750 Loss: 0.4145\n",
      "Epoch: 2 / 3, Step: 336 / 750 Loss: 0.6916\n",
      "Epoch: 2 / 3, Step: 337 / 750 Loss: 0.6887\n",
      "Epoch: 2 / 3, Step: 338 / 750 Loss: 0.6607\n",
      "Epoch: 2 / 3, Step: 339 / 750 Loss: 0.4797\n",
      "Epoch: 2 / 3, Step: 340 / 750 Loss: 0.4452\n",
      "Epoch: 2 / 3, Step: 341 / 750 Loss: 0.4741\n",
      "Epoch: 2 / 3, Step: 342 / 750 Loss: 0.4257\n",
      "Epoch: 2 / 3, Step: 343 / 750 Loss: 0.6135\n",
      "Epoch: 2 / 3, Step: 344 / 750 Loss: 0.4894\n",
      "Epoch: 2 / 3, Step: 345 / 750 Loss: 0.4889\n",
      "Epoch: 2 / 3, Step: 346 / 750 Loss: 0.4482\n",
      "Epoch: 2 / 3, Step: 347 / 750 Loss: 0.4441\n",
      "Epoch: 2 / 3, Step: 348 / 750 Loss: 0.3134\n",
      "Epoch: 2 / 3, Step: 349 / 750 Loss: 0.4580\n",
      "Epoch: 2 / 3, Step: 350 / 750 Loss: 0.3205\n",
      "Epoch: 2 / 3, Step: 351 / 750 Loss: 0.5538\n",
      "Epoch: 2 / 3, Step: 352 / 750 Loss: 0.4100\n",
      "Epoch: 2 / 3, Step: 353 / 750 Loss: 0.3327\n",
      "Epoch: 2 / 3, Step: 354 / 750 Loss: 0.7074\n",
      "Epoch: 2 / 3, Step: 355 / 750 Loss: 0.5453\n",
      "Epoch: 2 / 3, Step: 356 / 750 Loss: 0.3817\n",
      "Epoch: 2 / 3, Step: 357 / 750 Loss: 0.5636\n",
      "Epoch: 2 / 3, Step: 358 / 750 Loss: 0.2613\n",
      "Epoch: 2 / 3, Step: 359 / 750 Loss: 0.3086\n",
      "Epoch: 2 / 3, Step: 360 / 750 Loss: 0.4638\n",
      "Epoch: 2 / 3, Step: 361 / 750 Loss: 0.5616\n",
      "Epoch: 2 / 3, Step: 362 / 750 Loss: 0.8242\n",
      "Epoch: 2 / 3, Step: 363 / 750 Loss: 0.3113\n",
      "Epoch: 2 / 3, Step: 364 / 750 Loss: 0.5145\n",
      "Epoch: 2 / 3, Step: 365 / 750 Loss: 0.5059\n",
      "Epoch: 2 / 3, Step: 366 / 750 Loss: 0.5318\n",
      "Epoch: 2 / 3, Step: 367 / 750 Loss: 0.4835\n",
      "Epoch: 2 / 3, Step: 368 / 750 Loss: 0.4697\n",
      "Epoch: 2 / 3, Step: 369 / 750 Loss: 0.5731\n",
      "Epoch: 2 / 3, Step: 370 / 750 Loss: 0.6634\n",
      "Epoch: 2 / 3, Step: 371 / 750 Loss: 0.1555\n",
      "Epoch: 2 / 3, Step: 372 / 750 Loss: 0.7912\n",
      "Epoch: 2 / 3, Step: 373 / 750 Loss: 0.1888\n",
      "Epoch: 2 / 3, Step: 374 / 750 Loss: 0.2747\n",
      "Epoch: 2 / 3, Step: 375 / 750 Loss: 0.4943\n",
      "Epoch: 2 / 3, Step: 376 / 750 Loss: 0.2770\n",
      "Epoch: 2 / 3, Step: 377 / 750 Loss: 0.2398\n",
      "Epoch: 2 / 3, Step: 378 / 750 Loss: 0.1590\n",
      "Epoch: 2 / 3, Step: 379 / 750 Loss: 0.4841\n",
      "Epoch: 2 / 3, Step: 380 / 750 Loss: 0.2915\n",
      "Epoch: 2 / 3, Step: 381 / 750 Loss: 0.6879\n",
      "Epoch: 2 / 3, Step: 382 / 750 Loss: 0.6345\n",
      "Epoch: 2 / 3, Step: 383 / 750 Loss: 0.5248\n",
      "Epoch: 2 / 3, Step: 384 / 750 Loss: 0.4625\n",
      "Epoch: 2 / 3, Step: 385 / 750 Loss: 0.4426\n",
      "Epoch: 2 / 3, Step: 386 / 750 Loss: 0.4174\n",
      "Epoch: 2 / 3, Step: 387 / 750 Loss: 0.4021\n",
      "Epoch: 2 / 3, Step: 388 / 750 Loss: 0.4310\n",
      "Epoch: 2 / 3, Step: 389 / 750 Loss: 0.6193\n",
      "Epoch: 2 / 3, Step: 390 / 750 Loss: 0.3266\n",
      "Epoch: 2 / 3, Step: 391 / 750 Loss: 0.5000\n",
      "Epoch: 2 / 3, Step: 392 / 750 Loss: 0.2834\n",
      "Epoch: 2 / 3, Step: 393 / 750 Loss: 0.4205\n",
      "Epoch: 2 / 3, Step: 394 / 750 Loss: 0.5628\n",
      "Epoch: 2 / 3, Step: 395 / 750 Loss: 0.5573\n",
      "Epoch: 2 / 3, Step: 396 / 750 Loss: 0.4370\n",
      "Epoch: 2 / 3, Step: 397 / 750 Loss: 0.4456\n",
      "Epoch: 2 / 3, Step: 398 / 750 Loss: 0.3323\n",
      "Epoch: 2 / 3, Step: 399 / 750 Loss: 0.4966\n",
      "Epoch: 2 / 3, Step: 400 / 750 Loss: 0.4107\n",
      "Epoch: 2 / 3, Step: 401 / 750 Loss: 0.3713\n",
      "Epoch: 2 / 3, Step: 402 / 750 Loss: 0.4085\n",
      "Epoch: 2 / 3, Step: 403 / 750 Loss: 0.2872\n",
      "Epoch: 2 / 3, Step: 404 / 750 Loss: 0.6244\n",
      "Epoch: 2 / 3, Step: 405 / 750 Loss: 0.3382\n",
      "Epoch: 2 / 3, Step: 406 / 750 Loss: 0.5716\n",
      "Epoch: 2 / 3, Step: 407 / 750 Loss: 0.2157\n",
      "Epoch: 2 / 3, Step: 408 / 750 Loss: 0.5043\n",
      "Epoch: 2 / 3, Step: 409 / 750 Loss: 0.4436\n",
      "Epoch: 2 / 3, Step: 410 / 750 Loss: 0.1987\n",
      "Epoch: 2 / 3, Step: 411 / 750 Loss: 0.6585\n",
      "Epoch: 2 / 3, Step: 412 / 750 Loss: 0.8019\n",
      "Epoch: 2 / 3, Step: 413 / 750 Loss: 0.2343\n",
      "Epoch: 2 / 3, Step: 414 / 750 Loss: 0.3413\n",
      "Epoch: 2 / 3, Step: 415 / 750 Loss: 0.6572\n",
      "Epoch: 2 / 3, Step: 416 / 750 Loss: 0.6523\n",
      "Epoch: 2 / 3, Step: 417 / 750 Loss: 0.5471\n",
      "Epoch: 2 / 3, Step: 418 / 750 Loss: 0.3966\n",
      "Epoch: 2 / 3, Step: 419 / 750 Loss: 0.5990\n",
      "Epoch: 2 / 3, Step: 420 / 750 Loss: 0.5459\n",
      "Epoch: 2 / 3, Step: 421 / 750 Loss: 0.6269\n",
      "Epoch: 2 / 3, Step: 422 / 750 Loss: 0.5209\n",
      "Epoch: 2 / 3, Step: 423 / 750 Loss: 0.4805\n",
      "Epoch: 2 / 3, Step: 424 / 750 Loss: 0.2862\n",
      "Epoch: 2 / 3, Step: 425 / 750 Loss: 0.4727\n",
      "Epoch: 2 / 3, Step: 426 / 750 Loss: 0.3861\n",
      "Epoch: 2 / 3, Step: 427 / 750 Loss: 0.6489\n",
      "Epoch: 2 / 3, Step: 428 / 750 Loss: 0.5314\n",
      "Epoch: 2 / 3, Step: 429 / 750 Loss: 0.7565\n",
      "Epoch: 2 / 3, Step: 430 / 750 Loss: 0.6305\n",
      "Epoch: 2 / 3, Step: 431 / 750 Loss: 0.2777\n",
      "Epoch: 2 / 3, Step: 432 / 750 Loss: 0.4985\n",
      "Epoch: 2 / 3, Step: 433 / 750 Loss: 0.4748\n",
      "Epoch: 2 / 3, Step: 434 / 750 Loss: 0.3342\n",
      "Epoch: 2 / 3, Step: 435 / 750 Loss: 0.3418\n",
      "Epoch: 2 / 3, Step: 436 / 750 Loss: 0.5054\n",
      "Epoch: 2 / 3, Step: 437 / 750 Loss: 0.3558\n",
      "Epoch: 2 / 3, Step: 438 / 750 Loss: 0.5118\n",
      "Epoch: 2 / 3, Step: 439 / 750 Loss: 0.4873\n",
      "Epoch: 2 / 3, Step: 440 / 750 Loss: 0.5107\n",
      "Epoch: 2 / 3, Step: 441 / 750 Loss: 0.5431\n",
      "Epoch: 2 / 3, Step: 442 / 750 Loss: 0.2693\n",
      "Epoch: 2 / 3, Step: 443 / 750 Loss: 0.2415\n",
      "Epoch: 2 / 3, Step: 444 / 750 Loss: 0.4473\n",
      "Epoch: 2 / 3, Step: 445 / 750 Loss: 0.4903\n",
      "Epoch: 2 / 3, Step: 446 / 750 Loss: 0.3965\n",
      "Epoch: 2 / 3, Step: 447 / 750 Loss: 0.4488\n",
      "Epoch: 2 / 3, Step: 448 / 750 Loss: 0.2612\n",
      "Epoch: 2 / 3, Step: 449 / 750 Loss: 0.4014\n",
      "Epoch: 2 / 3, Step: 450 / 750 Loss: 0.4613\n",
      "Epoch: 2 / 3, Step: 451 / 750 Loss: 0.3564\n",
      "Epoch: 2 / 3, Step: 452 / 750 Loss: 0.4247\n",
      "Epoch: 2 / 3, Step: 453 / 750 Loss: 0.3567\n",
      "Epoch: 2 / 3, Step: 454 / 750 Loss: 0.3670\n",
      "Epoch: 2 / 3, Step: 455 / 750 Loss: 0.3883\n",
      "Epoch: 2 / 3, Step: 456 / 750 Loss: 0.4495\n",
      "Epoch: 2 / 3, Step: 457 / 750 Loss: 0.3195\n",
      "Epoch: 2 / 3, Step: 458 / 750 Loss: 0.1979\n",
      "Epoch: 2 / 3, Step: 459 / 750 Loss: 0.4090\n",
      "Epoch: 2 / 3, Step: 460 / 750 Loss: 0.3151\n",
      "Epoch: 2 / 3, Step: 461 / 750 Loss: 0.2382\n",
      "Epoch: 2 / 3, Step: 462 / 750 Loss: 0.7924\n",
      "Epoch: 2 / 3, Step: 463 / 750 Loss: 0.4419\n",
      "Epoch: 2 / 3, Step: 464 / 750 Loss: 0.4836\n",
      "Epoch: 2 / 3, Step: 465 / 750 Loss: 0.4185\n",
      "Epoch: 2 / 3, Step: 466 / 750 Loss: 0.3013\n",
      "Epoch: 2 / 3, Step: 467 / 750 Loss: 0.4264\n",
      "Epoch: 2 / 3, Step: 468 / 750 Loss: 0.7365\n",
      "Epoch: 2 / 3, Step: 469 / 750 Loss: 0.2900\n",
      "Epoch: 2 / 3, Step: 470 / 750 Loss: 0.4741\n",
      "Epoch: 2 / 3, Step: 471 / 750 Loss: 0.5063\n",
      "Epoch: 2 / 3, Step: 472 / 750 Loss: 0.6205\n",
      "Epoch: 2 / 3, Step: 473 / 750 Loss: 0.4224\n",
      "Epoch: 2 / 3, Step: 474 / 750 Loss: 0.3471\n",
      "Epoch: 2 / 3, Step: 475 / 750 Loss: 0.4090\n",
      "Epoch: 2 / 3, Step: 476 / 750 Loss: 0.4063\n",
      "Epoch: 2 / 3, Step: 477 / 750 Loss: 0.3679\n",
      "Epoch: 2 / 3, Step: 478 / 750 Loss: 0.3548\n",
      "Epoch: 2 / 3, Step: 479 / 750 Loss: 0.5177\n",
      "Epoch: 2 / 3, Step: 480 / 750 Loss: 0.8165\n",
      "Epoch: 2 / 3, Step: 481 / 750 Loss: 0.4662\n",
      "Epoch: 2 / 3, Step: 482 / 750 Loss: 0.5639\n",
      "Epoch: 2 / 3, Step: 483 / 750 Loss: 0.4629\n",
      "Epoch: 2 / 3, Step: 484 / 750 Loss: 0.4489\n",
      "Epoch: 2 / 3, Step: 485 / 750 Loss: 0.2199\n",
      "Epoch: 2 / 3, Step: 486 / 750 Loss: 0.3615\n",
      "Epoch: 2 / 3, Step: 487 / 750 Loss: 0.2581\n",
      "Epoch: 2 / 3, Step: 488 / 750 Loss: 0.7501\n",
      "Epoch: 2 / 3, Step: 489 / 750 Loss: 0.4911\n",
      "Epoch: 2 / 3, Step: 490 / 750 Loss: 0.3955\n",
      "Epoch: 2 / 3, Step: 491 / 750 Loss: 0.3696\n",
      "Epoch: 2 / 3, Step: 492 / 750 Loss: 0.4180\n",
      "Epoch: 2 / 3, Step: 493 / 750 Loss: 0.3623\n",
      "Epoch: 2 / 3, Step: 494 / 750 Loss: 0.3893\n",
      "Epoch: 2 / 3, Step: 495 / 750 Loss: 0.3158\n",
      "Epoch: 2 / 3, Step: 496 / 750 Loss: 0.4834\n",
      "Epoch: 2 / 3, Step: 497 / 750 Loss: 0.2246\n",
      "Epoch: 2 / 3, Step: 498 / 750 Loss: 0.4747\n",
      "Epoch: 2 / 3, Step: 499 / 750 Loss: 0.4032\n",
      "Epoch: 2 / 3, Step: 500 / 750 Loss: 0.4120\n",
      "Epoch: 2 / 3, Step: 501 / 750 Loss: 0.3150\n",
      "Epoch: 2 / 3, Step: 502 / 750 Loss: 0.3399\n",
      "Epoch: 2 / 3, Step: 503 / 750 Loss: 0.2683\n",
      "Epoch: 2 / 3, Step: 504 / 750 Loss: 0.3927\n",
      "Epoch: 2 / 3, Step: 505 / 750 Loss: 0.4174\n",
      "Epoch: 2 / 3, Step: 506 / 750 Loss: 0.2986\n",
      "Epoch: 2 / 3, Step: 507 / 750 Loss: 0.4320\n",
      "Epoch: 2 / 3, Step: 508 / 750 Loss: 0.5423\n",
      "Epoch: 2 / 3, Step: 509 / 750 Loss: 0.6685\n",
      "Epoch: 2 / 3, Step: 510 / 750 Loss: 0.2015\n",
      "Epoch: 2 / 3, Step: 511 / 750 Loss: 0.2466\n",
      "Epoch: 2 / 3, Step: 512 / 750 Loss: 0.1647\n",
      "Epoch: 2 / 3, Step: 513 / 750 Loss: 0.3680\n",
      "Epoch: 2 / 3, Step: 514 / 750 Loss: 0.3495\n",
      "Epoch: 2 / 3, Step: 515 / 750 Loss: 0.5826\n",
      "Epoch: 2 / 3, Step: 516 / 750 Loss: 0.1945\n",
      "Epoch: 2 / 3, Step: 517 / 750 Loss: 0.4450\n",
      "Epoch: 2 / 3, Step: 518 / 750 Loss: 0.3802\n",
      "Epoch: 2 / 3, Step: 519 / 750 Loss: 0.5753\n",
      "Epoch: 2 / 3, Step: 520 / 750 Loss: 0.5720\n",
      "Epoch: 2 / 3, Step: 521 / 750 Loss: 0.2125\n",
      "Epoch: 2 / 3, Step: 522 / 750 Loss: 0.4574\n",
      "Epoch: 2 / 3, Step: 523 / 750 Loss: 0.3366\n",
      "Epoch: 2 / 3, Step: 524 / 750 Loss: 0.3776\n",
      "Epoch: 2 / 3, Step: 525 / 750 Loss: 0.3602\n",
      "Epoch: 2 / 3, Step: 526 / 750 Loss: 0.3416\n",
      "Epoch: 2 / 3, Step: 527 / 750 Loss: 0.2292\n",
      "Epoch: 2 / 3, Step: 528 / 750 Loss: 0.3997\n",
      "Epoch: 2 / 3, Step: 529 / 750 Loss: 0.1739\n",
      "Epoch: 2 / 3, Step: 530 / 750 Loss: 0.4304\n",
      "Epoch: 2 / 3, Step: 531 / 750 Loss: 0.3847\n",
      "Epoch: 2 / 3, Step: 532 / 750 Loss: 0.3259\n",
      "Epoch: 2 / 3, Step: 533 / 750 Loss: 0.3910\n",
      "Epoch: 2 / 3, Step: 534 / 750 Loss: 0.2276\n",
      "Epoch: 2 / 3, Step: 535 / 750 Loss: 0.7634\n",
      "Epoch: 2 / 3, Step: 536 / 750 Loss: 0.3128\n",
      "Epoch: 2 / 3, Step: 537 / 750 Loss: 0.2054\n",
      "Epoch: 2 / 3, Step: 538 / 750 Loss: 0.5676\n",
      "Epoch: 2 / 3, Step: 539 / 750 Loss: 0.7386\n",
      "Epoch: 2 / 3, Step: 540 / 750 Loss: 0.4166\n",
      "Epoch: 2 / 3, Step: 541 / 750 Loss: 0.8404\n",
      "Epoch: 2 / 3, Step: 542 / 750 Loss: 0.2636\n",
      "Epoch: 2 / 3, Step: 543 / 750 Loss: 0.3569\n",
      "Epoch: 2 / 3, Step: 544 / 750 Loss: 0.3943\n",
      "Epoch: 2 / 3, Step: 545 / 750 Loss: 0.6036\n",
      "Epoch: 2 / 3, Step: 546 / 750 Loss: 0.4011\n",
      "Epoch: 2 / 3, Step: 547 / 750 Loss: 0.3220\n",
      "Epoch: 2 / 3, Step: 548 / 750 Loss: 0.3353\n",
      "Epoch: 2 / 3, Step: 549 / 750 Loss: 0.4665\n",
      "Epoch: 2 / 3, Step: 550 / 750 Loss: 0.4851\n",
      "Epoch: 2 / 3, Step: 551 / 750 Loss: 0.2652\n",
      "Epoch: 2 / 3, Step: 552 / 750 Loss: 0.3561\n",
      "Epoch: 2 / 3, Step: 553 / 750 Loss: 0.1942\n",
      "Epoch: 2 / 3, Step: 554 / 750 Loss: 0.2307\n",
      "Epoch: 2 / 3, Step: 555 / 750 Loss: 0.3882\n",
      "Epoch: 2 / 3, Step: 556 / 750 Loss: 0.5843\n",
      "Epoch: 2 / 3, Step: 557 / 750 Loss: 0.5175\n",
      "Epoch: 2 / 3, Step: 558 / 750 Loss: 0.6136\n",
      "Epoch: 2 / 3, Step: 559 / 750 Loss: 0.3269\n",
      "Epoch: 2 / 3, Step: 560 / 750 Loss: 0.3336\n",
      "Epoch: 2 / 3, Step: 561 / 750 Loss: 0.4277\n",
      "Epoch: 2 / 3, Step: 562 / 750 Loss: 0.4399\n",
      "Epoch: 2 / 3, Step: 563 / 750 Loss: 0.4849\n",
      "Epoch: 2 / 3, Step: 564 / 750 Loss: 0.6095\n",
      "Epoch: 2 / 3, Step: 565 / 750 Loss: 0.4057\n",
      "Epoch: 2 / 3, Step: 566 / 750 Loss: 0.3683\n",
      "Epoch: 2 / 3, Step: 567 / 750 Loss: 0.4049\n",
      "Epoch: 2 / 3, Step: 568 / 750 Loss: 0.6548\n",
      "Epoch: 2 / 3, Step: 569 / 750 Loss: 0.2659\n",
      "Epoch: 2 / 3, Step: 570 / 750 Loss: 0.7436\n",
      "Epoch: 2 / 3, Step: 571 / 750 Loss: 0.6106\n",
      "Epoch: 2 / 3, Step: 572 / 750 Loss: 0.2767\n",
      "Epoch: 2 / 3, Step: 573 / 750 Loss: 0.3860\n",
      "Epoch: 2 / 3, Step: 574 / 750 Loss: 0.4240\n",
      "Epoch: 2 / 3, Step: 575 / 750 Loss: 0.3867\n",
      "Epoch: 2 / 3, Step: 576 / 750 Loss: 0.5211\n",
      "Epoch: 2 / 3, Step: 577 / 750 Loss: 0.4045\n",
      "Epoch: 2 / 3, Step: 578 / 750 Loss: 0.2363\n",
      "Epoch: 2 / 3, Step: 579 / 750 Loss: 0.3973\n",
      "Epoch: 2 / 3, Step: 580 / 750 Loss: 0.5896\n",
      "Epoch: 2 / 3, Step: 581 / 750 Loss: 0.5169\n",
      "Epoch: 2 / 3, Step: 582 / 750 Loss: 0.4277\n",
      "Epoch: 2 / 3, Step: 583 / 750 Loss: 0.3340\n",
      "Epoch: 2 / 3, Step: 584 / 750 Loss: 0.1661\n",
      "Epoch: 2 / 3, Step: 585 / 750 Loss: 0.2154\n",
      "Epoch: 2 / 3, Step: 586 / 750 Loss: 0.2547\n",
      "Epoch: 2 / 3, Step: 587 / 750 Loss: 0.2637\n",
      "Epoch: 2 / 3, Step: 588 / 750 Loss: 0.4233\n",
      "Epoch: 2 / 3, Step: 589 / 750 Loss: 0.2889\n",
      "Epoch: 2 / 3, Step: 590 / 750 Loss: 0.3594\n",
      "Epoch: 2 / 3, Step: 591 / 750 Loss: 0.3419\n",
      "Epoch: 2 / 3, Step: 592 / 750 Loss: 0.3183\n",
      "Epoch: 2 / 3, Step: 593 / 750 Loss: 0.8789\n",
      "Epoch: 2 / 3, Step: 594 / 750 Loss: 0.1147\n",
      "Epoch: 2 / 3, Step: 595 / 750 Loss: 0.4045\n",
      "Epoch: 2 / 3, Step: 596 / 750 Loss: 0.1061\n",
      "Epoch: 2 / 3, Step: 597 / 750 Loss: 0.5102\n",
      "Epoch: 2 / 3, Step: 598 / 750 Loss: 0.3618\n",
      "Epoch: 2 / 3, Step: 599 / 750 Loss: 0.3247\n",
      "Epoch: 2 / 3, Step: 600 / 750 Loss: 0.3810\n",
      "Epoch: 2 / 3, Step: 601 / 750 Loss: 0.2661\n",
      "Epoch: 2 / 3, Step: 602 / 750 Loss: 0.3444\n",
      "Epoch: 2 / 3, Step: 603 / 750 Loss: 0.1165\n",
      "Epoch: 2 / 3, Step: 604 / 750 Loss: 0.6775\n",
      "Epoch: 2 / 3, Step: 605 / 750 Loss: 0.5058\n",
      "Epoch: 2 / 3, Step: 606 / 750 Loss: 0.6610\n",
      "Epoch: 2 / 3, Step: 607 / 750 Loss: 0.5023\n",
      "Epoch: 2 / 3, Step: 608 / 750 Loss: 0.3092\n",
      "Epoch: 2 / 3, Step: 609 / 750 Loss: 0.5160\n",
      "Epoch: 2 / 3, Step: 610 / 750 Loss: 0.4368\n",
      "Epoch: 2 / 3, Step: 611 / 750 Loss: 0.3275\n",
      "Epoch: 2 / 3, Step: 612 / 750 Loss: 0.6032\n",
      "Epoch: 2 / 3, Step: 613 / 750 Loss: 0.3746\n",
      "Epoch: 2 / 3, Step: 614 / 750 Loss: 0.6834\n",
      "Epoch: 2 / 3, Step: 615 / 750 Loss: 0.3657\n",
      "Epoch: 2 / 3, Step: 616 / 750 Loss: 0.2704\n",
      "Epoch: 2 / 3, Step: 617 / 750 Loss: 0.5616\n",
      "Epoch: 2 / 3, Step: 618 / 750 Loss: 0.4065\n",
      "Epoch: 2 / 3, Step: 619 / 750 Loss: 0.1696\n",
      "Epoch: 2 / 3, Step: 620 / 750 Loss: 0.6250\n",
      "Epoch: 2 / 3, Step: 621 / 750 Loss: 0.5858\n",
      "Epoch: 2 / 3, Step: 622 / 750 Loss: 0.2854\n",
      "Epoch: 2 / 3, Step: 623 / 750 Loss: 0.5362\n",
      "Epoch: 2 / 3, Step: 624 / 750 Loss: 0.4364\n",
      "Epoch: 2 / 3, Step: 625 / 750 Loss: 0.8332\n",
      "Epoch: 2 / 3, Step: 626 / 750 Loss: 0.4246\n",
      "Epoch: 2 / 3, Step: 627 / 750 Loss: 0.2822\n",
      "Epoch: 2 / 3, Step: 628 / 750 Loss: 0.5247\n",
      "Epoch: 2 / 3, Step: 629 / 750 Loss: 0.4430\n",
      "Epoch: 2 / 3, Step: 630 / 750 Loss: 0.4844\n",
      "Epoch: 2 / 3, Step: 631 / 750 Loss: 0.4600\n",
      "Epoch: 2 / 3, Step: 632 / 750 Loss: 0.3115\n",
      "Epoch: 2 / 3, Step: 633 / 750 Loss: 0.3561\n",
      "Epoch: 2 / 3, Step: 634 / 750 Loss: 0.3786\n",
      "Epoch: 2 / 3, Step: 635 / 750 Loss: 0.2460\n",
      "Epoch: 2 / 3, Step: 636 / 750 Loss: 0.2449\n",
      "Epoch: 2 / 3, Step: 637 / 750 Loss: 0.3632\n",
      "Epoch: 2 / 3, Step: 638 / 750 Loss: 0.3285\n",
      "Epoch: 2 / 3, Step: 639 / 750 Loss: 0.3479\n",
      "Epoch: 2 / 3, Step: 640 / 750 Loss: 0.5095\n",
      "Epoch: 2 / 3, Step: 641 / 750 Loss: 0.4116\n",
      "Epoch: 2 / 3, Step: 642 / 750 Loss: 0.3941\n",
      "Epoch: 2 / 3, Step: 643 / 750 Loss: 0.2227\n",
      "Epoch: 2 / 3, Step: 644 / 750 Loss: 0.3258\n",
      "Epoch: 2 / 3, Step: 645 / 750 Loss: 0.2937\n",
      "Epoch: 2 / 3, Step: 646 / 750 Loss: 0.4703\n",
      "Epoch: 2 / 3, Step: 647 / 750 Loss: 0.3757\n",
      "Epoch: 2 / 3, Step: 648 / 750 Loss: 0.3478\n",
      "Epoch: 2 / 3, Step: 649 / 750 Loss: 0.2957\n",
      "Epoch: 2 / 3, Step: 650 / 750 Loss: 0.1942\n",
      "Epoch: 2 / 3, Step: 651 / 750 Loss: 0.6687\n",
      "Epoch: 2 / 3, Step: 652 / 750 Loss: 0.6527\n",
      "Epoch: 2 / 3, Step: 653 / 750 Loss: 0.3018\n",
      "Epoch: 2 / 3, Step: 654 / 750 Loss: 0.4279\n",
      "Epoch: 2 / 3, Step: 655 / 750 Loss: 0.4781\n",
      "Epoch: 2 / 3, Step: 656 / 750 Loss: 0.4303\n",
      "Epoch: 2 / 3, Step: 657 / 750 Loss: 0.4286\n",
      "Epoch: 2 / 3, Step: 658 / 750 Loss: 0.2627\n",
      "Epoch: 2 / 3, Step: 659 / 750 Loss: 0.4147\n",
      "Epoch: 2 / 3, Step: 660 / 750 Loss: 0.2170\n",
      "Epoch: 2 / 3, Step: 661 / 750 Loss: 0.2372\n",
      "Epoch: 2 / 3, Step: 662 / 750 Loss: 0.5408\n",
      "Epoch: 2 / 3, Step: 663 / 750 Loss: 0.8175\n",
      "Epoch: 2 / 3, Step: 664 / 750 Loss: 0.2294\n",
      "Epoch: 2 / 3, Step: 665 / 750 Loss: 0.4090\n",
      "Epoch: 2 / 3, Step: 666 / 750 Loss: 0.2993\n",
      "Epoch: 2 / 3, Step: 667 / 750 Loss: 0.2268\n",
      "Epoch: 2 / 3, Step: 668 / 750 Loss: 0.4321\n",
      "Epoch: 2 / 3, Step: 669 / 750 Loss: 0.3241\n",
      "Epoch: 2 / 3, Step: 670 / 750 Loss: 0.3996\n",
      "Epoch: 2 / 3, Step: 671 / 750 Loss: 0.5283\n",
      "Epoch: 2 / 3, Step: 672 / 750 Loss: 0.4756\n",
      "Epoch: 2 / 3, Step: 673 / 750 Loss: 0.3405\n",
      "Epoch: 2 / 3, Step: 674 / 750 Loss: 0.2143\n",
      "Epoch: 2 / 3, Step: 675 / 750 Loss: 0.3210\n",
      "Epoch: 2 / 3, Step: 676 / 750 Loss: 0.2554\n",
      "Epoch: 2 / 3, Step: 677 / 750 Loss: 0.1123\n",
      "Epoch: 2 / 3, Step: 678 / 750 Loss: 0.3702\n",
      "Epoch: 2 / 3, Step: 679 / 750 Loss: 0.2267\n",
      "Epoch: 2 / 3, Step: 680 / 750 Loss: 0.5316\n",
      "Epoch: 2 / 3, Step: 681 / 750 Loss: 0.2984\n",
      "Epoch: 2 / 3, Step: 682 / 750 Loss: 0.2762\n",
      "Epoch: 2 / 3, Step: 683 / 750 Loss: 0.2794\n",
      "Epoch: 2 / 3, Step: 684 / 750 Loss: 0.4013\n",
      "Epoch: 2 / 3, Step: 685 / 750 Loss: 0.5197\n",
      "Epoch: 2 / 3, Step: 686 / 750 Loss: 0.5088\n",
      "Epoch: 2 / 3, Step: 687 / 750 Loss: 0.1495\n",
      "Epoch: 2 / 3, Step: 688 / 750 Loss: 0.4683\n",
      "Epoch: 2 / 3, Step: 689 / 750 Loss: 0.3067\n",
      "Epoch: 2 / 3, Step: 690 / 750 Loss: 0.2902\n",
      "Epoch: 2 / 3, Step: 691 / 750 Loss: 0.3417\n",
      "Epoch: 2 / 3, Step: 692 / 750 Loss: 0.4888\n",
      "Epoch: 2 / 3, Step: 693 / 750 Loss: 0.4166\n",
      "Epoch: 2 / 3, Step: 694 / 750 Loss: 0.4684\n",
      "Epoch: 2 / 3, Step: 695 / 750 Loss: 0.3797\n",
      "Epoch: 2 / 3, Step: 696 / 750 Loss: 0.4663\n",
      "Epoch: 2 / 3, Step: 697 / 750 Loss: 0.1751\n",
      "Epoch: 2 / 3, Step: 698 / 750 Loss: 0.6060\n",
      "Epoch: 2 / 3, Step: 699 / 750 Loss: 0.2830\n",
      "Epoch: 2 / 3, Step: 700 / 750 Loss: 0.3786\n",
      "Epoch: 2 / 3, Step: 701 / 750 Loss: 0.3364\n",
      "Epoch: 2 / 3, Step: 702 / 750 Loss: 0.5008\n",
      "Epoch: 2 / 3, Step: 703 / 750 Loss: 0.4179\n",
      "Epoch: 2 / 3, Step: 704 / 750 Loss: 0.2807\n",
      "Epoch: 2 / 3, Step: 705 / 750 Loss: 0.2034\n",
      "Epoch: 2 / 3, Step: 706 / 750 Loss: 0.3177\n",
      "Epoch: 2 / 3, Step: 707 / 750 Loss: 0.5183\n",
      "Epoch: 2 / 3, Step: 708 / 750 Loss: 0.3709\n",
      "Epoch: 2 / 3, Step: 709 / 750 Loss: 0.3621\n",
      "Epoch: 2 / 3, Step: 710 / 750 Loss: 0.5807\n",
      "Epoch: 2 / 3, Step: 711 / 750 Loss: 0.3353\n",
      "Epoch: 2 / 3, Step: 712 / 750 Loss: 0.3724\n",
      "Epoch: 2 / 3, Step: 713 / 750 Loss: 0.3124\n",
      "Epoch: 2 / 3, Step: 714 / 750 Loss: 0.4076\n",
      "Epoch: 2 / 3, Step: 715 / 750 Loss: 0.4591\n",
      "Epoch: 2 / 3, Step: 716 / 750 Loss: 0.3278\n",
      "Epoch: 2 / 3, Step: 717 / 750 Loss: 0.3470\n",
      "Epoch: 2 / 3, Step: 718 / 750 Loss: 0.2647\n",
      "Epoch: 2 / 3, Step: 719 / 750 Loss: 0.4266\n",
      "Epoch: 2 / 3, Step: 720 / 750 Loss: 0.2668\n",
      "Epoch: 2 / 3, Step: 721 / 750 Loss: 0.3962\n",
      "Epoch: 2 / 3, Step: 722 / 750 Loss: 0.2558\n",
      "Epoch: 2 / 3, Step: 723 / 750 Loss: 0.3502\n",
      "Epoch: 2 / 3, Step: 724 / 750 Loss: 0.4741\n",
      "Epoch: 2 / 3, Step: 725 / 750 Loss: 0.4488\n",
      "Epoch: 2 / 3, Step: 726 / 750 Loss: 0.2608\n",
      "Epoch: 2 / 3, Step: 727 / 750 Loss: 0.4418\n",
      "Epoch: 2 / 3, Step: 728 / 750 Loss: 0.4523\n",
      "Epoch: 2 / 3, Step: 729 / 750 Loss: 0.4097\n",
      "Epoch: 2 / 3, Step: 730 / 750 Loss: 0.2902\n",
      "Epoch: 2 / 3, Step: 731 / 750 Loss: 0.2357\n",
      "Epoch: 2 / 3, Step: 732 / 750 Loss: 0.1864\n",
      "Epoch: 2 / 3, Step: 733 / 750 Loss: 0.3029\n",
      "Epoch: 2 / 3, Step: 734 / 750 Loss: 0.2444\n",
      "Epoch: 2 / 3, Step: 735 / 750 Loss: 0.4879\n",
      "Epoch: 2 / 3, Step: 736 / 750 Loss: 0.3719\n",
      "Epoch: 2 / 3, Step: 737 / 750 Loss: 0.2779\n",
      "Epoch: 2 / 3, Step: 738 / 750 Loss: 0.3348\n",
      "Epoch: 2 / 3, Step: 739 / 750 Loss: 0.4355\n",
      "Epoch: 2 / 3, Step: 740 / 750 Loss: 0.3798\n",
      "Epoch: 2 / 3, Step: 741 / 750 Loss: 0.5969\n",
      "Epoch: 2 / 3, Step: 742 / 750 Loss: 0.3947\n",
      "Epoch: 2 / 3, Step: 743 / 750 Loss: 0.4744\n",
      "Epoch: 2 / 3, Step: 744 / 750 Loss: 0.4833\n",
      "Epoch: 2 / 3, Step: 745 / 750 Loss: 0.3321\n",
      "Epoch: 2 / 3, Step: 746 / 750 Loss: 0.2809\n",
      "Epoch: 2 / 3, Step: 747 / 750 Loss: 0.2104\n",
      "Epoch: 2 / 3, Step: 748 / 750 Loss: 0.4509\n",
      "Epoch: 2 / 3, Step: 749 / 750 Loss: 0.4979\n",
      "Epoch: 3 / 3, Step: 0 / 750 Loss: 0.3519\n",
      "Epoch: 3 / 3, Step: 1 / 750 Loss: 0.4912\n",
      "Epoch: 3 / 3, Step: 2 / 750 Loss: 0.4427\n",
      "Epoch: 3 / 3, Step: 3 / 750 Loss: 0.5089\n",
      "Epoch: 3 / 3, Step: 4 / 750 Loss: 0.2957\n",
      "Epoch: 3 / 3, Step: 5 / 750 Loss: 0.3827\n",
      "Epoch: 3 / 3, Step: 6 / 750 Loss: 0.1493\n",
      "Epoch: 3 / 3, Step: 7 / 750 Loss: 0.4841\n",
      "Epoch: 3 / 3, Step: 8 / 750 Loss: 0.2797\n",
      "Epoch: 3 / 3, Step: 9 / 750 Loss: 0.2311\n",
      "Epoch: 3 / 3, Step: 10 / 750 Loss: 0.3231\n",
      "Epoch: 3 / 3, Step: 11 / 750 Loss: 0.2706\n",
      "Epoch: 3 / 3, Step: 12 / 750 Loss: 0.1764\n",
      "Epoch: 3 / 3, Step: 13 / 750 Loss: 0.3652\n",
      "Epoch: 3 / 3, Step: 14 / 750 Loss: 0.2359\n",
      "Epoch: 3 / 3, Step: 15 / 750 Loss: 0.3272\n",
      "Epoch: 3 / 3, Step: 16 / 750 Loss: 0.3398\n",
      "Epoch: 3 / 3, Step: 17 / 750 Loss: 0.4503\n",
      "Epoch: 3 / 3, Step: 18 / 750 Loss: 0.2946\n",
      "Epoch: 3 / 3, Step: 19 / 750 Loss: 0.3733\n",
      "Epoch: 3 / 3, Step: 20 / 750 Loss: 0.3045\n",
      "Epoch: 3 / 3, Step: 21 / 750 Loss: 0.1186\n",
      "Epoch: 3 / 3, Step: 22 / 750 Loss: 0.3139\n",
      "Epoch: 3 / 3, Step: 23 / 750 Loss: 0.3331\n",
      "Epoch: 3 / 3, Step: 24 / 750 Loss: 0.1605\n",
      "Epoch: 3 / 3, Step: 25 / 750 Loss: 0.4749\n",
      "Epoch: 3 / 3, Step: 26 / 750 Loss: 0.2795\n",
      "Epoch: 3 / 3, Step: 27 / 750 Loss: 0.3902\n",
      "Epoch: 3 / 3, Step: 28 / 750 Loss: 0.4770\n",
      "Epoch: 3 / 3, Step: 29 / 750 Loss: 0.4348\n",
      "Epoch: 3 / 3, Step: 30 / 750 Loss: 0.4525\n",
      "Epoch: 3 / 3, Step: 31 / 750 Loss: 0.2065\n",
      "Epoch: 3 / 3, Step: 32 / 750 Loss: 0.4188\n",
      "Epoch: 3 / 3, Step: 33 / 750 Loss: 0.2558\n",
      "Epoch: 3 / 3, Step: 34 / 750 Loss: 0.4166\n",
      "Epoch: 3 / 3, Step: 35 / 750 Loss: 0.3037\n",
      "Epoch: 3 / 3, Step: 36 / 750 Loss: 0.5632\n",
      "Epoch: 3 / 3, Step: 37 / 750 Loss: 0.4225\n",
      "Epoch: 3 / 3, Step: 38 / 750 Loss: 0.2619\n",
      "Epoch: 3 / 3, Step: 39 / 750 Loss: 0.1232\n",
      "Epoch: 3 / 3, Step: 40 / 750 Loss: 0.5657\n",
      "Epoch: 3 / 3, Step: 41 / 750 Loss: 0.2323\n",
      "Epoch: 3 / 3, Step: 42 / 750 Loss: 0.3856\n",
      "Epoch: 3 / 3, Step: 43 / 750 Loss: 0.3020\n",
      "Epoch: 3 / 3, Step: 44 / 750 Loss: 0.2450\n",
      "Epoch: 3 / 3, Step: 45 / 750 Loss: 0.3029\n",
      "Epoch: 3 / 3, Step: 46 / 750 Loss: 0.3204\n",
      "Epoch: 3 / 3, Step: 47 / 750 Loss: 0.4055\n",
      "Epoch: 3 / 3, Step: 48 / 750 Loss: 0.4093\n",
      "Epoch: 3 / 3, Step: 49 / 750 Loss: 0.3921\n",
      "Epoch: 3 / 3, Step: 50 / 750 Loss: 0.5013\n",
      "Epoch: 3 / 3, Step: 51 / 750 Loss: 0.4335\n",
      "Epoch: 3 / 3, Step: 52 / 750 Loss: 0.2702\n",
      "Epoch: 3 / 3, Step: 53 / 750 Loss: 0.3180\n",
      "Epoch: 3 / 3, Step: 54 / 750 Loss: 0.1703\n",
      "Epoch: 3 / 3, Step: 55 / 750 Loss: 0.2793\n",
      "Epoch: 3 / 3, Step: 56 / 750 Loss: 0.5759\n",
      "Epoch: 3 / 3, Step: 57 / 750 Loss: 0.3241\n",
      "Epoch: 3 / 3, Step: 58 / 750 Loss: 0.2195\n",
      "Epoch: 3 / 3, Step: 59 / 750 Loss: 0.2488\n",
      "Epoch: 3 / 3, Step: 60 / 750 Loss: 0.1754\n",
      "Epoch: 3 / 3, Step: 61 / 750 Loss: 0.4003\n",
      "Epoch: 3 / 3, Step: 62 / 750 Loss: 0.3041\n",
      "Epoch: 3 / 3, Step: 63 / 750 Loss: 0.4193\n",
      "Epoch: 3 / 3, Step: 64 / 750 Loss: 0.2690\n",
      "Epoch: 3 / 3, Step: 65 / 750 Loss: 0.1389\n",
      "Epoch: 3 / 3, Step: 66 / 750 Loss: 0.7025\n",
      "Epoch: 3 / 3, Step: 67 / 750 Loss: 0.2573\n",
      "Epoch: 3 / 3, Step: 68 / 750 Loss: 0.2069\n",
      "Epoch: 3 / 3, Step: 69 / 750 Loss: 0.3215\n",
      "Epoch: 3 / 3, Step: 70 / 750 Loss: 0.1867\n",
      "Epoch: 3 / 3, Step: 71 / 750 Loss: 0.6683\n",
      "Epoch: 3 / 3, Step: 72 / 750 Loss: 0.3876\n",
      "Epoch: 3 / 3, Step: 73 / 750 Loss: 0.3553\n",
      "Epoch: 3 / 3, Step: 74 / 750 Loss: 0.3295\n",
      "Epoch: 3 / 3, Step: 75 / 750 Loss: 0.1773\n",
      "Epoch: 3 / 3, Step: 76 / 750 Loss: 0.3574\n",
      "Epoch: 3 / 3, Step: 77 / 750 Loss: 0.2638\n",
      "Epoch: 3 / 3, Step: 78 / 750 Loss: 0.4559\n",
      "Epoch: 3 / 3, Step: 79 / 750 Loss: 0.2947\n",
      "Epoch: 3 / 3, Step: 80 / 750 Loss: 0.4558\n",
      "Epoch: 3 / 3, Step: 81 / 750 Loss: 0.3839\n",
      "Epoch: 3 / 3, Step: 82 / 750 Loss: 0.1375\n",
      "Epoch: 3 / 3, Step: 83 / 750 Loss: 0.5280\n",
      "Epoch: 3 / 3, Step: 84 / 750 Loss: 0.3657\n",
      "Epoch: 3 / 3, Step: 85 / 750 Loss: 0.2471\n",
      "Epoch: 3 / 3, Step: 86 / 750 Loss: 0.5148\n",
      "Epoch: 3 / 3, Step: 87 / 750 Loss: 0.1948\n",
      "Epoch: 3 / 3, Step: 88 / 750 Loss: 0.4696\n",
      "Epoch: 3 / 3, Step: 89 / 750 Loss: 0.2338\n",
      "Epoch: 3 / 3, Step: 90 / 750 Loss: 0.3590\n",
      "Epoch: 3 / 3, Step: 91 / 750 Loss: 0.1055\n",
      "Epoch: 3 / 3, Step: 92 / 750 Loss: 0.1518\n",
      "Epoch: 3 / 3, Step: 93 / 750 Loss: 0.3333\n",
      "Epoch: 3 / 3, Step: 94 / 750 Loss: 0.2653\n",
      "Epoch: 3 / 3, Step: 95 / 750 Loss: 0.4503\n",
      "Epoch: 3 / 3, Step: 96 / 750 Loss: 0.3024\n",
      "Epoch: 3 / 3, Step: 97 / 750 Loss: 0.2338\n",
      "Epoch: 3 / 3, Step: 98 / 750 Loss: 0.1821\n",
      "Epoch: 3 / 3, Step: 99 / 750 Loss: 0.1248\n",
      "Epoch: 3 / 3, Step: 100 / 750 Loss: 0.4981\n",
      "Epoch: 3 / 3, Step: 101 / 750 Loss: 0.1695\n",
      "Epoch: 3 / 3, Step: 102 / 750 Loss: 0.4959\n",
      "Epoch: 3 / 3, Step: 103 / 750 Loss: 0.2955\n",
      "Epoch: 3 / 3, Step: 104 / 750 Loss: 0.3999\n",
      "Epoch: 3 / 3, Step: 105 / 750 Loss: 0.2629\n",
      "Epoch: 3 / 3, Step: 106 / 750 Loss: 0.2068\n",
      "Epoch: 3 / 3, Step: 107 / 750 Loss: 0.4570\n",
      "Epoch: 3 / 3, Step: 108 / 750 Loss: 0.2123\n",
      "Epoch: 3 / 3, Step: 109 / 750 Loss: 0.2536\n",
      "Epoch: 3 / 3, Step: 110 / 750 Loss: 0.3997\n",
      "Epoch: 3 / 3, Step: 111 / 750 Loss: 0.2359\n",
      "Epoch: 3 / 3, Step: 112 / 750 Loss: 0.5892\n",
      "Epoch: 3 / 3, Step: 113 / 750 Loss: 0.4548\n",
      "Epoch: 3 / 3, Step: 114 / 750 Loss: 0.1911\n",
      "Epoch: 3 / 3, Step: 115 / 750 Loss: 0.5896\n",
      "Epoch: 3 / 3, Step: 116 / 750 Loss: 0.4658\n",
      "Epoch: 3 / 3, Step: 117 / 750 Loss: 0.4257\n",
      "Epoch: 3 / 3, Step: 118 / 750 Loss: 0.2394\n",
      "Epoch: 3 / 3, Step: 119 / 750 Loss: 0.3281\n",
      "Epoch: 3 / 3, Step: 120 / 750 Loss: 0.5890\n",
      "Epoch: 3 / 3, Step: 121 / 750 Loss: 0.3318\n",
      "Epoch: 3 / 3, Step: 122 / 750 Loss: 0.4038\n",
      "Epoch: 3 / 3, Step: 123 / 750 Loss: 0.2956\n",
      "Epoch: 3 / 3, Step: 124 / 750 Loss: 0.1965\n",
      "Epoch: 3 / 3, Step: 125 / 750 Loss: 0.4544\n",
      "Epoch: 3 / 3, Step: 126 / 750 Loss: 0.2918\n",
      "Epoch: 3 / 3, Step: 127 / 750 Loss: 0.2492\n",
      "Epoch: 3 / 3, Step: 128 / 750 Loss: 0.3069\n",
      "Epoch: 3 / 3, Step: 129 / 750 Loss: 0.3637\n",
      "Epoch: 3 / 3, Step: 130 / 750 Loss: 0.4853\n",
      "Epoch: 3 / 3, Step: 131 / 750 Loss: 0.3786\n",
      "Epoch: 3 / 3, Step: 132 / 750 Loss: 0.5407\n",
      "Epoch: 3 / 3, Step: 133 / 750 Loss: 0.2668\n",
      "Epoch: 3 / 3, Step: 134 / 750 Loss: 0.2218\n",
      "Epoch: 3 / 3, Step: 135 / 750 Loss: 0.5812\n",
      "Epoch: 3 / 3, Step: 136 / 750 Loss: 0.3209\n",
      "Epoch: 3 / 3, Step: 137 / 750 Loss: 0.4775\n",
      "Epoch: 3 / 3, Step: 138 / 750 Loss: 0.6097\n",
      "Epoch: 3 / 3, Step: 139 / 750 Loss: 0.2646\n",
      "Epoch: 3 / 3, Step: 140 / 750 Loss: 0.4652\n",
      "Epoch: 3 / 3, Step: 141 / 750 Loss: 0.5772\n",
      "Epoch: 3 / 3, Step: 142 / 750 Loss: 0.2824\n",
      "Epoch: 3 / 3, Step: 143 / 750 Loss: 0.2930\n",
      "Epoch: 3 / 3, Step: 144 / 750 Loss: 0.4701\n",
      "Epoch: 3 / 3, Step: 145 / 750 Loss: 0.2133\n",
      "Epoch: 3 / 3, Step: 146 / 750 Loss: 0.3268\n",
      "Epoch: 3 / 3, Step: 147 / 750 Loss: 0.2087\n",
      "Epoch: 3 / 3, Step: 148 / 750 Loss: 0.4078\n",
      "Epoch: 3 / 3, Step: 149 / 750 Loss: 0.4091\n",
      "Epoch: 3 / 3, Step: 150 / 750 Loss: 0.2269\n",
      "Epoch: 3 / 3, Step: 151 / 750 Loss: 0.2367\n",
      "Epoch: 3 / 3, Step: 152 / 750 Loss: 0.4753\n",
      "Epoch: 3 / 3, Step: 153 / 750 Loss: 0.5837\n",
      "Epoch: 3 / 3, Step: 154 / 750 Loss: 0.4249\n",
      "Epoch: 3 / 3, Step: 155 / 750 Loss: 0.3745\n",
      "Epoch: 3 / 3, Step: 156 / 750 Loss: 0.4959\n",
      "Epoch: 3 / 3, Step: 157 / 750 Loss: 0.3625\n",
      "Epoch: 3 / 3, Step: 158 / 750 Loss: 0.3061\n",
      "Epoch: 3 / 3, Step: 159 / 750 Loss: 0.3472\n",
      "Epoch: 3 / 3, Step: 160 / 750 Loss: 0.1880\n",
      "Epoch: 3 / 3, Step: 161 / 750 Loss: 0.2002\n",
      "Epoch: 3 / 3, Step: 162 / 750 Loss: 0.1828\n",
      "Epoch: 3 / 3, Step: 163 / 750 Loss: 0.2506\n",
      "Epoch: 3 / 3, Step: 164 / 750 Loss: 0.2206\n",
      "Epoch: 3 / 3, Step: 165 / 750 Loss: 0.1689\n",
      "Epoch: 3 / 3, Step: 166 / 750 Loss: 0.3664\n",
      "Epoch: 3 / 3, Step: 167 / 750 Loss: 0.4981\n",
      "Epoch: 3 / 3, Step: 168 / 750 Loss: 0.1506\n",
      "Epoch: 3 / 3, Step: 169 / 750 Loss: 0.4089\n",
      "Epoch: 3 / 3, Step: 170 / 750 Loss: 0.3833\n",
      "Epoch: 3 / 3, Step: 171 / 750 Loss: 0.3591\n",
      "Epoch: 3 / 3, Step: 172 / 750 Loss: 0.3702\n",
      "Epoch: 3 / 3, Step: 173 / 750 Loss: 0.2048\n",
      "Epoch: 3 / 3, Step: 174 / 750 Loss: 0.4054\n",
      "Epoch: 3 / 3, Step: 175 / 750 Loss: 0.3367\n",
      "Epoch: 3 / 3, Step: 176 / 750 Loss: 0.4290\n",
      "Epoch: 3 / 3, Step: 177 / 750 Loss: 0.2695\n",
      "Epoch: 3 / 3, Step: 178 / 750 Loss: 0.3373\n",
      "Epoch: 3 / 3, Step: 179 / 750 Loss: 0.3080\n",
      "Epoch: 3 / 3, Step: 180 / 750 Loss: 0.3659\n",
      "Epoch: 3 / 3, Step: 181 / 750 Loss: 0.3196\n",
      "Epoch: 3 / 3, Step: 182 / 750 Loss: 0.2550\n",
      "Epoch: 3 / 3, Step: 183 / 750 Loss: 0.4044\n",
      "Epoch: 3 / 3, Step: 184 / 750 Loss: 0.3341\n",
      "Epoch: 3 / 3, Step: 185 / 750 Loss: 0.1702\n",
      "Epoch: 3 / 3, Step: 186 / 750 Loss: 0.2198\n",
      "Epoch: 3 / 3, Step: 187 / 750 Loss: 0.2847\n",
      "Epoch: 3 / 3, Step: 188 / 750 Loss: 0.2669\n",
      "Epoch: 3 / 3, Step: 189 / 750 Loss: 0.1972\n",
      "Epoch: 3 / 3, Step: 190 / 750 Loss: 0.1598\n",
      "Epoch: 3 / 3, Step: 191 / 750 Loss: 0.3061\n",
      "Epoch: 3 / 3, Step: 192 / 750 Loss: 0.2841\n",
      "Epoch: 3 / 3, Step: 193 / 750 Loss: 0.3580\n",
      "Epoch: 3 / 3, Step: 194 / 750 Loss: 0.2967\n",
      "Epoch: 3 / 3, Step: 195 / 750 Loss: 0.1707\n",
      "Epoch: 3 / 3, Step: 196 / 750 Loss: 0.2566\n",
      "Epoch: 3 / 3, Step: 197 / 750 Loss: 0.1594\n",
      "Epoch: 3 / 3, Step: 198 / 750 Loss: 0.3957\n",
      "Epoch: 3 / 3, Step: 199 / 750 Loss: 0.2220\n",
      "Epoch: 3 / 3, Step: 200 / 750 Loss: 0.4217\n",
      "Epoch: 3 / 3, Step: 201 / 750 Loss: 0.2679\n",
      "Epoch: 3 / 3, Step: 202 / 750 Loss: 0.3871\n",
      "Epoch: 3 / 3, Step: 203 / 750 Loss: 0.4708\n",
      "Epoch: 3 / 3, Step: 204 / 750 Loss: 0.3196\n",
      "Epoch: 3 / 3, Step: 205 / 750 Loss: 0.5021\n",
      "Epoch: 3 / 3, Step: 206 / 750 Loss: 0.3336\n",
      "Epoch: 3 / 3, Step: 207 / 750 Loss: 0.3931\n",
      "Epoch: 3 / 3, Step: 208 / 750 Loss: 0.3661\n",
      "Epoch: 3 / 3, Step: 209 / 750 Loss: 0.2919\n",
      "Epoch: 3 / 3, Step: 210 / 750 Loss: 0.2108\n",
      "Epoch: 3 / 3, Step: 211 / 750 Loss: 0.5405\n",
      "Epoch: 3 / 3, Step: 212 / 750 Loss: 0.3439\n",
      "Epoch: 3 / 3, Step: 213 / 750 Loss: 0.2658\n",
      "Epoch: 3 / 3, Step: 214 / 750 Loss: 0.4891\n",
      "Epoch: 3 / 3, Step: 215 / 750 Loss: 0.1368\n",
      "Epoch: 3 / 3, Step: 216 / 750 Loss: 0.4754\n",
      "Epoch: 3 / 3, Step: 217 / 750 Loss: 0.2834\n",
      "Epoch: 3 / 3, Step: 218 / 750 Loss: 0.2226\n",
      "Epoch: 3 / 3, Step: 219 / 750 Loss: 0.2579\n",
      "Epoch: 3 / 3, Step: 220 / 750 Loss: 0.2982\n",
      "Epoch: 3 / 3, Step: 221 / 750 Loss: 0.3499\n",
      "Epoch: 3 / 3, Step: 222 / 750 Loss: 0.7131\n",
      "Epoch: 3 / 3, Step: 223 / 750 Loss: 0.2106\n",
      "Epoch: 3 / 3, Step: 224 / 750 Loss: 0.2614\n",
      "Epoch: 3 / 3, Step: 225 / 750 Loss: 0.4274\n",
      "Epoch: 3 / 3, Step: 226 / 750 Loss: 0.4302\n",
      "Epoch: 3 / 3, Step: 227 / 750 Loss: 0.2653\n",
      "Epoch: 3 / 3, Step: 228 / 750 Loss: 0.2162\n",
      "Epoch: 3 / 3, Step: 229 / 750 Loss: 0.4102\n",
      "Epoch: 3 / 3, Step: 230 / 750 Loss: 0.3824\n",
      "Epoch: 3 / 3, Step: 231 / 750 Loss: 0.1916\n",
      "Epoch: 3 / 3, Step: 232 / 750 Loss: 0.5932\n",
      "Epoch: 3 / 3, Step: 233 / 750 Loss: 0.1400\n",
      "Epoch: 3 / 3, Step: 234 / 750 Loss: 0.5398\n",
      "Epoch: 3 / 3, Step: 235 / 750 Loss: 0.3945\n",
      "Epoch: 3 / 3, Step: 236 / 750 Loss: 0.3233\n",
      "Epoch: 3 / 3, Step: 237 / 750 Loss: 0.4449\n",
      "Epoch: 3 / 3, Step: 238 / 750 Loss: 0.3889\n",
      "Epoch: 3 / 3, Step: 239 / 750 Loss: 0.3209\n",
      "Epoch: 3 / 3, Step: 240 / 750 Loss: 0.1364\n",
      "Epoch: 3 / 3, Step: 241 / 750 Loss: 0.1881\n",
      "Epoch: 3 / 3, Step: 242 / 750 Loss: 0.2596\n",
      "Epoch: 3 / 3, Step: 243 / 750 Loss: 0.4906\n",
      "Epoch: 3 / 3, Step: 244 / 750 Loss: 0.3039\n",
      "Epoch: 3 / 3, Step: 245 / 750 Loss: 0.2913\n",
      "Epoch: 3 / 3, Step: 246 / 750 Loss: 0.5316\n",
      "Epoch: 3 / 3, Step: 247 / 750 Loss: 0.3164\n",
      "Epoch: 3 / 3, Step: 248 / 750 Loss: 0.2658\n",
      "Epoch: 3 / 3, Step: 249 / 750 Loss: 0.3241\n",
      "Epoch: 3 / 3, Step: 250 / 750 Loss: 0.2775\n",
      "Epoch: 3 / 3, Step: 251 / 750 Loss: 0.5084\n",
      "Epoch: 3 / 3, Step: 252 / 750 Loss: 0.2347\n",
      "Epoch: 3 / 3, Step: 253 / 750 Loss: 0.6122\n",
      "Epoch: 3 / 3, Step: 254 / 750 Loss: 0.2420\n",
      "Epoch: 3 / 3, Step: 255 / 750 Loss: 0.2212\n",
      "Epoch: 3 / 3, Step: 256 / 750 Loss: 0.3180\n",
      "Epoch: 3 / 3, Step: 257 / 750 Loss: 0.3827\n",
      "Epoch: 3 / 3, Step: 258 / 750 Loss: 0.2847\n",
      "Epoch: 3 / 3, Step: 259 / 750 Loss: 0.3778\n",
      "Epoch: 3 / 3, Step: 260 / 750 Loss: 0.2122\n",
      "Epoch: 3 / 3, Step: 261 / 750 Loss: 0.6168\n",
      "Epoch: 3 / 3, Step: 262 / 750 Loss: 0.4287\n",
      "Epoch: 3 / 3, Step: 263 / 750 Loss: 0.2168\n",
      "Epoch: 3 / 3, Step: 264 / 750 Loss: 0.2609\n",
      "Epoch: 3 / 3, Step: 265 / 750 Loss: 0.3523\n",
      "Epoch: 3 / 3, Step: 266 / 750 Loss: 0.2052\n",
      "Epoch: 3 / 3, Step: 267 / 750 Loss: 0.4121\n",
      "Epoch: 3 / 3, Step: 268 / 750 Loss: 0.5031\n",
      "Epoch: 3 / 3, Step: 269 / 750 Loss: 0.4264\n",
      "Epoch: 3 / 3, Step: 270 / 750 Loss: 0.3283\n",
      "Epoch: 3 / 3, Step: 271 / 750 Loss: 0.1760\n",
      "Epoch: 3 / 3, Step: 272 / 750 Loss: 0.2137\n",
      "Epoch: 3 / 3, Step: 273 / 750 Loss: 0.3019\n",
      "Epoch: 3 / 3, Step: 274 / 750 Loss: 0.1923\n",
      "Epoch: 3 / 3, Step: 275 / 750 Loss: 0.4442\n",
      "Epoch: 3 / 3, Step: 276 / 750 Loss: 0.5298\n",
      "Epoch: 3 / 3, Step: 277 / 750 Loss: 0.2684\n",
      "Epoch: 3 / 3, Step: 278 / 750 Loss: 0.1923\n",
      "Epoch: 3 / 3, Step: 279 / 750 Loss: 0.4019\n",
      "Epoch: 3 / 3, Step: 280 / 750 Loss: 0.3182\n",
      "Epoch: 3 / 3, Step: 281 / 750 Loss: 0.4418\n",
      "Epoch: 3 / 3, Step: 282 / 750 Loss: 0.1557\n",
      "Epoch: 3 / 3, Step: 283 / 750 Loss: 0.1893\n",
      "Epoch: 3 / 3, Step: 284 / 750 Loss: 0.6958\n",
      "Epoch: 3 / 3, Step: 285 / 750 Loss: 0.3412\n",
      "Epoch: 3 / 3, Step: 286 / 750 Loss: 0.1878\n",
      "Epoch: 3 / 3, Step: 287 / 750 Loss: 0.3777\n",
      "Epoch: 3 / 3, Step: 288 / 750 Loss: 0.5296\n",
      "Epoch: 3 / 3, Step: 289 / 750 Loss: 0.3284\n",
      "Epoch: 3 / 3, Step: 290 / 750 Loss: 0.5301\n",
      "Epoch: 3 / 3, Step: 291 / 750 Loss: 0.2261\n",
      "Epoch: 3 / 3, Step: 292 / 750 Loss: 0.3799\n",
      "Epoch: 3 / 3, Step: 293 / 750 Loss: 0.1788\n",
      "Epoch: 3 / 3, Step: 294 / 750 Loss: 0.3472\n",
      "Epoch: 3 / 3, Step: 295 / 750 Loss: 0.3873\n",
      "Epoch: 3 / 3, Step: 296 / 750 Loss: 0.4261\n",
      "Epoch: 3 / 3, Step: 297 / 750 Loss: 0.2649\n",
      "Epoch: 3 / 3, Step: 298 / 750 Loss: 0.2789\n",
      "Epoch: 3 / 3, Step: 299 / 750 Loss: 0.5122\n",
      "Epoch: 3 / 3, Step: 300 / 750 Loss: 0.3378\n",
      "Epoch: 3 / 3, Step: 301 / 750 Loss: 0.3890\n",
      "Epoch: 3 / 3, Step: 302 / 750 Loss: 0.2841\n",
      "Epoch: 3 / 3, Step: 303 / 750 Loss: 0.2067\n",
      "Epoch: 3 / 3, Step: 304 / 750 Loss: 0.4612\n",
      "Epoch: 3 / 3, Step: 305 / 750 Loss: 0.2322\n",
      "Epoch: 3 / 3, Step: 306 / 750 Loss: 0.7063\n",
      "Epoch: 3 / 3, Step: 307 / 750 Loss: 0.2059\n",
      "Epoch: 3 / 3, Step: 308 / 750 Loss: 0.4284\n",
      "Epoch: 3 / 3, Step: 309 / 750 Loss: 0.3871\n",
      "Epoch: 3 / 3, Step: 310 / 750 Loss: 0.2964\n",
      "Epoch: 3 / 3, Step: 311 / 750 Loss: 0.1708\n",
      "Epoch: 3 / 3, Step: 312 / 750 Loss: 0.3962\n",
      "Epoch: 3 / 3, Step: 313 / 750 Loss: 0.1580\n",
      "Epoch: 3 / 3, Step: 314 / 750 Loss: 0.3092\n",
      "Epoch: 3 / 3, Step: 315 / 750 Loss: 0.2640\n",
      "Epoch: 3 / 3, Step: 316 / 750 Loss: 0.1462\n",
      "Epoch: 3 / 3, Step: 317 / 750 Loss: 0.3584\n",
      "Epoch: 3 / 3, Step: 318 / 750 Loss: 0.4503\n",
      "Epoch: 3 / 3, Step: 319 / 750 Loss: 0.4580\n",
      "Epoch: 3 / 3, Step: 320 / 750 Loss: 0.2674\n",
      "Epoch: 3 / 3, Step: 321 / 750 Loss: 0.3120\n",
      "Epoch: 3 / 3, Step: 322 / 750 Loss: 0.3598\n",
      "Epoch: 3 / 3, Step: 323 / 750 Loss: 0.1736\n",
      "Epoch: 3 / 3, Step: 324 / 750 Loss: 0.4534\n",
      "Epoch: 3 / 3, Step: 325 / 750 Loss: 0.3369\n",
      "Epoch: 3 / 3, Step: 326 / 750 Loss: 0.4311\n",
      "Epoch: 3 / 3, Step: 327 / 750 Loss: 0.3166\n",
      "Epoch: 3 / 3, Step: 328 / 750 Loss: 0.3299\n",
      "Epoch: 3 / 3, Step: 329 / 750 Loss: 0.2506\n",
      "Epoch: 3 / 3, Step: 330 / 750 Loss: 0.1123\n",
      "Epoch: 3 / 3, Step: 331 / 750 Loss: 0.2342\n",
      "Epoch: 3 / 3, Step: 332 / 750 Loss: 0.4068\n",
      "Epoch: 3 / 3, Step: 333 / 750 Loss: 0.1363\n",
      "Epoch: 3 / 3, Step: 334 / 750 Loss: 0.2683\n",
      "Epoch: 3 / 3, Step: 335 / 750 Loss: 0.4291\n",
      "Epoch: 3 / 3, Step: 336 / 750 Loss: 0.2065\n",
      "Epoch: 3 / 3, Step: 337 / 750 Loss: 0.3011\n",
      "Epoch: 3 / 3, Step: 338 / 750 Loss: 0.1867\n",
      "Epoch: 3 / 3, Step: 339 / 750 Loss: 0.2304\n",
      "Epoch: 3 / 3, Step: 340 / 750 Loss: 0.3145\n",
      "Epoch: 3 / 3, Step: 341 / 750 Loss: 0.4008\n",
      "Epoch: 3 / 3, Step: 342 / 750 Loss: 0.4091\n",
      "Epoch: 3 / 3, Step: 343 / 750 Loss: 0.2211\n",
      "Epoch: 3 / 3, Step: 344 / 750 Loss: 0.1767\n",
      "Epoch: 3 / 3, Step: 345 / 750 Loss: 0.3945\n",
      "Epoch: 3 / 3, Step: 346 / 750 Loss: 0.2214\n",
      "Epoch: 3 / 3, Step: 347 / 750 Loss: 0.3753\n",
      "Epoch: 3 / 3, Step: 348 / 750 Loss: 0.2032\n",
      "Epoch: 3 / 3, Step: 349 / 750 Loss: 0.5966\n",
      "Epoch: 3 / 3, Step: 350 / 750 Loss: 0.4497\n",
      "Epoch: 3 / 3, Step: 351 / 750 Loss: 0.3100\n",
      "Epoch: 3 / 3, Step: 352 / 750 Loss: 0.1875\n",
      "Epoch: 3 / 3, Step: 353 / 750 Loss: 0.3132\n",
      "Epoch: 3 / 3, Step: 354 / 750 Loss: 0.2612\n",
      "Epoch: 3 / 3, Step: 355 / 750 Loss: 0.3938\n",
      "Epoch: 3 / 3, Step: 356 / 750 Loss: 0.3211\n",
      "Epoch: 3 / 3, Step: 357 / 750 Loss: 0.3472\n",
      "Epoch: 3 / 3, Step: 358 / 750 Loss: 0.3353\n",
      "Epoch: 3 / 3, Step: 359 / 750 Loss: 0.3488\n",
      "Epoch: 3 / 3, Step: 360 / 750 Loss: 0.2893\n",
      "Epoch: 3 / 3, Step: 361 / 750 Loss: 0.1927\n",
      "Epoch: 3 / 3, Step: 362 / 750 Loss: 0.5267\n",
      "Epoch: 3 / 3, Step: 363 / 750 Loss: 0.3842\n",
      "Epoch: 3 / 3, Step: 364 / 750 Loss: 0.2564\n",
      "Epoch: 3 / 3, Step: 365 / 750 Loss: 0.3402\n",
      "Epoch: 3 / 3, Step: 366 / 750 Loss: 0.2769\n",
      "Epoch: 3 / 3, Step: 367 / 750 Loss: 0.3892\n",
      "Epoch: 3 / 3, Step: 368 / 750 Loss: 0.2596\n",
      "Epoch: 3 / 3, Step: 369 / 750 Loss: 0.2634\n",
      "Epoch: 3 / 3, Step: 370 / 750 Loss: 0.2134\n",
      "Epoch: 3 / 3, Step: 371 / 750 Loss: 0.3560\n",
      "Epoch: 3 / 3, Step: 372 / 750 Loss: 0.4219\n",
      "Epoch: 3 / 3, Step: 373 / 750 Loss: 0.4321\n",
      "Epoch: 3 / 3, Step: 374 / 750 Loss: 0.4185\n",
      "Epoch: 3 / 3, Step: 375 / 750 Loss: 0.4377\n",
      "Epoch: 3 / 3, Step: 376 / 750 Loss: 0.2265\n",
      "Epoch: 3 / 3, Step: 377 / 750 Loss: 0.2818\n",
      "Epoch: 3 / 3, Step: 378 / 750 Loss: 0.1268\n",
      "Epoch: 3 / 3, Step: 379 / 750 Loss: 0.4683\n",
      "Epoch: 3 / 3, Step: 380 / 750 Loss: 0.2152\n",
      "Epoch: 3 / 3, Step: 381 / 750 Loss: 0.2072\n",
      "Epoch: 3 / 3, Step: 382 / 750 Loss: 0.2626\n",
      "Epoch: 3 / 3, Step: 383 / 750 Loss: 0.3385\n",
      "Epoch: 3 / 3, Step: 384 / 750 Loss: 0.2637\n",
      "Epoch: 3 / 3, Step: 385 / 750 Loss: 0.3274\n",
      "Epoch: 3 / 3, Step: 386 / 750 Loss: 0.2869\n",
      "Epoch: 3 / 3, Step: 387 / 750 Loss: 0.6408\n",
      "Epoch: 3 / 3, Step: 388 / 750 Loss: 0.2056\n",
      "Epoch: 3 / 3, Step: 389 / 750 Loss: 0.2550\n",
      "Epoch: 3 / 3, Step: 390 / 750 Loss: 0.2302\n",
      "Epoch: 3 / 3, Step: 391 / 750 Loss: 0.2385\n",
      "Epoch: 3 / 3, Step: 392 / 750 Loss: 0.1771\n",
      "Epoch: 3 / 3, Step: 393 / 750 Loss: 0.3556\n",
      "Epoch: 3 / 3, Step: 394 / 750 Loss: 0.1495\n",
      "Epoch: 3 / 3, Step: 395 / 750 Loss: 0.3468\n",
      "Epoch: 3 / 3, Step: 396 / 750 Loss: 0.2296\n",
      "Epoch: 3 / 3, Step: 397 / 750 Loss: 0.3663\n",
      "Epoch: 3 / 3, Step: 398 / 750 Loss: 0.3119\n",
      "Epoch: 3 / 3, Step: 399 / 750 Loss: 0.2153\n",
      "Epoch: 3 / 3, Step: 400 / 750 Loss: 0.1363\n",
      "Epoch: 3 / 3, Step: 401 / 750 Loss: 0.3533\n",
      "Epoch: 3 / 3, Step: 402 / 750 Loss: 0.4486\n",
      "Epoch: 3 / 3, Step: 403 / 750 Loss: 0.4853\n",
      "Epoch: 3 / 3, Step: 404 / 750 Loss: 0.3421\n",
      "Epoch: 3 / 3, Step: 405 / 750 Loss: 0.2651\n",
      "Epoch: 3 / 3, Step: 406 / 750 Loss: 0.3556\n",
      "Epoch: 3 / 3, Step: 407 / 750 Loss: 0.3286\n",
      "Epoch: 3 / 3, Step: 408 / 750 Loss: 0.3260\n",
      "Epoch: 3 / 3, Step: 409 / 750 Loss: 0.4098\n",
      "Epoch: 3 / 3, Step: 410 / 750 Loss: 0.3509\n",
      "Epoch: 3 / 3, Step: 411 / 750 Loss: 0.6855\n",
      "Epoch: 3 / 3, Step: 412 / 750 Loss: 0.2165\n",
      "Epoch: 3 / 3, Step: 413 / 750 Loss: 0.3149\n",
      "Epoch: 3 / 3, Step: 414 / 750 Loss: 0.3518\n",
      "Epoch: 3 / 3, Step: 415 / 750 Loss: 0.4597\n",
      "Epoch: 3 / 3, Step: 416 / 750 Loss: 0.5189\n",
      "Epoch: 3 / 3, Step: 417 / 750 Loss: 0.1460\n",
      "Epoch: 3 / 3, Step: 418 / 750 Loss: 0.3705\n",
      "Epoch: 3 / 3, Step: 419 / 750 Loss: 0.1857\n",
      "Epoch: 3 / 3, Step: 420 / 750 Loss: 0.4294\n",
      "Epoch: 3 / 3, Step: 421 / 750 Loss: 0.1709\n",
      "Epoch: 3 / 3, Step: 422 / 750 Loss: 0.5265\n",
      "Epoch: 3 / 3, Step: 423 / 750 Loss: 0.2195\n",
      "Epoch: 3 / 3, Step: 424 / 750 Loss: 0.5045\n",
      "Epoch: 3 / 3, Step: 425 / 750 Loss: 0.2447\n",
      "Epoch: 3 / 3, Step: 426 / 750 Loss: 0.4341\n",
      "Epoch: 3 / 3, Step: 427 / 750 Loss: 0.2653\n",
      "Epoch: 3 / 3, Step: 428 / 750 Loss: 0.2214\n",
      "Epoch: 3 / 3, Step: 429 / 750 Loss: 0.1940\n",
      "Epoch: 3 / 3, Step: 430 / 750 Loss: 0.2514\n",
      "Epoch: 3 / 3, Step: 431 / 750 Loss: 0.4090\n",
      "Epoch: 3 / 3, Step: 432 / 750 Loss: 0.1933\n",
      "Epoch: 3 / 3, Step: 433 / 750 Loss: 0.2400\n",
      "Epoch: 3 / 3, Step: 434 / 750 Loss: 0.2045\n",
      "Epoch: 3 / 3, Step: 435 / 750 Loss: 0.2981\n",
      "Epoch: 3 / 3, Step: 436 / 750 Loss: 0.3647\n",
      "Epoch: 3 / 3, Step: 437 / 750 Loss: 0.2116\n",
      "Epoch: 3 / 3, Step: 438 / 750 Loss: 0.3081\n",
      "Epoch: 3 / 3, Step: 439 / 750 Loss: 0.1895\n",
      "Epoch: 3 / 3, Step: 440 / 750 Loss: 0.1349\n",
      "Epoch: 3 / 3, Step: 441 / 750 Loss: 0.2695\n",
      "Epoch: 3 / 3, Step: 442 / 750 Loss: 0.2081\n",
      "Epoch: 3 / 3, Step: 443 / 750 Loss: 0.3727\n",
      "Epoch: 3 / 3, Step: 444 / 750 Loss: 0.4751\n",
      "Epoch: 3 / 3, Step: 445 / 750 Loss: 0.2513\n",
      "Epoch: 3 / 3, Step: 446 / 750 Loss: 0.1383\n",
      "Epoch: 3 / 3, Step: 447 / 750 Loss: 0.5909\n",
      "Epoch: 3 / 3, Step: 448 / 750 Loss: 0.2517\n",
      "Epoch: 3 / 3, Step: 449 / 750 Loss: 0.3221\n",
      "Epoch: 3 / 3, Step: 450 / 750 Loss: 0.1690\n",
      "Epoch: 3 / 3, Step: 451 / 750 Loss: 0.3219\n",
      "Epoch: 3 / 3, Step: 452 / 750 Loss: 0.3988\n",
      "Epoch: 3 / 3, Step: 453 / 750 Loss: 0.3181\n",
      "Epoch: 3 / 3, Step: 454 / 750 Loss: 0.3823\n",
      "Epoch: 3 / 3, Step: 455 / 750 Loss: 0.1946\n",
      "Epoch: 3 / 3, Step: 456 / 750 Loss: 0.2275\n",
      "Epoch: 3 / 3, Step: 457 / 750 Loss: 0.3197\n",
      "Epoch: 3 / 3, Step: 458 / 750 Loss: 0.5948\n",
      "Epoch: 3 / 3, Step: 459 / 750 Loss: 0.3544\n",
      "Epoch: 3 / 3, Step: 460 / 750 Loss: 0.3634\n",
      "Epoch: 3 / 3, Step: 461 / 750 Loss: 0.2123\n",
      "Epoch: 3 / 3, Step: 462 / 750 Loss: 0.4650\n",
      "Epoch: 3 / 3, Step: 463 / 750 Loss: 0.2038\n",
      "Epoch: 3 / 3, Step: 464 / 750 Loss: 0.2622\n",
      "Epoch: 3 / 3, Step: 465 / 750 Loss: 0.2390\n",
      "Epoch: 3 / 3, Step: 466 / 750 Loss: 0.3343\n",
      "Epoch: 3 / 3, Step: 467 / 750 Loss: 0.1693\n",
      "Epoch: 3 / 3, Step: 468 / 750 Loss: 0.2545\n",
      "Epoch: 3 / 3, Step: 469 / 750 Loss: 0.1196\n",
      "Epoch: 3 / 3, Step: 470 / 750 Loss: 0.2808\n",
      "Epoch: 3 / 3, Step: 471 / 750 Loss: 0.2647\n",
      "Epoch: 3 / 3, Step: 472 / 750 Loss: 0.4859\n",
      "Epoch: 3 / 3, Step: 473 / 750 Loss: 0.6380\n",
      "Epoch: 3 / 3, Step: 474 / 750 Loss: 0.4783\n",
      "Epoch: 3 / 3, Step: 475 / 750 Loss: 0.1491\n",
      "Epoch: 3 / 3, Step: 476 / 750 Loss: 0.2977\n",
      "Epoch: 3 / 3, Step: 477 / 750 Loss: 0.4204\n",
      "Epoch: 3 / 3, Step: 478 / 750 Loss: 0.2274\n",
      "Epoch: 3 / 3, Step: 479 / 750 Loss: 0.5306\n",
      "Epoch: 3 / 3, Step: 480 / 750 Loss: 0.1945\n",
      "Epoch: 3 / 3, Step: 481 / 750 Loss: 0.3864\n",
      "Epoch: 3 / 3, Step: 482 / 750 Loss: 0.1912\n",
      "Epoch: 3 / 3, Step: 483 / 750 Loss: 0.2752\n",
      "Epoch: 3 / 3, Step: 484 / 750 Loss: 0.3929\n",
      "Epoch: 3 / 3, Step: 485 / 750 Loss: 0.5337\n",
      "Epoch: 3 / 3, Step: 486 / 750 Loss: 0.1461\n",
      "Epoch: 3 / 3, Step: 487 / 750 Loss: 0.1579\n",
      "Epoch: 3 / 3, Step: 488 / 750 Loss: 0.2932\n",
      "Epoch: 3 / 3, Step: 489 / 750 Loss: 0.3073\n",
      "Epoch: 3 / 3, Step: 490 / 750 Loss: 0.1872\n",
      "Epoch: 3 / 3, Step: 491 / 750 Loss: 0.3022\n",
      "Epoch: 3 / 3, Step: 492 / 750 Loss: 0.1621\n",
      "Epoch: 3 / 3, Step: 493 / 750 Loss: 0.1660\n",
      "Epoch: 3 / 3, Step: 494 / 750 Loss: 0.2513\n",
      "Epoch: 3 / 3, Step: 495 / 750 Loss: 0.1006\n",
      "Epoch: 3 / 3, Step: 496 / 750 Loss: 0.2901\n",
      "Epoch: 3 / 3, Step: 497 / 750 Loss: 0.3150\n",
      "Epoch: 3 / 3, Step: 498 / 750 Loss: 0.1163\n",
      "Epoch: 3 / 3, Step: 499 / 750 Loss: 0.4953\n",
      "Epoch: 3 / 3, Step: 500 / 750 Loss: 0.3861\n",
      "Epoch: 3 / 3, Step: 501 / 750 Loss: 0.1984\n",
      "Epoch: 3 / 3, Step: 502 / 750 Loss: 0.4460\n",
      "Epoch: 3 / 3, Step: 503 / 750 Loss: 0.2512\n",
      "Epoch: 3 / 3, Step: 504 / 750 Loss: 0.2703\n",
      "Epoch: 3 / 3, Step: 505 / 750 Loss: 0.2081\n",
      "Epoch: 3 / 3, Step: 506 / 750 Loss: 0.1609\n",
      "Epoch: 3 / 3, Step: 507 / 750 Loss: 0.3012\n",
      "Epoch: 3 / 3, Step: 508 / 750 Loss: 0.5378\n",
      "Epoch: 3 / 3, Step: 509 / 750 Loss: 0.2670\n",
      "Epoch: 3 / 3, Step: 510 / 750 Loss: 0.6998\n",
      "Epoch: 3 / 3, Step: 511 / 750 Loss: 0.3096\n",
      "Epoch: 3 / 3, Step: 512 / 750 Loss: 0.4859\n",
      "Epoch: 3 / 3, Step: 513 / 750 Loss: 0.2607\n",
      "Epoch: 3 / 3, Step: 514 / 750 Loss: 0.4845\n",
      "Epoch: 3 / 3, Step: 515 / 750 Loss: 0.7807\n",
      "Epoch: 3 / 3, Step: 516 / 750 Loss: 0.3523\n",
      "Epoch: 3 / 3, Step: 517 / 750 Loss: 0.2796\n",
      "Epoch: 3 / 3, Step: 518 / 750 Loss: 0.1015\n",
      "Epoch: 3 / 3, Step: 519 / 750 Loss: 0.4701\n",
      "Epoch: 3 / 3, Step: 520 / 750 Loss: 0.2814\n",
      "Epoch: 3 / 3, Step: 521 / 750 Loss: 0.3881\n",
      "Epoch: 3 / 3, Step: 522 / 750 Loss: 0.2192\n",
      "Epoch: 3 / 3, Step: 523 / 750 Loss: 0.3060\n",
      "Epoch: 3 / 3, Step: 524 / 750 Loss: 0.1332\n",
      "Epoch: 3 / 3, Step: 525 / 750 Loss: 0.2396\n",
      "Epoch: 3 / 3, Step: 526 / 750 Loss: 0.2792\n",
      "Epoch: 3 / 3, Step: 527 / 750 Loss: 0.1953\n",
      "Epoch: 3 / 3, Step: 528 / 750 Loss: 0.5364\n",
      "Epoch: 3 / 3, Step: 529 / 750 Loss: 0.2578\n",
      "Epoch: 3 / 3, Step: 530 / 750 Loss: 0.3793\n",
      "Epoch: 3 / 3, Step: 531 / 750 Loss: 0.5028\n",
      "Epoch: 3 / 3, Step: 532 / 750 Loss: 0.1653\n",
      "Epoch: 3 / 3, Step: 533 / 750 Loss: 0.5670\n",
      "Epoch: 3 / 3, Step: 534 / 750 Loss: 0.3384\n",
      "Epoch: 3 / 3, Step: 535 / 750 Loss: 0.2576\n",
      "Epoch: 3 / 3, Step: 536 / 750 Loss: 0.2677\n",
      "Epoch: 3 / 3, Step: 537 / 750 Loss: 0.3684\n",
      "Epoch: 3 / 3, Step: 538 / 750 Loss: 0.3446\n",
      "Epoch: 3 / 3, Step: 539 / 750 Loss: 0.3512\n",
      "Epoch: 3 / 3, Step: 540 / 750 Loss: 0.1491\n",
      "Epoch: 3 / 3, Step: 541 / 750 Loss: 0.2282\n",
      "Epoch: 3 / 3, Step: 542 / 750 Loss: 0.4333\n",
      "Epoch: 3 / 3, Step: 543 / 750 Loss: 0.3480\n",
      "Epoch: 3 / 3, Step: 544 / 750 Loss: 0.1710\n",
      "Epoch: 3 / 3, Step: 545 / 750 Loss: 0.3871\n",
      "Epoch: 3 / 3, Step: 546 / 750 Loss: 0.6019\n",
      "Epoch: 3 / 3, Step: 547 / 750 Loss: 0.2426\n",
      "Epoch: 3 / 3, Step: 548 / 750 Loss: 0.1359\n",
      "Epoch: 3 / 3, Step: 549 / 750 Loss: 0.3019\n",
      "Epoch: 3 / 3, Step: 550 / 750 Loss: 0.3397\n",
      "Epoch: 3 / 3, Step: 551 / 750 Loss: 0.1957\n",
      "Epoch: 3 / 3, Step: 552 / 750 Loss: 0.4071\n",
      "Epoch: 3 / 3, Step: 553 / 750 Loss: 0.3274\n",
      "Epoch: 3 / 3, Step: 554 / 750 Loss: 0.2468\n",
      "Epoch: 3 / 3, Step: 555 / 750 Loss: 0.2773\n",
      "Epoch: 3 / 3, Step: 556 / 750 Loss: 0.4771\n",
      "Epoch: 3 / 3, Step: 557 / 750 Loss: 0.3901\n",
      "Epoch: 3 / 3, Step: 558 / 750 Loss: 0.2164\n",
      "Epoch: 3 / 3, Step: 559 / 750 Loss: 0.4398\n",
      "Epoch: 3 / 3, Step: 560 / 750 Loss: 0.2684\n",
      "Epoch: 3 / 3, Step: 561 / 750 Loss: 0.2375\n",
      "Epoch: 3 / 3, Step: 562 / 750 Loss: 0.3103\n",
      "Epoch: 3 / 3, Step: 563 / 750 Loss: 0.2777\n",
      "Epoch: 3 / 3, Step: 564 / 750 Loss: 0.2053\n",
      "Epoch: 3 / 3, Step: 565 / 750 Loss: 0.1363\n",
      "Epoch: 3 / 3, Step: 566 / 750 Loss: 0.2501\n",
      "Epoch: 3 / 3, Step: 567 / 750 Loss: 0.5528\n",
      "Epoch: 3 / 3, Step: 568 / 750 Loss: 0.1322\n",
      "Epoch: 3 / 3, Step: 569 / 750 Loss: 0.0774\n",
      "Epoch: 3 / 3, Step: 570 / 750 Loss: 0.2440\n",
      "Epoch: 3 / 3, Step: 571 / 750 Loss: 0.4114\n",
      "Epoch: 3 / 3, Step: 572 / 750 Loss: 0.4369\n",
      "Epoch: 3 / 3, Step: 573 / 750 Loss: 0.1664\n",
      "Epoch: 3 / 3, Step: 574 / 750 Loss: 0.2111\n",
      "Epoch: 3 / 3, Step: 575 / 750 Loss: 0.3548\n",
      "Epoch: 3 / 3, Step: 576 / 750 Loss: 0.3112\n",
      "Epoch: 3 / 3, Step: 577 / 750 Loss: 0.5291\n",
      "Epoch: 3 / 3, Step: 578 / 750 Loss: 0.1521\n",
      "Epoch: 3 / 3, Step: 579 / 750 Loss: 0.4661\n",
      "Epoch: 3 / 3, Step: 580 / 750 Loss: 0.2434\n",
      "Epoch: 3 / 3, Step: 581 / 750 Loss: 0.5475\n",
      "Epoch: 3 / 3, Step: 582 / 750 Loss: 0.2671\n",
      "Epoch: 3 / 3, Step: 583 / 750 Loss: 0.1353\n",
      "Epoch: 3 / 3, Step: 584 / 750 Loss: 0.3268\n",
      "Epoch: 3 / 3, Step: 585 / 750 Loss: 0.1741\n",
      "Epoch: 3 / 3, Step: 586 / 750 Loss: 0.3785\n",
      "Epoch: 3 / 3, Step: 587 / 750 Loss: 0.4813\n",
      "Epoch: 3 / 3, Step: 588 / 750 Loss: 0.3451\n",
      "Epoch: 3 / 3, Step: 589 / 750 Loss: 0.4127\n",
      "Epoch: 3 / 3, Step: 590 / 750 Loss: 0.2761\n",
      "Epoch: 3 / 3, Step: 591 / 750 Loss: 0.2605\n",
      "Epoch: 3 / 3, Step: 592 / 750 Loss: 0.1241\n",
      "Epoch: 3 / 3, Step: 593 / 750 Loss: 0.2986\n",
      "Epoch: 3 / 3, Step: 594 / 750 Loss: 0.3272\n",
      "Epoch: 3 / 3, Step: 595 / 750 Loss: 0.1070\n",
      "Epoch: 3 / 3, Step: 596 / 750 Loss: 0.1416\n",
      "Epoch: 3 / 3, Step: 597 / 750 Loss: 0.4204\n",
      "Epoch: 3 / 3, Step: 598 / 750 Loss: 0.1356\n",
      "Epoch: 3 / 3, Step: 599 / 750 Loss: 0.1865\n",
      "Epoch: 3 / 3, Step: 600 / 750 Loss: 0.3295\n",
      "Epoch: 3 / 3, Step: 601 / 750 Loss: 0.3151\n",
      "Epoch: 3 / 3, Step: 602 / 750 Loss: 0.4700\n",
      "Epoch: 3 / 3, Step: 603 / 750 Loss: 0.3730\n",
      "Epoch: 3 / 3, Step: 604 / 750 Loss: 0.3848\n",
      "Epoch: 3 / 3, Step: 605 / 750 Loss: 0.3904\n",
      "Epoch: 3 / 3, Step: 606 / 750 Loss: 0.1422\n",
      "Epoch: 3 / 3, Step: 607 / 750 Loss: 0.2868\n",
      "Epoch: 3 / 3, Step: 608 / 750 Loss: 0.4293\n",
      "Epoch: 3 / 3, Step: 609 / 750 Loss: 0.3472\n",
      "Epoch: 3 / 3, Step: 610 / 750 Loss: 0.3888\n",
      "Epoch: 3 / 3, Step: 611 / 750 Loss: 0.5516\n",
      "Epoch: 3 / 3, Step: 612 / 750 Loss: 0.4054\n",
      "Epoch: 3 / 3, Step: 613 / 750 Loss: 0.3438\n",
      "Epoch: 3 / 3, Step: 614 / 750 Loss: 0.4711\n",
      "Epoch: 3 / 3, Step: 615 / 750 Loss: 0.1143\n",
      "Epoch: 3 / 3, Step: 616 / 750 Loss: 0.2267\n",
      "Epoch: 3 / 3, Step: 617 / 750 Loss: 0.3230\n",
      "Epoch: 3 / 3, Step: 618 / 750 Loss: 0.1484\n",
      "Epoch: 3 / 3, Step: 619 / 750 Loss: 0.1518\n",
      "Epoch: 3 / 3, Step: 620 / 750 Loss: 0.3159\n",
      "Epoch: 3 / 3, Step: 621 / 750 Loss: 0.2523\n",
      "Epoch: 3 / 3, Step: 622 / 750 Loss: 0.1569\n",
      "Epoch: 3 / 3, Step: 623 / 750 Loss: 0.3073\n",
      "Epoch: 3 / 3, Step: 624 / 750 Loss: 0.3668\n",
      "Epoch: 3 / 3, Step: 625 / 750 Loss: 0.1885\n",
      "Epoch: 3 / 3, Step: 626 / 750 Loss: 0.2029\n",
      "Epoch: 3 / 3, Step: 627 / 750 Loss: 0.4429\n",
      "Epoch: 3 / 3, Step: 628 / 750 Loss: 0.4551\n",
      "Epoch: 3 / 3, Step: 629 / 750 Loss: 0.1777\n",
      "Epoch: 3 / 3, Step: 630 / 750 Loss: 0.4074\n",
      "Epoch: 3 / 3, Step: 631 / 750 Loss: 0.1441\n",
      "Epoch: 3 / 3, Step: 632 / 750 Loss: 0.4529\n",
      "Epoch: 3 / 3, Step: 633 / 750 Loss: 0.0565\n",
      "Epoch: 3 / 3, Step: 634 / 750 Loss: 0.3851\n",
      "Epoch: 3 / 3, Step: 635 / 750 Loss: 0.2320\n",
      "Epoch: 3 / 3, Step: 636 / 750 Loss: 0.3573\n",
      "Epoch: 3 / 3, Step: 637 / 750 Loss: 0.1184\n",
      "Epoch: 3 / 3, Step: 638 / 750 Loss: 0.4633\n",
      "Epoch: 3 / 3, Step: 639 / 750 Loss: 0.2614\n",
      "Epoch: 3 / 3, Step: 640 / 750 Loss: 0.1516\n",
      "Epoch: 3 / 3, Step: 641 / 750 Loss: 0.2610\n",
      "Epoch: 3 / 3, Step: 642 / 750 Loss: 0.2859\n",
      "Epoch: 3 / 3, Step: 643 / 750 Loss: 0.1971\n",
      "Epoch: 3 / 3, Step: 644 / 750 Loss: 0.3102\n",
      "Epoch: 3 / 3, Step: 645 / 750 Loss: 0.4612\n",
      "Epoch: 3 / 3, Step: 646 / 750 Loss: 0.4154\n",
      "Epoch: 3 / 3, Step: 647 / 750 Loss: 0.2845\n",
      "Epoch: 3 / 3, Step: 648 / 750 Loss: 0.3844\n",
      "Epoch: 3 / 3, Step: 649 / 750 Loss: 0.4821\n",
      "Epoch: 3 / 3, Step: 650 / 750 Loss: 0.4142\n",
      "Epoch: 3 / 3, Step: 651 / 750 Loss: 0.5753\n",
      "Epoch: 3 / 3, Step: 652 / 750 Loss: 0.2397\n",
      "Epoch: 3 / 3, Step: 653 / 750 Loss: 0.2598\n",
      "Epoch: 3 / 3, Step: 654 / 750 Loss: 0.2900\n",
      "Epoch: 3 / 3, Step: 655 / 750 Loss: 0.2948\n",
      "Epoch: 3 / 3, Step: 656 / 750 Loss: 0.2062\n",
      "Epoch: 3 / 3, Step: 657 / 750 Loss: 0.3736\n",
      "Epoch: 3 / 3, Step: 658 / 750 Loss: 0.3996\n",
      "Epoch: 3 / 3, Step: 659 / 750 Loss: 0.5464\n",
      "Epoch: 3 / 3, Step: 660 / 750 Loss: 0.2140\n",
      "Epoch: 3 / 3, Step: 661 / 750 Loss: 0.2132\n",
      "Epoch: 3 / 3, Step: 662 / 750 Loss: 0.1987\n",
      "Epoch: 3 / 3, Step: 663 / 750 Loss: 0.2863\n",
      "Epoch: 3 / 3, Step: 664 / 750 Loss: 0.5060\n",
      "Epoch: 3 / 3, Step: 665 / 750 Loss: 0.1538\n",
      "Epoch: 3 / 3, Step: 666 / 750 Loss: 0.1774\n",
      "Epoch: 3 / 3, Step: 667 / 750 Loss: 0.2775\n",
      "Epoch: 3 / 3, Step: 668 / 750 Loss: 0.1455\n",
      "Epoch: 3 / 3, Step: 669 / 750 Loss: 0.3420\n",
      "Epoch: 3 / 3, Step: 670 / 750 Loss: 0.2796\n",
      "Epoch: 3 / 3, Step: 671 / 750 Loss: 0.2978\n",
      "Epoch: 3 / 3, Step: 672 / 750 Loss: 0.5969\n",
      "Epoch: 3 / 3, Step: 673 / 750 Loss: 0.2359\n",
      "Epoch: 3 / 3, Step: 674 / 750 Loss: 0.1613\n",
      "Epoch: 3 / 3, Step: 675 / 750 Loss: 0.4804\n",
      "Epoch: 3 / 3, Step: 676 / 750 Loss: 0.4201\n",
      "Epoch: 3 / 3, Step: 677 / 750 Loss: 0.3040\n",
      "Epoch: 3 / 3, Step: 678 / 750 Loss: 0.3860\n",
      "Epoch: 3 / 3, Step: 679 / 750 Loss: 0.3267\n",
      "Epoch: 3 / 3, Step: 680 / 750 Loss: 0.3042\n",
      "Epoch: 3 / 3, Step: 681 / 750 Loss: 0.2243\n",
      "Epoch: 3 / 3, Step: 682 / 750 Loss: 0.8037\n",
      "Epoch: 3 / 3, Step: 683 / 750 Loss: 0.2610\n",
      "Epoch: 3 / 3, Step: 684 / 750 Loss: 0.3567\n",
      "Epoch: 3 / 3, Step: 685 / 750 Loss: 0.4197\n",
      "Epoch: 3 / 3, Step: 686 / 750 Loss: 0.1921\n",
      "Epoch: 3 / 3, Step: 687 / 750 Loss: 0.4112\n",
      "Epoch: 3 / 3, Step: 688 / 750 Loss: 0.0970\n",
      "Epoch: 3 / 3, Step: 689 / 750 Loss: 0.2444\n",
      "Epoch: 3 / 3, Step: 690 / 750 Loss: 0.4566\n",
      "Epoch: 3 / 3, Step: 691 / 750 Loss: 0.3495\n",
      "Epoch: 3 / 3, Step: 692 / 750 Loss: 0.2762\n",
      "Epoch: 3 / 3, Step: 693 / 750 Loss: 0.2245\n",
      "Epoch: 3 / 3, Step: 694 / 750 Loss: 0.4079\n",
      "Epoch: 3 / 3, Step: 695 / 750 Loss: 0.3650\n",
      "Epoch: 3 / 3, Step: 696 / 750 Loss: 0.3995\n",
      "Epoch: 3 / 3, Step: 697 / 750 Loss: 0.4666\n",
      "Epoch: 3 / 3, Step: 698 / 750 Loss: 0.2888\n",
      "Epoch: 3 / 3, Step: 699 / 750 Loss: 0.4040\n",
      "Epoch: 3 / 3, Step: 700 / 750 Loss: 0.4303\n",
      "Epoch: 3 / 3, Step: 701 / 750 Loss: 0.3106\n",
      "Epoch: 3 / 3, Step: 702 / 750 Loss: 0.2131\n",
      "Epoch: 3 / 3, Step: 703 / 750 Loss: 0.6644\n",
      "Epoch: 3 / 3, Step: 704 / 750 Loss: 0.2617\n",
      "Epoch: 3 / 3, Step: 705 / 750 Loss: 0.2273\n",
      "Epoch: 3 / 3, Step: 706 / 750 Loss: 0.3013\n",
      "Epoch: 3 / 3, Step: 707 / 750 Loss: 0.2192\n",
      "Epoch: 3 / 3, Step: 708 / 750 Loss: 0.3207\n",
      "Epoch: 3 / 3, Step: 709 / 750 Loss: 0.3470\n",
      "Epoch: 3 / 3, Step: 710 / 750 Loss: 0.3514\n",
      "Epoch: 3 / 3, Step: 711 / 750 Loss: 0.1757\n",
      "Epoch: 3 / 3, Step: 712 / 750 Loss: 0.3379\n",
      "Epoch: 3 / 3, Step: 713 / 750 Loss: 0.3222\n",
      "Epoch: 3 / 3, Step: 714 / 750 Loss: 0.5039\n",
      "Epoch: 3 / 3, Step: 715 / 750 Loss: 0.4111\n",
      "Epoch: 3 / 3, Step: 716 / 750 Loss: 0.2042\n",
      "Epoch: 3 / 3, Step: 717 / 750 Loss: 0.5693\n",
      "Epoch: 3 / 3, Step: 718 / 750 Loss: 0.4129\n",
      "Epoch: 3 / 3, Step: 719 / 750 Loss: 0.2642\n",
      "Epoch: 3 / 3, Step: 720 / 750 Loss: 0.3538\n",
      "Epoch: 3 / 3, Step: 721 / 750 Loss: 0.3631\n",
      "Epoch: 3 / 3, Step: 722 / 750 Loss: 0.5329\n",
      "Epoch: 3 / 3, Step: 723 / 750 Loss: 0.3214\n",
      "Epoch: 3 / 3, Step: 724 / 750 Loss: 0.4876\n",
      "Epoch: 3 / 3, Step: 725 / 750 Loss: 0.2855\n",
      "Epoch: 3 / 3, Step: 726 / 750 Loss: 0.3910\n",
      "Epoch: 3 / 3, Step: 727 / 750 Loss: 0.2526\n",
      "Epoch: 3 / 3, Step: 728 / 750 Loss: 0.2035\n",
      "Epoch: 3 / 3, Step: 729 / 750 Loss: 0.5441\n",
      "Epoch: 3 / 3, Step: 730 / 750 Loss: 0.2040\n",
      "Epoch: 3 / 3, Step: 731 / 750 Loss: 0.3059\n",
      "Epoch: 3 / 3, Step: 732 / 750 Loss: 0.3982\n",
      "Epoch: 3 / 3, Step: 733 / 750 Loss: 0.3072\n",
      "Epoch: 3 / 3, Step: 734 / 750 Loss: 0.3394\n",
      "Epoch: 3 / 3, Step: 735 / 750 Loss: 0.2090\n",
      "Epoch: 3 / 3, Step: 736 / 750 Loss: 0.2854\n",
      "Epoch: 3 / 3, Step: 737 / 750 Loss: 0.2088\n",
      "Epoch: 3 / 3, Step: 738 / 750 Loss: 0.3651\n",
      "Epoch: 3 / 3, Step: 739 / 750 Loss: 0.1827\n",
      "Epoch: 3 / 3, Step: 740 / 750 Loss: 0.4672\n",
      "Epoch: 3 / 3, Step: 741 / 750 Loss: 0.2341\n",
      "Epoch: 3 / 3, Step: 742 / 750 Loss: 0.3681\n",
      "Epoch: 3 / 3, Step: 743 / 750 Loss: 0.2041\n",
      "Epoch: 3 / 3, Step: 744 / 750 Loss: 0.2231\n",
      "Epoch: 3 / 3, Step: 745 / 750 Loss: 0.2540\n",
      "Epoch: 3 / 3, Step: 746 / 750 Loss: 0.2371\n",
      "Epoch: 3 / 3, Step: 747 / 750 Loss: 0.0970\n",
      "Epoch: 3 / 3, Step: 748 / 750 Loss: 0.2264\n",
      "Epoch: 3 / 3, Step: 749 / 750 Loss: 0.3631\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "loss_total = 0\n",
    "model.train()\n",
    "for i in range(3):\n",
    "    for j, data in enumerate(train_dataloader):\n",
    "        inputs = {'input_ids': data[0].cuda(), \n",
    "                      'attention_mask': data[1].cuda(), \n",
    "                      'labels': data[2].cuda()}\n",
    "        output = model(**inputs)\n",
    "        loss = output[0]\n",
    "        optimizer.zero_grad()\n",
    "        loss_total += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"Epoch: {} / {}, Step: {} / {} Loss: {:.4f}\".format(i+1, 3, j, len(train_dataloader),\n",
    "                                                                      loss))\n",
    "torch.save(model.state_dict(), 'distilmodelC.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e1aba12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('distilmodelC.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "loss_total = 0\n",
    "results = defaultdict(dict)\n",
    "predictions, true_vals = [], []\n",
    "for j, data in enumerate(test_dataloader):\n",
    "    inputs = {'input_ids': data[0].cuda(), \n",
    "              'attention_mask': data[1].cuda(), \n",
    "              'labels': data[2].cuda()}\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "    loss = output[0]\n",
    "    logits = output[1]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    labels = inputs['labels'].cpu().numpy()\n",
    "    loss_total += loss.item()\n",
    "    predictions.append(logits)\n",
    "    true_vals.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "403b8e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_vals = np.concatenate(true_vals, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cf469c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.80      0.81      0.80      1156\n",
      "         pos       0.58      0.81      0.68       456\n",
      "         neu       0.93      0.89      0.91      4388\n",
      "\n",
      "    accuracy                           0.87      6000\n",
      "   macro avg       0.77      0.84      0.80      6000\n",
      "weighted avg       0.88      0.87      0.87      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "preds_flat = np.argmax(predictions, axis = 1).flatten()\n",
    "labels_flat = true_vals.flatten()\n",
    "target_names = ['neg', 'pos', 'neu']\n",
    "print(classification_report(labels_flat, preds_flat, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef77b793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5413aff",
   "metadata": {},
   "source": [
    "# Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c97cd9",
   "metadata": {},
   "source": [
    "Let's compare the results on the entire dataset:\n",
    "\n",
    "| Model | Accuracy |\n",
    "|-------|----------|\n",
    "|Naive Bayes| 75% |\n",
    "|Logistic Regression| 7% |\n",
    "|CNN | 75% |\n",
    "|BERT | 97% |\n",
    "|DistilBERT| 91% |\n",
    "\n",
    "This shows that BERT performs the best of the 5 models for sentiment analysis of the COVID-19 tweets.\n",
    "\n",
    "But, to get a good sense of the performance, it is necessary to check the per class precision obtained by each model.\n",
    "\n",
    "\n",
    "\n",
    "|    Model   | neu | pos | neg |\n",
    "|-------|-----------|------|------|\n",
    "|Naive Bayes | 75% | 0% | 0% |\n",
    "|Logistic Regression | 0% | 7% | 0% |\n",
    "|CNN | 100% | 0% | 0% |\n",
    "|BERT | 98% | 91% | 93% |\n",
    "|DistilBERT | 94% | 68% | 85% |\n",
    "\n",
    "The results show that BERT performs the best with respect to per-class precision as well. \n",
    "\n",
    "The results even demonstrate that even though DistilBERT is a \"lighter\" version of BERT, it does compromise with accuracy and precision for our dataset. Also, the BERT model will perform the best for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d63c090",
   "metadata": {},
   "source": [
    "Now, to discuss the results on the performance of the models on Parts A, B, C:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90600c6",
   "metadata": {},
   "source": [
    "Part A:\n",
    "\n",
    "| Model | Accuracy |\n",
    "|-------|----------|\n",
    "|Naive Bayes| 76% |\n",
    "|Logistic Regression| 6% |\n",
    "|CNN | 76% |\n",
    "|BERT | 94% |\n",
    "|DistilBERT| 89% |\n",
    "\n",
    "This again shows that the BERT model performs well also on the types of tweets related to government actions on COVID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2f9022",
   "metadata": {},
   "source": [
    "Part B:\n",
    "\n",
    "| Model | Accuracy |\n",
    "|-------|----------|\n",
    "|Naive Bayes| 75% |\n",
    "|Logistic Regression| 6% |\n",
    "|CNN | 75% |\n",
    "|BERT | 94% |\n",
    "|DistilBERT| 88% |\n",
    "\n",
    "This again shows that the BERT model performs well also on the types of tweets related COVID-19 crises, social distancing, lockdown, and stay at home."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffdce5b",
   "metadata": {},
   "source": [
    "Part C:\n",
    "\n",
    "| Model | Accuracy |\n",
    "|-------|----------|\n",
    "|Naive Bayes| 74% |\n",
    "|Logistic Regression| 7% |\n",
    "|CNN | 74% |\n",
    "|BERT | 93% |\n",
    "|DistilBERT| 87% |\n",
    "\n",
    "This again shows that the BERT model performs well also on the types of tweets related to COVID-19 cases, outbreak, and stay at home."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec7c5f",
   "metadata": {},
   "source": [
    "Thus, it can be concluded that BERT is the best model to do sentiment analysis of COVID-19 tweets. Also, BERT performs slightly less accurate with repsect to tweets related to COVID-19 cases, outbreak and stay at home. But, the difference is not significant than that of parts A and B. <br>\n",
    "Also, DistilBERT, though claimed to be equivalent to BERT gives a significant low performance than BERT for our task. Hence, the use of BERT for sentiment analysis is important as it significantly imrpoves performance over DistilBERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8321fd2",
   "metadata": {},
   "source": [
    "The results given in [1] on distilBERT and BERT are as follows:\n",
    "\n",
    "| Models/Dataset | COVIDSenti-A | COVIDSenti-B | COVIDSenti-C | COVIDSenti |\n",
    "|----------------|--------------|--------------|--------------|------------|\n",
    "|distilBERT | 93.7% | 92.9% | 92.6% | 93.9% |\n",
    "| BERT | 94.1% | 93.7% | 93.2% | 94.8% |\n",
    "\n",
    "The results obtained by finetuning these both models in this project are:\n",
    "\n",
    "| Models/Dataset | COVIDSenti-A | COVIDSenti-B | COVIDSenti-C | COVIDSenti |\n",
    "|----------------|--------------|--------------|--------------|------------|\n",
    "|distilBERT | 89% | 88% | 93% | 91% |\n",
    "| BERT | 94% | 94% | 87% | 97% |\n",
    "\n",
    "Thus, it can be seen that the BERT model fine tuned in this project above outperforms the one with the given in the original paper. On top of that, we have also tested the model on Naive Bayes, Logistic Regression and CNN. These models are not tested in [1]. Naive Bayes and CNN also achieve a satisfactory accuracy on all the datasets.\n",
    "\n",
    "The main reason for BERT's high accuracy is its ability to capture contextual word representation which the other models can't do. These results align with the previous studies that state that BERT outperforms other methods like TF-IDF or Word2Vec for any NLP task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d795303a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee54b804",
   "metadata": {},
   "source": [
    "## References:\n",
    "[1] U. Naseem, I. Razzak, M. Khushi, P. W. Eklund and J. Kim, \"COVIDSenti: A Large-Scale Benchmark Twitter Data Set for COVID-19 Sentiment Analysis,\" in IEEE Transactions on Computational Social Systems, vol. 8, no. 4, pp. 1003-1015, Aug. 2021, doi: 10.1109/TCSS.2021.3051189. <br>\n",
    "[2] https://github.com/usmaann/COVIDSenti <br>\n",
    "[3] https://huggingface.co/docs/transformers/model_doc/distilbert <br>\n",
    "[4] https://medium.com/analytics-vidhya/bert-the-theory-you-need-to-know-ddd316794395 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9abf84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
